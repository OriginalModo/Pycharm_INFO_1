"""

 Самое Важное!!!
 Всегда ДУМАТЬ! перед тем, как что-либо сделать, необходимо всё тщательно обдумать

 Радоваться Жизни  Радоваться разным мелочам

 Осторожность!   Быть осторожным, Думать о последствиях
 Ценить:         Ценить то что есть и стремиться к лучшему, Ценить сегодняшний день и брать МАКСИМУМ
 Быть проще:     Ко всему относиться Проще и Спокойнее без Волнения
 Слушать Других: Прислушиваться к мнению других людей они могут быть правы  И делать выводы
 Время:          Тайм-менеджмент   Грамотное распределение времени, Контроль Времени, Правильно раставлять Приоритеты
 Уверенность:    Быть уверенным в себе НО Оценивать свои силы!
 Развития:       Развиваться, Учиться, учиться и ещё раз - учиться, Саморазвитие
 Не Надеяться:   Надеяться только на себя
 Контроль:       Быть менее Эмоциональным, Совладать с Эмоциями, Контролировать свои эмоции в любой ситуации
 Внимательность: Быть Внимательным
 Спокойствие:    Быть Спокойнее, Перестать Нервничать , Быть Расслабленным, Не Злиться на себя и на других
 Режим:          Правильный Сон, Пить Воду
 Зарядка:        Бег, Тренировки, Стойка на Голове
 Тельце в тепле: НЕ переохлаждаться

 Молчание золото:  Лучше промолчать, чем сказать и потом жалеть о том, что сказал
 Соломон:          Все пройдёт, и это тоже пройдёт
 Вообще это замечательный подход: осознать, что проблема не такая уж и проблема, и вполне решаема.
 Кто ищет-тот всегда найдет!
 Искать Другие способы
 Не спеши, а то успеешь...   Успеешь, но не туда куда хотел...
 Подумай, нужно ли тебе ЭТО и для Чего
 Надо принимать вещи такими, как они есть, и пользоваться ими с наибольшей для себя выгодой.
 Если научиться принимать вещи как они есть, страдание исчезнет.
 У каждого свои недостатки


 Примерные цифры + у каждого свои способности! Миф
 Пирамида Обучения Дейла или Конус опыта Дейла:
 Мы запоминаем в среднем:
 10% того, что читаем,
 20% того, что видим,
 30% того, что слышим,
 50% того, что видим и слышим,
 70% того, что обсуждаем с другими людьми
 90% того, что делаем.

 ЧТЕНИЕ ВСЛУХ для запоминания лучше чем про себя.
 ЧТЕНИЕ ВСЛУХ - увеличению словарного запаса, развитию памяти и повышению грамотности человека.
________________________________________________________________________________________________________________________

 Python является МУЛЬТИПАРАДИГМЕННЫМ языком программирования, поддерживающим:
 Императивное                                                - Инструкции должны выполняться последовательно.
 Процедурное                                                 - Последовательно выполняемые операторы можно собрать в подпрограммы.
 Структурное                                                 - Представление программы в виде иерархической структуры блоков.
 Объектно-ориентированное программирование                   - Набор объектов, взаимодействующих друг с другом
 Метапрограммирование                                        - Метаклассы являются основным инструментом
 Функциональное программирование(заточено под многопоточку)  - в центре внимания которой находятся функции.
 Асинхронное программирование                                - Несколько задач выполняются параллельно и независимо друг от друга


 РЕАКТИВНОЕ программирование — это парадигма программирования, основанная на асинхронных потоках данных и распространении
 изменений. В этой парадигме акцент делается на обработку событий и реагирование на изменения состояния.


 СТАТИЧЕСКАЯ  ТИПИЗАЦИЯ выполняет проверку "ДО"       выполнения программы
 ДИНАМИЧЕСКАЯ ТИПИЗАЦИЯ выполняет проверку "ВО ВРЕМЯ" выполнения программы

 Python(ПУТХОН) — мультипарадигменный язык программирования со СТРОГОЙ ДИНАМИЧЕСКОЙ ТИПИЗАЦИЕЙ
 СТРОГАЯ ТИПИЗАЦИЯ - означает, что операции между разными типами данных требуют явного их преобразования.

 print(int('10') + 10)                 # -> 20
 print(dict(a=1, b=2) + list([1, 2]))  # -> TypeError: unsupported operand type(s) for +: 'dict' and 'list'

 ДИНАМИЧЕСКАЯ ТИПИЗАЦИЯ - при котором переменная связывается с типом в момент присваивания значения,
 а не в момент объявления переменной.

 a = 10
 print(a)  # -> 10
 a = set((1, 2))
 print(a)  # -> {1, 2}

 Статический прием типизации данных устанавливает тип переменной в процессе компиляции,
 Динамический — во время работы программы.
 Статически типизированный язык программирования проверяет переменную и присваивает ей тип, который в дальнейшем нельзя изменить

 в Python выражения (x) и x Обычно означают одно и тоже
 x = 5
 print((x))  # -> 5
 print(x)    # -> 5


 LLVM 16 — это версия проекта LLVM, который представляет собой набор инструментов для компиляции и оптимизации кода.
 Он используется для разработки компиляторов и других языков программирования.

 LLVM 16 — это версия инфраструктуры компиляторов, используемой для оптимизации и генерации кода. В Python LLVM
 используется косвенно через библиотеки, такие как Numba или PyPy, но сам LLVM 16 как отдельный компонент в Python отсутствует.

 В Python есть интерфейсы для работы с LLVM, например, библиотека llvmlite, которая позволяет использовать возможности
 LLVM в Python-программах. Библиотека llvmlite используется, например, в проекте Numba для JIT-компиляции (Just-In-Time)
 Python-кода в машинный код с использованием LLVM.


 Комментарии - это не способ "спасти" плохой код, а инструмент для пояснения того, что нельзя выразить кодом.
 Стремитесь к коду, который понятен без комментариев! Не комментируйте плохой код — перепишите его.


 -- ЧИСТЫЙ КОД по Роберту Мартину --

 - Читаемость — как книга, а не ребус.
 - Простота — одна функция = одна задача (SRP), нет дублирования (DRY).
 - Короткие функции — ≤ 20 строк, уровни абстракции разделены.
 - Тестируемость — DI, легко покрыть тестами.
 - SOLID — гибкая архитектура.
 - Нет "запахов" — магических чисел, жестких зависимостей, спагетти-кода.

 DI (Dependency Injection, Внедрение зависимостей) - это внедрение зависимостей, техника, при которой объект получает
 свои зависимости извне (а не создаёт их сам).

 Философия: Код пишется для людей, а не для машин.


 --  Тип-сумма (Sum Type)    Тип-произведение (Product Type) --

 Тип-сумма:        Может быть одним из нескольких типов  (например, Union[int, str]) или  int | str        <-----
 Тип-произведение: Сочетает несколько типов одновременно (например, Tuple[int, str]) или  tuple[int, str]  <-----

 - Тип-сумма (Sum Type) -  Это тип, который может принимать значения из нескольких возможных типов.

 Python с использованием Union: Union[int, str] означает, что значение может быть либо целым числом (int), либо строкой (str).
 В других языках это может называться "объединением" или "вариантом".


 - Тип-произведение (Product Type) - Это тип, который сочетает в себе несколько типов одновременно.
  кортеж Tuple[int, str] означает, что значение состоит из двух элементов: целого числа и строки.
  В других языках это может быть структура, запись или объект, содержащий несколько полей разных типов.


 # ПОВЕДЕНИЕ ИНТЕРПРЕТАТОРА   МОЖНО СДЕЛАТЬ ИММИТАЦИЮ!    <------------
 после перезапуска интерпретатора меняется PYTHONHASHSEED → меняется порядок set/dict и все места, где ты полагаешься на
 hash()/итерацию множеств, начинают давать другой пул/тай-брейки.

 # ЧТО ТАКОЕ SEED
 Seed — это начальное значение для генератора псевдослучайных чисел: одинаковый seed → та же последовательность “случайных” результатов.
 Причины: hash() с солью (PYTHONHASHSEED), незафиксированный random/os.urandom, джиттер (getrandbits),
 меняющаяся ENV/соль, порядок обхода set/dict.

 Фикс: стабильный сид = blake2b(входы + фикс. соль), убрать джиттер, явно random.seed(seed), сортировать set/dict, не менять соль/ENV.


  -- Грань между интерпретацией и компиляцией почти стерта --

 К большинству компилируемых    языков у нас появляется интерпретатор
 К большинству интерпретируемых языков у нас появляется компилятор

 К компилируемым языкам добавляются интерпретаторы: Например, у C/C++ есть интерпретаторы (вроде Cling для C++), которые
 позволяют выполнять код без предварительной компиляции. Это удобно для интерактивной разработки и экспериментов.

 К интерпретируемым языкам добавляются компиляторы: Например, Python, который традиционно интерпретируется,
 теперь имеет компиляторы (например, Numba, Cython) или JIT-компиляцию (через PyPy). Это ускоряет выполнение кода.

 Грань между интерпретацией и компиляцией действительно становится всё более размытой. Современные языки и инструменты
 стремятся объединить преимущества обоих подходов, чтобы обеспечить гибкость, скорость разработки и производительность.

 - К компилируемым языкам добавляются интерпретаторы:

 Например, Cling для C++ позволяет выполнять код интерактивно, без предварительной компиляции.

 Это особенно полезно для исследовательской работы, прототипирования и обучения.


 - К интерпретируемым языкам добавляются компиляторы:

 Python, изначально интерпретируемый, теперь имеет инструменты вроде Numba (JIT-компиляция), Cython (статическая компиляция)
 и PyPy (JIT-интерпретатор).

 Эти инструменты значительно ускоряют выполнение кода, особенно для вычислительно сложных задач.

 Почему это происходит?
 Гибкость: Интерпретаторы позволяют быстро тестировать и изменять код.

 Производительность: Компиляторы оптимизируют код для быстрого выполнения.

 Современные требования: Разработчики хотят и то, и другое — скорость разработки и скорость выполнения.

 Современные языки и инструменты всё чаще сочетают в себе черты интерпретации и компиляции, стирая границы между ними.


 C-API - Для мутаций   <-----
 C-API в Python позволяет работать с внутренними структурами Python на уровне C. Это мощный инструмент, который можно
 использовать для мутаций (изменений) объектов, которые в Python считаются неизменяемыми (например, кортежи). Однако
 такие действия не рекомендуются, так как они могут привести к неожиданным последствиям и нарушению работы программы.


 -- Как компьютеры хранят данные? --

 Ключевой принцип хранения и обработки данных в компьютере действительно основан на ДВОИЧНОЙ системе счисления
 (бинарном коде), где информация представляется с помощью двух состояний:

  0 (выключено / нет сигнала / ложь)
  1 (включено / есть сигнал / истина)

 Коротко: Вся информация (числа, текст, изображения) разбивается на биты (0/1), которые хранятся в памяти
 как электрические/магнитные состояния.


 Производители дисков используют десятичную систему (СИ, 1 ГБ = 1 000 000 000 байт).

 Windows использует двоичную систему (1 ГБ = 1 073 741 824 байт, или 2³⁰ байт).

 Пример расчёта для 500 ГБ:
 Производитель:
 500 × 1 000 000 000 = 500 000 000 000 байт

 Windows пересчёт:
 500 000 000 000 % 1 073 741 824 = 465,66 ГБ
 500 ГБ (десятичных) = 465 ГБ (двоичных) — это одно и то же количество байт, просто разные системы счисления.

 ГБ (GB) — десятичный гигабайт (1 млрд байт).
 ГиБ (GiB) — двоичный гибибайт (1 073 741 824 байт).

 Но Windows по традиции пишет «ГБ», хотя считает в GiB.
 Один и тот же объём, но разная математика → 500 ГБ (на коробке) = 465 ГБ (в Windows).


 --- Системы счисления. Как работают ---

 # Пример с числом 4321  в 8 системе счисления    Восьмеричная система (основание 8)
 # 4321 / 8 = 540, остаток 1
 # 540 / 8 = 67, остаток 4
 # 67 / 8 = 8, остаток 3
 # 8 / 8 = 1, остаток 0
 # 1 / 8 = 0, остаток 1


 # Пример с числом 4321  в 16 системе счисления   Шестнадцатеричная система (основание 16)
 # 4321 / 16 = 270, остаток 1
 # 270 / 16 = 16, остаток 14 (E в шестнадцатеричной системе)
 # 16 / 16 = 1, остаток 0
 # 1 / 16 = 0, остаток 1


 # Пример с числом 4321  в 2 системе счисления    Двоичная система (основание 2)
 # 4321 / 2 = 2160, остаток 1
 # 2160 / 2 = 1080, остаток 0
 # 1080 / 2 = 540, остаток 0
 # 540 / 2 = 270, остаток 0
 # 270 / 2 = 135, остаток 0
 # 135 / 2 = 67, остаток 1
 # 67 / 2 = 33, остаток 1
 # 33 / 2 = 16, остаток 1
 # 16 / 2 = 8, остаток 0
 # 8 / 2 = 4, остаток 0
 # 4 / 2 = 2, остаток 0
 # 2 / 2 = 1, остаток 0
 # 1 / 2 = 0, остаток 1


 # Пример с числом 4321  в 8 системе счисления
 print(4321-(4321//8*8))
 print(4321//8 - ((4321//8//8)*8))
 print((4321//8//8) - (4321//8//8//8*8))
 print((4321//8//8//8) - (4321//8//8//8//8*8))
 print((4321//8//8//8//8) - (4321//8//8//8//8//8))

 По умолчанию система счисления десятичная, но можно задать любое основание от 2 до 36 включительно.
 Системы счисления:
 int('0b1010', base=2)  # -> 10   int('0b1010', 2)  # -> 10              bin(10)    # -> 0b1010
 int('0o12', base=8)    # -> 10   int('0o12', 8)    # -> 10              int('10')  # -> 10
 int('10', base=10)     # -> 10   int('10', 10)     # -> 10              oct(10)    # -> 0o12
 int('0xa', base=16)    # -> 10   int('0xa', 16)    # -> 10              hex(10)    # -> 0xa

 --- END Системы счисления. Как работают ---

 -- Модуль ast --

 # Модуль ast в Python предоставляет инструменты для работы с абстрактным синтаксическим деревом
 # (AST — Abstract Syntax Tree) Python-кода. Он позволяет анализировать, модифицировать и генерировать код на Python.
 # Модуль ast представляет в виде структуры данных, которую можно легко обходить и анализировать.

 import ast

 print(ast.dump(ast.parse('1+0j'), indent=2))

 # Module(
 #   body=[
 #     Expr(
 #       value=BinOp(
 #         left=Constant(value=1),       # Левый операнд: число 1
 #         op=Add(),                     # Операция: сложение
 #         right=Constant(value=0j)))],  # Правый операнд: комплексное число 0j
 #   type_ignores=[])


 # Вот очень простой пример, который демонстрирует, как использовать модуль ast для разбора (парсинга) Python-кода:

 # Пример кода на Python в виде строки
 code = "a = 1 + 2"

 # Парсим код в абстрактное синтаксическое дерево
 tree = ast.parse(code)

 print(ast.dump(tree, indent=4))

 # Module(
 #     body=[
 #         Assign(
 #             targets=[
 #                 Name(id='a', ctx=Store())],  # Цель присваивания: переменная 'a'
 #             value=BinOp(
 #                 left=Constant(value=1),      # Левый операнд: число 1
 #                 op=Add(),                    # Операция: сложение
 #                 right=Constant(value=2)))],  # Правый операнд: число 2
 #     type_ignores=[])


 import ast

 s = "deque([1, 2, 3])"
 print(eval(s))              # -> deque([1, 2, 3])
 print(ast.literal_eval(s))  # -> ValueError: malformed node or string on line 1: <ast.Call object at 0x000001ED2A76A470>


  -- Модуль symtable --

 Модуль symtable в Python предоставляет инструменты для анализа таблиц символов, которые используются для хранения
 информации о переменных, функциях, классах и других именах в коде. Таблица символов помогает понять, как имена
 используются в программе (например, являются ли они локальными, глобальными или свободными).

 Модуль symtable в Python позволяет анализировать таблицы символов, которые содержат информацию о переменных, функциях
 и других именах в коде. Он помогает понять, как эти имена используются (локально, глобально и т.д.).

 import symtable

 # Исходный код функции
 code = '''
 def example(a):
     b = a + 1
     return b
 '''

 # Создаем таблицу символов для анализа кода
 table = symtable.symtable(code, "example", "exec")

 # Функция для вывода информации о символах
 def print_symbols(table, indent=0):
     # Выводим имя текущей таблицы символов (например, имя функции)
     print(" " * indent + f"Symbol table: {table.get_name()}")

     # Перебираем символы в текущей таблице
     for symbol in table.get_symbols():
         print(" " * indent + f"  Symbol: {symbol.get_name()}")
         print(" " * indent + f"    Is referenced: {symbol.is_referenced()}")  # Используется ли символ
         print(" " * indent + f"    Is assigned: {symbol.is_assigned()}")  # Присваивается ли значение
         print(" " * indent + f"    Is parameter: {symbol.is_parameter()}")  # Является ли параметром функции

     # Рекурсивно обрабатываем вложенные таблицы символов (например, вложенные функции)
     for child in table.get_children():
         print_symbols(child, indent + 4)

 # Выводим информацию о символах
 print_symbols(table)

 # Вывод:
 # Symbol table: top         # Глобальная таблица символов
 #   Symbol: example         # Символ функции example
 #     Is referenced: False  # Не используется
 #     Is assigned: True     # Присваивается (определение функции)
 #     Is parameter: False   # Не параметр
 # Symbol table: example     # Таблица символов функции example
 #   Symbol: a               # Параметр функции
 #     Is referenced: True   # Используется (в выражении b = a + 1)
 #     Is assigned: False    # Не присваивается
 #     Is parameter: True    # Является параметром
 #   Symbol: b               # Локальная переменная
 #     Is referenced: True   # Используется (в return b)
 #     Is assigned: True     # Присваивается (b = a + 1)
 #     Is parameter: False   # Не параметр



 -- Блоки Памяти --
 Блоки памяти в компьютерах используются для хранения битов. Это могут быть переменные в вашей программе, а могут быть
 пиксели изображения.
 Таким образом, абстракция блока памяти - ЭТО РЕГИСТРЫ НА ВАШЕЙ МАТЕРИНСКОЙ ПЛАТЕ, ОПЕРАТИВНАЯ ПАМЯТЬ, ЖЕСТКИЙ ДИСК.
 Одно из основных различий между всеми этими типами блоков памяти заключается в скорости,
 с которой они могут ЧИТАТЬ/ЗАПИСЫВАТЬ данные.

 Например, большинство блоков памяти работает намного лучше, когда читает один большой кусок данных, а не множество
 маленьких (это называется ПОСЛЕДОВАТЕЛЬНЫМ ЧТЕНИЕМ И СЛУЧАЙНЫМИ ДАННЫМИ)

 Скорость чтения/запись                 Размеры
 1) Кэш L1/L2/L3/L4                     1) Обычный жесткий диск (HDD)
 2) ОЗУ                                 2) Твердотельный жесткий диск (SSD)
 3) Твердотельный жесткий диск (SSD)    3) ОЗУ
 4) Обычный жесткий диск (HDD)          4) Кэш L1/L2/L3/L4

  -- END Блоки Памяти --


 Эвристический метод, или просто эвристика, — это метод, который приводит к решению, НЕ гарантируя, что оно — лучшее или оптимальное

 Приближенное решение: ЭВРИСТИЧЕСКИЕ АЛГОРИТМЫ часто находят решения, которые достаточно хороши для практических нужд,
 но не обязательно являются оптимальными.

 ЛИНЕЙНЫЙ АЛГОРИТМ — алгоритм, в котором вычисления выполняются строго последовательно.
 ЖАДНЫЙ АЛГОРИТМ   — алгоритм, заключающийся в принятии локально оптимальных решений на каждом этапе.

 От низкого к высокому вы можете классифицировать языки следующим образом:
 Машинный код -> Язык ассемблера -> Скомпилированный язык -> Интерпретируемый язык

 Парадигма программирования — это классификация, стиль или способ программирования.

 ДВЕ ОСНОВНЫЕ ПАРАДИГМЫ: ИМПЕРАТИВНАЯ и ДЕКЛАРАТИВНАЯ.
 Императивный подход описывает, каким образом ты что-то делаешь.    ->  Java, Python, JavaScript, C, C++, ...
 Декларативный описывает, что именно ты делаешь.                    ->  SQL, HTML, CSS


 В Python аргументы передаются по ссылке на объект (pass-by-object-reference):

 Неизменяемые (числа, строки, кортежи)   - ведут себя как передача по значению (изменения внутри функции не влияют на оригинал).
 Изменяемые (списки, словари, множества) - ведут себя как передача по ссылке (изменения внутри функции отражаются на оригинале).

 # Пример:
 def func(x, lst):
     x = 10          # Не изменит внешний int (неизменяемый)
     lst.append(1)   # Изменит внешний список (изменяемый)

 a = 5
 b = []
 func(a, b)
 print(a)  # -> 5
 print(b)  # -> [1]


 Переменные(variables) в Python — это именованные ссылки на объекты, которые хранятся в памяти компьютера.
 Переменные связываются с объектами ТОЛЬКО после создания объектов.    <-----
 class Gizmo:
     def __init__(self):
         print(f'Gizmo id {id(self)}')

 x = Gizmo()       # -> Gizmo id 1818795939984

 # Создается объект Gizmo   НО!  Переменная y так и НЕ будет создана
 y = Gizmo() * 10  # -> Gizmo id 2153632369872
 # TypeError: unsupported operand type(s) for *: 'Gizmo' and 'int'

 В Python присваивание происходит СПРАВА НАЛЕВО. Сначала вычисляется значение правой части
 (объект создается или извлекается), а затем это значение связывается с переменной слева.
 Всегда сначала читайте ПРАВУЮ часть, ту, где объект создается или извлекается                    <-----


 ### ПРИКОЛ  [:1:]
 print([1, 2, 3][:1:]) # -> [1]
 print([1, 2, 3][:2:]) # -> [1, 2]
 print([1, 2, 3][:3:]) # -> [1, 2, 3]
 print([1, 2, 3][:4:]) # -> [1, 2, 3]


 Одинарный символ подчеркивания _:
 Иногда используется в качестве имени временных или незначительных переменных («неважных»). Кроме того, он представляет
 результат последнего выражения в сеансе интерпретатора REPL Python.

 print([1 for _ in 'abc'])  # -> [1, 1, 1]                                  # Помечать можно функции   <-----
                                                                            def _():
 # Можно вторым или первым или любым так использовать _                         pass
 for i, _ in enumerate([1, 2, 3]):
     print(i, end=' ')  # -> 0 1 2


 # Кстати так тоже можно создавать функции  С такими отступами:                                        <-----
 def                func(lst)               :
                    pass


 # Создание ПУСТЫХ Функций/Классов    РАЗНЫЕ СПОСОБЫ
 def function(): pass     class Foo(): pass
 def function(): ...      class Foo(): ...

 def function():          class Foo():
    '''docstring'''           '''docstring'''


 Аннотация(annotation) - подсказка типа(type hint) - подсказки типов данных в Python.   import typing
 def my_func(a: int, b: str) -> None: pass

 В Python аннотации типов не влияют на выполнение кода и не занимают место в памяти во время выполнения. Однако они
 могут увеличивать размер исходного кода, но это не сказывается на производительности.    <-----

 В Python есть несколько способов аннотировать типы. Вот основные варианты:
 from typing import Optional, Dict, List, TypedDict, Any, Union, Literal, Final

 1. Стандартные аннотации (typing)
 Optional[T] -> T | None (значение может быть типа T или None)
 Dict[K, V]  -> dict[K, V] (словарь с ключами K и значениями V)
 List[T]     -> list[T] (список элементов типа T)

 TypedDict   -> Словарь с фиксированными ключами и типами (как аналог dataclass для dict)
 # Пример TypedDict
 from typing import TypedDict                           # Аналог dataclass, но для словарей.

 # Определяем тип для словаря                           # Этот словарь гарантирует, что:
 class Person(TypedDict):                               # - Ключи name, age и is_student обязательны
     name: str                                          # - name должен быть строкой
     age: int                                           # - age должен быть целым числом
     is_student: bool                                   # - is_student должен быть булевым значением

 # Создаём словарь, соответствующий этому типу
 person: Person = {
     "name": "Анна",
     "age": 25,
     "is_student": False
 }

 print(person)  # -> {'name': 'Анна', 'age': 25, 'is_student': False}

 2. Новый синтаксис (Python 3.10+)
 str | None     -> Замена Optional[str]                 x: str | None           # вместо Optional[str]
 dict[str, int] -> Замена Dict[str, int]                y: dict[str, int]       # вместо Dict[str, int]
 list[str]      -> Замена List[str]                     z: list[int]            # вместо List[int]

 Сравнение
 Старый стиль (typing)	Новый стиль (Python 3.10+)
 Optional[str]	        str | None
 Dict[str, int]	        dict[str, int]
 List[int]	            list[int]

 Другие полезные аннотации:
 Any               -> Любой тип (отключение проверки)
 Union[T1, T2]     -> T1 | T2 (значение одного из типов)
 Literal["a", "b"] -> Конкретное значение ("a" или "b")
 Final             -> Константа (нельзя изменять)

 Новый синтаксис короче и удобнее, но работает только в Python 3.10+.


 Typeshed — это коллекция файлов с аннотациями типов (stub-файлов) для стандартной библиотеки Python и популярных
 сторонних библиотек. Эти файлы помогают статическим анализаторам (например, mypy) проверять типы в коде, даже если
 сами библиотеки не имеют аннотаций.

 .pyi файлы — это подсказки для типов, которые улучшают анализ кода.


 -- ruff линтер --

 Ruff – это очень быстрый линтер и форматтер для Python, который:

 Находит и исправляет ошибки в коде.    
 Поддерживает современные аннотации (list[str] вместо List[str]).
 Работает в 10–100x быстрее других инструментов (flake8, pylint).

 Ruff — для скорости, Flake8 — для гибкости, Pylint — для глубины анализа.     pip install ruff flake8 pylint

 isort — сортирует и группирует импорты в Python. Встроен в Ruff (правила I)

 vulture — находит неиспользуемый (“мёртвый”) код.

 mypy поддерживает новые аннотации, но для замены в проекте используйте pyupgrade или ruff
 ruff может автоматически заменить старые аннотации на новые с помощью правил:               <------

 UP006 -> Optional[T]   -> T | None
 UP035 -> List/Dict/Set -> list/dict/set

 Запуск:

 bash
 ruff check --fix --select UP006,UP035 .

 Пример замены:

 from typing import Optional, List  # Было
 x: Optional[int]
 y: List[str]

 x: int | None  # Стало (UP006)
 y: list[str]   # Стало (UP035)
 Итог: ruff делает замену правильно и быстро.

 -- END ruff линтер --


 PEP 8 - является официальным стандартом написания кода на Python. Этот документ содержит рекомендации и правила.
 Flake8 — это инструмент для проверки кода на соответствие стандартам стиля PEP 8 и поиска ошибок в Python.
 Mypy — статический анализатор типов для Python, который позволяет находить ошибки несоответствия типов в коде.
 Нет, `mypy` не ускоряет скорость работы кода в Python

 Этот комментарий отключает проверку mypy для всего файла.
 # mypy: ignore-errors на первой строке модуля

 Этот комментарий отключает проверку mypy только для одной строки.
 # type: ignore рядом с конкретной строкой

 mypy можно использовать с Django, но есть нюансы.                                       <-----
 Плюсы:
 - Проверяет типы в моделях (models.py), вьюхах (views.py), формах и др.
 - Ловит ошибки до запуска кода.

 Минусы:
 - Не все части Django полностью покрыты типами (иногда нужны # type: ignore).
 - Может требовать настройки mypy.ini для игнорирования миграций и некоторых динамических частей.

 Для Django важно не игнорировать типы полностью, но и не требовать 100% покрытия — некоторые части
 (миграции, динамические запросы) лучше исключить из проверки.
 Лучший выбор для Django: Pyright (если в VSCode – Pylance) или Django-stubs + mypy.     <-----


 Модуль dataclasses - использует type hint

 Dataclass - это способ гибко управлять нашими классами, который позволяет писать меньше кода
 dataclass - Это полный аналог обычных классов, с ними можно делать все то, что мы умеем
 Под капотом реализуют некоторые встроенные методы  _eq__, __repr__, __init_

 Плюсы:
 - легко читаются
 - меньше кода

 Минусы:
 - работают медленнее обычных классов


 В общем, @dataclass предоставляет удобство и компактность, но может иметь некоторые накладные расходы на
 производительность, особенно при большом количестве классов.                                                     <-----

 @dataclasses.dataclass(*, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True,
 kw_only=False, slots=False, weakref_slot=False)

 В Python `dataclasses` — это модуль, который упрощает создание классов, в основном используемых для хранения
 данных. Декоратор `@dataclass` позволяет автоматически генерировать специальные методы
 (такие как `__init__`, `__repr__`, `__eq__` и другие) на основе объявленных полей. Вот краткое описание каждого аргумента:

 1. **init**: Если установлен в `True`, автоматически создается метод `__init__`, который позволяет создавать
  экземпляры класса с заданными полями.

 2. **repr**: Если установлен в `True`, автоматически создается метод `__repr__`, который возвращает строковое
  представление объекта, что особенно полезно для отладки.

 3. **eq**: Если установлен в `True`, автоматически создается метод `__eq__`, который определяет, как сравниваются
  экземпляры класса на равенство.

 4. **order**: Если установлен в `True`, автоматически создаются методы для упорядочивания экземпляров класса
  (`__lt__`, `__le__`, `__gt__`, `__ge__`), позволяя использовать операторы сравнения.

 5. **unsafe_hash**: Если установлен в `True`, создается метод `__hash__`, даже если класс содержит изменяемые поля.
  Это может привести к непредсказуемому поведению, если объект изменяется после добавления в множество или как ключ в словаре.

 6. **frozen**: Если установлен в `True`, класс становится неизменяемым (immutable), что означает, что после его
  создания нельзя изменять его поля.  ЭК НЕ могут изменять поля

 7. **match_args**: Если установлен в `True`, позволяет использовать новую конструкцию `match`
  (введенную в версии Python 3.10) для сопоставления с образцом (pattern matching) с аргументами класса.

 8. **kw_only**: Если установлен в `True`, все поля класса должны быть указаны как именованные аргументы при создании
  экземпляра, что делает вызов конструктора более явным.

 9. **slots**: Если установлен в `True`, используется механизм `__slots__`, который может уменьшить размер экземпляров
  классов и сделать доступ к атрибутам немного быстрее, но не поддерживает динамическое добавление атрибутов.

 10. **weakref_slot**: Этот параметр не является стандартным в Python до версии 3.11. Если он будет добавлен, ожидается,
  что он будет работать с `__slots__`, чтобы поддерживать слабые ссылки на атрибуты.

 Эти аргументы предоставляют множество возможностей для настройки поведения классов, которые создаются с помощью `dataclasses`

 # Сделать по умолчанию пустой список  Сравнение __eq__()  уже встроенно в dataclass
 from dataclasses import dataclass, field

 @dataclass
 class Foo:
     n: int
     s: str = 'a'
     items: list[str] = field(default_factory=list)  # <-- и всё это - чтобы по умолчанию был пустой список

 f = Foo(1)
 f1 = Foo(1)

 print(f.__dict__)    # -> {'n': 1, 's': 'a', 'items': []}
 print(f.__eq__(f1))  # -> True
 print(f == f1)       # -> True

 ff = Foo(11111)
 ff1 = Foo(1)
 print(ff.__eq__(ff1))  # -> False
 print(ff == ff1)       # -> False


 Marshmallow и Pydantic - это библиотеки для сериализации и валидации данных в Python, но с ключевыми отличиями:

 Подход к валидации:
  - Marshmallow: Декларативные схемы для определения структуры и валидации данных.
  - Pydantic: Использует аннотации типов, делая код более "питоническим".

 Сериализация и десериализация:
  - Marshmallow: Основной акцент на преобразование объектов в форматы, такие как JSON.
  - Pydantic: Создает модели для простого преобразования в словари и другие форматы.

 Производительность:
  - Pydantic: Обычно быстрее благодаря аннотациям типов и оптимизированным алгоритмам.
  - Marshmallow: Может быть медленнее из-за дополнительных уровней абстракции.

 Типизация:
  - Marshmallow: Гибкая типизация, но менее безопасная.
  - Pydantic: Строгая типизация, что улучшает валидацию и поддержку IDE.


 С Pydantic вы можете определять модели данных с помощью подсказок типов (type hints) Python
 FastAPI - основан на Python 3.6+ type hints**

 Pydantic построен на основе `dataclasses` из стандартной библиотеки Python. Он использует их для создания моделей
 данных, добавляя валидацию и аннотации типов.

 1. **Валидация данных**: Проверяет, соответствуют ли данные заданным типам и требованиям.
 2. **Оптимизация кода**: Упрощает работу с данными, минимизируя ошибки и улучшая читаемость кода.
 3. **Поддержка сериализации/десериализации**: Позволяет легко преобразовывать данные из форматов (например, JSON)
    в Python-объекты и наоборот.
 4. **Сложные структуры**: Поддерживает вложенные модели и сложные типы данных.

 Pydantic часто используется в веб-фреймворках, таких как FastAPI, для работы с запросами и ответами.

 Сравните это с пидантиком (Pydantic), в котором, кажется, думают о людях:

  from pydantic import BaseModel

  class Foo(BaseModel):
      items: list[str] = []


 В Pydantic три точки (...) в полях модели означают, что поле является обязательным и не имеет значения по умолчанию.
 Это эквивалентно использованию Field(..., default=...) или просто указанию типа без значения по умолчанию.


 # Пример использования Field с ... в Pydantic              # Пример
 from pydantic import BaseModel, Field                      from pydantic import BaseModel

 class User(BaseModel):                                     class User(BaseModel):
     name: str = Field(...)  # обязательное поле                name: str  # обязательное поле
     age: int = Field(...)   # обязательное поле                age: int = ...  # тоже обязательное поле

 # Пример создания экземпляра
 user = User(name="Alice", age=25)  # Оба поля обязательны
 print(user)


 В Pydantic алиасы позволяют использовать разные имена для одного поля при:
 - сериализации (alias)
 - десериализации (validation_alias, включая AliasChoices)

 # Пример:

 # Этот класс доступен начиная с Pydantic v2.
 from pydantic import AliasChoices

 datasource_type: str = Field(
     alias="type",  # имя при сериализации (в JSON -> "type")
     validation_alias=AliasChoices("type", "datasource_type")  # допустимые имена при парсинге (из JSON)
 )
 AliasChoices позволяет принимать поле из входящих данных с разными именами ("type" или "datasource_type"),
 но при сериализации всегда будет использоваться alias ("type").

 Коротко: гибкость именования полей без изменения внутренней модели.



 -- библиотека alembic --

 Alembic — это библиотека для управления миграциями базы данных в Python. Она часто используется вместе с
 SQLAlchemy для создания, применения и отмены изменений в структуре базы данных
 (например, добавление или удаление таблиц, колонок, индексов и т.д.). Alembic позволяет отслеживать изменения в
 схеме базы данных и применять их в виде последовательных миграций.

 Основные функции Alembic:
 - Создание миграций: Alembic автоматически генерирует скрипты миграций на основе изменений в моделях SQLAlchemy.
   alembic revision -m "create users table"

 - Применение миграций: Миграции можно применить к базе данных, чтобы обновить её структуру.
   alembic upgrade head

 - Откат миграций: Alembic позволяет откатить изменения, если что-то пошло не так.
   alembic downgrade -1

 - Управление версиями: Alembic отслеживает, какие миграции уже были применены к базе данных.

 Создание миграции:
 bash
 alembic revision -m "create users table"

 Редактирование сгенерированного файла миграции (например, alembic/versions/xxxx_create_users_table.py) для определения изменений:

 def upgrade():
     op.create_table(
         'users',
         sa.Column('id', sa.Integer, primary_key=True),
         sa.Column('name', sa.String),
         sa.Column('age', sa.Integer),
     )

 def downgrade():
     op.drop_table('users')


 Применение миграции:
 bash
 alembic upgrade head

 Откат миграции:
 bash
 alembic downgrade -1

 Показывает текущую версию базы данных, то есть последнюю применённую миграцию:
 bash
 alembic current

 Показывает историю миграций – список всех версий (ревизий) с их хешами, сообщениями и путями от старых к новым
 bash
 alembic history

 alembic history head:5  # последние 5 миграций


 МОЖНО ПРОСТО ПОДМЕНИТЬ НОМЕР РЕВИЗИИ ЕСЛИ ЕСТЬ ОШИБКИ 2 HEADS

 Нет, Alembic НЕ используется в Django. В Django для управления миграциями базы данных используется встроенная система
 миграций, которая доступна через команды manage.py.

 -- END библиотека alembic --


 -- Что делать при миграциях alembic --

 # если надо делаем
 git stash

 # Переключаемся на main
 git pull origin main

 # переключаемся на свою ветку и ребейзимся
 git rebase origin main

 # переключаемся на свою ветку и ребейзимся  лучше так
 git rebase main

 # удаляем свою миграцию из Pycharma (делаем копию на всякий случай)

 # откат всех миграций   # base  - состояние базы данных до применения первой миграции     # можно не делать
 alembic downgrade base

 # если нужно откатит последнюю примененную миграцию
 alembic downgrade -1


 # если такая ошибка FAILED: Can't locate revision identified by '857a7a8e15cf'
 # для удаления схемы
 drop schema ae_meta cascade

 # для создания схемы
 create schema ae_meta


 # накатываем миграции
 alembic upgrade head


 # делаем новую ревизию
 alembic revision --autogenerate -m "имя"    #  "имя" - это имя которое добавиться через _ к ревизии
 # Пример  857a7a8e15cf_workbook_and_workbook_query.py


 # делаем commit
 # пушим в репозиторий



 # для бобра  команда

 # для удаления схемы
 drop schema ae_meta cascade

 # для создания схемы
 create schema ae_meta
 -- END Что делать при миграциях alembic --


 -- Добавление полей в миграции Alembic БЕЗ потери данных --

 Чтобы добавить новые поля в существующую таблицу с помощью Alembic без потери данных, следуйте этим шагам:

 1. Создайте новую миграцию  # alembic revision -m "add_new_fields"

 2. В сгенерированном файле миграции используйте op.add_column() для ДОБАВЛЕНИЯ новых полей

 def upgrade():
    op.add_column('table_name', sa.Column('new_field', sa.String()))
    op.add_column('table_name', sa.Column('another_field', sa.Integer()))

 def downgrade():
    op.drop_column('table_name', 'another_field')
    op.drop_column('table_name', 'new_field')

 3. Для ИЗМЕНЕНИЯ существующих полей используйте op.alter_column() для ИЗМЕНЕНИЯ существующих полей

 def upgrade():
    op.alter_column('table_name', 'existing_field',
                   type_=sa.String(length=100),
                   nullable=False)

 def downgrade():
    # Возвращаем предыдущий тип и nullable статус
    op.alter_column('table_name', 'existing_field',
                   type_=sa.String(),  # или предыдущий тип, например sa.Integer()
                   nullable=True)

 # Пример полной миграции:

 from alembic import op
 import sqlalchemy as sa

 def upgrade():
     op.add_column('users', sa.Column('phone', sa.String(20), nullable=True))
     op.add_column('users', sa.Column('is_active', sa.Boolean(), server_default='true'))
     op.alter_column('users', 'email', nullable=False)

 def downgrade():
     op.alter_column('users', 'email', nullable=True)
     op.drop_column('users', 'is_active')
     op.drop_column('users', 'phone')

 Несколько дополнительных рекомендаций:

 Для больших таблиц можно добавить batch_alter_table для лучшей производительности:

 with op.batch_alter_table('table_name') as batch:
     batch.add_column(sa.Column('new_field', sa.String()))

 - Для полей с ограниченной длиной лучше указывать максимальную длину (как вы сделали с String(20))
 - Можно добавлять индексы для новых полей:
 op.create_index('ix_users_phone', 'users', ['phone'])


 -- Django Migrations vs Alembic --

 Django Migrations - автоматически генерируются на основе моделей (models.py), управляются встроенной системой Django.

 Alembic (для SQLAlchemy) - требует ручного создания/настройки миграций (команда revision + правка вручную),
 больше контроля, но меньше автоматики.

 Суть:
 Django сам решает, как применять миграции, Alembic даёт гибкость, но требует больше действий.


 Django Migrations
 - Автоматическая генерация: Django сам создаёт миграции на основе изменений в models.py (через makemigrations).
 - Встроенный механизм: Не требует дополнительных библиотек, всё работает "из коробки".
 - Автоматическое применение: Миграции применяются командой migrate, Django сам отслеживает, какие уже выполнены.
 - Меньше гибкости: Хоть и можно вручную править миграции, обычно полагаются на автоматику.

 Alembic (SQLAlchemy)
 - Ручное управление: Нужно явно создавать миграции (alembic revision -m "message") и часто править их вручную.
 - Больше контроля: Можно тонко настраивать миграции, добавлять сложные SQL-запросы, условия и т. д.
 - Нет привязки к моделям: Alembic не всегда понимает изменения автоматически (иногда требует --autogenerate и ручной проверки).
 - Гибкость в БД: Лучше подходит для сложных сценариев (например, нескольких БД, нестандартных SQL-операций).

 Вывод!!!
 Django Migrations - удобны для стандартных проектов, где важна скорость разработки.
 Alembic           - мощнее для сложных случаев, но требует больше ручной работы.

 Нужна простота и скорость? -> Django Migrations.
 Нужна гибкость и контроль? -> Alembic.


 --- FastAPI быстрее работает, чем Django ---
 FastAPI быстрее работает, чем Django, по нескольким причинам:

 1. **Асинхронность**: FastAPI основывается на асинхронном программировании, что позволяет обрабатывать множество
 запросов одновременно. Это особенно полезно для I/O-ориентированных приложений, таких как API, где время ожидания
 ответов от базы данных или внешних сервисов может быть значительным. Django, хотя и поддерживает асинхронность
 начиная с версии 3.1, имеет более сильную синхронную ориентацию и изначально не проектировался с учетом асинхронного подхода.

 2. **Упрощенная архитектура**: FastAPI имеет более легковесную архитектуру и меньшее количество "из коробки"
 фунциональных возможностей, чем Django. Это значит, что вы можете настроить только те компоненты, которые вам
 действительно нужны, а не загружать приложение лишними модулями.

 3. **Использование Starlette**: FastAPI построен на базе Starlette, который является высокопроизводительным асинхронным
 фреймворком для веб-приложений. Starlette предоставляет множество функций, таких как маршрутизация, обработка запросов
 и ответов, которые выполнены оптимально для асинхронного контекста.

 4. **Оптимизация для JSON**: FastAPI использует библиотеку Pydantic для валидации и сериализации данных, что делает
 работу с JSON более быстрой и эффективной по сравнению с Django, где подобные операции реализованы более громоздко.

 5. **Автоматическая документация**: FastAPI автоматически генерирует документацию API на основе аннотаций типов,
 что упрощает разработку и тестирование, но также эффективно поддерживает взаимодействие с клиентами без лишних затрат
 времени на ручное описание.

 Таким образом, для высоконагруженных API приложений FastAPI будет предпочтительным выбором из-за своей
 производительности и простоты использования асинхронных возможностей. Django, с другой стороны, более подходит для
 создания полнофункциональных веб-приложений, где могут быть важны его богатые возможности и встроенные компоненты.


 -- FastAPI Depends и Dependencies --

 В FastAPI Depends и Dependencies (зависимости) используются для внедрения дополнительной логики в обработчики маршрутов
 (например, проверку аутентификации, доступ к БД и т. д.).

 Depends – это специальный инструмент FastAPI (не декоратор!), который указывает, что обработчик маршрута зависит от какой-то зависимости.
 Dependencies (Зависимости) – это функции, классы или другие callable-объекты, которые выполняют какую-то логику
 (например, проверку авторизации, доступ к БД, извлечение параметров запроса и т. д.).

 Dependencies – это функции/классы, предоставляющие данные или логику.
 Depends – механизм FastAPI для указания, какие зависимости нужны обработчику.


 -- FastAPI Lifespan --

 Lifespan в FastAPI - это механизм для выполнения кода при запуске и остановке приложения (например, для подключения/отключения от БД).

 Пример:

 from fastapi import FastAPI
 from contextlib import asynccontextmanager

 # Функции для запуска/остановки
 async def startup():
     print("Сервер запущен!")

 async def shutdown():
     print("Сервер остановлен!")

 # Используем lifespan
 @asynccontextmanager
 async def lifespan(app: FastAPI):
     await startup()
     yield  # Здесь работает приложение
     await shutdown()

 app = FastAPI(lifespan=lifespan)                                                   # lifespan  <-----  <-----

 @app.get("/")
 async def root():
     return {"message": "Hello World"}

 Что происходит:
 При старте FastAPI вызовет startup().

 Во время работы приложения - код между yield и shutdown() не выполняется.

 При остановке - вызывается shutdown().

 Альтернативно можно использовать app.add_event_handler(), но lifespan удобнее для современных версий FastAPI (2.0+).



 # ЕЩЕ ОТВЕТ
 В FastAPI lifespan (жизненный цикл) - это механизм для выполнения кода при запуске и остановке приложения.

 Кратко:
 startup  - код выполняется при старте приложения (например, подключение к БД).
 shutdown - код выполняется при завершении (например, закрытие соединений).

 # ПРИМЕР 1. Через @app.on_event("startup") и @app.on_event("shutdown") (устаревший, но рабочий способ)
 from fastapi import FastAPI

 app = FastAPI()

 @app.on_event("startup")
 async def startup():
     print("Сервер запущен!")

 @app.on_event("shutdown")
 async def shutdown():
     print("Сервер остановлен.")


 # Через lifespan (рекомендуемый современный способ):

 from contextlib import asynccontextmanager
 from fastapi import FastAPI

 @asynccontextmanager
 async def lifespan(app: FastAPI):
     print("Старт!")
     yield
     print("Завершение.")

 app = FastAPI(lifespan=lifespan)                                                   # lifespan  <-----  <-----
 Это полезно для управления ресурсами (БД, кэш, внешние API).                       #           <-----  <-----




 -- Uvicorn ASGI-сервер --

 Uvicorn - это быстрый ASGI-сервер для Python, предназначенный для запуска веб-приложений на асинхронных фреймворках
 (например, FastAPI, Starlette).

 ### запуск сервера uvicorn
 uvicorn api_server.server:app --host 0.0.0.0 --port 8000 --reload --log-level debug

 Ключевые особенности:

 - Поддержка ASGI (асинхронный стандарт для Python).
 - Высокая производительность (работает на uvloop и httptools).
 - Простота использования: uvicorn main:app --reload.
 - Используется для развертывания FastAPI и других асинхронных приложений.

 import uvicorn

 if __name__ == "__main__":
     uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)

 Где:
 "main:app"  - указывает на объект app в модуле main.py,
 reload=True - включает авто-перезагрузку. НЕ используйте reload в продакшене - это снижает производительность.


 -- Gunicorn vs Uvicorn --

 Gunicorn - это WSGI-сервер для Python, предназначенный для запуска синхронных веб-приложений (например, Flask, Django).
 Uvicorn  - это ASGI-сервер, созданный для асинхронных приложений (например, FastAPI, Starlette).

 Коротко о различиях:
 Uvicorn  - ASGI-сервер (для асинхронных приложений, например FastAPI).
 Gunicorn - WSGI-сервер (для синхронных приложений например, Flask, Django).

 Gunicorn + Uvicorn:
 Для асинхронных приложений Gunicorn может работать как процесс-менеджер, запуская Uvicorn в качестве воркеров

 Почему так делают?
 Gunicorn управляет процессами (запуск, рестарт, балансировка нагрузки).
 Uvicorn обеспечивает асинхронную обработку запросов.

 Это популярный подход для FastAPI.
 Gunicorn может работать как менеджер процессов
 Uvicorn - как ASGI-воркер для асинхронных приложений.

  ### запуск сервера uvicorn
 uvicorn api_server.server:app --host 0.0.0.0 --port 8000 --reload --log-level debug


 --- collections.abc — Абстрактные базовые классы для контейнеров ---
 Использовать для наследования или проверки типа объекта.

 АВС                    Наследует от            Абстрактные методы                 Миксинные методы

 Container                                      __contains__

 Hashable                                        __hash__

 Iterable                                       __iter__

 Iterator               Iterable                __next__                           __iter__

 Reversible             Iterable                __reversed__

 Generator              Iterator                send,throw                         close, __iter__, __next__

 Sized                                          __len__

 Callable                                       __call__

 Collection             Sized, Iterable,        __contains__, __iter__,
                        Container               __len__

 Sequence               Reversible,             __getitem__, __len__               __contains__, __iter__,
                        Collection                                                 __reversed__, index, и count

 MutableSequence        Sequence                __getitem__, __setitem__,          Унаследованные Sequence методы и
                                                __delitem__, __len__,              append, clear, reverse, extend,
                                                 insert                            pop, remove, и __iadd__

 ByteString             Sequence                __getitem__, __len__               Унаследованные Sequence методы

 Set                    Collection              __contains__, __iter__,            __le__, __lt__, __eq__, __ne__,
                                                 __len__                           __gt__, __ge__, __and__, __or__,
                                                                                    __sub__, __xor__ и isdisjoint

 MutableSet             Set                     __contains__, __iter__,            Унаследованные Set методы и clear, pop,
                                                __len__, add, discard              remove, __ior__, __iand__, __ixor__, и __isub__

 Mapping                Collection              __getitem__, __iter__,             __contains__, keys, items, values,
                                                 __len__                           get, __eq__, и __ne__

 MutableMapping         Mapping                 __getitem__, __setitem__,          Унаследованные Mapping методы и pop,
                                                __delitem__, __iter__, __len__     popitem, clear, update, и setdefault

 MappingView            Sized                                                       __len__

 ItemsView              MappingView, Set                                            __contains__, __iter__

 KeysView               MappingView, Set                                            __contains__, __iter__

 ValuesView             MappingView, Collection                                     __contains__, __iter__

 Awaitable                                      __await__

 Coroutine              Awaitable               send,throw                          close

 AsyncIterable                                  __aiter__

 AsyncIterator         AsyncIterable            __anext__                           __aiter__

 AsyncGenerator        AsyncIterator            asend,athrow                        aclose, __aiter__,__anext__

 Buffer                                         __buffer__

 print(issubclass(list, collections.abc.Iterable))    # -> True
 print(isinstance(list(), collections.abc.Iterable))  # -> True

 Проверка isinstance(obj, Iterable) обнаруживает классы, зарегистрированные как Iterable или имеющие метод __iter__().
 но он НЕ обнаруживает классы, которые выполняют итерацию с помощью метода __getitem__().
 Единственный надежный способ определить, является ли объект итеративным, — это вызвать iter(obj).


 Инструкция assert - это удобный способ вставить отладочные утверждения в программу.
 assert 2.0 + 2 == 4, "Good"
 assert 2 + 2 == 5, "Houston we've got a problem"  # -> AssertionError: Houston we've got a problem

 Вы запустите assert кортеж (condition, message) в качестве первого параметра.
 assert True, 'no good'    # -> Ничего не происходит
 assert (True, 'no good')  # -> SyntaxWarning: assertion is always true, perhaps remove parentheses?

 # Можно разные проверики делать!
 assert isinstance('1', int | float), 'BAD'  # -> AssertionError: BAD
 assert 1 in [1, 2, 3]

 __debug__ - __debug__ True, если Python не был запущен с опция командной строки -O
 Инструкция assert будет игнорироваться (не будет выполняться) интерпретатором Python, если запустить его в
 оптимизированном режиме (опция командной строки -O при запуске скрипта Python).
 __debug__ = False -> SyntaxError: cannot assign to __debug__

 PYTHONOPTIMIZE – это переменная окружения, которую можно использовать для управления поведением интерпретатора Python.
 Применяется для оптимизации скомпилированного кода. С его помощью можно удалить отладочную информацию, что в итоге
 ускоряет выполнение кода. Обычно -O удаляет из скомпилированного байт-кода docstrings и assert
 PYTHONOPTIMIZE=1 - Игнорирует документационные строки (docstrings), что может снизить использование памяти и немного
 ускорить выполнение
 PYTHONOPTIMIZE=2 - Кроме игнорирования docstrings, дополнительно отключает assert, что может привести к небольшому
 ускорению работы программы


  -- Флаги Интерпретатора Python --
  Вот несколько основных флагов, которые можно использовать при запуске Python:

 -m <module>      - запускает указанный модуль как сценарий.
 -c <command>     - выполняет указанную строку кода.
 -O               - включает оптимизацию, игнорируя инструкции assert.
 -B               - отключает создание файлов .pyc.
 -h или --help    - выводит справку по использованию Python.
 -V или --version - показывает версию Python.
 -i               - запускает интерактивную сессию после выполнения скрипта.
 Эти флаги позволяют изменять поведение интерпретатора в зависимости от потребностей пользователя.


 -- Модуль sys обеспечивает доступ к некоторым переменным и функциям, взаимодействующим с интерпретатором python.

 -- Модуль os предоставляет множество функций для работы с операционной системой

 -- Модуль locale в Python, региональные настройки

 -- Модуль time   -  Реальное время ОС.    Работа с системным временем операционной системы

 -- Модуль datetime  -  Работа с датой и временем. Форматирование, преобразования и обработка даты и времени

  # Интересный модуль dateutil
  from dateutil.relativedelta import relativedelta
  relativedelta — это прибавление/вычитание календарных интервалов к датам: months, years
  (в отличие от timedelta, где только дни/секунды). Например date(2024,1,31) + relativedelta(months=1) корректно даст конец февраля.

  --- Чтобы красиво вывести pprint ---
 from pprint import pprint
 pprint()

 # При дебаге pprint НАШЕ ВСЁ!
 from pprint import pprint
 x = [1]
 x.append(x)
 print(x)   # -> [1, [...]]
 pprint(x)  # -> [1, <Recursion on list with id=1847043418560>]


  ДВУХРЕЖИМНЫЙ API   re   os
 - Модуль re поддерживает как строковые, так и байтовые операции с регулярными выражениями.   str или bytes
 - Модуль os может работать с аргументами типа str или bytes


 К переменным среды можно обращаться как к текстовым строкам в отображении os.environ:
 import os
 path = os.environ['PATH']
 user = os.environ['USER']
 editor = os.environ['EDITOR']
 val = os.environ['SOMEVAR']
 ... и т. д. ...

 Чтобы изменить переменные среды, задайте значение переменной os.environ.
 Пример:
 os.environ['NAME'] = 'VALUE'


 -- Почему так нужно делать!   range(len(lst)-1) --
 `len(lst) - 1` нужно писать, чтобы избежать выхода за границы списка. Когда ты используешь `range(len(lst)-1)`,
 это означает, что цикл будет итерироваться по индексам от `0` до `len(lst) - 2`. Это позволяет безопасно обращаться к
 элементу `lst[i + 1]`, поскольку в этом случае `i + 1` всегда будет находиться в пределах существующих индексов списка.

 Индексы
 range задаётся как [начало, конец):

 range(0, 3) - это [0, 1, 2], и если вам нужны числа от 1 до 10, то вы пишете range(1, 11)

 По этой же причине если вам нужно посчитать с 10 до 1 включительно, то вы пишете range(10, 0, -1) Как только нужно
 работать с индексами (что в питоне, к счастью, нечасто), то везде появляются i+1 или i-1, потому что начало включается,
 а конец - нет. Это бесит.


 -- ТИПЫ ДАННЫХ Python --
 Основные ВСТРОЕННЫЕ ТИПЫ ДАННЫХ языка Python:

 ИЗМЕНЯЕМЫЕ(mutable): list, dict, set, Массивы байтов (bytearray), Самописные классы (Classes), (Class Instances - экземпляры классов)
 Изменяемые (англ. Mutable) - содержимое объекта МОЖНО изменить после создания.       НЕ СОЗДАЁМ НОВЫЙ ОБЬЕКТ В ПАМЯТИ

 НЕ ИЗМЕНЯЕМЫЕ(immutable): Числа (int, float, complex), Булевые (bool), Строки (str), Кортежи (tuple),
 НЕизменяемые множества (frozenset), Диапазоны (range), Байтовые строки (bytes)
 НЕизменяемые (англ. Immutable): содержимое объекта НЕЛЬЗЯ изменить после создания.   СОЗДАЁМ НОВЫЙ ОБЬЕКТ В ПАМЯТИ

 memoryview — представление (view) байтов без копирования; изменяемость зависит от источника:
 bytes — только чтение, bytearray — можно менять.


 АДТ – это МАТЕМАТИЧЕСКОЕ описание некоторых данных и операций над ними (например, стек, очередь, список, множество).
 В Python АДТ может быть реализован с помощью классов и встроенных типов.  АДТ - АБСТРАКТНЫЙ ТИП ДАННЫХ


 -- Какое максимальное значение int в python?   sys.maxsize - Хорошая попытка но НЕТ! --

 Максимального int в Python НЕТ: int произвольной точности, ограничение — только память.
 sys.maxsize — не максимум int, а практический предел для размеров/индексов в CPython (тип Py_ssize_t), обычно 2^63 - 1 на 64-битных.
 Поэтому sys.maxsize + 1 может дать OverflowError там, где число нужно преобразовать в Py_ssize_t (например, индекс).


 -- СТРУКТУРЫ ДАННЫХ Python --

 ВСТРОЕННЫE:
 4 (ЧЕТЫРЕ) Built-in (ВСТРОЕННЫХ) СТРУКТУРЫ ДАННЫХ, которые вы можете использовать для хранения данных:
 list(список), tuple(кортеж), dict(словарь), set(множество), frozenset(неизменяемое множество)


 ВСТРОЕННЫE + ДОПОЛНИТЕЛЬНЫЕ СТРУКТУРЫ ДАННЫХ:


 Список                 (list)
 Массив                 (array)
 Стек                   (LifoQueue)
 Очередь                (Queue)
 Дек                    (deque)
 Очередь с приоритетом  (heapq)
 Кортеж                 (tuple)
 Множество              (set, frozenset)
 Словарь                (dict)
 Модуль collections     (OrderedDict, ChainMap, Counter, defaultdict, deque, namedtuple)


 -- Как Python ХИТРИТ С НЕИЗМЕНЯЕМЫМИ Объектами --
 # Ложь во имя спасения! -  ОНИ ЭКОНОМЯТ ПАМЯТЬ И УСКОРЯЮТ РАБОТУ ИНТЕРПРЕТАТОРА

 # Для КОРТЕЖА [:]  и tuple(t)  - Возвращает ссылку на ИСХОДНЫЙ Кортеж           # Пример с list() и [:]  ДЕЛАЕТ КОПИЮ
 t1 = (1, 2, 3)                                                                  lst1 = [1, 2, 3]
 t2 = tuple(t1)                                                                  lst2 = list(lst1)
 print(t1 is t2)  # -> True                                                      print(lst1 is lst2)  # -> False
 t3 = t1[:]                                                                      lst3 = lst1[:]
 print(t3 is t1)  # -> True                                                      print(lst3 is lst1)  # -> False


 # ТАКОЕ ЖЕ ПОВЕДЕНИЕ У frozenset, str, bytes                                    # Пример str()
 f1 = frozenset({1, 2, 3})                                                       s1 = str({1, 2, 3})
 f2 = frozenset(f1)                                                              s2 = str(s1)
 print(f1 is f2)  # -> True                                                      print(s1 is s2)      # -> True
 f3 = f1.copy()                                                                  s3 = s1[:]
 # f[:] - НЕ работает Потому что frozenset НЕ ПОСЛЕДОВАТЕЛЬНОСТЬ
 # f3 = f1[:]   # -> TypeError: 'frozenset' object is not subscriptable
 print(f3 is f1)  # -> True                                                     print(s3 is s1)       # -> True


 -- Как можно изменить ТИП ОБЪЕКТА  Атрибут  __class__   НЕ рекомендуется!!  --

 # Можно изменить тип объекта, изменив атрибут __class__
 Да, в Python можно изменить тип объекта, присвоив ему другой класс через атрибут __class__. Однако это НЕ рекомендуется,
 так как может привести к непредсказуемым последствиям и нарушению принципов объектно-ориентированного программирования.
 Лучше избегать таких подходов.

 class A:
     def greet(self):
         return "Hello from A"

 class B:
     def greet(self):
         return "Hello from B"

 # Создаем объект класса A
 obj = A()
 print(obj.greet())  # Вывод: Hello from A

 # Изменяем класс объекта на B
 obj.__class__ = B

 print(obj.greet())  # Вывод: Hello from B


 Четыре способа форматирования строк:
 string.Template    from string import Template
                    s = Template('who and what')
                    s.substitute(who='spam', what='eggs')  # -> ('spam', 'eggs')

 сишный стиль      # '%s and %s' % ('spam', 'eggs')      # ->  'spam and eggs'
 f-строки          #  f'spam and eggs'                   # ->  'spam and eggs'
 str.format()      # '{} and {}'.format('spam', 'eggs')  # ->  'spam and eggs'

 Если форматирующие строки поступают от пользователей, то используйте  string.Template, чтобы избежать проблем с безопасностью.
 f-строки     - если Python 3.6+
 str.format() - если меньше чем Python 3.6+




 CPython содержит небольшой целочисленный кеш в диапазоне [-5, 256], поэтому числа в диапазоне [-5, 256] будут ссылаться
 на один и тот же адрес памяти.

 # Работает не только с числами. Работает с НЕ ИЗМЕНЯЕМЫЕ(immutable)                            <-----
 a = 100000000000
 b = 'aaaaaaaaaaaaaa'
 print(a is 100000000000)  # -> True
 print(b is 'aaaaaaaaaaaaaa')  # -> True
 # SyntaxWarning: "is" with a literal. Did you mean "=="?  Но работает Просто предупреждение!

 --- Доказательство [-5, 256] ---
 # Несколько способов доказать, что в Python числа от -5 до 256 являются синглтонами (кешируются)

 # Тоже самое
 res = len([i for i in range(-5, 300) if i is int(str(i))])
 res_2 = len([i for i in range(-5, 300) if i is i+0])
 print(res)     # -> 262
 print(res_2)   # -> 262


 --- Из чего состоят строки в Python? ---
 Строки в Python состоят из последовательности Unicode-символов и хранятся как неизменяемые (immutable) объекты. <-----

  - Строки в Python — последовательности Unicode.

  - Могут храниться как 1, 2 или 4 байта на символ (зависит от содержимого).

  - Неизменяемы — любое изменение создаёт новую строку.

  - Короткие/ASCII-строки кешируются (интернируются).

  - Если нужна изменяемая строка, используется bytearray или list с последующим join.





 BSON - Двоичный JSON   - расширение JSON
 JSON - тот же словарь пайтон, обращайтесь к нему по ключам:
  Конвертировать из Python в JSON:
  json.dump() - метод записывает объект Python в файл в формате JSON  - сериализации
  json.dumps() - метод возвращает строку в формате JSON - сериализации

  Конвертировать из JSON в Python:
  json.load() - метод считывает файл в формате JSON и возвращает объекты Python - десериализации
  json.loads() - метод считывает строку в формате JSON и возвращает объекты Python - десериализации

 Не все форматы преобразуются Например datetime НЕ сериализуется Нужно преобразовать к формату которые сериализуються:

 JSON               Python

 object             dict
 array              list
 string             str
 number (int)       int
 number (real)      float
 true               True
 false              False
 null               None


 # Совместимые с json ПСЕВДОНИМЫ   Можно копировать json прямо в консоль
 true, false, null = True, False, None

 fruit = {
     'hehe': true,
     'AAA': false,
     'BBB': null,
 }

 print(fruit)              # -> {'hehe': True, 'AAA': False, 'BBB': None}
 print(true, false, null)  # -> True False None


 Pickle – сериализация, превращение обьектов python в байты и наоборот байтов в обьекты python десериализации
 Также для работы с бинарными файлами Python предоставляет специальный встроенный модуль pickle
 "Pickling" - процесс преобразования объекта Python в поток байтов
 "unpickling" - обратная операция, в результате которой поток байтов преобразуется обратно в Python-объект.

 Pickle сохраняет __dict__ объекта (атрибуты → значения).
 Но если у объекта есть метод __getstate__(), то сохраняется только то, что он вернёт (игнорируя __dict__).
 __getstate__() — это способ контролировать, что именно сохранит pickle

 -- Модуль marshal
 примитивный модуль сериализации. Этот модуль существует главным образом для поддержки файлов Python .pyc


 -- JSON - это текстовый формат сериализации, а pickle - это двоичный формат сериализации.              <-----


 Пакет - набор модулей
 Модули в Python являются полноправными объектами!                                                                <-----
 В Python модули являются первоклассными объектами, что означает, что они могут быть присваиваемыми переменным,
 храниться в структурах данных (например, списках или словарях) и передаваться как аргументы в функции.

 Исходный код модуля загружается и выполняется только один раз, независимо от числа раз использования import      <-----

 Есть распространенное заблуждение, что команда from модуль import имя более эффективна, так как может загружать
 только часть модуля. Это НЕ так. В любом случае весь модуль загружается и сохраняется в кеше.                    <-----


 ### Быстрый чек: модуль vs пакет

 python -m some_module — запускает модуль some_module.py
 python -m some_package — запускает some_package/__main__.py (если есть)


  -- Что происходит при import под капотом --

 При выполнении команды import в Python происходит несколько шагов:

 1) Поиск модуля: Python ищет указанный модуль в системных путях, определённых в sys.path, включая
    стандартные библиотеки и пользовательские директории.

 2) Компиляция: Если модуль не был ранее загружен, Python компилирует его в байт-код (файлы с расширением .pyc).

 3) Инициализация: Выполняется код модуля, и создаются его объекты (функции, классы и переменные).

 Кэширование: Загруженный модуль сохраняется в кэше sys.modules, чтобы при следующем импорте избежать повторной загрузки и компиляции.
 -- END Что происходит при import под капотом --


 -- ЦИКЛИЧЕСКИЙ ИМПОРТ --
 КАК ПОЧИНИТЬ?
 Вынеси общее в третий модуль (common.py), чтобы модули не импортировали друг друга.
 Сделай “ленивый” импорт: from b import X внутри функции/метода.


 Модуль — это файл, содержащий определения функций, классов и переменных, а также исполняемый код
 Модуль — это файл с расширением .py

 ТЕРНАРНЫЙ ОПЕРАТОР(Python условие в одну строку) — единственный оператор в Python, который требует три операнда
 Тернарный оператор(Ternary Operator) — способ превратить простую условную инструкцию в выражение:

 Выражение(expression) — это комбинация значений, операторов и литералов, которая что-то дает.
 Инструкция(statement) — это действие или команда, которая что-то делает. Пример: If-Else, циклы... и т. д.

 Основное различие между ними заключается в том, что expression всегда возвращает значение и может быть частью
 statement, в то время как statement выполняет действие и не обязательно возвращает значение. Это как разница между
 задачей (statement) и решением задачи (expression).

  -- командная строка cmd   Terminal commands --
 dir - Отображение файлов и папок в текущем каталоге
 cd - Изменить каталог, если нажать Tab будет автодополнение
 cd .. - переход на 1 папку назад
 cd ..\.. - переход на 2 папки назад
 cd \ - Переход в корневую директорию
 cd . - Переход в текущую директорию (никаких изменений)
 mkdir - создание папки


    -- Отличие Функции генератора от Функции сопрограммы   Generator function vs Coroutine function --
 Функции генератора, с выражением yield from <expr> внутри него, являются почти сопрограммами, но не совсем.

 Единственное отличие состоит в том, что ФУНКЦИЯ ГЕНЕРАТОР НЕ может контролировать где продолжиться выполнение после
 ее завершения, при этом дальнейшее управление всегда передается вызывающей стороне.


  -- Сопрограммы также имеют перечисленные ниже методы, аналогичные методам генераторов (Generator-iterator methods) --
 Однако, в отличие от генераторов, сопрограммы не поддерживают итерацию напрямую.

 Coroutine                                    Generator-iterator                          Asynchronous generator-iterator
                                              generator.__next__()                        coroutine agen.__anext__()
 coroutine.send(value)                        generator.send(value)                       coroutine agen.asend(value)
 coroutine.throw(value)                       generator.throw(value)                      coroutine agen.athrow(value)
 coroutine.throw(type[, value[, traceback]])  generator.throw(type[, value[, traceback]]) coroutine agen.athrow(type[, value[, traceback]])
 coroutine.close()                            generator.close()                           coroutine agen.aclose()


  Интроспекция — это способность программы исследовать тип или свойства объекта во время работы программы. - inspect
 Вы можете поинтересоваться, каков тип объекта, является ли он экземпляром класса
 Интроспекция позволяет вам изучать атрибуты объекта во время выполнения программы, а РЕФЛЕКСИЯ — манипулировать ими.
 В Python самой распространённой формой интроспекции является использование метода dir() , .__dir__()

 dir() , .__dir__()  -  Чтобы посмотреть все методы и атрибуты, связанные с определенным объектом в Python
 import datetime
 [_ for _ in dir(datetime) if 'date' in _.lower()]

 Python поддерживает полную интроспекцию времени исполнения. Это означает, что для любого объекта можно получить
 всю информацию о его внутренней структуре.
 Примеры: __dict__ , __class__ ,    type() , dir() или встроенного модуля inspect

  --- Менеджер пакетов: pip, conda, poetry - новый менеджер пакетов ---

  Менеджер пакетов - это инструмент, который позволяет управлять установкой, обновлением и удалением библиотек
  и зависимостей в проектах на языке Python.

 В общем, Poetry предлагает более мощные и удобные возможности для работы с проектами на Python по сравнению с pip.


  -- UV VS PIP VS Poetry --

 UV:     Быстрый установщик пакетов Python (альтернатива pip), написан на Rust.
 PIP:    Стандартный менеджер пакетов Python, медленнее, но универсальный.
 Poetry: Инструмент для управления зависимостями и сборки проектов (аналог pip + venv + setuptools).


 REPL (Read–eval–print loop)  - Цикл «чтение—вычисление—вывод» - Read(Чтение) Evaluate(Оценка) Print(Печать) Loop(Цикл)

 -- Модуль struct - сериализованные С-структуры

 Cython - Python+C/C++(Cтатическая типизация)статически компилируемый, ускоряющий выполнение кода на языке Python
 Из-за этих явных указаний типов в переменной и происходит ускорение кода

 Cython позволяет разработчикам оптимизировать код Python, используя функциональность языка C и библиотек CPython

 CPython является интерпретатором байт-кода, написан на C
 CPython написан на языке C, отсюда и его название

 Байт код — это промежуточный язык программирования, предназначенный для исполнения виртуальной машиной.   import dis

 Python компилирует исходный код в байт-код для внутреннего использования, который затем интерпретируется
 виртуальной машиной Python. Этот байт-код сохраняется в .pyc файлах.

 Исходный код Python компилируется в байт-код — внутреннее представление программы Python в интерпретаторе CPython.
 Байт-код также кэшируется в .pyc  файлах, чтобы второй раз выполнить тот же файл быстрее          <-----

 Вкратце, байт тип — это такая последовательность данных, которая уже готова тут же для сохранения в память.

 Стек является структурой данных, которая используется в качестве внутренней рабочей памяти виртуальной машины.
 Существуют разные классы виртуальных машин, и один из них называется стековой машиной. Виртуальная машина Python
 является реализацией такой стековой машины.

 Для СPython : Реализация самой ВМ управляет стеком и кучей (как и сборкой мусора). Сверху стек, снизу куча.
 В стеке - ссылки на объекты в куче. Фактических значений в стеке нет. Все объекты и структуры данных
 (т.е. и переменные и значения) - в куче. Управляет ей внутренний менеджер памяти.

  PyPy – использует Just-In-Time (JIT) компилятор - обходит GIL для ускорения кода Python
 PyPy для ускорения и оптимизации работы кода Python
 Сборщик мусора PyPy по умолчанию называется incminimark это инкрементальный сборщик мусора с перемещением поколений.
 JIT (Just-In-Time) компилятор – это инструмент, который компилирует код Python в машинный код «на лету», во время выполнения программы.


 --- Как устроена память в CPython ---

 Стек (Call Stack)

- Хранит фреймы вызовов (кадры функций) с локальными переменными, аргументами и инструкциями байт-кода.
- Переменные в стеке – это ссылки (указатели) на объекты в куче. Самих объектов в стеке нет.

 Куча (Heap)

 - Здесь хранятся все объекты (числа, строки, списки и т.д.).
 - Управляется менеджером памяти CPython (аллокатор pymalloc, который работает поверх malloc от ОС).
 - Сборщик мусора (GC) следит за объектами в куче (основное – подсчёт ссылок, дополнительное – обработка циклических
   ссылок через generational GC).

 Ссылки vs Объекты

 Переменная a = [1, 2] – в стеке хранится ссылка на список в куче.
 Сам список [1, 2] лежит в куче, а его элементы – тоже ссылки на объекты (1 и 2 – отдельные неизменяемые объекты в куче).

 # Пример в деталях

 def foo():
     x = 10  # В стеке фрейма `foo` – ссылка `x` на объект `10` в куче
     y = [x] # В куче создаётся список, содержащий ссылку на `10`
     return y

 Стек фрейма foo:

 x → int 10 (в куче)
 y → список [ссылка на 10] (в куче)

 Куча:

 Объект 10 (тип int)
 Объект-список [ссылка на 10]

 Важные уточнения:
 - Неизменяемые типы (например, int, str, tuple) могут кэшироваться или переиспользоваться (например, малые числа в CPython).
 - Изменяемые типы (list, dict) обычно мутируют на месте, но операции типа x = x + [1] создают новый объект
 - Сборка мусора работает не только через подсчёт ссылок, но и через generational GC для борьбы с циклами.
 - Неизменяемые типы: кэшируются (числа -5..256, строки)

 Итог:
 важно помнить, что стек хранит именно фреймы вызовов, а не просто "переменные", и что куча управляется не только GC,
 но и аллокатором CPython.


 -- MALLOC VS PYMALLOC --

 malloc  — стандартная функция C для выделения памяти (использует системные вызовы, например brk/mmap).

 pymalloc — собственный аллокатор CPython:
 - Оптимизирован для мелких объектов (≤512 байт).
 - Работает быстрее malloc за счёт пулов памяти (предвыделенные блоки).
 - Для больших объектов (>512 байт) автоматически переключается на malloc.
 - Работает только для выделения памяти внутри Python (например, не используется для сторонних C-библиотек).

 Разница malloc vs pymalloc:
 pymalloc — это оптимизированная версия malloc для Python, которая уменьшает накладные расходы на частое
 выделение/освобождение мелких объектов.


 malloc vs pymalloc:

 Особенность	    malloc	         pymalloc
 Размер объектов	Любые	         Только ≤ 512 байт
 Скорость	        Медленнее	     Быстрее (пулы памяти)
 Использование	    Системный вызов	 Внутренний аллокатор Python



 --- Что происходит в момент вызова функции? ---

 При вызове функции Python сначала связывает позиционные аргументы по порядку, затем ключевые аргументы по именам
 параметров. Если остались незаполненные параметры - используются значения по умолчанию. Любые несоответствия
 количества/имен аргументов вызывают ошибку.

 Интерпретатор Python создаёт словарь связывания имен параметров и заполняет его в таком порядке:

 - Позиционные аргументы связываются по порядку.
 - Ключевые аргументы связываются по имени (НЕ становятся позиционными!).
 - Незаполненные параметры получают значения по умолчанию (если есть).
 - Если параметр остался без значения → ошибка.

 Дополнительные нюансы:

 - Ключевые аргументы могут перезаписать позиционные (если имя совпадает).
 - Значения по умолчанию вычисляются при определении функции (не при вызове!).
 - Если есть *args и **kwargs, "лишние" аргументы попадают туда.

 Но ключевые аргументы никогда НЕ превращаются в позиционные — они всегда обрабатываются отдельно.


 Правильный процесс вызова функции:

 1) Фаза связывания аргументов (в строгом порядке):

 - Сначала обрабатываются позиционные аргументы (слева направо)
 - Затем обрабатываются ключевые аргументы (по именам параметров)

 2) Проверка на конфликты:

 Запрещено дублирование (один параметр не может получить значение и позиционно, и по ключу)


 def func(a, b): pass
    func(1, a=2)  # Ошибка: параметр 'a' задан дважды

 3) Заполнение значений по умолчанию:

 - Только для параметров, которые не получили значений на предыдущих этапах
 - Значения берутся из определения функции (вычисляются один раз при создании функции!)

 4) Финальная проверка:

 - Если остались обязательные параметры без значений → TypeError


 Когда все аргументы обработаны, начинается заполнение оставшихся пустых слотов соответствующим значением по умолчанию
 из определения функции.


 ВАЖНО!!!      Значения по умолчанию вычисляются один раз при определении функции!
 Обратите внимание, на то, что изменяемый объект используемый в качестве значения по умолчанию, будет использоваться всеми вызовами
 Изменяемые объекты как значения по умолчанию: потенциально не безопасно, так как их состояние сохраняется между вызовами функции.

 Значения по умолчанию вычисляются в точке выполнения инструкции def, а не в точке ее вызова, так что:
 i = 5
 def f(arg=i):        # точка вызова инструкции def         Значения по умолчанию вычисляются тут!
     print(arg)
 i = 6
 print(f())  # -> 5   # точка вызова функции



  -- Это происходит из-за того, что определения функций загружаются в память при интерпретации всего модуля, и функции
    становятся доступными для вызова в любом месте, даже до их определения в коде. --

 #  Функция f1 видит f2, потому что Python загружает все определения функций при интерпретации модуля, независимо
 # от их порядка. Поэтому вызов f2() в f1() работает, даже если f2 определена ниже.                               <-----

 # Тоже самое            # Тоже самое  # ПОМЕНЯЛИ ФУНКЦИИ МЕСТАМИ  # Классы должны быть определены ПЕРЕД использованием
 def f1():               def f2():                                 class F1(F):
     return f2()             return 'hehe'                             pass

 def f2():               def f1():                                 class F:
     return 'hehe'           return f2()                               pass

 print(f1())  # -> hehe  print(f1())  # -> hehe                    F1()  # Ошибка: NameError: name 'F' is not defined

 # В Python функции могут быть вызваны до их определения, поскольку интерпретатор загружает все определения функций
 в память при выполнении модуля. Поэтому, когда вы вызываете f1(), интерпретатор уже знает о существовании f2(), даже
 если она определена ниже в коде. Это позволяет функции f1() корректно вызывать f2() и возвращать её результат.   <-----


 ### Это распаковка: в цикле пара ["a","b"] сразу раскладывается в a и b, а второе значение из zip попадает в val.
 equations = [["a", "b"], ["b", "c"]]
 values = [2.0, 3.0]
                                               # ТОЖЕ САМОЕ                             ### ВАЖНО <-------
 for (a, b), val in zip(equations, values):    for eq, val in zip(equations, values):
     print(a, b, val)                              a, b = eq
                                                   print(a, b, val)
 # -> a b 2.0
 # -> b c 3.0


  --- Что за звери *args и **kwargs ---
 1) args и kwargs в параметрах функции - общепринятые имена, но можно использовать и другие
 2) * позволяет распаковать iterable/sequence, а ** распакуют словарь
 2) Итерируемый объект (iterable) - это объект, который способен возвращать элементы по одному
 2) Кроме того, это объект, из которого можно получить итератор. Примеры итерируемых объектов:
 все последовательности: список, строка, кортеж
 2) Таким образом, получается, что итерируемый объект это любой объект который реализует метод __iter__
 2) Последовательность (sequence) - это итерируемый объект, к элементам которого можно обратиться по целочисленному
 индексу, а также можно узнать общее количество элементов (длину последовательности)
 2) Последовательность (sequence) - можно вызвать метод __getitem__() и __len__() ---> list, tuple, range, str и bytes.
 3) если нет никаких спецсимволов, то аргументы функции можно передавать как позиционно, так и keyword (то есть ключ=значение).
 Важно помнить, что позиционные всегда идут раньше keyword(именованных),
 при этом keyword аргументы между собой не обязаны хранить порядок.
 4) спецсимвол / в параметрах функции говорит, что все, что ДО него должно передаваться как позиционные аргументы
 5) спецсимвол * (без указания переменной), говорит о том что все, что ПОСЛЕ него должно передаваться как keyword аргумент
 6) *args в параметрах функции соберет все позиционные аргументы в кортеж (tuple)
 7) **kwargs в параметрах функции соберет все keyword (именованные) аргументы в словарь (dict)


 # В вызовах функций можно использовать * НЕСКОЛЬКО РАЗ
 def fun(a, b, c, d, *rest):
     return a, b, c, d, rest

 print(fun(*[1, 2], 3, *range(4, 7)))     # -> (1, 2, 3, 4, (5, 6))
 print(fun(*[1, 2], *[3, 4], *range(2)))  # -> (1, 2, 3, 4, (0, 1))


 # В вызовах функций можно использовать ** НЕСКОЛЬКО РАЗ     Оператор **
 def fun(a, b, c, d, **rest):
     return a, b, c, d, rest

 print(fun(*[1, 2, 3, 4], **{'2': '2'}, **{'1': '2'}))  # -> (1, 2, 3, 4, {'2': '2', '1': '2'})


 # Можно использовать много раз ** если ключи НЕ повторяются и КЛЮЧИ = СТРОКИ    Работает так в функциях
 def fun(**kwargs):
     return kwargs

 print(fun(**{'x': 1}, y=2, **{'z': 2}))  # -> {'x': 1, 'y': 2, 'z': 2}
 print(fun(**{'x': 1}, y=2))              # -> {'x': 1, 'y': 2}
 print(fun(**{'x': 1}, y=2, **{1: 2}))    # -> TypeError: keywords must be strings

 # Пример без функции МОЖНО ЛЮБЫЕ КЛЮЧИ ИСПОЛЬЗОВАТЬ
 print({**{'x': 1}, 1: 2, **{1: 2}})  # -> {'x': 1, 1: 2}


 Оператор == используется для сравнения значений двух переменных
 Оператор is используется для проверки, являются ли две переменные одним и тем же объектом

 # Пример l2 = list(l1)    Объекты РАЗНЫЕ                       # Пример l2 = l1    Объекты ОДИНАКОВЫЕ
 l1 = [3, [55, 44], (7, 8, 9)]                                  l1 = [3, [55, 44], (7, 8, 9)]
 l2 = list(l1)                                                  l2 = l1
 print(l2 == l1)   # -> True                                    print(l2 == l1)   # -> True
 print(l2 is l1)   # -> False                                   print(l2 is l1)   # -> True

 # Ссылки внутри ОДИНАКОВЫЕ                                     # Ссылки внутри ОДИНАКОВЫЕ
 print(id(l1[1]), id(l1[2]))  # -> 2018437580864 2017853012992  print(id(l1[1]), id(l1[2]))  # -> 3024612180992 3024027483136
 print(id(l2[1]), id(l2[2]))  # -> 2018437580864 2017853012992  print(id(l2[1]), id(l2[2]))  # -> 3024612180992 3024027483136

 # Объекты РАЗНЫЕ                                               # Объекты ОДИНАКОВЫЕ
 print(id(l1), id(l2))        # -> 2018437200960 2018418215616  print(id(l1), id(l2))        # -> 3024611801216 3024611801216


 # Интересный пример
 a = {'A': 1}
 c = a
 print(c is a)       # -> True

 c['B'] = 2
 print(a)            # {'A': 1, 'B': 2}
 b = {'A': 1, 'B': 2}
 print(b == c)       # -> True                   # Тоже самое
 print(b.__eq__(c))  # -> True                   # Тоже самое
 print(b is c)       # -> False
 print(b is not c)   # -> True

 # is Работает быстрее чем == потому что его НЕвозможно перегрузить  Интерпретатор НЕ ищет и НЕ вызывает   Спец. методы
 #    ==  синтаксический сахар метода __eq__()      a.__eq__(b)

 res = [1, 2]
 res.append(3)

 res_2 = [1, 2, 3]
 print(res, res_2)         # -> [1, 2, 3] [1, 2, 3]
 print(res == res_2)       # -> True
 print(res.__eq__(res_2))  # -> True
 print(res is res_2)       # -> False


 Скорость   ==,    is,    in   Объяснение почему in быстрее ==      Why is 'x' in ('x',) faster than 'x' == 'x'?
 Проверка is может быть гораздо более быстрой, чем проверка на == (которая может потребовать рекурсивной проверки членов).

 Оба (in, ==) отправляются в if (left_pointer == right_pointer); разница лишь в том, сколько работы они прикладывают,
 чтобы достичь этой цели. in просто делает меньше.

 Отличие функции от метода:
 Метод — это функция определенная внутри тела класса.

 Методы отличаются от обычных функций только тем, что экземпляр объекта (self) добавляется к другим аргументам.

 Функции Python — это объекты первого класса
 Функции Python — это Дескрипторы.

 # Пример
 def func():
     '''my func'''

 print(type(func))            # -> <class 'function'>

 # Если вызывать через ЭК  будет метод  Если через Класс функция                                                 <-----
 class Func:
     def class_fun(self):
         '''my func'''

 f = Func()
 print(type(Func.class_fun))  # -> <class 'function'>                                                          # <-----
 print(type(f.class_fun))     # -> <class 'method'>                                                            # <-----
 Благодаря протоколу дескрипторов присоединенная к классу функция становиться методом при обращении через ЭК     <-----


 Любой тип данных (int, list, str, tuple и т.д.) является примером объекта первого класса в python.
 Объект называют «объектом первого класса» (first-class object, first-class entity, first-class citizen) если:
 - С ним можно работать как с переменными
 - Он может быть передан в функцию как аргумент
 - Он может быть возвращен из функции как результат
 - Он может быть включен в другие структуры данных. Например, быть элементом словаря или списка


 Карринг — это преобразование функции от многих аргументов в набор вложенных функций, каждая из которых является
 функцией от одного аргумента:

 Мемоизация — это техника оптимизации, которая позволяет сохранять результаты функций с целью избежать повторных вычислений.
 Мемоизация довольно простая и эффективная практика. Благодаря functools.lru_cache, ей удобно пользоваться в Python.
 Под капотом у нее словарь в виде хэш-таблицы, соответственно ключ должен реализовать хеширование


  Monkey patch (обезьяний патч) — Это просто динамическая замена атрибутов/функций во время выполнения
 class A:
    def func(self):
        return "Hello"

 def new_func(self):
     return "Hello, World"

 A.func = new_func          # monkey-patch        <-----

 Проще говоря, Monkey patch вносит изменения в модуль или класс во время работы программы.
 MonkeyPatch — это часть кода Python, которая расширяет или изменяет другой код во время выполнения (обычно при запуске).

 Простой пример выглядит так:
 from SomeOtherProduct.SomeModule import SomeClass

 def speak(self):
     return "ook ook eee eee eee!"
 SomeClass.speak = speak     # monkey-patch       <-----

 Поскольку классы Python изменяемы, а методы — это всего лишь атрибуты класса, вы можете делать это сколько угодно — и,
 по сути, вы можете даже заменять классы и функции в модуле совершенно таким же образом.
 Будьте осторожны при использовании monkeypatching!                                                            <-----


 --- Closure Замыкание ---
 Замыкание(Closure) - Внутренняя функция которая возвращается из внешней и при этом
 использует переменные из внешнего Scope(Область видимости)
 каждое замыкание хранит своё состояние, они не пересекаются
 хранит состояние(данные), предоставляет интерфейс для работы с ними,
 "скрывает" данные и помогает избегать global

 По сути замыкание - это внутренняя функция, которая возвращается из внешней и использует переменные из внешнего скоупа
 (которые ей не принадлежат). Функция как бы "замыкает", захватывает переменные из внешней функции. Вы могли встречать
 такое например в декораторах.

 Каждый объект замыкания независим, они не пересекаются, у каждого свои данные.
 Замыкания это еще один шаг в сторону ООП, так как тут мы имеем некоторое состояние (данные)
 сокрытое от посторонних глаз и с которым можно взаимодействовать только с помощью заранее написанного интерфейса (функция).
 Замыкания могут быть полезны для того чтобы избегать использования global, а также и в других случаях,
 когда нам важно, чтобы наши данные не изменили невалидным способом, чтобы с данными работали только через нашу логику.
 НО(!) до этих данных тоже можно добраться при определенном желании, нужно понимать что нет полного сокрытия данных.
 boys.__closure__[0].cell_contents

 def names():
    all_names = []

    def inner(name: str) -> list:
        all_names.append(name)
        return all_names

    return inner

 boys = names()
 boys('Vasya')
 boys('Sasya')
 print(boys.__closure__[0].cell_contents)  # -> ['Vasya', 'Sasya']

 Замыкание lambda
 def pow_(base):
    return lambda value: value**base


 # Интересный пример     При изменении   переменной Видит её как локальную!
 # Всякая переменная, которой присваивается значение в теле Функции считается ЛОКАЛЬНОЙ                       <-----

 b = 6                      b = 6                                   b = 6
 def func(a):               def func(a):                            def func(a):
     print(a, end=' ')          print(a, end=' ')                       global b
     print(b)                   print(b)                                print(a, end=' ')
                                b = 9                                   print(b)
                                                                        b = 9
 func(3)  # -> 3 6          func(3)  # -> 3  UnboundLocalError       func(3)  # -> 3 6



 # Замыкание class vs def    Реализация своей функции avg
 # Функция НЕЭФФЕКТИВНА              # ЭФФЕКТИВНА                               # Через КЛАСС
 def make_averager():                def make_averager():                       class Averager:
     series = []                         total = 0                                  def __init__(self):
                                         count = 0                                      self.series = []
     def averager(new_value):
         series.append(new_value)        def averager(new_value):                   def __call__(self, new_value):
         total = sum(series)                 nonlocal total, count                      self.series.append(new_value)
         return total / len(series)          count +=1                                  total = sum(self.series)
     return averager                         total += new_value                         return total / len(self.series)
                                             return total/count
 avg = make_averager()                   return averager                        avg = Averager()
 print(avg(10))  # -> 10.0                                                      print(avg(10))  # -> 10.0
 print(avg(11))  # -> 10.5                                                      print(avg(11))  # -> 10.5
 print(avg(12))  # -> 11.0                                                      print(avg(12))  # -> 11.0

 # __code__ Откомпилированное ТЕЛО ФУНКЦИИ
 print(avg.__code__.co_varnames)          # -> ('new_value', 'total')  # Локальные переменные функции
 print(avg.__code__.co_freevars)          # -> ('series',)             # Свободные переменные замыкания
 print(avg.__closure__[0].cell_contents)  # -> [10, 11, 12]            # Текущее содержимое


  --- ДЕКОРАТОР (ФУНКЦИЯ ВЫСШЕГО ПОРЯДКА) СТРУКТУРНЫЙ ПАТТЕРН ПРОЕКТИРОВАНИЯ ---
 ДЕКОРАТОР — это ФУНКЦИЯ ВЫСШЕГО ПОРЯДКА, которая принимает другую функцию в качестве аргумента,
 добавляет к ней некоторую дополнительную функциональность и возвращают функцию с измененным поведением.
 Функция - полноправный обьект
 внутренняя функция может захватывать переменные из внешней
 Суть декоратора в том, что мы можем менять поведение декорируемого объекта,
 при этом не меняя его собственную реализацию, его код.

 Проще говоря: декораторы обертывают функцию, изменяя ее поведение.

 Декораторы в Python выполняются сразу после загрузки модуля, когда интерпретатор встречает их.    <-----
 Декораторы выполняются при импорте модуля, тогда как декорируемые функции вызываются только в момент их использования.
 Это и демонстрирует различие между ЭТАПОМ ИМПОРТА (где работают декораторы) и ЭТАПОМ ВЫПОЛНЕНИЯ
 (где вызываются декорируемые функции).

 ФАБРИКА ДЕКОРАТОРОВ - функция которая возвращает декоратор
 Декораторы классов позволяют изменять поведение класса без изменения его исходного кода.    Модуль dataclasses

 @dataclasses.dataclass(*, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True,
 kw_only=False, slots=False, weakref_slot=False)

  # Модуль dataclasses
 from dataclasses import dataclass

 @dataclass
 class InventoryItem:
     name: str
     unit_price: float
     quantity: int = 0

 item = InventoryItem(name="HEHE", unit_price=12, quantity=100)
 print(item.__dict__)  # -> {'name': 'HEHE', 'unit_price': 12, 'quantity': 100}


  -- Интересный пример Наследование в Dataclasses  Декорировать нужно все классы при Наследовании --

 # Вот ещё пример логичности и простоты. Датаклассы Base и Child:

 from dataclasses import dataclass

 @dataclass
 class Base:
   a: str

 class Child(Base):
   b: str


 # Внезапно оказывается, что Base ведёт себя как приличный датакласс, а Child — как неприличный:

 print(Base())                     # -> TypeError: Base.__init__() missing 1 required positional argument: 'a'
 print(Base(a='test'))             # -> Base(a='test')
 print(Child(a='test'))            # -> Child(a='test')
 print(Child(a='test', b='test'))  # -> TypeError: Base.__init__() got an unexpected keyword argument 'b'

 # Почему так? Потому что «датаклассовость» не наследуется. Если хочется, чтобы Child тоже был датаклассом,
 # его нужно тоже декорировать.             Так что будьте осторожны при наследовании.                      <-----



 --- ЕСЛИ ПОВЕСИТЬ ДЕКОРАТОР НА ASYNC (АСИНХРОННУЮ ФУНКЦИЮ) КАКИЕ МОГУТ БЫТЬ ПРОБЛЕМЫ? ---                 <------
 При декорировании async функций в Python могут возникнуть следующие проблемы:

 - Потеря контекста – если декоратор не поддерживает await, асинхронная функция превратится в синхронную.
   - Потерю контекста выполнения (например, asyncio.get_running_loop() может сломаться,
     если декоратор неаккуратно обрабатывает асинхронность).
   - Проблемы с contextvars, если декоратор не сохраняет контекст.
 - Неправильная работа return/raise – если декоратор не пробрасывает результаты и исключения через await.
 - Утечка корутин – если забыть await внутри декоратора, корутина не выполнится.
   - Возврат корутины без её выполнения (как в исходном примере universal_decorator).
   - Неправильная обработка исключений внутри корутины.
 - Сложность с functools.wraps – без него теряются метаданные функции (__name__, __doc__).

 Правильный подход:

 # Если декоратор должен работать и с синхронными, и с асинхронными функциями – потребуется более сложная логика.
 from functools import wraps

 # Простой асинхронный декоратор (только для async-функций)
 def async_decorator(func):
     @wraps(func)
     async def wrapper(*args, **kwargs):
         # До вызова
         result = await func(*args, **kwargs)
         # После вызова
         return result
     return wrapper


 - Декоратор не async – сломает асинхронность, если не вызывает await.
 - Утечка корутин – если забыть await, функция не выполнится.
 - Неправильные метаданные – без @wraps теряется __name__, __doc__.
 - Ошибки в цепочке вызовов – если декоратор не пробрасывает return/raise через await.

 # Асинхронный декоратор с логированием
 def async_decorator(func):
     @wraps(func)
     async def wrapper(*args, **kwargs):
         return await func(*args, **kwargs)  # Обязательно await!
     return wrapper


 # Пример универсального декоратора, который работает и с СИНХРОННЫМИ, и с АСИНХРОННЫМИ функциями

 import inspect
 from functools import wraps
 import contextvars

 # Универсальный декоратор (для sync и async функций)
 def universal_decorator(func):
     is_async = inspect.iscoroutinefunction(func)

     if is_async:
         @wraps(func)
         async def wrapper(*args, **kwargs):
             print("До вызова (async)")
             try:
                 result = await func(*args, **kwargs)
                 print("После вызова (async)")
                 return result
             except Exception as e:
                 print(f"Ошибка в async: {e}")
                 raise
     else:
         @wraps(func)
         def wrapper(*args, **kwargs):
             print("До вызова (sync)")
             try:
                 result = func(*args, **kwargs)
                 print("После вызова (sync)")
                 return result
             except Exception as e:
                 print(f"Ошибка в sync: {e}")
                 raise

     return wrapper

 # Пример использования
 @universal_decorator
 async def async_func():
     print("Асинхронная функция")
     return 42

 @universal_decorator
 def sync_func():
     print("Синхронная функция")
     return 24

 # Вызов
 import asyncio

 async def main():
     print(await async_func())  # Асинхронный вызов
     print(sync_func())         # Синхронный вызов

 asyncio.run(main())

 Ключевые моменты:
 - inspect.iscoroutinefunction(func) - проверяет, асинхронная ли функция
 - Двойная обёртка - отдельные ветки для sync/async случаев
 - @wraps сохраняет метаданные
 - await используется только для асинхронного случая

 --- END ЕСЛИ ПОВЕСИТЬ ДЕКОРАТОР НА ASYNC (АСИНХРОННУЮ ФУНКЦИЮ) КАКИЕ МОГУТ БЫТЬ ПРОБЛЕМЫ? ---


 -- Атрибут экземпляра  и   Атрибут класса        dataclass  с аннотацией и БЕЗ --

 from dataclasses import dataclass
 from typing import ClassVar       #  ClassVar - атрибут является атрибутом класса

 # Обьяление С аннотацией типа  БУДЕТ  Атрибут экземпляра
 @dataclass
 class Spam:
     repeat: int       # Атрибут экземпляра

                                                # Объяление БЕЗ аннотации типа  БУДЕТ  Атрибут Класса    ClassVar
 @dataclass                                     @dataclass                           @dataclass
 class Spam:                                    class Spam:                          class Spam:
     repeat: int = 99  # Атрибут экземпляра         repeat = 99  # Атрибут Класса        repeat: ClassVar[int] = 99

 s = Spam()                                     s = Spam()
 s1 = Spam()                                    s1 = Spam()

 # Значение НЕ изменяется для всех ЭК           # Значение изменяется для всех ЭК
 Spam.repeat = 10                               Spam.repeat = 10
 print(s.repeat)   # -> 99                      print(s.repeat)    # -> 10
 print(s1.repeat)  # -> 99                      print(s1.repeat)   # -> 10


 ДЕКОРАТОР С ПАРАМЕТРАМИ - ЭТО ДОБАВЛЕНИЕ ЕЩЕ ОДНОГО УРОВНЯ ВЛОЖЕННОСТИ - чтобы передать параметр


 Принципы написания кода
 DRY - don't repeat yourself             - не повторяйся
 YAGNI - You aren't gonna need it        - это не понадобится
 KISS - Keep it simple, stupid           - будь проще
 POLA - Principle Of Least Astonishment  - не удивляй пользователя
 EAFP - Easier to Ask for Forgiveness than Permission  - проще извиниться, чем просить разрешения (сначала действуй)
 EAFP - Оптимистичное программирование   Optimistic Programming  'Python Way'
 LBYL - Look Before You Leap - смотри, прежде чем прыгнуть (сначала спроси)
 LBYL - Defensive programming  Защитное программирование
 Помните, что нет правил без исключений, все принципы и даже дзен - рекомендации, а не неоспоримый закон!
 Дзен Python - философии программирования от Тима Петерса (PEP20), состоит из 19 «руководящих принципов» написания
 компьютерных программ, влияющих на структуру языка программирования Python.
 команда - import this

 GRASP — принципы для распределения обязанностей в ОБЪЕКТНО-ОРИЕНТИРОВАННОМ программировании.


  --- Паттерны ---
 Паттерны или ШАБЛОНЫ разработки - это общие способы решения частых задач и проблем

 Паттерны в Python – это шаблоны для решения задач, которые часто встречаются в практике программиста.
 Паттерны — это типовые решение для типовых задач.
 Существует три основные разновидности паттернов:
 1) Архитектурные паттерны, 2) паттерны Проектирования, 3) Идиомы
 ПАТТЕРНЫ ПРОЕКТИРОВАНИЯ:
 1) Порождающие паттерны — отвечают за процесс создания объектов.
 2) Структурные паттерны — определяют отношения между объектами, облегчая их взаимодействие
 3) Поведенческие паттерны — определяют способы коммуникации между объектами

 «Шаблон проектирования» — это конструкция относительно «высокого уровня»
 «Идиома» — это относительно «низкоуровневая» конструкция

 Идиомы — это низкоуровневые шаблоны, специфичные для языка программирования. Идиома описывает, как реализовать
 определенные аспекты компонентов или отношения между ними с учетом особенностей данного языка.
 примеры идиом: lambda, enumerate, yield, set, dict, with, тернарный оператор, zip, Listcomps, genexp  и другие
 from collections import Counter, OrderedDict, ChainMap, deque, defaultdict, namedtuple    и другие
 from itertools import chain, combinations, permutations, groupby, zip_longest             и другие
 CamelCase: OrderedDict, ChainMap   если в формате(стиле) CamelCase то значит написано на чистом  Python

  АРХИТЕКТУРНЫЕ ПАТТЕРНЫ - Определяет архитектуру приложения, задают его логику
 отвечает на вопрос «как будет устроен продукт»
 ПАТТЕРНЫ ПРОЕКТИРОВАНИЯ - предоставляют решения на уровне кода
 отвечает на вопрос «как лучше организовать составные части продукта»:
 как эффективнее создавать объекты, настраивать обмен данными между ними и их взаимодействие.


  -- Unit of Work ПАТТЕРН --
 Unit of Work — это паттерн, который группирует несколько операций с БД в одну транзакцию, чтобы:

 - Гарантировать целостность данных (все изменения применяются или откатываются).
 - Упростить управление транзакциями (не открывать/закрывать транзакцию для каждого вызова).

 Пример:
 async with unit_of_work:  # Начало транзакции
     user = await uow.users.get(id=1)
     user.name = "New Name"
     await uow.commit()  # Фиксация (или откат при ошибке)

 Аналог:
 Как "корзина" в интернет-магазине — оплачиваете всё разом, а не каждый товар отдельно.

 Где используется?

 - ORM (SQLAlchemy, Django)
 - Репозитории (DDD, Clean Architecture)



 --- Анти-паттерны — полная противоположность паттернам ---
  Если паттерны проектирования — это примеры практик хорошего программирования, так называемые шаблоны решения определённых
 задач, то антипаттерны — их полная противоположность. Это шаблоны ошибок, которые совершаются при решении различных задач.

 Анти-паттерны -     Божественный обьект(God object) , Singleton Одиночка

 Божественный объект — анти-паттерн, который довольно часто встречается у ООП разработчиков. Такой объект берет на себя
 слишком много функций и/или хранит в себе практически все данные. В итоге мы имеем НЕпереносимый код, в котором,
 к тому же, сложно разобраться. Также подобный код довольно сложно поддерживать, учитывая, что вся система зависит
 практически только от него.

 Бороться с анти-паттерном «Божественный объект» следует путем разбивания одного большого класса на несколько небольших.


  -- Конструкция __MAIN__ для чего и кому нужна. --
 1) любой код на питоне лежит в модуле (файл с расширением py)
 2) любой модуль при запуске программы получает атрибут __name__
 3) один(!) модуль, с которого программа началась (точка входа) получает имя __main__, все остальные
 (которые импортированы) получают имя, равное имени в файловой системе, без расширения. Например first
 4) Крайне важное для понимания! Любой модуль при импорте выполняется, как если бы мы его запустили отдельно.
 То есть все принты будут напечатаны(если они не в функциях), любой вызов функции выполнен.

 if __name__=='__main__' означает  "если этот модуль НЕ был импортирован, а запущен напрямую,
 то..." и все действия в данном блоке НЕ будут выполнены при импорте модуля.

      __main__
 если ты запущен напрямую а не импортирован
 у каждого модуля есть __name__
 __main__ - это имя среды, в которой выполняется код верхнего уровня. «Код верхнего уровня» - это первый
 пользовательский модуль Python, который начинает выполняться. Он «верхнеуровневый», потому что импортирует все
 остальные модули, необходимые программе. Иногда «код верхнего уровня» называют точкой входа в приложение.

   --- lambda ---
 в lambda можно написать всё ,то что можно написать после return в обычной функции
 аналог def!!
 можно писать всё что допустимо после return в def
 не выполняется до вызова ()!!!
 значения по умолчанию вычисляются в момент создания функции (аргумент функции)
 lambda не сериализуется pickle
 AttributeError: Can't pickle local object 'run.<locals>.<listcomp>.<lambda>'

 Аргумент по умолчанию будет создан 1 раз при интерпретации

 lambda-функции - это анонимные функции, которые могут включать только одно выражение.
 lambda - Анонимная встроенная функция, состоящая из одного выражения , которое вычисляется при вызове функции.

 Лямбды в Питоне могут состоять только из одного выражения. Используя синтаксис скобок, можно оформить тело лямбды в несколько строк.
 lambda НЕ поддерживает аннотацию и нет return   <----

  --- Hash ---
 Hashable - обьект является Hashable если у него есть Hash и за время жизни обьекта этот Hash никогда не меняется
 Hashable Строки, Числа, Кортежи(если они состоят из неизменяемых обьектов, строк, чисел)
 Hashable != Immutable
 Для встроенных типов Hashable == Immutable
 Hashable - (int, float, bool, str, frozenset, tuple(если он состоит из НЕизменяемых обьектов), bytes, complex, range, ЭК)
 unhashable type - (list, set, dict, tuple(если есть хоть 1 изменяемый обьект), bytearray)

 Большинство НЕИЗМЕНЯЕМЫХ встроенных объектов Python являются хешируемыми. ИЗМЕНЯЕМЫЕ контейнеры
 (такие как списки или словари) — нет. НЕИЗМЕНЯЕМЫЕ контейнеры (такие как кортежи и frozenset) хешируются только в том
 случае, если их элементы хешируются. Объекты, являющиеся экземплярами пользовательских классов, по умолчанию хешируются.
 Все они сравниваются неодинаково (кроме самих себя), а их хеш-значение получается из их id().

 # hash() чтобы узнать  hashable  или НЕТ
 first_tuple = (1, 2, 3)
 second_tuple = ([1], 2, 3)

 print(hash(first_tuple))   # -> 529344067295497451
 print(hash(second_tuple))  # -> TypeError: unhashable type: 'list'


 В Python значение -1 зарезервировано для внутренних механизмов (например, обозначения ошибок).
 Чтобы избежать конфликтов , hash(-1) возвращает -2, а не -1. Для остальных целых чисел hash(x) обычно равен x.

 # Если hash -1 Значит что-то не так     # В Python значение -1 зарезервировано для внутренних механизмов
 print(hash(-2))  # -> -2
 print(hash(-3))  # -> -3
 print(hash(-1))  # -> -2  (специальный случай)
 print(hash(0))   # -> 0


 --- Рекурсия Recursion   Рекурсивная функция ---
 Рекурсия - функция которая вызывает сама себя - Рекурсия не очень оптимизирована
 Хвостовая Рекурсия - функция вызывает сама себя в самом конце программы - Не отимизирована
 вместо Хвостовой Рекурсии - используем for или while
 Множественная рекурсия - это когда функция вызывает саму себя несколько раз в своем теле.
 Множественная рекурсия возникает, когда в теле функции происходит более одного рекурсивного вызова.
 Изменение лимита рекурсия по умолчанию глубина рекурсии = 1000

 import sys
 sys.setrecursionlimit(4000) - Эта функция устанавливает максимальную глубину рекурсии на указанное значение.

 # Показать глубину рекурсии
 print(sys.getrecursionlimit())  # -> 1000


 # НЕ ЯВЛЯЕТСЯ ХВОСТОВОЙ РЕКУРСИЕЙ   ХВОСТОВОГО ВЫЗОВА НЕТ       # ХВОСТОВАЯ РЕКУРСИЯ   ХВОСТОВОЙ ВЫЗОВ ЕСТЬ
 def factorial(n):                                               def factorial_tc(n, product=1):
     if n < 2:                                                       if n < 1:
         return 1                                                        return product
     return n * factorial(n - 1)                                     return factorial_tc(n - 1, product * n)

 print(factorial(5))  # -> 120                                   print(factorial_tc(5))  # -> 120

 # factorial - результат рекурсивного вызова нужно умножить на n после возвращения.
 # factorial_tc - рекурсивный вызов является последним выражением, и результат может быть возвращен напрямую
   БЕЗ дополнительных операций.


  -- Как работает РЕКУРСИЯ??? --                                                                            <----   <----
 Стек сначала накапливает вызовы рекурсивной функции, потом как только мы дошли до первого условия выхода(BASE CASE)
 Стек обратно разматывает вызовы рекурсивной функции и получает результат.
 (СНАЧАЛА ВСЕ НАКАПЛИВАЕТСЯ ПОТОМ ВСЕ РАСКРУЧИВАЕТСЯ НА СТЕКЕ ВЫЗОВОВ И МЫ ПОЛУЧАЕМ ГОТОВЫЙ РЕЗУЛЬТАТ)


  --- модуль warning ---
 В модуле warning собраны функции для работы с предупреждениями.


 --- Exceptions   Исключения ---
 Исключение — это событие, возникающее во время исполнения программы, нарушающее нормальный ход выполнения

 BaseException - Базовый класс для всех встроенных исключений. Есть 4 его подкласса исключений:

 SystemExit - исключение, порождаемое функцией sys.exit при выходе из программы. Вызывается функцией sys.exit()
 KeyboardInterrupt - порождается при прерывании программы пользователем (обычно сочетанием клавиш Ctrl+C).
 GeneratorExit - порождается при вызове метода close объекта generator.
 Exception - является родителем всех основных исключений, с которыми мы сталкивается при написании программ.

 Когда будет выполнена ветка else в конструкции  try  except  else?
 Ветка else будет выполнена только в том случае, если исключения не было возбуждено в блоке try
 Блок else выполняется, если в процессе выполнения блока try не возникло исключений

 # Пример с всеми блоками  try  except  else  finally :
 def divide(a, b):
     try:
         result = a / b
     except ZeroDivisionError:
         print("Ошибка: деление на ноль!")
     except TypeError as e:
         print(f"Ошибка типа: {e}")
     else:
         print(f"Результат: {result}")
         return result  # finally выполнится ДО возврата!
     finally:
         print("Завершение операции")

 divide(10, 2)  # Сработают else и finally
 divide(10, 0)  # Сработает except и finally

 # Результат: 5.0
 # Завершение операции
 # Ошибка: деление на ноль!
 # Завершение операции


 `try` и `except` занимают немного ресурсов, НО ИХ ИСПОЛЬЗОВАНИЕ ДОБАВЛЯЕТ НАКЛАДНЫЕ РАСХОДЫ ПРИ ВОЗНИКНОВЕНИИ ИСКЛЮЧЕНИЙ.
  В нормальных условиях их влияние незначительно.


 -- finally когда работает а когда НЕТ --

 # В Python блок finally выполняется ВСЕГДА, даже если был вызван sys.exit() в блоке except
 import sys

 try:
     print(2 / 0)
 except ZeroDivisionError:
     print("Деление на ноль!")
     sys.exit(0)  # Программа завершается, но finally всё равно выполнится
 finally:
     print("Этот код выполнится в любом случае!")

 Почему так происходит?
 sys.exit() вызывает исключение SystemExit, но перед завершением интерпретатор всегда выполняет блок finally.


 Блок finally НЕ выполняется только в исключительных случаях:

 - Принудительное завершение через os._exit().
 - Аварийный краш Python (segfault, stack overflow и т. д.).
 - Бесконечный цикл или зависание.
 - Убийство процесса извне (kill -9, диспетчер задач).


 os._exit() завершает процесс немедленно, без вызова обработчиков finally, финализаторов (__del__) и других завершающих действий.

 # Пример 1: Принудительное завершение через os._exit()
 import os

 try:
     print(2 / 0)
 except ZeroDivisionError:
     print("Деление на ноль!")
     os._exit(1)  # Программа завершится сразу, finally не выполнится
 finally:
     print("Этот код не выполнится!")


 # Пример 2: Принудительный краш через os.abort()
 try:
     print("Вызов os.abort()...")
     os.abort()  # Немедленное аварийное завершение (как SIGABRT)
 except:                             # except без указания исключения ловит все ошибки (но так делать не рекомендуется).
     print("Except не сработает!")
 finally:
     print("Этот finally не выполнится!")

 Во всех остальных случаях (включая sys.exit(), исключения и обычное завершение) finally гарантированно выполнится.


 # finally выполняется ДАЖЕ при raise
 try:
     print(2 / 0)  # → ZeroDivisionError
 except ZeroDivisionError:
     print("Деление на ноль!")  # Выполнится
     raise KeyError  # Выбрасывается новое исключение, но сначала выполнится finally!
 finally:
     print("Этот код выполнится в любом случае!")  # Выполнится до завершения программы
     print("РАБОТАЕТ FINALLY ВМЕСТЕ С RAISE")

 # Почему так?
 # - Блок finally гарантированно выполняется перед выходом из try-except, даже если внутри есть raise.
 # - После выполнения finally программа завершится с исключением KeyError.
 # - Это полезно, например, для освобождения ресурсов (закрытия файлов, соединений с БД и т. д.), чтобы избежать утечек.


 # finally всегда выполняется, даже если:

 # - есть raise,
 # - есть return,
 # - произошло другое исключение.

 -- END finally когда работает а когда НЕТ --


 -- Что такое коллизия? --
 Когда хеш-функция возвращает один и тот же ответ для разных данных.

 Коллизия - если у двух НЕ равных элементов hash одинаковый(больше относиться к самописным классам)
 Коллизия - Событие, когда два хеша совпали, а не должны, называется коллизией

 Зондирование - это технология, используемая в хэш-таблицах для нахождения свободных ячеек в случае коллизий. Когда два
 ключа имеют одинаковый хэш и пытаются занять одно место, применяется зондирование для поиска следующего доступного места.

 Зондирование — это метод разрешения коллизий в хэш-таблицах. Когда два ключа имеют одинаковый хэш и пытаются занять
 одно и то же место, зондирование помогает найти следующую свободную ячейку, проверяя соседние индексы.
 Существует несколько стратегий зондирования, включая линейное и квадратичное.


 Когда хеш-таблица приближается к своему максимальному заполнению, чтобы избежать ухудшения производительности
 (например, увеличения числа коллизий), обычно создается новый массив, который вдвое больше текущего.

 Таким образом, процесс расширения хеш-таблицы включает в себя:

 - Создание нового массива большего размера (обычно вдвое).

 - Перераспределение всех существующих элементов в новый массив с использованием хеш-функции.

 Это позволяет поддерживать эффективность операций вставки, удаления и поиска в хеш-таблице.


 -- Как решить проблему коллизии? --
 Проблемы коллизии в Python, связанные с хешированием объектов (например, при использовании СЛОВАРЕЙ или МНОЖЕСТВ),
 разрешаются с помощью следующих методов:

 1. **Хеш-функция**: Python использует хеш-функцию для преобразования объектов (как правило, неизменяемых) в хеш-значения.
 Если два объекта имеют одинаковое хеш-значение, это называется коллизием.

 2. **Методы разрешения коллизий**:
 - **Метод цепочек**: Каждое ведро (или ячейка) хеш-таблицы может содержать список или другой контейнер для хранения
 всех элементов, имеющих одно и то же хеш-значение.

 - **Линейное или квадратичное пробирование**: Используется для поиска следующей пустой ячейки в случае коллизии.
 При этом происходит последовательная проверка ячеек в хеш-таблице.

 - **Удвоение размера хеш-таблицы**: Когда хеш-таблица достигает определенного уровня заполненности, её размер может
 быть увеличен, чтобы уменьшить количество коллизий.

 3. **Кастомизация хешей и сравнений**: Можно переопределить методы `__hash__()` и `__eq__()` в пользовательских классах,
 чтобы изменить способ, которым определяются хеши и равенства объектов, также что может помочь избежать коллизий.

 Эти подходы помогают эффективно управлять хеш-значениями и минимизировать влияние коллизий на производительность операций.


 -- Проблему коллизии в Python можно решить с помощью следующих подходов --:

 1. **Метод цепочек**: Каждый элемент, имеющий одинаковый хеш, хранится в виде списка (или других контейнеров)
  внутри одной ячейки хеш-таблицы.

 2. **Линейное пробирование**: При коллизии проверяется следующая ячейка последовательно, пока не найдется пустая.

 3. **Квадратичное пробирование**: Аналогично линейному, но шаг увеличивается квадратично (1, 4, 9 и т. д.) для поиска
  свободной ячейки.

 4. **Пересчитывание хеш-таблицы**: Если таблица становится слишком заполненной, её размер можно увеличить
  (реализация ресайза) и перераспределить элементы по новым хеш-значениям.

 5. **Переопределение методов**: В пользовательских классах можно переопределить методы `__hash__()` и `__eq__()`,
  чтобы управлять хешированием и сравнением объектов, уменьшая вероятность коллизий.

 Эти стратегии помогают эффективно справляться с коллизиями и обеспечивают быструю работу хеш-таблиц.

 На практике вам не нужно беспокоиться о коллизиях в Python, так как встроенные структуры данных, такие как словари,
 множества уже эффективно обрабатывают их.


 set и dict используют один и тот же алгоритм хэширования

 O(1) - значит за постоянное время
 скорость(время) доступа к элементам одинаковая, скорость не зависит от количества элементов

 Сложность получения элемента в Dict и Set в наилучшем случае составляет O(1),
 поскольку элемент может быть получен просто с помощью хэш-функции в качестве индекса массива.
 Однако в худшем случае, когда возникают хэш-коллизии, сложность может вырасти до O(n), где n - количество элементов в таблице.

 Ну и сложность операций добавления, удаления и поиска элементов в Set и Dict также составляет O(1) в наилучшем случае
 и O(n) в худшем случае.


 # Словари и множества очень зависят от своих хэш-функций. Если хэш-функция некоторого типа данных не дает скорость О(1),
 # любой словарь или набор содержащий этот тип, больше не будет иметь гарантии скорость О(1)             <-----  <-----

 Зависимость от хэш-функций: Таким образом, эффективность словарей и множеств действительно сильно зависит от качества
 и производительности их хэш-функций. Если хэш-функция для определенных типов данных не обеспечивает быстрое построение
 хэш-значений или если она ведет к множеству коллизий, производительность структуры данных может упасть,
 и доступ к элементам перестанет быть O(1).
 В заключение, правильные хэш-функции критически важны для достижения оптимальной производительности словарей и множеств.



 для самописных классов если мы НЕ переопределяем __hash__
 hash будет равен адрес в памяти // поделенный на 16                      # Так тоже может Опеределить пустой Класс
 class Cat:                                                               class Cat:          class Cat:
     pass                                                                     ...                 '''docstring'''

 tom = Cat()
 print(hash(tom))      # ->  151995328801
 print(id(tom))        # ->  2431925260816
 print(id(tom)//16)    # ->  151995328801


 -- Примеры В Python 1, True и 1.0 считаются равными  set, dict, frozenset --

 В множествах остаётся первый встреченный элемент среди дубликатов.
 В словарях:

 - Ключ определяется первым вхождением.
 - Значение берётся последнее.

 Операции множеств работают только с ключами

 # 1. Множества (set и frozenset)  В множествах остаётся ПЕРВЫЙ встреченный элемент среди дубликатов
 print({1, True, 1, 1.0})          # -> {1}
 print({True, 1, 1, 1.0})          # -> {True}
 print(frozenset({1, True, 1.0}))  # -> frozenset({1})
 print(frozenset({True, 1, 1.0}))  # -> frozenset({True})

 # 2. Словари (dict)               В словарях ключ определяется первым вхождением, а значение — последним.
 print({1: 1, True: 2, 1: 3, 1.0: 4})  # -> {1: 4}
 print({True: 1, 1: 2, 1: 3, 1.0: 4})  # -> {True: 4}

 # 3. Операции с ключами словарей
 a = {1: 1, 2: 2}
 b = {1: 1, 3: 3}
 # Объединение словарей (|):
 print(a | b)                # -> {1: 1, 2: 2, 3: 3}
 # Операции МНОЖЕСТВА  работают только с ключами (если не переопределены)
 print(a.keys() | b.keys())  # -> {1, 2, 3}
 print(a.keys() ^ b.keys())  # -> {2, 3}
 print(a.keys() & b.keys())  # -> {1}

 # Операции с элементами (кортежами ключ-значение)
 print(a.items() & b.items())  # ->  {(1, 1)}
 print(a.items() - b.items())  # ->  {(2, 2)}
 print(a.items() | b.items())  # ->  {(1, 1), (3, 3), (2, 2)}
 print(a.items() ^ b.items())  # ->  {(3, 3), (2, 2)}

 # НЕ РАБОТАЕТ С values()  ОПЕРАЦИИ МНОЖЕСТВ ТРЕБУЮТ УНИКАЛЬНЫХ ЭЛЕМЕНТОВ, но values() не гарантирует уникальность.
 print(a.values() & b.values())  # ->  TypeError: unsupported operand type(s) for &: 'dict_values' and 'dict_values'
 print(a.values() - b.values())  # ->  TypeError: unsupported operand type(s) for &: 'dict_values' and 'dict_values'
 print(a.values() | b.values())  # ->  TypeError: unsupported operand type(s) for &: 'dict_values' and 'dict_values'
 print(a.values() ^ b.values())  # ->  TypeError: unsupported operand type(s) for &: 'dict_values' and 'dict_values'

 # МОЖНО ПРЕОБРАЗОВАТЬ в set
 print(set(a.values()) & set(b.values()))  # ->  {1}
 print(set(a.values()) - set(b.values()))  # ->  {2}
 print(set(a.values()) | set(b.values()))  # ->  {1, 2, 3}
 print(set(a.values()) ^ set(b.values()))  # ->  {2, 3}


 УПОРЯДОЧЕННЫЙ - означает, что элементы структуры хранятся в том порядке, в котором они были добавлены
 НЕ УПОРЯДОЧЕННЫЙ - означает, что элементы структуры хранятся в случайном порядке.

 --- set (Множество) ---
 set - множество, хешсет, ИЗМЕНЯЕМЫЙ, НЕУПОРЯДОЧЕННЫЙ набор hashable объектов, доступ и проверка наличия O(1)
 Элементами множества может быть любой НЕизменяемый тип данных
 Множество, Набор(set) - это НЕупорядоченная коллекция без повторяющихся элементов.
 Примечание: чтобы создать пустое Множество(set) - нужно использовать set(), а не {}
 {} - создаст пустой словарь(dict)


 # Устранить дубликаты и оставить Порядок элементов                 <-----
 my_lst = [10, 10, 10, 2, 3]
 # Порядок НЕ УПОРЯДОЧЕННЫЙ!!!
 print(set(my_lst))                          # -> {3, 10, 2}

 # Сохраняем порядок
 print(dict.fromkeys(my_lst).keys())        # -> dict_keys([10, 2, 3])
 print(list(dict.fromkeys(my_lst).keys()))  # -> [10, 2, 3]


 # КОНСТРУКТОР МОЖНО ИСПОЛЬЗОВАТЬ НО НЕЛЬЗЯ unhashable ВНУТРИ
 a = set([1, 2, 3])             # Конструктор преобразует итерируемый объект (список) в множество
 print(a)                       # -> {1, 2, 3}
 a_set = {1, 2, 3}              # Работает   создаёт множество с помощью литерала
 a_set_bad = {1, 2, [1, 2, 3]}  # TypeError: unhashable type: 'list'

 # ВЫВОД конструктор можно использовать для создания множества из итерируемого объекта, но нельзя использовать изменяемые типы


 # Множества (set, frozenset) — не поддерживают индексирование, потому что они неупорядоченные и хранят элементы
 # в произвольном порядке (основаны на хеш-таблицах).

 # ПРИМЕРЫ МНОЖЕСТ И СЛОВАРЕЙ
 my_set = {10, 20, 30}
 print(my_set[0])  # -> TypeError: 'set' object is not subscriptable

 my_set = frozenset({10, 20, 30})
 print(my_set[0])  # -> TypeError: 'frozenset' object is not subscriptable

 # Словари (dict) — начиная с Python 3.7 сохраняют порядок добавления ключей, но по-прежнему не поддерживают доступ
 # по числовому индексу (только по ключу).

 my_dict = {"a": 1, "b": 2, "c": 3}
 print(my_dict[0])  # KeyError: 0 (индексы не работают, только ключи!)
 my_dict[0] # -> KeyError: 0


 --- dict (Словарь) ---
 dict (Словари)  — это ИЗМЕНЯЕМЫЕ УПОРЯДОЧЕННЫЕ коллекции без повторяющихся элементов,
 где каждый элемент представляет собой пару «ключ-значение»
 1) dict - словарь, отображение, хеш-мап, ассоциативный массив, коллекция пар ключ-значение,
 где ключом может быть только hashable тип, доступ по ключу и проверка наличия ключа O(1),
 с питона 3.7 хранит порядок вставки
 2) пустой словарь создавать лучше через {},а не dict(), под капотом сразу будет создано 8 элементов
 3) Hashable != Immutable, эти понятия часто путают, помните что это не одно и то же.
 4) Значения по ключу могут быть любого типа данных. Чтобы взять значение по ключу,
 необходимо указать ключ в квадратных скобках после имени словаря
 a_dict = {'good': 'a', 2: 'b'}, a_dict['good']
 5) алгоритм работы словаря и сета: Получаем хеш -} высчитываем позицию в массиве -} если элемента нет
 то действуем соответственно задаче -} если элемент есть то сравниваем ключ == тому что ищем -}
 если ключ не равен искомому то ищем дополнительный бакет
 Бакет — это просто номер в массиве элементов
 6) По умолчанию самописные классы возвращают хеш основанный на id, если переопределяете хеш, то всегда проверяйте,
 что у равных объектов одинаковый хеш
 7) За скорость словаря и сета мы платим большей памятью и тем, что положить туда можно не любые элементы
 используй {} для создания пустого dict

 хэш-таблицы (hash-table) - позволяет находить элементы за O(1).   НО с  хэш-коллизиями за O(n)
 Поиск занимает постоянное время - ищете вы в 10 млн элементов  или среди 10

 Хеш-таблица (hash table) — это структура данных, которая позволяет находить элементы за O(1). Поиск занимает постоянное
 время вне зависимости от того, ищете вы среди 10 млн элементов или всего среди 10.
 Так же, как массив, хеш для хранения данных требует предварительного выделения большого блока последовательной памяти.
 Но, в отличие от массива, его элементы хранятся не в упорядоченной последовательности. Позиция, занимаемая элементом,
 «волшебным образом» задается хеш-функцией. Это специальная функция, которая на входе получает данные, предназначенные
 для хранения, и возвращает число, кажущееся случайным. Оно интерпретируется как позиция в памяти, куда будет помещен
 элемент. Это позволяет нам получать элементы немедленно. Заданное значение сначала пропускается через хеш-функцию.
 Она выдает точную позицию, где элемент должен находиться в памяти. Если элемент был сохранен, то вы найдете его в этой
 позиции.


 Нужно гарантировать, чтобы в хэш-таблице оставалось незанятым по крайней мере 50% пространства.    <-----
 В противном случае коллизии начнут происходить слишком часто и производительность хэш-таблица значительно упадет!

 Для корректной работы хэш-таблицы требуют выделения очень большого блока непрерывной памяти <-----

 # .keys() и .items() объекта  dict()  Удивительно похожи на frozenset()
 # .keys() и items() -  Поддерживают наиболее полезные операторы и методы класса frozenset()   <-----
 d1 = dict(a=1, b=2, c=3, d=4)
 d2 = dict(b=20, d=40, e=50)

 # Легко получить пересекающиеся ключи
 print(d1.keys() & d2.keys())     # ->  {'d', 'b'}

 print(d1.keys() - d2.keys())     # ->  {'a', 'c'}
 print(d1.keys() | d2.keys())     # ->  {'c', 'e', 'd', 'a', 'b'}
 print(d1.keys() ^ d2.keys())     # ->  {'a', 'c', 'e'}

 print(d1.items() & d2.items())   # ->  set()
 print(d1.items() - d2.items())   # ->  {('b', 2), ('d', 4), ('a', 1), ('c', 3)}
 print(d1.items() | d2.items())   # ->  {('b', 2), ('b', 20), ('a', 1), ('e', 50), ('c', 3), ('d', 40), ('d', 4)}
 print(d1.items() ^ d2.items())   # ->  {('b', 2), ('b', 20), ('a', 1), ('e', 50), ('c', 3), ('d', 40), ('d', 4)}


 types.MappingProxyType — обертка для создания словарей только для чтения

 Mappingproxy — это просто словарь без __setattr__ метода.

 Это помогает интерпретатору гарантировать, что ключи для атрибутов и методов уровня класса могут быть только строками.
 Если ключи являются строками, мы можем упростить и ускорить код общего случая для поиска атрибутов и методов на уровне
 класса. В частности, логика поиска __mro__ для классов нового стиля упрощается и ускоряется за счет предположения,
 что ключи dict классов являются строками.
 Но и защищает __dict__ класса от изменений, обеспечивая согласованность работы механизмов наследования и дескрипторов.

 # Небольшой пример!
 class A: pass

 a = A()

 print(type(a.__dict__))      # -> <class 'dict'>
 print(type(A.__dict__))      # -> <class 'mappingproxy'>

 # У Экземпляра класса обычный словарь                                                                 <-----
 print(a.__dict__)            # -> {}
 a.__dict__['a'] = 1
 a.__dict__.update({'a': 1})
 a.__setattr__('b', 20)
 print(a.__dict__)            # -> {'a': 1, 'b': 20}

 # У Класса словарь только для чтения                                                                  <-----
 A.__dict__['a'] = 1          # -> TypeError: 'mappingproxy' object does not support item assignment
 A.__dict__.update({'a': 1})  # -> AttributeError: 'mappingproxy' object has no attribute 'update'


 __dict__ класса ключи действительно всегда строки (имена атрибутов). Это оптимизация CPython, которая ускоряет
 поиск атрибутов в иерархии наследования (через __mro__).

 Однако mappingproxy сам по себе не гарантирует, что ключи — строки. Если завернуть в mappingproxy обычный словарь с
 нестроковыми ключами, они сохранятся:

 d = {123: "abc"}
 mp = types.MappingProxyType(d)
 print(mp[123])  # "abc" (работает, хотя ключ — не строка)


 -- Что еще может быть в словаре ключем и значением
 def func(): return 200

 x = lambda x: x

 a_dict = {lambda x: x * 2: x(10), func: func(),}         # в словаре может быть функция, lambda-функция
 print(a_dict)  # -> {<function <lambda> at 0x0000026691F499E0>: 10, <function func at 0x0000026691C50D60>: 200}
 b_dict = {(i for i in range(3)): [i for i in range(2)]}  # в словаре может быть genexp, listcomps
 print(b_dict)  # -> {<generator object <genexpr> at 0x0000020C6C2A0040>: [0, 1]}

 Mapping Types — dict,  from collections import defaultdict, OrderedDict, Counter


-- Как Устроены СЛОВАРИ В ПИТОНЕ --

 Современные словари Python (3.6+) используют комбинацию двух таблиц:

 1. `indices` - разреженная таблица индексов (размер минус степень 2)
 2. `entries` - плотный массив записей, сохраняющий порядок вставки

 # Пример работы словаря
 d = {}
 d["яблоко"] = 5
 d["банан"] = 3

 Предположим:
 - hash("яблоко") = 12345 → младшие биты: 0b11000000111001 (12345)
 - hash("банан") = 54321 → младшие биты: 0b1101010000110001 (54321)
 - размер indices = 8 (минимальная степень 2 для 2 элементов)

 # Пример для Python 3.6-3.10:
 Внутренняя структура:

 indices = [None, 0, None, None, None, 1, None, None]  # Разреженный массив
 entries = [
     {'hash': 12345, 'key': "яблоко", 'value': 5},  # Индекс 0
     {'hash': 54321, 'key': "банан", 'value': 3}    # Индекс 1
 ]

 # Поиск "яблоко":
 1) hash("яблоко") = 12345
 2) pos = hash & (len(indices)-1) = 12345 & 7 = 1
 3) indices[1] = 0 → проверяем entries[0]
 4) Сравниваем:
    - entries[0]['hash'] == 12345
    - entries[0]['key'] == "яблоко"
 5) Возвращаем entries[0]['value'] → 5

 # При коллизии (Python 3.6+):
 Используется линейный пробинг:  # ПСЕВДОСЛУЧАЙНЫЙ (perturb) пробинг, а не просто линейный
 next_pos = (current_pos + 1) & (len(indices)-1)

 # Пример с коллизией:
 d = {}
 d["a"] = 1  # hash("a") & 7 = 1
 d["i"] = 2  # hash("i") & 7 = 1 (коллизия)

 Внутренняя структура:
 indices = [None, 0, None, None, None, None, None, 1]
 entries = [
     {'hash': hash("a"), 'key': "a", 'value': 1},
     {'hash': hash("i"), 'key': "i", 'value': 2}
 ]

 # Компактные словари (Python 3.11+):
 Для словарей ≤ 8 элементов используется единая таблица (единый плотный массив (аналог entries)):
 d = {"a": 1, "b": 2}
 # Внутри может выглядеть так:
 compact = [
     {"hash": hash("a"), "key": "a", "value": 1},
     {"hash": hash("b"), "key": "b", "value": 2}
 ]
 # Индексы вычисляются через дополнительную таблицу или битовые маски.


 # Пример для Python 3.11:
 d = {"a": 1, "b": 2}
 # Внутри может выглядеть так:
 indices = [0, 1, -1, -1]  # -1 означает пусто
 entries = [
     {"hash": hash("a"), "key": "a", "value": 1},
     {"hash": hash("b"), "key": "b", "value": 2}
 ]

 Важные особенности:
 1. Размер indices всегда степень двойки (для быстрого вычисления позиции)
 2. При заполнении на 2/3 происходит увеличение и перестроение
 3. Порядок итерации соответствует порядку вставки (благодаря entries)
 4. Python 3.11+ использует компактное хранение для маленьких словарей
 4. Python 3.12+ ещё более компактные словари для некоторых случаев (например, если ключи — только строки).
 5. При удалении элементов запись в entries помечается как удаленная (без нарушения порядка)


 ### ChatGPT
 dict в CPython устроен так: есть таблица слотов (indices) и плотный список записей (entries) в порядке вставки.
 реализация использует разреженную хеш-таблицу + плотный массив записей (split table / compact dict).

 # Пример (УПРОЩЁННО, как можно представить dict внутри)
 d = {"a": 1, "b": 2}

 # разреженная таблица слотов: slot -> индекс в entries (-1 = пусто)
 indices = [-1, 0, -1, -1, -1, 1, -1, -1]

 # плотный массив записей (в порядке вставки)
 entries = [
     (h_a, "a", 1),   # entries[0]
     (h_b, "b", 2),   # entries[1]
 ]

 # смысл:
 # indices[1] = 0  -> в entries[0] лежит ("a":1)
 # indices[5] = 1  -> в entries[1] лежит ("b":2)

-- END Как Устроены СЛОВАРИ В ПИТОНЕ --


 --- list (Список) ---
 list (Список) - список, ИЗМЕНЯЕМЫЙ УПОРЯДОЧЕННЫЙ, обычно хранит значения одного типа, О(1) доступ к элементу
 используй [] для создания пустого списка
 если заранее известен размер, то НЕ используй append (для 8000 добавлений выделяется 8600 ячеек памяти)
 используй листкомпс
 не пытайся заменять список кортежом, там где идет изменение размера

 У list и tuple скорость(время) доступа к элементам одинаковая, скорость не зависит от количества элементов

 Python когда стартует он сразу при старте в ОС (операционной системе)
 резервирует место под tuple. Python не ходит в ОС и не просит дай мне еще памяти как это происходит со списками

 каждый раз когда мы создаем список Python обращается к ОС и просит место                               <-----


 pop([i]), index(x[, start[, end]])
 Квадратные скобки вокруг [i], [, start[, end]. [] - Означает, что параметр является необязательным.    <-----

  -- Очень интересный момент С id Посмотри Внимательно --

 # Конструкция lst_1 *= 3  Изменение обьекта       # Конструкция lst_1 = lst_1 * 3  Создание НОВОГО обьекта

 # id НЕ Изменился                                  # id Изменился
 lst_1 = [1, 2, 3]                                  lst_1 = [1, 2, 3]
 print(id(lst_1))  # -> 2002285652864               print(id(lst_1))  # -> 2671279192832
 lst_1 *= 3                                         lst_1 = lst_1 * 3
 print(id(lst_1))  # -> 2002285652864               print(id(lst_1))  # -> 2671281626048
 print(lst_1)      # -> [1, 2, 3, 1, 2, 3, 1, 2, 3] print(lst_1)      # -> [1, 2, 3, 1, 2, 3, 1, 2, 3]


 # Построение списка списков
 good_lst = [['_'] * 3 for i in range(3)]
 print(good_lst)  # -> [['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]

 good_lst[1][2] = 'X'
 print(good_lst)  # -> [['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']]


 bad_lst = [['_'] * 3] * 3
 print(bad_lst)  # -> [['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]

 bad_lst[1][2] = 'X'
 print(bad_lst)  # -> [['_', '_', 'X'], ['_', '_', 'X'], ['_', '_', 'X']]



 # Получится список содержащий три ссылки на один о тот же внутренний список
 my_lst = [[]] * 3
 my_lst[0].append(1)

 # Таким образом, все три элемента ссылаются на один и тот же объект.
 print(my_lst)       # Выведет: [[1], [1], [1]]                                                <-----

 # Интересный момент  +=  vs  extend()   В Python Console  Странное поведение оператора  +=    <-----
 # Будет TypeError   и Список ИЗМЕНИЛСЯ                            # Будет Работать
 t = (1, 2, [30, 40])                                              t = (1, 2, [30, 40])
 t[2] += [50, 60]                                                  t[2].extend([50, 60])
 # -> TypeError: 'tuple' object does not support item assignment
 t   # -> (1, 2, [30, 40, 50, 60])                                 t                # -> (1, 2, [30, 40, 50, 60])

 # Тоже самое с *=                                                 # Будет Работать
 t = (1, 2, [30, 40])                                              t = (1, 2, [30, 40])
 t[2] *= 2                                                         t[2].extend(t[2])
 # -> TypeError: 'tuple' object does not support item assignment
 t   # -> (1, 2, [30, 40, 30, 40])                                 t                # -> (1, 2, [30, 40, 30, 40])

 Составное присваивание (например, a += 1) в Python НЕ является АТОМАРНОЙ ОПЕРАЦИЕЙ. Это означает, что оно выполняет
 несколько шагов: чтение значения переменной, выполнение операции и запись результата обратно в переменную.
 Поэтому между этими шагами может произойти изменение значения переменной, особенно в многопоточных приложениях.

 # Если что-то НЕпонятно смотим байт-код    += *=    НЕ АТОМАРНЫЕ ОПЕРАЦИИ     Составное присваивание
 print(dis.dis('s[a] += b'))
 print(dis.dis('s[a] *= b'))


 x += 1 для int в CPython — атомарно (из-за GIL, если x — простой int).
 x += [1] для list        — НЕ атомарно (нужна синхронизация, иначе возможна гонка данных).

 1. Атомарность x += 1 для int
 - В CPython операция x += 1 компилируется в один неделимый байткод (INPLACE_ADD + STORE).
 - GIL не отпускается между чтением, изменением и записью → операция атомарна.
 - Но: Если x — это свойство (@property) или объект с перегруженным __iadd__, атомарность теряется.

 2. Неатомарность x += [1] для list
 - Операция распадается на несколько шагов (LOAD, INPLACE_ADD, STORE).
 - Между ними GIL может переключиться → другой поток вмешается → гонка данных.
 - Результат: возможны потерянные обновления (например, len(x) меньше ожидаемого).

 Как избежать проблем?
 Для списков используйте threading.Lock:

 lock = threading.Lock()
 def append_safe():
     with lock:
         x += [1]
 Для сложных структур данных лучше подходят queue.Queue или многопроцессорность (multiprocessing).


 -- Модель dis - Дизассемблер для Python байткода   Смотрим байт-код  --

 import dis

 # Выводим все команды байт-кода
 for opcode in dis.opmap:
     print(f"{opcode}: {dis.opname[dis.opmap[opcode]]}")


 Основные команды байт-кода:
 Команда	      -   Описание
 LOAD_FAST	      -   Загружает локальную переменную (аргумент функции или переменную).
 STORE_FAST	      -   Сохраняет значение в локальную переменную.
 LOAD_CONST	      -   Загружает константу (например, число, строку или None).
 BINARY_ADD	      -   Выполняет сложение двух значений.
 BINARY_SUBTRACT  -   Выполняет вычитание двух значений.
 BINARY_MULTIPLY  -   Выполняет умножение двух значений.
 BINARY_DIVIDE	  -   Выполняет деление двух значений.
 BINARY_OP        -   Заменяет несколько отдельных команд, таких как BINARY_ADD, BINARY_SUBTRACT, BINARY_MULTIPLY и т.д.
 COMPARE_OP	      -   Выполняет операцию сравнения (например, ==, <, >).
 CALL_FUNCTION	  -   Вызывает функцию.
 RETURN_VALUE	  -   Возвращает значение из функции.
 POP_TOP	      -   Удаляет верхний элемент из стека.
 JUMP_ABSOLUTE	  -   Безусловный переход к указанной инструкции.
 JUMP_IF_FALSE	  -   Переход, если значение на вершине стека равно False.
 MAKE_FUNCTION	  -   Создаёт объект функции.
 LOAD_GLOBAL	  -   Загружает глобальную переменную или функцию.
 LOAD_ATTR	      -   Загружает атрибут объекта (например, obj.attr).
 STORE_ATTR	      -   Сохраняет значение в атрибут объекта.
 BUILD_LIST	      -   Создаёт список.
 BUILD_TUPLE	  -   Создаёт кортеж.
 BUILD_MAP	      -   Создаёт словарь.
 GET_ITER	      -   Получает итератор из объекта.
 FOR_ITER	      -   Используется в циклах for для итерации по элементам.

 LOAD_DEREF       -   Загружает значение переменной из cell-объекта (используется в замыканиях).


 -- Модуль copy --
 copy.copy(x)             - возвращает мелкую НЕглубокую(shallow) копию x
 copy.deepcopy(x[, memo]) - возвращает глубокую(deep) копию x


 # В большинстве программ использовать deepcopy() НЕ стоит. Копирование объектов — операция медленная и часто лишняя
 # Учтите, что deepcopy() не будет работать с объектами, в которых задействовано состояние системы или исполнительной
 # среды (открытые файлы, сетевые подключения, программные потоки, генераторы и т. д.).

 # ВСЕ примеры создают поверхностную копию списка                                                      <-----
 res = [1, 2, 3]
 list_1 = res[:]
 list_2 = res[::]
 list_3 = res.copy()
 list_4 = list(res)
 list_5 = res + []
 print(id(res), id(list_1), id(list_2), id(list_3), id(list_4), id(list_5))
 # 2225921212864 2225893028160 2225897553216 2225903276864 2225902572736 2225902572544    # id  ВСЕ РАЗНЫЕ

 Разница между поверхностным и глубоким копированием актуальна только для составных объектов
 (объектов, которые содержат другие объекты, например списки или экземпляры классов):
 - Неглубокая копия создает новый составной объект, а затем (насколько это возможно) вставляет в него ссылки на объекты,
 найденные в оригинале.
 - Глубокая копия создает новый составной объект, а затем рекурсивно вставляет в него копии объектов, найденных в оригинале.

 При операциях глубокого копирования часто возникают две проблемы, которых нет при операциях поверхностного копирования:
 - Рекурсивные объекты (составные объекты, которые прямо или косвенно содержат ссылку на себя) могут вызывать рекурсивный цикл.
 - Потому что глубокое копирование копирует все, что может копировать слишком много, например данные, которые
  предназначены для совместного использования между копиями.

 Метод sort() - сортирует элементы списка по алфавиту в порядке возрастания НЕ СОЗДАВАЯ ЕГО КОПИЙ. # in-place  O(n log(n))
 sorted() возвращает новый отсортированный список, не затрагивая исходный.          O(n log n)
 Метод sort() сортирует элементы списка на месте (in-place), то есть изменяет исходный список и ничего не возвращает.

 Функции sorted() и метод list.sort() реализуют алгоритм TimSort                                            <-----
 # Python 3.10 и раньше: TimSort.
 # Python 3.11 и позже: по сути TimSort-подобная сортировка, но с merge policy Powersort (иногда это и называют “Powersort в Python”)

 Алгоритмы сортировки Python:

 1. **Пузырьковая сортировка (Bubble Sort)**:
 - **Описание**: Сравнивает пары соседних элементов и меняет их местами, если они находятся в неправильном порядке.
   Процесс повторяется, пока массив не будет отсортирован.
 - **Сложность**:
 - Время: O(n²) в худшем и среднем случаях, O(n) в лучшем (при уже отсортированном массиве).
 - Пространство: O(1). (сортировка выполняется in-place, без использования дополнительной памяти).

 2. **Сортировка выбором (Selection Sort)**:
 - **Описание**: Находит минимальный элемент в неотсортированной части массива и меняет его местами с первым элементом.
 Этот процесс повторяется для каждого элемента до конца массива.
 - **Сложность**:
 - Время: O(n²) во всех случаях.
 - Пространство: O(1).

 3. **Сортировка вставками (Insertion Sort)**:
 - **Описание**: Постепенно строит отсортированную последовательность, вставляя элементы из неотсортированной части в
 нужное место отсортированной части.
 - **Сложность**:
 - Время: O(n²) в худшем случае, O(n) в лучшем (если массив почти отсортирован).
 - Пространство: O(1).

 4. **Быстрая сортировка (Quick Sort)**:
 - **Описание**: Использует принцип "разделяй и властвуй". Выбирает опорный элемент и делит массив на две части —
 меньшие и большие по сравнению с опорным — и рекурсивно сортирует эти части.
 - **Сложность**:
 - Время: O(n log n) в среднем случае, O(n²) в худшем (при плохом выборе опорного элемента).
 - Пространство: O(log n) для рекурсии.

 5. **Сортировка слиянием (Merge Sort)**:
 - **Описание**: Разделяет массив на две половины, сортирует каждую половину и затем сливает их обратно в
 отсортированный массив. Также использует "разделяй и властвуй".
 - **Сложность**:
 - Время: O(n log n) во всех случаях.
 - Пространство: O(n).

 6. **Пирамидальная сортировка (Heap Sort)**:
 - **Описание**: Преобразует массив в структуру кучи (heap) и извлекает элементы по одному, чтобы получить
 отсортированный массив. Работает in-place.
 - **Сложность**:
 - Время: O(n log n) во всех случаях.
 - Пространство: O(1).

 7. **Тим-сорт (Tim Sort)**:
 - **Описание**: Адаптивный алгоритм, основанный на сортировке вставками и сортировке слиянием. Используется в Python в
 функции `sorted()` и методе `list.sort()`.
 - **Сложность**:
 - Время: O(n log n) в среднем, O(n) в лучшем случае (при почти отсортированных данных).
 - Пространство: O(n).

 8. **Сортировка Шелла (Shell Sort)**:
 - **Описание**: Улучшенная версия сортировки вставками, которая сравнивает и сортирует элементы, находящиеся на
 определенном расстоянии друг от друга, уменьшая расстояние до 1.
 - **Сложность**:
 - Время: O(n²) в худшем, O(n log n) в среднем (зависит от выбора последовательности).
 - Пространство: O(1).

 9. **Сортировка битом (Radix Sort)**:
 - **Описание**: Сортирует элементы по каждому разряду значений, начиная с наименьших и переходя к более значительным.
 Чаще всего используется для сортировки целых чисел.
 - **Сложность**:
 - Время: O(nk), где k — количество разрядов.
 - Пространство: O(n + k).

 10. **Сортировка подсчётом (Counting Sort)**:
 - **Описание**: Использует дополнительный массив для подсчета числа вхождений каждого уникального элемента.
   Сравнительно быстрая, но работает только для ограниченных диапазонов значений.
 - **Сложность**:
 - Время: O(n + k), где k — максимальное значение в массиве.
 - Пространство: O(k).

 11. **Сортировка по ведрам (Bucket Sort)**:
 - **Описание**: Делит элементы на несколько "ведер" и сортирует каждое ведро индивидуально, после чего объединяет все
   ведра в окончательный отсортированный массив.
 - **Сложность**:
 - Время: O(n + k) для равномерно распределенных данных, где k — количество ведер.
 - Пространство: O(n + k).


 -- Быстрая сортировка (Quicksort) обычно работает быстрее, чем сортировка слиянием (Merge Sort) на практике --
 У быстрой сортировки константа меньше, чем у сортировки слиянием, поэтому, несмотря на то что оба алгоритма
 характеризуются временем O(n log n), быстрая сортировка работает быстрее. А на практике быстрая сортировка
 работает быстрее, потому что средний случай встречается намного чаще худшего.


 Стратегия «разделяй и властвуй» основана на разбиении задачи на уменьшающиеся фрагменты. Если вы используете стратегию
 «разделяй и властвуй» со списком, то базовым случаем, скорее всего, является пустой массив или массив из одного элемента.

 Если вы реализуете алгоритм быстрой сортировки, выберите в качестве опорного случайный элемент.
 Среднее время выполнения быстрой сортировки составляет O(n log n)!

 Константы в «O-большом» иногда могут иметь значение. Именно по этой причине быстрая сортировка быстрее сортировки слиянием.

 При сравнении простой сортировки с бинарной константа почти никогда роли не играет, потому что O(log n) СЛИШКОМ СИЛЬНО
 превосходит O(n) по скорости при большом размере списка.


 Плоский Список(Flat list) - одномерный массив
 Вложенный список(Nested list) - многомерный массив
 Связный список(Linked List) - эффективность добавления элементов в начало, середину и конец.
 Связный список (связанный, список узлов и ссылок или указателей) (Linked List) - deque
 deque - doubly-linked list (двусвязный список)

 Односвязный список(Singly linked list) - однонаправленный связный список, можно передвигаться только в сторону конца списка.
 В практике применим редко, В Python встроенной реализации не имеет. Можно написать свою реализацию или deque

 Односвязный - Потому что каждый элемент хранит ровно 1 связь.   1 указатель/ссылка на следующий элемент
 Двусвязный - Хранит  указатель/ссылку на предыдущий и на следующий элемент


 Связный список
 Cвязный список (linked list) позволяет хранить элементы в цепи ячеек, которые не обязательно должны находиться
 в последовательных адресах памяти. Память для ячеек выделяется по мере необходимости. Каждая ячейка имеет указатель,
 сообщающей об адресе следующей в цепи. Ячейка с пустым указателем (NULL) отмечает конец цепи. Связные списки
 используются для реализации стеков, списков и очередей. При наращивании связного списка не возникает никаких проблем:
 любая ячейка может храниться в любой части памяти. Таким образом, размер списка ограничен только объемом имеющейся
 свободной памяти. Также не составит труда вставить элементы в середину списка или удалить их — достаточно просто
 изменить указатели ячеек.

 Связный список тоже имеет свои недостатки: мы не можем сразу получить n-й элемент. Сначала придется прочитать первую
 ячейку, извлечь из нее адрес второй ячейки, затем прочитать вторую ячейку, извлечь из нее указатель на следующую
 ячейку и т. д., пока мы не доберемся до n-й ячейки. Кроме того, когда известен адрес всего одной ячейки, не так просто
 ее удалить или переместиться по списку назад. Не имея другой информации, нельзя узнать адрес предыдущей ячейки в цепи.


 Двусвязный список
 Двусвязный список (double linked list) — это связный список, где ячейки имеют два указателя: один на предыдущую ячейку,
 другой — на следующую.

 Он обладает тем же преимуществом, что и связный список: не требует предварительного выделения большого блока памяти,
 потому что пространство для новых ячеек может выделяться по мере необходимости. При этом дополнительные указатели
 позволяют двигаться по цепи ячеек вперед и назад. В таком случае, если известен адрес всего одной ячейки, мы сможем быстро ее удалить.

 И тем не менее мы по-прежнему не имеем прямого доступа к n-му элементу. Кроме того, для поддержки двух указателей в
 каждой ячейке требуется более сложный код и больше памяти.


 Массив (array) — это самый простой способ хранения набора элементов в памяти компьютера. Он заключается в выделении
 единого пространства в памяти и последовательной записи в него ваших элементов. Конец последовательности отмечается
 специальным маркером NULL.

 Каждый объект в массиве занимает такой же объем памяти, что и любой другой. Представим массив, начинающийся с
 адреса ячейки памяти s, где каждый элемент занимает b байт. Чтобы получить n-й элемент, нужно извлечь b байт,
 начиная с позиции в памяти    s + (b × n).
 Это позволяет напрямую обращаться к любому элементу массива.

 Когда нам понадобится пятый элемент мы перейдем к элементу по адресу M + 5. То есть, если мы хотим получить i-й
 элемент из нашего массива, мы переходим к M + i
 Итак, если данные хранятся в памяти по порядку, то зная порядок расположенных данных, мы можем найти их за один шаг O(1)


 Массив
  - Предоставляет Константный доступ по индексу.
  - Занимает НЕПРЕРЫВНЫЙ кусок в памяти.
  - Размер куска памяти фиксируется при создании.

  Массив — это структура данных, которая хранит набор элементов одного типа и фиксированного размера.
  В массиве все элементы занимают одинаковое количество памяти, поскольку массив состоит из элементов одного типа.
  Это значит, что каждый элемент массива будет иметь одинаковый размер в байтах.


 Список и Массив  list vs array
 Список - можно хранить объекты разного размера и типа/размер списка НЕ ограничен
 Массив - Данные одного типа/размер массива ограничен.  Размер массива задается в момент создания.
 для создания массива нужно использовать  array.array, NumPy np.array()


 -- Stack vs Массив (Python) --

 Стек - это частный случай массива с ограниченным доступом:                                               <-----

 Работает по принципу LIFO (Last In - First Out)
 - Разрешены только 2 операции:
    - append() (добавить в конец)
    - pop() (убрать последний)

 - Пример: stack = []; stack.append(1); stack.pop()


 Массив (list) - более общая структура:
  - Доступ к любому элементу по индексу arr[0]
  - Любые операции: вставка, удаление, срезы
  - Пример: arr = [1,2]; arr.insert(0,9); arr[1:]


 Где использовать:
 - Стек   - когда нужна дисциплина LIFO (отмена действий, вызов функций)
 - Массив - когда нужен произвольный доступ к элементам



  -- Как хранятся элементы списка и массива в памяти --                                       <-----     <-----

 В Python списки (объект типа `list`) представляют собой динамические структуры данных,
 и их элементы могут храниться в разных местах памяти.                                                  <-----

 Все элементы массива располагаются рядом друг с другом в оперативной памяти. Это свойство массивов позволяет эффективно
 использовать память и обеспечивает быстрый доступ к элементам массива.                                 <-----

 Random Access Memory(RAM, ОЗУ) - Оперативная память   ОЗУ - Оперативное запоминающее устройство

 Все элементы массива располагаются один за другим в непрерывном блоке памяти. Это позволяет быстро получать
 доступ к элементам массива с использованием индексов, поскольку адрес каждого элемента можно вычислить на основе
 адреса первого элемента и размера каждого элемента

 Элементы списков хранятся не обязательно в смежных участках памяти. Когда вы добавляете элементы в список, Python
 может выделять память динамически, что может означать размещение элементов в разных областях памяти.

 Массивы имеют фиксированный тип и размещение элементов в памяти, тогда как Списки представляют собой более гибкие
 структуры данных, которые могут содержать разные типы элементов, и размещение их в памяти НЕ обязательно будет смежным.



 Внутреннее строение списка — массив (точнее, vector) указателей, т. е. список является динамическим массивом.
 Вектор - одномерный массив проиндексированных элементов. В NumPy np.array()
 Векторы: это массивы с динамическим размером. размер памяти не ограничен

 Чтобы получить производительность с амортизируемым временем O(1) для вставок и удалений, новые элементы должны
 добавляться в конец списка методом append() и снова удалятся из конца методом pop().

 Добавление и удаление элементов вначале списка намного медленнее и занимает O(n) времени, поскольку существующие
 элементы должны сдвигаться, чтобы создать место для нового элемента. Такого антишаблона производительности следует избегать.

 К линейным структурам можно отнести массивы, очереди, стеки, деки, линейные списки. Структуры стека.   <-----


 -- Модуль bisect - реализация алгоритма бинарного поиска
 Обеспечивает поддержку вставки значений в отсортированный список, без необходимости сортировать этот список после каждой вставки.
 Бинарный поиск существенно быстрее, чем обычный, но требует предварительной сортировки коллекции, по которой осуществляется поиск.

 bin_lst = [-1, -3, 2, 4, 5, 7, 8, 9]  # Будем искать 9
 # БИНАРНЫЙ поиск  O(log n) - кратко как работает берем серединный элемент например  4
 # сравниваем 9 > 4  и будем искать  ТОЛЬКО УЖЕ ТУТ   [4, 5, 7, 8, 9]
 # берем серединный элемент например  7
 # сравниваем 9 > 7  и будем искать  ТОЛЬКО УЖЕ ТУТ   [8, 9]


 При бинарном поиске каждый раз исключается половина чисел
 Для числе 100 - Будет 7 шагов                              Бинарный O(log n) поиск для числа 100 - 7   Шагов
 Пример:  100 -> 50 -> 25 -> 13 -> 7 -> 4 -> 2 -> 1         Обычный  O(n)    поиск для числа 100 - 100 Шагов

 Левый и Правый двоичный поиск
 from bisect import bisect_left, bisect_right


 -- Лучше НЕ использовать конструкторы --

 # Тоже самое будет работать со всеми конструкторами set(), tuple() и тд...

 # dis.dis('list()')                                           # dis.dis('[]')

 #   0           0 RESUME                   0                #   0           0 RESUME                   0
 #   1           2 PUSH_NULL                                 #   1           2 BUILD_LIST               0
 #               4 LOAD_NAME                0 (list)         #               4 RETURN_VALUE
 #               6 CALL                     0
 #              14 RETURN_VALUE



 --- tuple (Кортеж) ---
 tuple кортеж, НЕИЗМЕНЯЕМЫЙ УПОРЯДОЧЕННЫЙ, обычно хранит значения РАЗНЫХ типов, О(1) доступ к элементу
 используй кортежи везде, где это возможно и обоснованно
 tuple хранит ссылки на элементы,
 список внутри кортежа мы можем изменить потому что кортеж хранит ссылки,
 tuple в Python оптимизированы Python знает сколько места сразу нужно занять в памяти
 используй () для создания пустого tuple

 tuple - ИЗМЕНЯЕМЫЙ(Mutable) на уровне С                                                        <-------
 Как только создали tuple - он ИЗМЕНЯЕМЫЙ а потом он НЕИЗМЕНЯЕМЫЙ

 PyTuple_SetItem используется только для инициализации кортежа, но это не делает его изменяемым.
 Это просто способ заполнить кортеж данными до того, как он станет доступен для использования в Python

 PyObject *tuple = PyTuple_New(3);  // Создаем новый кортеж размером 3
 PyTuple_SetItem(tuple, 0, PyLong_FromLong(1));  // Устанавливаем элемент при инициализации
 PyTuple_SetItem(tuple, 1, PyLong_FromLong(2));
 PyTuple_SetItem(tuple, 2, PyLong_FromLong(3));

 # Хороший пример  Список внутри кортежа мы МОЖЕМ изменить потому что кортеж хранит ссылки
 a_tuple = (1, 2, ['a', 'b'])
 a_tuple[2].append(9999999)
 print(a_tuple)  # -> (1, 2, ['a', 'b', 9999999])

 ### КОРТЕЖ БЛИН Запятая в конце
 addr = "AAA",
 print(type(addr))  # -> <class 'tuple'>

  # Кортеж внутри МЕНЯЕТ ссылку   l2 = list(l1)         Можно попробовать вариант  l2=l1  Будет другой результат)
 l1 = [3, [66, 55, 44], (7, 8, 9)]
 l2 = list(l1)
 #l2 = l1

 l1.append(100)
 l1[1].remove(55)
 # Ссылки внутри ОДИНАКОВЫЕ
 print(id(l1[1]), id(l1[1]), id(l1[2]))  # -> 2413815154112 2413815154112 2415377546240
 print(id(l2[1]), id(l2[1]), id(l2[2]))  # -> 2413815154112 2413815154112 2415377546240

 l2[1] += [33, 22]
 l2[2] += (10, 11)

 print(id(l1[1]), id(l1[1]), id(l1[2]))  # -> 2413815154112 2413815154112 2415377546240

 # ИЗМЕНИЛАСЬ Только ссылка на Кортеж    Был создан НОВЫЙ КОРТЕЖ                                    <-----
 print(id(l2[1]), id(l2[1]), id(l2[2]))  # -> 2413815154112 2413815154112 2413814827904

 # Объекты РАЗНЫЕ
 print(id(l1), id(l2))                   # -> 2415381110464 2413796420288



 -- Модуль ctypes в Python, интеграция с языком C , Использование функции, написанных на языке C в коде Python
 ctypes позволяет создавать указатели вызываемых C-функций из вызываемых объектов Python.
 Иногда их называют функциями обратного вызова.   Мы можем изменять id, НЕИЗМЕНЯЕМЫЕ ОБЪЕКТЫ   <------------

 from ctypes import *

 ctypes — это внешняя библиотека функций для Python. Он предоставляет типы данных, совместимые с C, и позволяет
 вызывать функции в DLL или общих библиотеках. Его можно использовать для обертывания этих библиотек в чистый Python.

 - c_size_t - это беззнаковый тип, который обычно используется для представления размеров объектов в памяти
            (например, длины массивов). Он соответствует типу size_t в C.

 - c_ssize_t - это знаковый тип, который используется для представления размеров, которые могут быть отрицательными
 (например, возвращаемые значения функций, которые могут сигнализировать об ошибках). Он соответствует типу ssize_t в C.


 Таким образом, основное различие заключается в знаковости: c_size_t — беззнаковый, а c_ssize_t — знаковый.

 - c_void_p — это тип в библиотеке ctypes в Python, представляющий собой указатель на некий неуказанный тип данных в C.
 Он используется для работы с адресами в памяти и взаимодействия с C-библиотеками, когда тип данных неизвестен.


 В Python арифметика указателей возможна только через низкоуровневые инструменты.
 Арифметика указателей (pointer arithmetic) — это низкоуровневая операция, при которой указатель (адрес памяти)
 изменяется для доступа к данным по новому адресу. В Python это не поддерживается напрямую, так как
 Python — высокоуровневый язык с автоматическим управлением памятью. Однако, с помощью модулей (например, ctypes),
 можно работать с указателями и выполнять арифметику на уровне C.    <-----

 # Прямое манипулирование памятью в Python
 # Пример   # Изменение указателя через ctypes
 import ctypes
 x = 42
 ptr = ctypes.addressof(ctypes.c_int(x))  # Получаем указатель
 print(ptr)      # -> 2427735535640

 new_ptr = ptr + 4  # Сдвиг указателя на 4 байта
 print(new_ptr)  # -> 2427735535644


  -- Чем tuple отличатется от list --                                 <-----

 # tuple immutable - внутри верхнеуровнего Python       внутри - Python-API
 # tuple mutable   - внутри низкоуровнего Python        внутри - Python-СAPI

 # Из С tuple      - полностью мутабельный объект


 # Изменение tuple через ctypes   # Прямое манипулирование памятью в Python

 # опасное и нестандартное изменение неизменяемого объекта (tuple) с использованием низкоуровневых операций через модуль ctypes

 tup1 = (1, 2)
 tup1_2 = tup1
 tup2 = (3, 4)                                           # Можно изменить на кортеж большего размера tup2 = (3, 4, 5, 6)
 print(tup1, tup1_2, tup2)  # -> (1, 2) (1, 2) (3, 4)
 # id Не изменились
 print(id(tup1), id(tup1_2), id(tup2))  # -> 2930856963712 2930856963712 2930857325248

 # Заставим tup1 измениться
 import ctypes  # Импортируем модуль для низкоуровневой работы с памятью

 # Вычисляем смещение (offset) до данных кортежа
 offset = (
     ctypes.sizeof(ctypes.c_size_t)     # Размер счетчика ссылок (обычно 8 байт на 64-битных системах)
     + ctypes.sizeof(ctypes.c_void_p)   # Размер указателя на тип объекта (обычно 8 байт)
     + ctypes.sizeof(ctypes.c_void_p)   # Размер указателя на длину кортежа (обычно 8 байт)
 )

 # Вычисляем размер данных, которые нужно скопировать
 size = ctypes.sizeof(ctypes.c_void_p) * len(tup2)  # Размер одного элемента кортежа (указателя) * количество элементов

 # Изменяем длину кортежа tup1
 length_offset = ctypes.sizeof(ctypes.c_size_t) + ctypes.sizeof(ctypes.c_void_p)
 ctypes.cast(id(tup1) + length_offset, ctypes.POINTER(ctypes.c_ssize_t))[0] = len(tup2)


 # Копируем данные из tup2 в tup1, начиная с вычисленного смещения
 ctypes.memmove(id(tup1) + offset, id(tup2) + offset, size)  # Низкоуровневое копирование памяти

 # Изменили tuple
 print(tup1, tup1_2, tup2)  # -> (3, 4) (3, 4) (3, 4)
 # id Не изменились
 print(id(tup1), id(tup1_2), id(tup2))  # -> 2930856963712 2930856963712 2930857325248


 -- Почему код с frozenset не работает как с tuple --

 Разная структура: tuple хранит элементы в виде массива указателей, а frozenset использует хэш-таблицу.
 Простое копирование данных через memmove не учитывает эту разницу.

 Длина объекта: В tuple длина хранится в явном виде, и ее можно изменить через ctypes.
 В frozenset длина зависит от количества элементов в хэш-таблице, и ее изменение требует более сложных манипуляций.



 -- Кардинальное(Фундаментальное) различие между list vs tuple --

 # Аннотации tuple
 # .pyi
 empty_tuple: tuple[()]       # Кортеж без элементов
 two_tuple: tuple[int, str]   # Кортеж из двух элементов: int и str             # shape (размерность)
 middleware: tuple[str, ...]  # Кортеж из любого количества строк


 # У list нету shape (размерность)                                                   <-------

 # Аннотации list
 # .pyi
 empty_list: list[()]        # Некорректно! У списков нет фиксированной длины.  # shape (размерность)
 two_list: list[int, str]    # Ошибка! У списков нельзя указывать типы по позициям.
 middleware: list[str, ...]  # Некорректно! У списков НЕТ ellipsis.
 middleware: list[str]       # Список из любого количества строк


 - Аннотации для tuple

 # Пустой кортеж:
 empty_tuple: tuple[()]  # Кортеж без элементов

 # Кортеж с фиксированным количеством элементов (shape):
 two_tuple: tuple[int, str]  # Кортеж из двух элементов: int и str

 # Кортеж с переменным количеством элементов одного типа:
 middleware: tuple[str, ...]  # Кортеж из любого количества строк

 # Здесь ... (ellipsis) означает, что кортеж может содержать любое количество элементов указанного типа.


 - Аннотации для list
 # Пустой список:
 empty_list: list[()]  # Некорректно! У списков нет фиксированной длины.

 # Правильно:
 empty_list: list[Any]  # Пустой список (без указания типа элементов)
 Список с фиксированным количеством элементов:


 two_list: list[int, str]  # Некорректно! У списков нет фиксированной длины.
 # Правильно:
 two_list: list[Union[int, str]]  # Список, содержащий int и str

 # Список с переменным количеством элементов одного типа:
 middleware: list[str, ...]  # Некорректно! У списков нет ellipsis.

 # Правильно:
 middleware: list[str]  # Список из любого количества строк


 # Кардинальное различие между list и tuple

 # tuple:
 # Может иметь фиксированную длину (shape) в аннотациях типов, например:

 tuple[int, str]  # Кортеж из двух элементов: int и str

 # Может иметь переменную длину с указанием типа элементов:
 tuple[str, ...]  # Кортеж из любого количества строк


 # list:
 # Не имеет фиксированной длины (shape) в аннотациях типов.

 # Всегда описывается как список элементов одного типа:
 list[str]  # Список из любого количества строк
 В стандартной типизации list принимает только один параметр - тип элементов, которые содержатся в списке

 # Итог:
 # Кортежи (tuple) могут иметь фиксированную длину (shape) или переменную длину в аннотациях типов.
 # Списки (list) НЕ имеют фиксированной длины (shape) и всегда описываются как список элементов одного типа.



 Tuple против List. Мы на самом деле немного врем людям, говоря, что разница в мутабельности и иммутабельности.
 Что на самом деле является лишь деталью реализации в Python. Ведь есть языки, где оба типа данных иммутабельны:
 Erlang, Haskell, тд. Фундаметальное различие лежит внутри Теории Категории. Из нее мы узнаем,
 что Tuple - тип произведение, а List (по хорошему) - монада (в питоне всего лишь моноид в смысле l + [] и
 недо-функтор в смысле map(f, l)). Вот почему тип List[x] имеет kind * -> *, а Tuple[x, ...] имеет kind * -> ... -> *.

 Монада — это способ структурировать вычисления, чтобы сделать код более предсказуемым и удобным для работы.
 В Python монады не являются встроенной концепцией, но их можно реализовать с помощью классов и методов.


  -- Интересный факт про append --
 Иногда append может быть за O(n)
 Таким образом, хотя `append` обычно выполняется за O(1), в особых ситуациях, связанных с перераспределением памяти,
 она может быть O(n). В среднем же, для обычного использования, её можно считать O(1).

 в Python операция `append` для списков в основном выполняется за O(1), но в редких случаях, когда требуется
 перераспределение памяти (например, при достижении максимальной емкости списка), она может занять O(n).
 Поэтому в худшем случае, когда происходит перераспределение, `append` может быть O(n), но это случается редко и в
 среднем время выполнения составляет O(1).

 А если свободного места не осталось, все данные придется скопировать в новую область памяти!       <-----
 что приведет к временной сложности O(n) для этой операции.                                         <-----


  --- Символьный ад ---
 Уберите детей от экрана.

 {'a': 1}   # это словарь
 dict(a=1)  # это то же самое
 {'a'}      # это множество (set)
 {}         # это не пустое множество, это пустой словарь
 set()      # а вот это пустое множество

 1        # это просто число (int)
 (1)      # это тоже просто число
 (1, 2)   # это кортеж (tuple)
 1,       # это тоже кортеж
 ()       # это пустой кортеж
 (1+2)    # это число
 tuple()  # а это тоже пустой кортеж

 []                     # это пустой список
 [1]                    # это список с одним элементом
 [1, 2, 3]              # это список с 3 элементами
 [i for i in range(3)]  # это тоже список с 3 элементами
 ()                     # это пустой кортеж
 (1)                    # это просто число
 (1, 2, 3)              # это (ха-ха) кортеж
 (i for i in range(3))  # это не кортеж, это генератор :D


 --- Символьный ад  2 ---

 {'a': 1}               # это словарь
 {'a'}                  # это множество (set)
 {}                     # это не пустое множество, это пустой словарь
 set()                  # а вот это пустое множество

 1                      # это просто число (int)
 (1+2)                  # это число
 (1, 2)                 # это кортеж (tuple)
 (1)                    # это тоже просто число
 1,                     # это тоже кортеж
 ()                     # это пустой кортеж
 (,)                    # SyntaxError

 []                     # это пустой список
 [1]                    # это список с одним элементом
 [1, 2, 3]              # это список с 3 элементами
 [i for i in range(3)]  # это тоже список с 3 элементами
 ()                     # это пустой кортеж
 (1)                    # это просто число
 (1, 2, 3)              # это кортеж
 (i for i in range(3))  # это не кортеж, это generator expression


 Модель Random Access Machine (RAM) - это абстрактная модель вычислений, которая используется для анализа алгоритмов.
 Основная цель RAM — упростить анализ сложности алгоритмов, предполагая мгновенный доступ к данным.


 ПРОСТРАНСТВЕННАЯ (емкостаная) СЛОЖНОСТЬ - относиться к объему используемой ПАМЯТИ, необходимой для выполнения алгоритма


 «O-большое» определяет время выполнения в худшем случае

 --- Big O или Big Oh    Временная сложность ---
 Простыми словами Big O показывает как будет меняться производительность алгоритма в зависимости от роста входящих данных.
 Описания алгоритмической сложности. Она показывает, как сильно увеличится количество операций при увеличении размера данных.
 Примеры нотаций Big O:

 O(1):  Константная сложность. - Время выполнения алгоритма остается постоянным и не зависит от объема данных.
 O(log n): Логарифмическая сложность.  - Время выполнения алгоритма растет медленно с увеличением размера входных данных.
 Например Бинарный поиск в отсортированном массиве.   Везде где есть ДЕЛЕНИЕ будет   O(log n)   Бинарное дерево   <-----
 O(n):  Линейная сложность. - Время выполнения алгоритма растет линейно с увеличением размера входных данных.  Простой Поиск
 O(n log n): Линейно-логарифмическая сложность. - Время выполнения алгоритма растет быстрее, чем линейно, но медленнее,
 чем квадратично. Например, сортировка слиянием (merge sort).
 O(n^2): Квадратичная сложность. - Время выполнения алгоритма зависит от квадрата размера входных данных.
 Например, сортировка пузырьком (bubble sort).
 O(n^3): Кубическая сложность. Время выполнения алгоритма зависит от размера входных данных в кубе. Например, алгоритмы,
 которые имеют три вложенных цикла, такие как некоторые методы многомерной обработки данных.
 O(n!): Факториальная сложность. Это самая высокая степень роста времени выполнения алгоритма. Время выполнения
 алгоритма растет факториально от размера входных данных. Этот тип сложности встречается, например, при переборе всех
 возможных комбинаций элементов, что делает его чрезвычайно неэффективным для больших значений n.

 Скорость Алгоритмов:
 0(1) -> O(log n) -> O(n) -> O(n log n) -> O(n^2) -> O(n^3) -> O(n!)


 O(u log u): Линейно-логарифмическая сложность по числу уникальных элементов u.                                 <-----
 Используется, когда после удаления дублей (например, через set) сортируется НЕ весь массив размера n, а только u уникальных значений (u ≤ n).

 # O(n log n) — оценка через размер входа: считаем, будто сортируем/обрабатываем n элементов.       # <--------
 # O(u log u) — точнее здесь: сортируем только u уникальных элементов после set(nums) (u ≤ n).      # <--------


 Худший случай (worst case)    - Когда входные данные требуют максимальных затрат времени и памяти.
 Лучший случай (best case)     - Полная противоположность worst case, самые удачные входные данные.
 Средний случай (average case) - Самый хитрый из тройки. Между best case и worst case, расчёт average case - Дело сложное

 Amortized Амортизированный анализ — это метод анализа сложности данного алгоритма или того , сколько ресурсов, особенно
 времени или памяти, требуется для его выполнения . Усредняет время выполнения операций в последовательности


 Big O или Big Oh - Верхняя оценка сложности алгоритма      Асимптотика    Асимптотический анализ
 Нотация большого O обозначает верхнюю границу времени выполнения алгоритма. Таким образом,
 она указывает на сложность алгоритма в худшем случае.

 Ни одно из следующих значений НЕ является правильной записью «O-большое»:
 O(n + 26), O(n – 26), O(n * 26), O(n / 26). Все они эквивалентны O(n)                  <-----

 --- Как определить Вычислительную сложность АЛГОРИТМА? ---

 O(1):  Константная сложность.
 Пример как работает   НЕ важно сколько команд входит в эту операцию:

 # Не важно сколько операций ГЛАВНОЕ ЧТОБЫ ВСЕ ОНИ БЫЛИ ЗА O(1)
 var_a = 10
 inf = float('inf')
 print(inf)

 # Все отбрасывается  Будет только 0(1)
 0(10) = 0(1)
 0(C) = 0(1)
 0(1 + 1 + 1) = 0(3) = 0(1)



 O(n):  Линейная сложность.  n == переменная

 # Число итераций зависит от Размерности lst    ЧЕМ  БОЛЬШЕ N тем больше итераций нужно сделать в цикле
 lst = [1, 4, 10, -5, 0, 2, 3, 18, 32]

 # Все команды будут 0(n)
 for i in lst:
     print(i, end=' ')
     i += 1

 # Все отбрасывается  Будет только 0(n)
 0(2n) = 0(n)
 0(1 + n) = 0(n)
 0(A * n) = 0(n)
 0(n + C) = 0(n)
 0(A * n + C) = 0(n)

 # Более сложный пример КАК РАСЧИТАТЬ СЛОЖНОСТЬ АЛГОРИТМА  Вычислительная сложность АЛГОРИТМА       <-----     <-----

 # 0(1)  Выполнение команды
 lst = [1, 4, 10, -5, 0, 2, 3, 18, 32]   # 0(1)  Выполнение команды

 # 0(n)  Выполнение Цикла
 for i, v in enumerate(lst):             # 0(n)  Выполнение Цикла
     lst[i] += 1

 # 0(n)  Выполнение Цикла
 for i in lst:                           # 0(n)  Выполнение Цикла
     print(i, end=' ')

 # Важно!!!  <-----                                                                                            <-----
 # Выходит следующее выражение   0(1) + 0(n) + 0(n) = 0(1 + n + n) = 0(1 + 2n)   Все отбрасывается  Будет только 0(n)


 # Правила сложения и умножения   ОТБРАСЫВАТЬ МОЖНО ТОЛЬКО КОНСТАНТЫ НО НЕ ПЕРЕМЕННЫЕ                          <-----

 # Для последовательных циклов сложение(+)   0(n + m)
 # 0(n)  Выполнение Цикла
 for i in range(n):             # 0(n)  Выполнение Цикла    переменная  n
     print(i)

 # 0(n)  Выполнение Цикла
 for i in range(m):             # 0(n)  Выполнение Цикла    переменная  m
     print(i)

 # Важно!!!  <-----         ОТБРАСЫВАТЬ МОЖНО ТОЛЬКО КОНСТАНТЫ НО НЕ ПЕРЕМЕННЫЕ                                <-----
 # Выходит следующее выражение  0(n) + 0(m) = 0(n + m)   СОКРАТИТЬ НИЧЕГО НЕЛЬЗЯ ПОТОМУ ЧТО ОБЕ ВЕЛИЧИНЫ == ПЕРЕМЕННЫЕ!!


 # Для вложенных циклов умножение(*)   0(n * m)
 for i in range(n):                             # 0(n * m) Потому что делаем перебор
     for j in range(m):
         print(i, y)

 # Если n = m:   0(n + n) = 0(2n) = 0(n)      При ВЛОЖЕННЫХ ЦИКЛАХ будет 0(n^2)   0(n * n) = 0(n^2)   Важно!!! <-----



 НЕ ВАЖНАЯ СЛОЖНОСТЬ
 # Для вложенных циклов умножение(*)   0(n * n) = 0(n^2)
 for i in range(n):                             # 0(n * n) = 0(n^2)
     for j in range(n):
         print(i, y)

 # Добавили еше 1 цикл
 # 0(n^2 + n) = 0(n^2)
 for t in range(n):                             # 0(n^2 + n) = 0(n^2)
     print(t)

 # Объяснение n^2  ЗНАЧИТЕЛЬНО БОЛЬШЕ ЧЕМ   n - Можно отбросить  будет 0(n^2 + n) = 0(n^2)
 # Если одно слагаемое принимает значение БОЛЕЕ ЧЕМ В 2 (ДВА) РАЗА БОЛЬШЕ чем другое значит МОЖНО ОТБРОСИТЬ
 # Примеры: 0(n + log n) = 0(n)         0(3n + 10n^2 + 2^n) = 0(n +10n^2 + 2^n) = 0(2^n)    Важно!!!   <-----
 # Если ничего не знаем об A      0(2^n + A) = 0(2^n + A)   Ничего не отбрасываем



 O(log n): Логарифмическая сложность.   Как пример бинарный поиск

 bin_lst = [-1, -3, 2, 4, 5, 7, 8, 9]  # Будем искать 9
 # БИНАРНЫЙ поиск  O(log n) - кратко как работает берем серединный элемент например  4
 # сравниваем 9 > 4  и будем искать  ТОЛЬКО УЖЕ ТУТ   [4, 5, 7, 8, 9]
 # берем серединный элемент например  7
 # сравниваем 9 > 7  и будем искать  ТОЛЬКО УЖЕ ТУТ   [8, 9]


 Для МАССИВОВ: чтение O(1), вставка O(n).                                               удаление O(n)
 Для СПИСКОВ:  чтение O(1), вставка в конец O(1), вставка в середину или начало O(n).   удаление O(1)

 # Пример с массивом (списком в Python)
 arr = [1, 2, 3, 4, 5]

 # Чтение                        # Вставка
 print(arr[2])  # O(1)           arr.insert(2, 10)  # O(n)

 # Удаление                      # Удаление из конца
 arr.pop(2)  # O(n)              arr.pop()   # O(1)
 print(arr)  # [1, 2, 3, 4, 5]   print(arr)  # [1, 2, 3, 4]


 # popleft() - это более скоростная версия pор(0) в обычном списке.В остальном эти реализации действительно очень похожи.
 # Почему popleft() быстрее?

 # - В списке pop(0) требует сдвига всех элементов влево → O(n).

 # - В deque popleft() просто меняет указатель на голову → O(1).

 # Чем больше список, тем заметнее разница в скорости! 🚀


 --- Big O различных операций в CPython ---
 n — это количество элементов, находящихся в контейнере в данный момент.
 k — это либо значение параметра, либо количество элементов в параметре.


 Список (list):
 Внутри список представлен как массив; самые большие затраты возникают из-за превышения текущего размера выделения
 (поскольку все должно перемещаться) или из-за вставки или удаления где-то в начале (поскольку все, что после этого,
 должно перемещаться). Если вам нужно добавить/удалить на обоих концах, рассмотрите возможность использования вместо
 этого Collections.deque.

 Operation  Average Case  Amortized Worst Case
 Copy       O(n)          O(n)
 Append     O(1)          O(1)
 Pop last   O(1)          O(1)
 Pop(i)     O(n)          O(n)
 Insert     O(n)          O(n)
 Get Item   O(1)          O(1)
 Set Item   O(1)          O(1)
 Del Item   O(n)          O(n)
 Iteration  O(n)          O(n)
 Get Slice  O(k)          O(k)
 Del Slice  O(n)          O(n)
 Set Slice  O(k+n)        O(k+n)
 Extend     O(k)          O(k)
 Sort       O(n log n)    O(n log n)
 Multiply * O(nk)         O(nk)
 x in s     O(n)
 min(s)     O(n)
 max(s)     O(n)
 Get Length O(1)          O(1)


 ### ChatGPT
 Для list в Python ключевое:
 append / extend / pop() с конца — амортизированно O(1), но в худшем случае O(n) из-за перераспределения (resize и копирование).
 sort — O(n log n) в среднем и худшем, лучший случай O(n) (Timsort на почти отсортированных).
------------------------------------------------------------------------------------------------------------------------

 collections.deque
 collections.deque (double-ended queue) (двусторонняя очередь) внутренне представляется как двусвязный список.
 (Ну, для большей эффективности это список массивов, а не объектов.) Оба конца доступны, но даже просмотр середины
 происходит медленно, а добавление или удаление из середины происходит еще медленнее.

 Operation  Average Case  Amortized Worst Case
 Copy       O(n)          O(n)
 append     O(1)          O(1)
 appendleft O(1)          O(1)
 pop        O(1)          O(1)
 popleft    O(1)          O(1)
 extend     O(k)          O(k)
 extendleft O(k)          O(k)
 rotate     O(k)          O(k)
 remove     O(n)          O(n)
 Get Length O(1)          O(1)
------------------------------------------------------------------------------------------------------------------------

 Множество (set)
 реализация намеренно очень похожа на Словарь (dict)
 ПОИСК/ВСТАВКА/УДАЛЕНИЕ в среднем за O(1) в худшем O(n)  set реализован как хеш-таблица

 set — хеш-таблица; in/add/remove обычно O(1) амортизированно, в худшем O(n) при коллизиях.

 Operation                         Average Case            Amortized Worst Case
 x in s                            O(1)                    O(n)
 Union s|t                         O(len(s)+len(t))
 Intersection s&t                  O(min(len(s), len(t)))  O(len(s) * len(t))     замените «min» на «max», если t не является set
 Multiple intersection s1&s2&..&sn                         (n-1)*O(l) where l is max(len(s1),..,len(sn))
 Difference s-t                    O(len(s))
 s.difference_update(t)            O(len(t))
 Symmetric Difference s^t          O(len(s))               O(len(s) * len(t))
 s.symmetric_difference_update(t)  O(len(t))               O(len(t) * len(s))
------------------------------------------------------------------------------------------------------------------------

 Словарь (dict)
 ПОИСК/ВСТАВКА/УДАЛЕНИЕ в среднем за O(1) в худшем O(n)  dict реализован как хеш-таблица

 Operation  Average Case  Amortized Worst Case
 k in d     O(1)          O(n)
 Copy       O(n)          O(n)
 Get Item   O(1)          O(n)
 Set Item   O(1)          O(n)
 Del Item   O(1)          O(n)
 Iteration  O(n)          O(n)


 --- Интересный факт ---                                                                        <------
 Обратите внимание, что существует быстрый путь для dict, которые (на практике) работают только с ключами str; это НЕ
 влияет на алгоритмическую сложность, но может существенно влиять на постоянные факторы: как быстро завершается типичная программа.

 -- Строковые (str) ключи в словаре (dict) - ПРЕДПОЧТИТЕЛЬНЕЕ    работают быстрее               <------
 # Таким образом, использование строковых ключей происходит примерно на 30% +/- быстрее даже по сравнению с int ключами
 print(timeit.timeit('a["500"]', 'a ={}\nfor i in range(1000): a[str(i)] = i')) # Поиск str key быстрее
 # 0.04318580008111894
 print(timeit.timeit('a[500]', 'a ={}\nfor i in range(1000): a[i] = i'))        # Поиск str key быстрее чем другие
 # 0.068284000037238

 -- Но специально преобразовывать их будет Дороже                                               <------
 Преобразование вашего ключа в строку будет стоить (намного) дороже, чем небольшой прирост, который вы могли бы получить,
 если бы он был строкой для dict.
 # Как вы можете видеть, несмотря на то, что строковый dict работает быстрее, преобразование ключа по сравнению с ним
 очень затратно, что полностью снижает выигрыш (и даже больше).

 print(timeit.timeit("a[500]", "a={key: 1 for key in range(1000)}" ))
 # 0.06836240005213767
 print(timeit.timeit("a[\"500\"]", "a={str(key): 1 for key in range(1000)}" ))
 # 0.04284069989807904
 print(timeit.timeit("a[str(500)]", "a={str(key): 1 for key in range(1000)}" ))
 # 0.16474479995667934

 -- Вывод --                                                                                    <------
 Так что да, если данные, которые вы используете, используются только в качестве ключей к словарю, и не имеет значения,
 в каком формате вы их храните, тогда предпочтительнее использовать строки в небольшом словаре. <------


 ---  ЗАМЕТКИ LEETCODE ---

 ## O(1) по памяти значит, что алгоритм использует постоянное количество дополнительной памяти, которое не растёт с размером входного массива.
 ## O(log10(n)) и O(log(n)) — одно и то же в Big-O, потому что логарифмы с разными основаниями отличаются только на константу:
 ## Константы в Big-O не важны.

 ## Bruteforce — это “решение в лоб”: перебрать все варианты и проверить, какой подходит (без хитрых оптимизаций).

 ## DFS = Depth-First Search   — поиск в глубину (идём “вниз” по ветке как можно глубже, потом назад).
 ## BFS = Breadth-First Search — поиск в ширину (идём по уровням: сначала корень, потом все дети, потом внуки и т.д.).

 ## В Two Pointers обычно два указателя двигаются одновременно (left/right) по структуре, чтобы что-то “сжимать/сравнивать/сводить”.


 ## Коротко:
 # in-place в массиве + без новых структур → O(1)
 # если создаёшь новый список/строку → O(n)

 # Главное правило

 # Входные данные (массив/строка), которые тебе дали, не считаются “дополнительной памятью”.
 # Считается только то, что ты создаёшь сверх входа.

 # Если вход — массив (list) и ты меняешь его “in-place”   Доп. память обычно O(1)
 # потому что ты используешь пару переменных.

 # Пример: removeDuplicates, removeElement, merge sorted array (3 указателя).

 # Но есть нюанс: результат
 # Если по условию нужно вернуть новый список/строку, то: сам результат занимает O(n) памяти
 # его нельзя сделать O(1), потому что результат надо где-то хранить


 ## Фиксированная структура → O(1) по памяти (в общем виде)
 Если ты создаёшь структуру данных, размер которой НЕ зависит от n (например, фиксированный набор/алфавит),
 то доп. память считается O(1).

 Пример:
 allowed = set("aeiou")   # размер всегда 5 (константа)
 count = 0                # несколько переменных
 → extra space O(1), потому что размер set и число переменных не растут с длиной входа n.


 ## Про память в задачах с сортировкой
 - После сортировки (in-place) алгоритм часто использует только несколько переменных → extra space O(1).
 - list.sort() сортирует in-place (не создаём второй массив), а sorted() создаёт новый список → O(n) памяти.

 Нюанс Python:
 - list.sort() использует служебную память для сортировки.
 - В LeetCode/интервью часто пишут: O(1) доп. (не считая памяти сортировки).
 - Если учитывать строго реализацию Python: память сортировки может быть до O(n) в худшем случае.

 Пример:
 points.sort(key=lambda x: x[1])
 arrows, arrow_x = 1, points[0][1]   # далее только переменные → O(1) extra



 --- Затраты памяти и времени на выполнение append и списковое включение (list comprehensions) memory_profiler ---

 #  В общем случае, списковое включение (list comprehension) обычно потребляет меньше памяти,
 #  чем использование метода append, потому что оно создает список в одном непрерывном блоке памяти, тогда как метод
 #  append может потребовать больше перераспределений памяти, если список растет.

 from memory_profiler import memory_usage, profile

 # Функция с использованием append
 def create_list_append():
     l = []
     for i in range(100_000):
         l.append(i * 2)
     return l

 # Функция с использованием спискового включения
 def create_list_comprehension():
     return [i * 2 for i in range(100_000)]

 # Замер памяти
 def memory_usage_test():
     # Замер памяти для append
     append_mem_usage = memory_usage((create_list_append, ))
     print(f"Peak memory usage for append: {max(append_mem_usage) - min(append_mem_usage)} MiB")

     # Замер памяти для спискового включения
     comprehension_mem_usage = memory_usage((create_list_comprehension, ))
     print(f"Peak memory usage for comprehension: {max(comprehension_mem_usage) - min(comprehension_mem_usage)} MiB")

 @profile
 def profile_memory():
     create_list_append()
     create_list_comprehension()

 # Запуск теста
 if __name__ == '__main__':
     memory_usage_test()
     profile_memory()

 # Выводы Всегда разные проверь сам!!!                                                       <-----   <-----
 # Peak memory usage for append:        1.59375 MiB
 # Peak memory usage for comprehension: 1.37109375 MiB


 # Line #    Mem usage    Increment  Occurrences   Line Contents
 # =============================================================
 #   1076     22.1 MiB     22.1 MiB           1   @profile
 #   1077                                         def profile_memory():
 #   1078     22.4 MiB      0.3 MiB           1       create_list_append()
 #   1079     22.4 MiB      0.0 MiB           1       create_list_comprehension()



 # Легкий пример как использовать profile                                                      <-----   <-----
 from memory_profiler import profile

 @profile
 def my_function():
     a = [i for i in range(100000)]
     b = [i * 2 for i in a]
     return b

 if __name__ == "__main__":
     my_function()

 # Line #    Mem usage    Increment  Occurrences   Line Contents
 # =============================================================
 #   1052     20.9 MiB     20.9 MiB           1   @profile
 #   1053                                         def my_function():
 #   1054     22.9 MiB      2.0 MiB      100001       a = [i for i in range(100000)]
 #   1055     25.1 MiB  -1819.1 MiB      100001       b = [i * 2 for i in a]
 #   1056     25.1 MiB      0.0 MiB           1       return b



 # Более того даже если мы создадим список без добавления (append), он всеравно будет занимать больше памяти чем кортеж
 # Потому что списки хранят дополнитулью информацию о своем текущем состоянии, чтобы эффективно изменять размер <------
 # Дополнительная информация занимает мало места (порядка одного дополнительного элемента) однако при использовании
 # миллиона списков мы начинаем чувствовать разницу!                                                            <------

 import timeit
 import sys

 # Функции для создания списка и кортежа
 def create_list():
     return [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

 def create_tuple():
     return (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)

 # Замер времени
 list_time = timeit.timeit(create_list, number=1000000)
 tuple_time = timeit.timeit(create_tuple, number=1000000)

 # Размеры
 list_size = sys.getsizeof(create_list())
 tuple_size = sys.getsizeof(create_tuple())

 # Вывод результатов
 print(f'Время создания списка: {list_time:.3f} секунд')    # ->  Время создания списка:  0.147 секунд
 print(f'Время создания кортежа: {tuple_time:.3f} секунд')  # ->  Время создания кортежа: 0.078 секунд
 print(f'Размер списка: {list_size} байт')                  # ->  Размер списка:          68 байт
 print(f'Размер кортежа: {tuple_size} байт')                # ->  Размер кортежа:         60 байт



 -- Вычисляем, сколько оперативной памяти используем! --

 memit берет данные об использовании ОЗУ от оперативной системы, а asizeof запрашивает у объектов их размер(который может
 быть сообщен неверно). Обычно asizeof работает медленнее, чем memit, но asizeof полезна при анализе небольших объектов
 Функция memit, вероятно, более подходит для реальных приложений, поскольку измеряет потребление памяти точнее (не на
 основе предположений)


 # memory_usage позволяет более точно оценить потребление памяти в реальном времени, а asizeof может быть полезен для
 # анализа небольших объектов, когда вам нужно измерить их размер более детально.

 from pympler.asizeof import asizeof                                                 <-----   <----- Интересный импорт

 # Создание списка
 my_list = [i for i in range(1000000)]

 # Измерение размера объекта
 size = asizeof(my_list)
 print(f"Size of my_list: {size} bytes")                # -> Size of my_list: 20224368 bytes



 from memory_profiler import memory_usage

 def my_function():
     return [i for i in range(1000000)]

 if __name__ == '__main__':
     # Измерение использования памяти
     mem_usage = memory_usage(my_function)
     print(f"Peak Memory Usage: {max(mem_usage)} MiB")  # -> Peak Memory Usage: 79.70703125 MiB




 --- Замеры размеров Python ---  РАЗМЕР СТАНОВИТСЯ МЕНЬШЕ С КАЖДОЙ НОВОЙ ВЕРСИЕЙ!

                            -- Примеры Списков deque vs list --                                 <-----
 my_list = [1, 2, 3, 4, 5]
 print(f'getsizeof list:  {sys.getsizeof(my_list)} байт')     # -> getsizeof list:  104 байт
 print(f'asizeof   list:  {asizeof.asizeof(my_list)} байт')   # -> asizeof   list:  264 байт

 from collections import deque
 my_deque = deque([1, 2, 3, 4, 5])
 print(f'getsizeof deque: {sys.getsizeof(my_deque)} байт')    # -> getsizeof deque: 760 байт
 print(f'asizeof   deque: {asizeof.asizeof(my_deque)} байт')  # -> asizeof   deque: 760 байт



                            -- Примеры Кортежей namedtuple vs tuple --                          <-----
 my_tuple = (1, 2, 3, 4, 5)
 print(f'getsizeof tuple:       {sys.getsizeof(my_tuple)} байт')    # -> getsizeof tuple:       80 байт
 print(f'asizeof   tuple:       {asizeof.asizeof(my_tuple)} байт')  # -> asizeof   tuple:       240 байт

 from collections import namedtuple
 nt_tuple = namedtuple('nt_tuple', ['a', 'b', 'c', 'd', 'e'])
 p = nt_tuple(1, 2, c=3, d=4, e=5)
 print(f'getsizeof namedtuple:  {sys.getsizeof(p)} байт')           # -> getsizeof namedtuple:  80 байт
 print(f'asizeof   namedtuple:  {asizeof.asizeof(p)} байт')         # -> asizeof   namedtuple:  240 байт

 Создание объекта namedtuple накладывает некоторые накладные расходы (например, ХРАНЕНИЕ ИМЕН ПОЛЕЙ), но для
 НЕБОЛЬШИХ ОБЪЕКТОВ эти накладные расходы могут быть минимальными.
 Однако в большинстве случаев namedtuple будет занимать больше места в памяти по сравнению с обычным кортежем


                            -- Примеры Словарей OrderedDict vs dict --                          <-----
 my_dict = {1: 'a', 2: 'b', 3: 'c'}
 print(f'getsizeof dict:         {sys.getsizeof(my_dict)} байт')     # -> getsizeof dict:       224 байт
 print(f'asizeof   dict:         {asizeof.asizeof(my_dict)} байт')   # -> asizeof   dict:       488 байт


 from collections import OrderedDict
 or_dict = OrderedDict({1: 'a', 2: 'b', 3: 'c'})

 print(f'getsizeof OrderedDict:  {sys.getsizeof(or_dict)} байт')    # -> getsizeof OrderedDict: 448 байт
 print(f'asizeof   OrderedDict:  {asizeof.asizeof(or_dict)} байт')  # -> asizeof   OrderedDict: 712 байт



                            -- Примеры Множества frozenset vs set --                            <-----
 my_set = {1, 2, 3, 4, 5}
 print(f'getsizeof set:        {sys.getsizeof(my_set)} байт')     # -> getsizeof set:       472 байт
 print(f'asizeof   set:        {asizeof.asizeof(my_set)} байт')   # -> asizeof   set:       632 байт


 fz_set = frozenset({1, 2, 3, 4, 5})

 print(f'getsizeof frozenset:  {sys.getsizeof(fz_set)} байт')    # -> getsizeof frozenset:  472 байт
 print(f'asizeof   frozenset:  {asizeof.asizeof(fz_set)} байт')  # -> asizeof   frozenset:  632 байт

 # Если list внутри
 fz_set = frozenset([1, 2, 3, 4, 5])

 print(f'getsizeof frozenset:  {sys.getsizeof(fz_set)} байт')    # -> getsizeof frozenset:  728 байт
 print(f'asizeof   frozenset:  {asizeof.asizeof(fz_set)} байт')  # -> asizeof   frozenset:  888 байт

 # Если tuple внутри
 fz_set = frozenset((1, 2, 3, 4, 5))

 print(f'getsizeof frozenset:  {sys.getsizeof(fz_set)} байт')    # -> getsizeof frozenset:  728 байт
 print(f'asizeof   frozenset:  {asizeof.asizeof(fz_set)} байт')  # -> asizeof   frozenset:  888 байт



                            -- Примеры Строки/Числа str vs int --                              <-----
 my_string = "Hello, World!"
 print(f'getsizeof str:  {sys.getsizeof(my_string)} байт')    # -> getsizeof str:  62 байт
 print(f'asizeof   str:  {asizeof.asizeof(my_string)} байт')  # -> asizeof   str:  64 байт


 my_int = 10000

 print(f'getsizeof int:  {sys.getsizeof(my_int)} байт')       # -> getsizeof int:  28 байт
 print(f'asizeof   int:  {asizeof.asizeof(my_int)} байт')     # -> asizeof   int:  32 байт


 # Чем больше строка тем больше размер так же и с другими обьектами

 my_string = "a"

 print(f'getsizeof str:  {sys.getsizeof(my_string)} байт')    # -> getsizeof str:  50 байт
 print(f'asizeof   str:  {asizeof.asizeof(my_string)} байт')  # -> asizeof   str:  56 байт


 my_int = 1

 print(f'getsizeof int:  {sys.getsizeof(my_int)} байт')       # -> getsizeof int:  28 байт
 print(f'asizeof   int:  {asizeof.asizeof(my_int)} байт')     # -> asizeof   int:  32 байт



                            -- Сравнение slots vs no_slots --                                  <-----
                            -- @dataclass(slots=True)  vs  @dataclass() --
 from dataclasses import dataclass

 @dataclass(slots=True)
 class WithSlots:
     value: int

 with_slots = WithSlots(10)
 print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  40 байт
 print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  72 байт


 @dataclass
 class NoSlots:
     value: int

 no_slots = NoSlots(10)
 print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    56 байт
 print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    440 байт



  --- Замеры ПУСТЫХ обьектов встроенных Python ---

                             -- Примеры list vs [] vs deque() vs heapq --                       <-----

 my_list = list()
 print(f'getsizeof list():      {sys.getsizeof(my_list)} байт')    # -> getsizeof list():       56 байт
 print(f'asizeof   list():      {asizeof.asizeof(my_list)} байт')  # -> asizeof   list():       56 байт

 my_list = []
 print(f'getsizeof []:          {sys.getsizeof(my_list)} байт')    # -> getsizeof []:           56 байт
 print(f'asizeof   []:          {asizeof.asizeof(my_list)} байт')  # -> asizeof   []:           56 байт


 from collections import deque

 my_deque = deque()
 print(f'getsizeof deque():     {sys.getsizeof(my_deque)} байт')    # -> getsizeof deque():     760 байт
 print(f'asizeof   deque():     {asizeof.asizeof(my_deque)} байт')  # -> asizeof   deque():     760 байт


 import heapq

 my_heapq = []
 heapq.heapify(my_heapq)
 print(f'getsizeof heapq:       {sys.getsizeof(my_heapq)} байт')    # -> getsizeof heapq:       56 байт
 print(f'asizeof   heapq:       {asizeof.asizeof(my_heapq)} байт')  # -> asizeof   heapq:       56 байт





                             -- Примеры set() vs frozenset() --                                 <-----

 my_set = set()
 print(f'getsizeof set():       {sys.getsizeof(my_set)} байт')    # -> getsizeof set():         216 байт
 print(f'asizeof   set():       {asizeof.asizeof(my_set)} байт')  # -> asizeof   set():         216 байт

 my_set = frozenset()
 print(f'getsizeof frozenset(): {sys.getsizeof(my_set)} байт')    # -> getsizeof frozenset():   216 байт
 print(f'asizeof   frozenset(): {asizeof.asizeof(my_set)} байт')  # -> asizeof   frozenset():   216 байт





                            -- Примеры tuple() vs namedtuple() vs () --                         <-----
 my_tuple = tuple()
 print(f'getsizeof tuple():    {sys.getsizeof(my_tuple)} байт')    # -> getsizeof tuple():      40 байт
 print(f'asizeof   tuple():    {asizeof.asizeof(my_tuple)} байт')  # -> asizeof   tuple():      40 байт
 print(f'asizeof   ():         {asizeof.asizeof(())} байт')        # -> asizeof   ():           40 байт


 from collections import namedtuple

 my_tuple = namedtuple('C', '')
 nt_tuple = my_tuple()
 print(f'getsizeof namedtuple: {sys.getsizeof(nt_tuple)} байт')    # -> getsizeof namedtuple:    40 байт
 print(f'asizeof   namedtuple: {asizeof.asizeof(nt_tuple)} байт')  # -> asizeof   namedtuple:    40 байт





                             -- Примеры dict() vs {} vs OrderedDict() vs defaultdict() vs ChainMap() --        <-----
 my_dict = dict()
 print(f'getsizeof dict():       {sys.getsizeof(my_dict)} байт')    # -> getsizeof dict():       64 байт
 print(f'asizeof   dict():       {asizeof.asizeof(my_dict)} байт')  # -> asizeof   dict():       64 байт


 my_dict = {}
 print(f'getsizeof {{}}:         {sys.getsizeof(my_dict)} байт')    # -> getsizeof {}:           64 байт
 print(f'asizeof   {{}}:         {asizeof.asizeof(my_dict)} байт')  # -> asizeof   {}:           64 байт


 from collections import OrderedDict

 my_OrDt = OrderedDict()
 print(f'getsizeof OrderedDict:  {sys.getsizeof(my_OrDt)} байт')    # -> getsizeof OrderedDict:  128 байт
 print(f'asizeof   OrderedDict:  {asizeof.asizeof(my_OrDt)} байт')  # -> asizeof   OrderedDict:  128 байт


 from collections import defaultdict

 my_defa = defaultdict(int)     # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict(str)     # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict(list)    # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict(set)     # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict(dict)    # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict()        # Все будут весить ОДИНАКОВО!!!
 print(f'getsizeof defaultdict():  {sys.getsizeof(my_defa)} байт')    # -> getsizeof defaultdict():  72 байт
 print(f'asizeof   defaultdict():  {asizeof.asizeof(my_defa)} байт')  # -> asizeof   defaultdict():  72 байт


 from collections import ChainMap

 my_chain = ChainMap()
 print(f'getsizeof ChainMap():  {sys.getsizeof(my_chain)} байт')    # -> ggetsizeof ChainMap():  48 байт
 print(f'asizeof   ChainMap():  {asizeof.asizeof(my_chain)} байт')  # -> aasizeof   ChainMap():  520 байт





                             -- Сравнение slots vs no_slots --                                  <-----
                             -- @dataclass(slots=True)  vs  @dataclass() --

 from dataclasses import dataclass

 @dataclass(slots=True)
 class WithSlots:pass

 with_slots = WithSlots()
 print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  32 байт
 print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  32 байт


 @dataclass
 class NoSlots:pass

 no_slots = NoSlots()
 print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    56 байт
 print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    352 байт





                             -- Обычные классы По размеру тоже самое что @dataclass(slots=True)  vs  @dataclass() --
                             -- Сравнение slots vs no_slots --

 class WithSlots:__slots__ = ()

 with_slots = WithSlots()
 print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  32 байт
 print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  32 байт


 class NoSlots:pass

 no_slots = NoSlots()
 print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    56 байт
 print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    352 байт





                            -- Примеры int() float() complex() True False str() range(0) bytes() bytearray() vs None --
                            -- И пустые объекты Тоже самое 0  ''  0.0  0j  b""  bytearray(b"")  object()  --     <-----

 my_int = int()
 print(f'getsizeof int():  {sys.getsizeof(my_int)} байт')            # -> getsizeof int():      28 байт
 print(f'asizeof   int():  {asizeof.asizeof(my_int)} байт')          # -> asizeof   int():      32 байт
 print(f'asizeof   0:      {asizeof.asizeof(0)} байт')               # -> asizeof   0:          32 байт


 my_float = float()
 print(f'getsizeof float():  {sys.getsizeof(my_float)} байт')        # -> getsizeof float():    24 байт
 print(f'asizeof   float():  {asizeof.asizeof(my_float)} байт')      # -> asizeof   float():    24 байт
 print(f'asizeof   0.0:      {asizeof.asizeof(my_float)} байт')      # -> asizeof   0.0:        24 байт


 my_comp = complex()
 print(f'getsizeof complex():  {sys.getsizeof(my_comp)} байт')       # -> getsizeof complex():  32 байт
 print(f'asizeof   complex():  {asizeof.asizeof(my_comp)} байт')     # -> asizeof   complex():  32 байт
 print(f'asizeof   0j:         {asizeof.asizeof(my_comp)} байт')     # -> asizeof   0j:         32 байт


 # True
 print(f'getsizeof True:  {sys.getsizeof(True)} байт')               # -> getsizeof True:       28 байт
 print(f'asizeof   True:  {asizeof.asizeof(True)} байт')             # -> asizeof   True:       32 байт


 # False
 print(f'getsizeof False:  {sys.getsizeof(False)} байт')             # -> getsizeof False:      28 байт
 print(f'asizeof   False:  {asizeof.asizeof(False)} байт')           # -> asizeof   False:      32 байт


 # None  занимает фиксированное количество памяти!!!        Один из самых маленьких объектов по памяти!!!      <-----
 print(f'getsizeof None:  {sys.getsizeof(None)} байт')               # -> getsizeof None:       16 байт
 print(f'asizeof   None:  {asizeof.asizeof(None)} байт')             # -> asizeof   None:       16 байт


 my_str = str()
 print(f'getsizeof str():  {sys.getsizeof(my_str)} байт')            # -> getsizeof str():      49 байт
 print(f'asizeof   str():  {asizeof.asizeof(my_str)} байт')          # -> asizeof   str():      56 байт
 print(f'asizeof   "":     {asizeof.asizeof("")} байт')              # -> asizeof   "":         56 байт


 my_range = range(0)
 print(f'getsizeof range(0):  {sys.getsizeof(my_range)} байт')       # -> getsizeof range(0):   48 байт
 print(f'asizeof   range(0):  {asizeof.asizeof(my_range)} байт')     # -> asizeof   range(0):   48 байт


 my_bytes = bytes()
 print(f'getsizeof bytes():  {sys.getsizeof(my_bytes)} байт')        # -> getsizeof bytes():    33 байт
 print(f'asizeof   bytes():  {asizeof.asizeof(my_bytes)} байт')      # -> asizeof   bytes():    40 байт
 print(f'asizeof   b"":      {asizeof.asizeof(b"")} байт')           # -> asizeof   b"":        40 байт


 my_b_arr = bytearray()
 print(f'getsizeof bytearray():  {sys.getsizeof(my_b_arr)} байт')    # -> getsizeof bytearray():              56 байт
 print(f'asizeof   bytearray():  {asizeof.asizeof(my_b_arr)} байт')  # -> asizeof   bytearray():              56 байт
 print(f'asizeof   bytearray(b""):  {asizeof.asizeof(bytearray(b""))} байт')  # -> asizeof   bytearray(b""):  56 байт


 my_object = object()
 print(f'getsizeof object():  {sys.getsizeof(my_object)} байт')      # -> getsizeof object():   16 байт
 print(f'asizeof   object():  {asizeof.asizeof(my_object)} байт')    # -> asizeof   object():   0 байт

 object() возвращает 0 байт, потому что функция `asizeof` не находит вложенных объектов или атрибутов для учета,
 так как `object` не содержит информации.  Сам по себе object() - весит 16 байт
 object() - является базовым пустым объектом без дополнительных атрибутов или содержимого.

 В Python каждый объект резервирует память под возможные атрибуты, даже если их нет. Это увеличивает расход памяти при
 массовом создании объектов. Поэтому встроенные типы (например, object) не позволяют добавлять произвольные атрибуты,
 чтобы избежать излишних затрат.


 # None много чего умеет!!!
 print(dir(None))
 # ['__bool__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',
 # '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__',
 # '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']

 # При сравнение с другим элементом вызывает NotImplemented
 print(None.__lt__(1))   # -> NotImplemented
 print(None.__str__())   # -> None
 print(None.__bool__())  # -> False
 print(None.__or__())    # -> AttributeError: 'NoneType' object has no attribute '__or__'. Did you mean: '__dir__'?



 --- ЗАМЕРЫ  ПЕРЕБОР (for x in ...)   ПОИСК (x in ...) ---

 import timeit

 lst = list(range(10_000_000))
 tup = tuple(lst)
 s = set(lst)
 d = {k: k for k in lst}  # {0:0, 1:1, ..., 9999999:9999999}

 # ПЕРЕБОР (for x in ...)
 print("List:", round(timeit.timeit(lambda: [x for x in lst], number=100), 2))                # -> List:          41.32
 print("Tuple:", round(timeit.timeit(lambda: [x for x in tup], number=100), 2))               # -> Tuple:         41.46
 print("Set:", round(timeit.timeit(lambda: [x for x in s], number=100), 2))                   # -> Set:           45.48
 print("Dict (keys):", round(timeit.timeit(lambda: [k for k in d], number=100), 2))           # -> Dict (keys):   45.3
 print("Dict (values):", round(timeit.timeit(lambda: [v for v in d.values()], number=100), 2))# -> Dict (values): 46.46
 print("Dict (items):", round(timeit.timeit(lambda: [i for i in d.items()], number=100), 2))  # -> Dict (items):  112.56

 # Перебор (for x in ...)
 # List и Tuple: Практически одинаковы по скорости, так как оба являются последовательностями и итерируются одинаково эффективно.
 # Set и Dict (keys): Чуть медленнее из-за дополнительной работы с хеш-таблицей, но разница незначительная.
 # Dict (items): Заметно медленнее, потому что items() возвращает кортежи (key, value), а их создание требует дополнительных операций.
 # Dict (values):  Немного медленнее, чем keys(), из-за необходимости доступа к значениям через хеш-таблицу (~1-2% в тестах)

 # Итог по скорости перебора (for x in ...):
 List ≈ Tuple < Set ≈ Dict (keys) ≈ Dict (values) << Dict (items)


 # ПОИСК (x in ...)
 print("List (in):", round(timeit.timeit(lambda: 9_999_999 in lst, number=100), 4))               # ->  List (in):        8.5484
 print("Tuple (in):", round(timeit.timeit(lambda: 9_999_999 in tup, number=100), 4))              # ->  Tuple (in):       9.0128
 print("Set (in):", round(timeit.timeit(lambda: 9_999_999 in s, number=1000), 4))                 # ->  Set (in):         0.0001
 print("Dict (in keys):", round(timeit.timeit(lambda: 9_999_999 in d, number=1000), 4))           # ->  Dict (in keys):   0.0001
 print("Dict (in values):", round(timeit.timeit(lambda: 9_999_999 in d.values(), number=100), 4)) # ->  Dict (in values): 17.844

 # Поиск (x in ...)
 # List и Tuple: O(n)      - линейный поиск (медленно).
 # Set и Dict (keys): O(1) - хеш-таблица (очень быстро).
 # Dict (values): O(n)     - медленно, так как значения не хешируются и поиск линейный.
 # (k, v) in d.items() - Поиск по ключу (k) - O(1)   Поиск по значению (v) - O(n) (линейный перебор, как в d.values()).
   k проверяется за O(1) (хеш-таблица ключей).  v проверяется за O(1) (если k найден), иначе O(n).

 # Итог по скорости поиска (x in ...):
 Set ≈ Dict (keys) << Dict (values) ≈ List ≈ Tuple


 Итоговая таблица скорости операций

 Структура	Перебор (for x in ...)	                    Поиск (x in ...)	                    Доступ по индексу
 list	    🟢 O(n) (быстро, ~tuple)	                🔴 O(n) (медленно, линейный поиск)	    🟢 O(1) (lst[i])
 tuple	    🟢 O(n) (быстро, ~list)	                    🔴 O(n) (медленно, линейный поиск)	    🟢 O(1) (tup[i])
 set	    🟡 O(n) (медленнее list/tuple из-за хешей)	🟢 O(1) (быстро, хеш-таблица)	        🔴 Не поддерживается
 dict
 → keys	    🟡 O(n) (~set, чуть медленнее list)	        🟢 O(1) (key in d)	                    🔴 Нет индексации
 → values	🟡 O(n) (аналогично keys)	                🔴 O(n) (value in d.values())	        🔴 Нет индексации
 → items	🔴 O(n) (самый медленный из-за кортежей)	🟢 O(1) ((k,v) in d.items() для ключей)	🔴 Нет индексации
 dict		                                            🟢 O(1) (d[key] для доступа к значению)	🟢 O(1) (d[key], но не индекс!)

 Главное — помнить, что:

 - Для перебора: list/tuple > set/dict.keys() > dict.items().
 - Для поиска: set/dict.keys() >> list/tuple/dict.values().
 - Доступ по индексу есть только у list/tuple, а у dict - только по ключу.



 --- Сравнение объектов ---
 # Если объекты не могут быть сравниваемы (например, если они разных типов), то произойдет ошибка типа `TypeError`.

 # В Python функции `max()`, `min()` и `sorted()` и другие подобные функции используют стандартные методы сравнения
 # объектов, такиме как `__lt__`, `__le__`, `__gt__`, `__ge__`, и `__eq__`.

 print(sorted([1, 2, 'a']))  # -> TypeError: '<' not supported between instances of 'str' and 'int'   Тут метод __lt__
 print(min([1, 2, 'a']))     # -> TypeError: '<' not supported between instances of 'str' and 'int'   Тут метод __gt__
 print(max([1, 2, 'a']))     # -> TypeError: '>' not supported between instances of 'str' and 'int'   Тут метод __gt__


 # Чтобы можно было сравнивать ЭК нужно прописать методы         по умолчанию будет ошибка
 class Person:
     def __init__(self, name, age):
         self.name = name
         self.age = age

     def __lt__(self, other):
         return self.age < other.age

 p1 = Person("Alice", 30)
 p2 = Person("Bob", 25)

 print(max(p1, p2).name)  # Вывод: Alice (поскольку Alice старше)
 print(min(p1, p2).name)  # Вывод: Bob (поскольку Bob младше)

 # Без методов будет ошибка
 class Person:
     def __init__(self, name, age):
         self.name = name
         self.age = age

 p1 = Person("Alice", 30)
 p2 = Person("Bob", 25)

 print(p1 > p2)           # -> TypeError: '>' not supported between instances of 'Person' and 'Person'
 print(max(p1, p2).name)  # -> TypeError: '>' not supported between instances of 'Person' and 'Person'
 print(min(p1, p2).name)  # -> TypeError: '<' not supported between instances of 'Person' and 'Person'


 # __eq__ по умолчанию сравнивает адрес в памяти

 # В dataclass eq встроен сразу           # В обычном классе Сравниваем адрес в памяти
 from dataclasses import dataclass

 @dataclass                               class Person:
 class Person:                                def __init__(self, name, age):
     name: str                                    self.name = name
     age: int                                     self.age = age

 p1 = Person("Bob", 25)                   p1 = Person("Bob", 25)
 p2 = Person("Bob", 25)                   p2 = Person("Bob", 25)
 print(p1 == p2)  # -> True               print(p1 == p2)  # -> False


 --- РАБОТА С ХЕШИРОВАНИЕМ в Python! ---

 # Итоговая таблица:
 # Сценарий	        __hash__	__eq__	Результат в словаре/множестве
 # Базовый класс	auto	    is	    Разные ключи (сравнение по id)
 # Только __hash__	задан	    is	    Разные ключи (но хеши одинаковые)
 # Только __eq__	None	    задан	Ошибка (unhashable)
 # Оба метода	    задан	    задан	Один ключ при равенстве по __eq__


 # ДЛЯ ЧЕГО ПЕРЕОПРЕДЕЛЯТЬ МЕТОД __hash__ и __eq__

 # Чтобы два разных объекта (a1 и a2) считались одним ключом в словаре, нужно выполнить два условия:
 # - У них должен быть одинаковый хэш (вы уже это сделали, переопределив __hash__).
 # - Они должны считаться равными (a1 == a2 → True), для этого нужно переопределить  __eq__.

 # return 42 в __hash__ корректен, но на практике так делать не стоит - это создаст коллизии и замедлит работу словарей/множеств.
 # все объекты будут попадать в один бакет

 class A:
     def __hash__(self):                                 # ЕСЛИ НЕТ __hash__    TypeError: unhashable type: 'A'
         return 42  # одинаковый хэш для всех объектов   # АНТИПАТТЕРН

     def __eq__(self, other):                            # ЕСЛИ НЕТ __eq__      БУДЕТ 2 КЛЮЧА a1 и a2
         return isinstance(other, A)  # Все экземпляры A равны между собой


 # Демонстрация работы со словарем
 a1 = A()
 a2 = A()

 b = {a1: '1', a2: '2'}  # a2 перезапишет a1, потому что они "равны"
 print(b)      # Останется только a2
 print(b.get(a1))  # '2'
 print(b.get(a2))  # '2'
 print( a1 == a2)   # True
 print( a1 is a2)   # False (разные объекты в памяти)

 # Демонстрация работы с множеством
 s = {a1, a2}  # Добавляем оба объекта в множество
 print(s)  # Останется только один элемент
 print(len(s))  # 1
 print(a1 in s)  # True
 print(a2 in s)  # True

 # Ключевые моменты:
 # Для словарей и множеств важны оба метода: __hash__ и __eq__
 # Множество также считает объекты одинаковыми, если:
 # - Их хеши равны (__hash__)
 # - Они равны по __eq__
 # В результате в множестве остается только один элемент, так как a1 и a2 считаются одинаковыми
 # При этом объекты остаются разными в памяти (is возвращает False)



 # ПРИМЕР 1 (Только __eq__):
 # Если удалить __hash__, но оставить __eq__, класс станет нехешируемым
 # Это происходит потому, что при переопределении __eq__ Python автоматически делает __hash__ = None
 class A:
     def __eq__(self, other):
         return True

     # __hash__ = object.__hash__  # ЕСЛИ ДОБАВИТЬ ЭТУ СТРОЧКУ ОШИБКИ НЕ БУДЕТ  TypeError: unhashable type: 'A'

 a1 = A()

 # Попытка использовать в СЛОВАРЕ или МНОЖЕСТВЕ вызовет:
 b_dict = {a1: '1'}  # -> TypeError: unhashable type: 'A'
 b_set = {a1}        # -> TypeError: unhashable type: 'A'



 # ПРИМЕР 2 (Только __hash__):
 # Без __eq__ словарь/множество будут сравнивать объекты через is (по идентичности в памяти)
 # Даже при одинаковом __hash__ объекты останутся разными ключами

 class A:
     def __hash__(self):
         return 42   # АНТИПАТТЕРН

 a1 = A()
 a2 = A()
 b = {a1: '1', a2: '2'}
 print(len(b))  # 2 (разные ключи, так как a1 is a2 == False)



 # Идеальный __hash__  ЗАМЕНА return 42:
 # Обычно __hash__ строится на основе неизменяемых полей объекта, например:

 class Point:
     def __init__(self, x, y):
         self.x = x
         self.y = y

     def __hash__(self):
         return hash((self.x, self.y))  # Кортеж из полей

     def __eq__(self, other):
         return isinstance(other, Point) and (self.x, self.y) == (other.x, other.y)


 # Про наследование:
 # Если класс переопределяет __eq__, но хочет сохранить стандартный __hash__, нужно явно указать это:

 __hash__ = object.__hash__

 --- END РАБОТА С ХЕШИРОВАНИЕМ в Python! ---



 Протокол итератора (iterator protocol) - объекты итератора должны поддерживать  iterator.__iter__()  и iterator.__next__()
 Для создания объекта типа Iterator можно воспользоваться встроенной функцией iter(). По итератору можно двигаться с помощью функции next().

 it = iter([i*i for i in range(10)])
 - Нельзя получить длину итератора функцией len():
 len(it) -> TypeError: object of type 'list_iterator' has no len()

 НО МОЖНО ИСПОЛЬЗОВАТЬ more_itertools.ilen(iterable) - Возвращает количество элементов в iterable .
 from more_itertools import ilen

 ilen(it) # -> 10

 - Итератор не поддерживает получение элемента по индексу:
 it[2] ->  TypeError: 'list_iterator' object is not subscriptable

 - К итератору нельзя применить обычные операции среза или функцию slice(). Для этих целей, можно использовать функцию
 itertools.islice() модуля itertools. - так же подходит и к генератору
 it = iter([i*i for i in range(10)])
 it[2:5]  -> TypeError: 'list_iterator' object is not subscriptable

 import itertools
 list(itertools.islice(it, 2, 5)) -> # [49, 64, 81]

 class more_itertools.islice_extended(iterable, stop)  для получения отрицательных индексов
 from more_itertools import islice_extended

 iterable = iter('abcdefgh')
 list(islice_extended(iterable, -4, -1))  # -> ['e', 'f', 'g']

 # Срезы напрямую Любые Отрицательные или Положительные
 iterable = iter('abcdefgh')
 list(islice_extended(iterable))[-4:-1]  # -> ['e', 'f', 'g']

 - После прохождения по итератору, он остается пустым:
 it = iter([i*i for i in range(10)])
 list(it)
 # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
 list(it)
 # []


 Недостаток типа Iterator состоит в том, что при первом его вызове вычисляются сразу все значения последовательности как
 физической, так и виртуальной, к тому же все они хранятся в памяти до их исчерпания. Этот недостаток решает тип generator (генератор).

 Недостаток некоторых реализаций итераторов в том, что они могут загружать все данные в память сразу.    <-----
 # Примеры:

 # Проблема: весь список хранится в памяти, даже если элементы используются по одному.
 numbers = [x for x in range(1_000_000)]  # Создаётся список из 1 млн чисел в памяти
 iterator = iter(numbers)                 # Итератор просто перебирает уже существующий список

 # итератор НЕ загружает всё в память ("ленивые" итераторы)
 numbers = range(1_000_000)  # Не хранит все числа в памяти
 iterator = iter(numbers)    # Каждое число вычисляется на лету

 Итог:
 - Загружают всё в память: list, tuple, pandas.DataFrame, dict.items() (если исходная коллекция уже в памяти).
 - Не загружают всё в память: range, генераторы (yield), map/filter, файловые итераторы, itertools.

 - Если нужно обрабатывать большие данные, лучше использовать генераторы (yield) или "ленивые" итераторы (range, map, filter и т. д.).
 - Если данные уже в памяти (например, list), то обычный итератор (iter()) просто перебирает их без дополнительной оптимизации.


 --- Generator Types Протокол генератора в Python и выражение yield ---
 Важно! Так как генератор - это "улучшенный" итератор, следовательно на тип generator распространяются такие же
 ограничения как и на тип iterator.

 - Нельзя получить длину генератора функцией len():
 - не поддерживает получение элемента по индексу
 - После прохождения по генератору, он остается пустым

 для срезов используем - itertools.islice()   но НЕ поддерживает отрицательные индексы -1...
 для Отрицательных или Положительных срезов используем - from more_itertools import islice_extended

 Например, такой генератор, как:
 def squares(start, stop):
    for i in range(start, stop):
        yield i * i

 generator = squares(a, b)

 или эквивалентное выражение генератора (genexp)
 generator = (i*i for i in range(a, b))

 # Как перевернуть генератор/итератор?    reversed(list)
 # Напрямую — НЕЛЬЗЯ: генератор одноразовый и не хранит элементы, поэтому у него НЕТ “конца”, к которому можно обратиться.

 # Что будет на выходе ПОСМОТРИ
 my_list = [1, 2, 3, 4, 5]
 my_generator = (x**2 for x in my_list)

 # # TypeError: генератор одноразовый, без len/индексов (не Sequence), поэтому reversed() не работает
 print(reversed(my_generator))        # -> TypeError: 'generator' object is not reversible

 print(reversed(list(my_generator)))  # -> <list_reverseiterator object at 0x000001C4286F3A00>
 print(my_generator)                  # -> <generator object <genexpr> at 0x000001CA17B23850>
 print(list(my_generator))            # -> []


  -- YIELD FROM И СУБГЕНЕРАТОРЫ --
 # Тоже самое                          # Тоже самое                         # return + yield   # Разбери пример
 def sub_gen():                        def sub_gen():                       def sub_gen():
     yield 1.1                             yield 1.1                            yield 1.1
     yield 1.2                             yield 1.2                            yield 1.2
                                                                                return 'Done'
 def gen():                            def gen():
     yield 1                               yield 1                          def gen():
     for i in sub_gen():                   yield from sub_gen()                 yield 1
         yield i                           yield 2                              result = yield from sub_gen()
     yield 2                                                                    print('<--', result, end=' ')
                                                                                yield 2

 for i in gen():                       for i in gen():                      for i in gen():
     print(i, end=' ') # 1 1.1 1.2 2      print(i, end=' ') # 1 1.1 1.2 2       print(i, end=' ') # 1 1.1 1.2 <-- Done 2


 -- yield from  VS for i in ...: yield i --

 - for ... yield ... — просто отдаёт элементы (только next()).
 - yield from — полностью “проксирует” подгенератор: next + send/throw/close и возвращает его return (StopIteration.value).

 yield from - позволяет внешнему генератору получить значение, которое подгенератор вернул через return.  <-----

 # 1) ОДНИ И ТЕ ЖЕ элементы (тут разницы почти нет)
 def a():
     for x in [1, 2, 3]:
         yield x

 def b():
     yield from [1, 2, 3]


 # 2) РАЗНИЦА: yield from умеет забирать return value подгенератора
 def subgen():
     yield 1
     yield 2
     return 99  # "результат" подгенератора

 def outer_for():
     # просто отдаём элементы, но 99 "теряется"
     for x in subgen():
         yield x

 def outer_yieldfrom():
     # отдаём элементы и получаем return value
     result = yield from subgen()
     yield ("result", result)

 print(list(outer_for()))
 # [1, 2]

 print(list(outer_yieldfrom()))
 # [1, 2, ('result', 99)]


 `return value` в ГЕНЕРАТОРЕ приводит к StopIteration(value)   <-----


 -- КОГДА НУЖНО ОБОРАЧИВАТЬ ГЕНЕРАТОР В СКОБКИ ()

 # Если АГРУМЕНТА НЕТ   СКОБКИ ()  НЕ НУЖНЫ
 print(i for i in range(10))            # -> <generator object <genexpr> at 0x000002D45E5B3B90>

 # Если ЕСТЬ АГРУМЕНТ то нужно ОБОРАЧИВАТЬ ГЕНЕРАТОР   В СКОБКИ ()
 print((i for i in range(10)), sep='')  # -> <generator object <genexpr> at 0x000002D45E5B3B90>

 # БУДЕТ SyntaxError
 # print(i for i in range(10), sep='')  # -> SyntaxError: Generator expression must be parenthesized


 А, чуть не забыл - генераторы ещё экономят память, потому что не вываливают все результаты целиком,
 а генерируют элементы на лету. Поэтому в памяти хранится не вся коллекция, а только текущее состояние генератора.

 Генераторы откладывают выполнение кода
 Поэтому заранее знать, где код будет выполнен - та ещё задача.



 -- Ключевое различии между return и yield в Python --

 return — сразу завершает функцию, уничтожая стек вызовов (локальные переменные и состояние теряются).
 yield  — приостанавливает функцию, сохраняя стек (при следующем вызове выполнение продолжится с этого места).


 Почему говорят "return убивает стек"?
 Потому что return полностью выходит из функции, очищая её контекст (локальные переменные, стек вызовов).
 yield же "замораживает" состояние функции, позволяя продолжить выполнение позже.

 Пример:
 def test():
     x = 1
     yield x  # Сохраняет x и приостанавливается
     x += 1
     return  # Убивает стек, x исчезает

 gen = test()
 print(next(gen))  # 1 (yield)
 print(next(gen))  # StopIteration (return убил выполнение)

 return = полный выход, стек уничтожается.
 yield  = пауза, стек сохраняется.


 --- Context Manager Types     Тип контекстный менеджер ---

 В питоне есть оператор with вход и выход, при работе с файлами, файл автоматически закрывается.
 Для создания своего контекстного менеджера нужно определить класс с двумя специальными методами:
  __enter__() и __exit__().

 Мы также можем создать контекстный менеджер, используя функцию и декоратор contextlib.contextmanager.
 В этом случае функция должна быть генератором, который yield‘ит объект для использования в блоке with.
 Все, что находится до yield, будет выполняться перед входом в блок, а после yield — после выхода из блока.

 from contextlib import contextmanager

 @contextmanager
 def open_file(name):
     f = open(name, 'w')
     try:
         yield f
     finally:
         f.close()


 Контекстные менеджеры в Python используются для управления ресурсами, автоматически выполняя определённые действия
 при входе и выходе из блока кода. Наиболее распространённые применения:

  ВСЕГДА УКАЗЫВАЕМ КОДИРОВКУ    encoding                                                        <----- Важно

 1. **Файлы**: Автоматическое закрытие файлов после их использования.
    with open('file.txt') as f:
        data = f.read()

 2. **Сессии базы данных**: Управление подключениями к БД, обеспечивая корректное закрытие соединений
      (например, с помощью `with` с библиотекой SQLAlchemy).

 3. **Блокировки**: Управление мьютексами и семафорами для обеспечения потокобезопасности.

 4. **Транзакции**: Упрощение работы с транзакциями, автоматически откатывая их в случае ошибок.

 5. **Кастомные ресурсы**: Создание собственных контекстных менеджеров для управления любыми ресурсами
    (например, сетевыми соединениями), используя `__enter__` и `__exit__`.

 Таким образом, контекстные менеджеры помогают избежать утечек ресурсов и делают код более читаемым и безопасным.

  # это менеджеры контекста, которые временно перенаправляют:
 contextlib.redirect_stdout — стандартный вывод (stdout, например, print) в указанный объект.
 contextlib.redirect_stderr — стандартный вывод ошибок (stderr, например, сообщения об ошибках) в указанный объект.

 Транзакции в Python — это последовательность операций, которые выполняются как единое целое. Если одна операция
 НЕ удается, все изменения отменяются, что гарантирует целостность данных. Обычно используются при работе с базами данных.

  Примеры Транзакций в Python:
 - Использование SQLite
 - Использование SQLAlchemy


 ExitStack — “один with для многих ресурсов”, которые добавляются по ходу, и он закроет их все при выходе.

 from contextlib import ExitStack

 with ExitStack() as s:
     f1 = s.enter_context(open("a.txt"))
     f2 = s.enter_context(open("b.txt"))
 # f2 закрыт, потом f1


 --- LEGB-rule. Как Python ищет имена переменных ---
 Буквы в аббревиатуре LEGB обозначают локальную, вложенную, глобальную и встроенную
 (Local, Enclosing, Global и Built-in Scope) области. Поиск идёт снизу-вверх сначала L-E-G-B
 даже для встроенных функций если не нашел переменную, то ошибка  NameError:

 Особенности LEGB:
 1) сначала поиск идет в локальном пространстве имен, максимально близко к использованию имени и
 далее идет снизу-вверх, изнутри-наружу к глобальному пространству имен
 2) после локального пространства имен интерпретатор посмотрит в enclosing, то есть в функцию,
 которая содержит текущую (если она есть) и далее проверит глобальное пространство имен
 3) последним шагом будут проверены имена в модуле builtins (встроенные функции)
 4) если на любом этапе имя найдено, то далее поиск не идет. Если все этапы неудачны то выбрасывается NameError
 5) важно понимать что даже если мы используем встроенную функцию, типа max/min/sum/print,
 то интерпретатор сначала проведет поиск по всем скоупам. Вот почему крайне важно НИКОГДА не давать своим переменным,
 функциям, модулям имена встроенных функций или библиотек (самые частые фейлы это имена типа len, list, sum, json, dict)

 Локальные переменные видны только в локальной области видимости, которой может выступать отдельно взятая функция
 Глобальные переменные видны во всей программе.
 Локальные переменные создаются каждый раз при входе в функцию и уничтожаются при выходе из нее.   <-----

 Локальные переменные НЕ требуют поиска в словаре, так как хранятся в очень ТОНКОМ массиве с малым временем поиска <-----

 END --- LEGB ---

 -- Почему локальные переменные работают быстрее, чем глобальные - Это деталь реализации CPython.
 Вы можете спросить, почему хранить локальные переменные быстрее, чем глобальные. Это деталь реализации CPython.

 Помните, что CPython компилируется в байт-код, который запускает интерпретатор. Когда функция компилируется, локальные
 переменные сохраняются в массиве фиксированного размера ( не в dict), а имена переменных присваиваются индексам.
 Это возможно, поскольку вы не можете динамически добавлять локальные переменные в функцию. Тогда получение локальной
 переменной — это буквально поиск указателя в списке и увеличение счетчика ссылок, что PyObject тривиально.

 Сравните это с глобальным поиском ( LOAD_GLOBAL), который представляет собой настоящий dict поиск, включающий хэш и т. д.
 Кстати, именно поэтому вам нужно указать, global i хотите ли вы, чтобы она была глобальной: если вы когда-либо
 присваиваете значение переменной внутри области видимости, компилятор выдаст STORE_FASTs для доступа к ней,
 если вы не скажете ему этого не делать.

 Кстати, глобальный поиск все еще довольно оптимизирован. Поиск атрибутов foo.bar очень медленный !


 Почему код Python в функции выполняется быстрее?
 Важно!!!
 Причина, по которой в локальных переменных это происходит быстрее, заключается в том, что локальные области фактически
 реализуются как массивы, а НЕ словари (поскольку их размер известен во время компиляции).

 ### ChatGPT
 Внутри функции локальные переменные хранятся как “fast locals”: по сути это массив/слот (индекс),
 и доступ к ним идёт инструкциями типа LOAD_FAST → это быстро.

 Глобальные/атрибуты/встроенные чаще требуют поиска в словарях (globals(), builtins, obj.__dict__) → это медленнее.


 --- global и nonlocal ---
 global и nonlocal нужны только для изменения значений
 global может создать переменную, nonlocal не может!
 nonlocal ищет только во внешних скоупах, но не в глобальном и не builtins
 НЕ используйте global, nonlocal


 -- Модуль heapq обеспечивает реализацию алгоритма очереди кучи, также известного как алгоритм очереди приоритетов.
 Можно рассматривать кучу как обычный список Python без сюрпризов.
 Python min-куча (наименьшее значение всегда лежит в корне) реализована на базе списка при помощи встроенного модуля heapq


 Вот основные виды тестирования в Python:

 1. **Unit Testing (Модульное тестирование)**: Тестирует отдельные функции или классы для проверки их корректности.
  Используются библиотеки `unittest` и `pytest`.

 2. **Integration Testing (Интеграционное тестирование)**: Проверяет взаимодействие между несколькими модулями или
  системами, чтобы убедиться в их совместной работе.

 3. **Functional Testing (Функциональное тестирование)**: Проверяет функциональность приложения в соответствии с
 требованиями, не углубляясь в внутренние детали реализации.

 4. **End-to-End Testing (Тестирование "от конца до конца")**: Тестирует полный пользовательский поток в приложении,
  включая все интеграции и зависимости.

 5. **Regression Testing (Регрессионное тестирование)**: Убеждается, что изменения в коде не нарушили существующую
  функциональность.

 6. **Тестирование пользовательского интерфейса (UI Testing)**: Проверяет интерфейс приложения, чтобы убедиться в
  правильном отображении и работе элементов.

 7. **Тестирование производительности (Performance Testing)**: Оценивает скорость и стабильность приложения под нагрузкой.

 8. **Приемочное тестирование (Acceptance Testing)**: Проверяет готовность системы к использованию конечными пользователями.

 Эти виды тестирования обеспечивают базовую проверку качества и надежности приложений.


 Unit Testing  vs  Integration Testing

 - Unit Testing проверяет отдельные части кода (функции, классы) изолированно.
 - Integration Testing проверяет, как эти части работают вместе.

 Unit — тестируем детали по отдельности, Integration — тестируем их взаимодействие.


 паттерн "Arrange-Act-Assert" (AAA), который часто используется в структуре юнит-тестов.

 Arrange (Построение / Подготовка)  - настройка начальных условий (создание объектов, моки, входные данные).
 Act (Операция / Действие)          - выполнение тестируемого метода или действия.
 Assert (Проверка / Утверждение)    - проверка, что результат соответствует ожиданиям.

 # Пример на Python (pytest):
 def test_add_numbers():
     # Arrange (Подготовка)
     a, b = 2, 3

     # Act (Действие)
     result = a + b

     # Assert (Проверка)
     assert result == 5

 Почему это важно?
 - Делает тесты читаемыми и структурированными.
 - Четко разделяет подготовку, действие и проверку.
 - Упрощает поиск ошибок при падении теста.


 --- Тестирование ---
 Selenium — это один из самых популярных и широко используемых инструментов для автоматизации тестирования веб-приложений.

 TDD (Test-Driven Development) – это метод разработки ПО, где сначала пишут тест, затем код, который проходит этот тест,
 и потом рефакторят код.  РАЗРАБОТКА ЧЕРЕЗ ТЕСТИРОВАНИЕ

 3 шага TDD:
 Красный     - написать тест (он не проходит).
 Зелёный     - написать минимальный код для прохождения теста.
 Рефакторинг - улучшить код, сохраняя его работоспособность.

 Цель: чистый, надёжный код и быстрое выявление ошибок.


 -- Методы Тестирование --

 1. **Тестирование по классам эквивалентности (Equivalence Class Testing)**: Делит входные данные на эквивалентные классы,
  чтобы минимизировать количество тестов. Проверяется только одно значение из каждого класса.

 2. **Тестирование граничных значений (Boundary Value Testing)**: Тестирует значения на границах эквивалентных классов
  (например, минимальные и максимальные значения), так как ошибки часто возникают именно на этих границах.

 3. **Тестирование случайных значений (Random Testing)**: Случайно выбирает входные данные для тестирования, чтобы
 выявить ошибки, которые могут быть пропущены более структурированными методами.

 4. **Тестирование по состояниям (State Transition Testing)**: Основано на моделировании состояний системы. Проверяет,
  как система реагирует на переходы между различными состояниями.

 5. **Тестирование на основе требований (Requirement-Based Testing)**: Разрабатывает тесты на основе спецификаций и
 требований к системе, чтобы убедиться, что функциональность реализована правильно.

 6. **Тестирование изомерной (Combinatorial Testing)**: Исследует разные комбинации входных значений и их влияние на
  систему, что особенно полезно для систем с множеством параметров.

 7. **Тестирование сценариев (Scenario Testing)**: Оценивает систему на основе реальных сценариев использования.

 8. **Проверка условий (Condition Testing)**: Проверяет, как программа обрабатывает разные логические условия.


 Эти методы помогают обеспечить качественное тестирование программного обеспечения с разных сторон и выявить
  потенциальные ошибки в коде.


 Классы эквивалентности - это разделение функционала или данных на определенные наборы,
 с которыми тестируемое приложение должно работать одинаково.

 Класс эквивалентности (Equivalence class) – это набор входных (или выходных) данных ПО, которые обрабатываются
 программой по одному алгоритму или приводят к одному результату.

 # Хороший ответ
 Значения называется эквивалентными друг другу если они приводят к одному и тому же результату.        <-----
 Пример калькулятор: Ужимать тесты
 Выделить классы эквивалентности: это значит написать 10 тестов вместо 100. С тем же результатом!


 Граничные значения (Boundary Values) — это значения, в которых один класс эквивалентности переходит в другой диапазон.
 Эта техника тестирования фокусируется на анализе поведения программы на ее границах или близких к ним цифрах.
 Это метод тестирования, в котором основное внимание уделяется значениям на границах допустимого диапазона.



 Фикстуры (fixtures) в pytest — это специальные функции, которые подготавливают данные или состояние,
 необходимые для выполнения тестов. Они позволяют создавать общее окружение для нескольких тестов, упрощают код,
 обеспечивают настройку и очистку ресурсов. Фикстуры определяются с помощью декоратора `@pytest.fixture` и могут быть
 использованы, передавая их как аргументы в тестовые функции. Это делает тесты более читаемыми и поддерживаемыми.

 ### Основные цели фикстур:
 1. **Подготовка данных**: Создание необходимых объектов, например, подключение к базе данных или подготовка тестовых данных.
 2. **Очистка ресурсов**: Освобождение ресурсов после завершения теста (например, закрытие соединения с базой данных).
 3. **Упрощение кода**: Устранение дублирования кода за счет централизации логики подготовки.

 pytest.capsys — это встроенная фикстура (fixture) в pytest, которая позволяет перехватывать и проверять
 стандартный вывод (stdout) и стандартный вывод ошибок (stderr) в тестах. Это полезно для тестирования функций,
 которые что-то выводят на экран.


 # ЗАПУСК ТЕСТОВ В ТЕРМИНАЛЕ  <-----
 pytest tests                                                                     # запустить В ТЕРМИНАЛЕ ВСЕ ТЕСТЫ
 pytest tests/unit/api/v2/query/test_query.py                                     # запустить ОТДЕЛЬНЫЙ МОДУЛЬ С ТЕСТАМИ
 pytest tests/unit/api/v2/query/test_query.py::test_get_query_conditions_by_name  # запустить КОНКРЕТНЫЙ ТЕСТ (ОДИН)

 Чтобы запустить тесты определенное количество раз В ТЕРМИНАЛЕ Git Bash / PyCharm Terminal.

 С плагином pytest-repeat - это лучший вариант, и он будет работать на всех платформах (Linux/macOS/Windows):

 Лучший способ, так как pytest-repeat дает детальную статистику после прогонов.
 pip install pytest-repeat
 pytest tests/unit/api/v2/workbook/test_workbook_query.py tests/unit/api/v2/workbook/test_workbook.py --count=5

 Если добавить -x, тесты остановятся при первой ошибке:
 pytest tests/unit/api/v2/workbook/test_workbook_query.py tests/unit/api/v2/workbook/test_workbook.py --count=5 -x

 Альтернативный способ (без плагина) Git Bash

 ЗАПУСТИТЬ ТЕСТЫ 5 РАЗ
 Если тест упадет, цикл закончится
 for i in {1..5}; do pytest tests/unit/api/v2/workbook/test_workbook_query.py tests/unit/api/v2/workbook/test_workbook.py || break; done

 Если тест упадет, цикл продолжится
 for i in {1..5}; do pytest tests/unit/api/v2/workbook/test_workbook_query.py tests/unit/api/v2/workbook/test_workbook.py; done


 --- Unittest ---
 Unittest — это модуль стандартной библиотеки Python. Внутри есть фреймворк для создания и запуска тестов.
 С его помощью можно создавать мок-объекты, которые имитируют поведение зависимых компонентов
 и помогают изолировать тестируемый код. Нельзя лишь имитировать внешние сервисы.

 from unittest import TestCase, main
 from Again.unit_tests.calculator_new import calculator

 class CalculatorTest(TestCase):

     def test_plus(self):
         self.assertEqual(calculator('2+2'), 4)

     def test_many_sings(self):
         with self.assertRaises(ValueError) as e:
             calculator('2+2*10')
         self.assertEqual('Выражение должно содержать 2 целых числа и 1 знак', e.exception.args[0])

 if __name__ == '__main__':
     main()

  Типы проверок в классе TestCase
 assertEqual(a, b)          a == b
 assertNotEqual(a, b)       a != b
 assertTrue(x)              bool(x) is True
 assertFalse(x)             bool(x) is False
 assertIs(a, b)             a is b
 assertIsNot(a, b)          a is not b
 assertIsNone(x)            x is None
 assertIsNotNone(x)         x is not None
 assertIn(a, b)             a in b
 assertNotIn(a, b)          a not in b
 assertIsInstance(a, b)     isinstance(a, b)
 assertNotIsInstance(a, b)  not isinstance(a, b)


 Модуль Mock: макеты-пустышки в тестировании

 unittest.mock предоставляет базовый Mock класс, устраняющий необходимость создания множества заглушек в вашем наборе тестов.
 Объект sentinel в unittest.mock — это уникальный "заглушечный" объект для тестов.

 Фикстуры (fixtures) в юнит-тестировании — это функции или объекты, которые предварительно настраивают необходимое
 окружение или состояние для тестов. Они помогают избежать дублирования кода, обеспечивают подготовку и очистку ресурсов,
 такие как создание тестовых данных или подключение к базе данных, и могут использоваться в нескольких тестах для
 повышения читаемости и поддерживаемости кода.

 ### Основные цели фикстур:
 1. **Инициализация ресурсов**: Подготовка необходимых объектов или данных, таких как создание тестовой базы данных,
  настройка файлов и т.д.
 2. **Очистка ресурсов**: Освобождение ресурсов после выполнения тестов для предотвращения утечек памяти или конфликтов
  в будущем тестировании.

 ### Использование:
 Фикстуры определяются с помощью методов `setUp()` и `tearDown()` в классе тестов. Метод `setUp()` выполняется перед
 каждым тестом, а `tearDown()` — после него. Эти методы позволяют централизовать логику подготовки и очистки,
 что упрощает поддержку тестов.


 -- Разница между стандартным `unittest` и `django.test` (Django unittest) --

 Разница между стандартным `unittest` и `django.test` (Django unittest) заключается в следующем:

 1. **Контекст**: Стандартный `unittest` используется для тестирования общих Python-приложений, тогда как `django.test`
  предоставляет специфичные инструменты и поддержку для тестирования Django-приложений.

 2. **Функциональность**: Django unittest включает дополнительные утилиты, такие как тестовые базы данных, утилиты для
  тестирования представлений (views), моделей и форм, а также инструменты для проверки работы с URL и шаблонами.

 3. **Тестовая база данных**: При использовании Django unittest автоматически создается тестовая база данных,
  которая изолирована и очищается после каждого теста, что упрощает тестирование при работе с базой данных.

 В целом, `django.test` предоставляет больше удобства и возможностей для тестирования в контексте Django-приложений.

 Основные отличия:

 1. **Контекст**: `unittest` — общий фреймворк для тестирования Python, в то время как `django.test` —
 специализированный для Django-приложений.

 2. **Инструменты**: `django.test` предлагает дополнительные функции, такие как тестовая база данных,
  средства для тестирования представлений, моделей и форм.

 3. **Автоматизация**: В Django unittest автоматически создается и очищается тестовая база данных, обеспечивая изолированное тестирование.

 Таким образом, Django unittest предоставляет более удобные инструменты для тестирования в контексте веб-приложений.
 Тестирование unittest в Django


 -- МУТАЦИОННОЕ тестирование --
 Мутационное тестирование — это метод оценки качества тестов путём внесения небольших изменений (мутаций) в исходный код
 и проверки, обнаруживают ли тесты эти изменения. Если тесты находят мутации, они эффективны; если нет — требуют доработки.

 Коротко: «Портишь код — проверяешь, ловят ли тесты ошибки».



 --- PATCH vs MOCK vs STUB в pytest ---

 Stub – простая заглушка, возвращает фиктивные данные без логики. Возвращает заранее заданные данные.

 # Пример Stub
 def stub(): return 42

 Mock – объект, имитирующий поведение и проверяющий вызовы (из unittest.mock или pytest-mock).
 это заглушка с возможностью проверки (был ли вызван, сколько раз и т. д.)

 # Пример Mock
 from unittest.mock import Mock
 mock = Mock(return_value=100)
 assert mock() == 100
 mock.assert_called_once()

 Mock vs MagicMock

 Mock      – базовая заглушка с проверкой вызовов.
 MagicMock – автоматически создаёт методы-заглушки (полезно для магических методов, например __len__).

 Patch – временная подмена объекта (через pytest-mock или monkeypatch).

 # Пример Patch:
 def test_func(mocker):  # pytest-mock
    mocker.patch("module.func", return_value=123)
    assert module.func() == 123

 Разница:

 Stub  – только возвращает данные.
 Mock  – подменяет + проверяет, как его использовали.
 Patch – способ временно заменить объект (на stub/mock).

 	                Stub	                Mock	                    Patch
 Возвращает данные	Да (фиксированные)	    Да (можно настраивать)	    Сам по себе нет, но заменяет объект на stub/mock
 Проверяет вызовы	Нет	                    Да (assert_called и др.)	Нет (но можно патчить mock)
 Где используется	Простая подмена	        Проверка взаимодействия	    Временная замена объекта



 --- Конкурентность (concurrency)  Параллельность (parallel) ---
 Конкурентность (concurrency) - запуск на выполнение сразу нескольких задач
 (не обязательно в 1 момент времени выполняется несколько). Зависит от ПО Первые ОС с процессором без ядер -использовали только ее.
 Проще говоря быстрое переключение между задачами

 Параллельность (parallel) - конкурентность, когда 2+ задачи выполняются одновременно. Зависит от железа
  Вы не можете одновременно (!) выполнять больше задач, чем есть ядер в системе.

 thread-safe - потокобезопасность, означает что при работе с обьектом не возникают известные проблемы
 при работе с конкурентностью

 Часть кода является потокобезопасной, если она корректно работает при одновременном выполнении несколькими потоками.

 Если код или объект считается thread-safe, это означает, что его использование не приведет к непредсказуемому поведению
 при одновременном выполнении нескольких потоков.


 -- Встроенные библиотеки Python для конкурентности --

 - threading          – потоки (I/O-bound, GIL).
 - multiprocessing    – процессы (CPU-bound).
 - asyncio            – асинхронность (I/O-bound, корутины).
 - concurrent.futures – пулы потоков/процессов (ThreadPoolExecutor, ProcessPoolExecutor).
 - queue              – очереди для обмена данными.



 --- Python НЕ является полностью потокобезопасным по умолчанию.  Python НЕ потокобезопасен!!! ---

 Python не потокобезопасен!

 Python не потокобезопасен из-за:

 1) GIL (Global Interpreter Lock)

 - Разрешает выполнять только один поток Python-кода одновременно (даже на многоядерных CPU).
 - Но: GIL не защищает ваши данные — при работе с общими ресурсами возможны race condition.

 2) Проблемы с общими данными

 # Пример                                          # Пример ПРАВИЛЬНЫЙ:
 x = 0                                             from threading import Lock
 def increment():                                  lock = Lock()
     global x                                      with lock:
     x += 1  # Небезопасно! (даже с GIL)               x += 1  # Теперь безопасно

 Как исправить:

 - Используйте threading.Lock для блокировок.
 - Для CPU-задач — multiprocessing (без GIL).
 - Для I/O — потоки + синхронизация.

 GIL ≠ потокобезопасность: он лишь ограничивает параллелизм, но НЕ заменяет управление данными.

 Пример с Lock:

 from threading import Lock
 lock = Lock()
 with lock:
     x += 1  # Теперь безопасно

 Итог:

 - GIL мешает многопоточному ускорению кода.
 - Ваши данные всё равно нужно защищать вручную.

 Для справки:

 - Даже операции вида x += 1 в Python не атомарны (из-за GIL они могут прерываться).
 - queue.Queue и collections.deque — потокобезопасны "из коробки".

 СУТЬ!!!                                                                                                          <-----
 Python не потокобезопасен из-за GIL (который лишь ограничивает выполнение одним потоком, но не защищает данные).
 Используйте Lock для общих ресурсов, multiprocessing для CPU-задач.

 GIL не делает Python потокобезопасным на 100% (только для байт-кода).
 GIL не гарантирует 100% потокобезопасности в Python, но он защищает операции с байт-кодом и объектами Python от
 самых опасных race conditions.

 1. Что защищает GIL?
 GIL предотвращает параллельное выполнение байт-кода Python в нескольких потоках. Это значит:
 - Невозможно, чтобы два потока одновременно изменяли один и тот же Python-объект на уровне байт-кода.
 - Счётчик ссылок (refcount) и другие внутренние структуры CPython защищены от повреждений.

 2. В чём GIL не помогает?
 Даже с GIL возможны проблемы с потокобезопасностью, если:

 - Состояние гонки (Race Condition) на высоком уровне
 - Проблемы в C-расширениях
 - Атомарные операции != потокобезопасность

 --- END Python НЕ является полностью потокобезопасным по умолчанию.  Python НЕ потокобезопасен!!! ---



 --- Global Interpreter Lock (GIL) ---
 GIL (Global Interpreter Lock) - глобальная блокировка интерпретатора -
 механизм гарантирующий, что в любой момент времени выполняется только 1 инструкция в питоне.

 GIL - Гарантирует нам что в один момент времени в Python работает РОВНО 1 ПОТОК(инструкция),
 даже если потоков больше

 GIL может быть удержан потоком не дольше 5 мс  - 0.0000005  (0.005)
 time.sleep(t) останавливает текущий поток минимум на t секунд (поток “спит”, CPU не жрёт).
 В CPython на время sleep поток отпускает GIL, поэтому другие потоки могут выполнять Python-код — из-за этого
 и видно переключение/перемешивание.

 - Переключение между потоками — НЕ “магия”: есть time-slicing (sys.setswitchinterval) + точки release GIL в C.  <-----
 import sys
 print(sys.getswitchinterval())  # -> 0.005

 Чтобы помешать потоку Python удерживать GIL БЕСКОНЕЧНО, Интерпретатор байт-кода Python периодически
 (По умолчанию раз в 5 миллисекунд) приостанавливает текущий поток и тем самым освобождает GIL.

 В CPython глобальная блокировка интерпретатора, или GIL, представляет собой мьютекс, который защищает доступ к
 объектам Python, НЕ позволяя нескольким потокам одновременно выполнять байт-коды Python. GIL предотвращает состояния
 гонки и обеспечивает безопасность потоков.
 Короче говоря, этот мьютекс необходим главным образом потому, что управление памятью CPython не является потокобезопасным.

 GIL - Механизм, используемый интерпретатором CPython для обеспечения того, чтобы только один поток выполнял байт-код Python одновременно.

 Краткие сведения о GIL:
 - Одновременно может выполняться только один поток.
 - Интерпретатор Python переключается между потоками для достижения конкурентности.
 - GIL применим к CPython (стандартной реализации). Но такие как, например, Jython и IronPython не имеют GIL.
 - GIL делает однопоточные программы быстрыми.
 - Операциям ввода/вывода, обычно GIL не мешает.
 - GIL позволяет легко интегрировать непотокобезопасные библиотеки на языке C
 - Благодаря GIL есть много высокопроизводительных расширений/модулей, написанных на языке C.

 Для CPU зависимых задач интерпретатор делает проверку каждые N тиков и переключает потоки.
 Таким образом один поток не блокирует другие.

 Потоки ОС переключает планировщик ОС (например, в Linux - CFS).                        <-----

 ОС переключает потоки своим планировщиком (в Linux обычно CFS для обычных задач).
 CPython/GIL лишь решает, какой поток может выполнять Python-байткод в данный момент;
 а когда именно поток получит CPU — решает планировщик ОС.

 GIL в Python НЕ управляет переключением потоков, а только блокирует выполнение Python-кода,
 чтобы в один момент времени работал только один поток.

 GIL — это мьютекс внутри CPython, который НЕ даёт двум потокам одновременно исполнять Python-байткод в одном процессе.


 Итог:
 Переключение потоков - ОС.
 GIL - только блокировка выполнения Python-кода.


 Да, GIL (Global Interpreter Lock) в Python помогает Garbage Collector (GC) работать корректно.
 Он предотвращает одновременный доступ нескольких потоков к объектам Python, что минимизирует риски гонок данных и
 обеспечивает безопасную работу с памятью. Это позволяет GC надежно отслеживать и освобождать память,
 избегая проблем с памятью, которые могут возникнуть при работе в многопоточной среде.


 Таким образом, GIL способствует безопасности и эффективности работы сборщика мусора в Python.    <-----   Важно
 GIL помогает сборщику мусора (GC) – ДА, защищает подсчёт ссылок.


 КРИТИЧЕСКИЕ СЕКЦИИ – участки кода, где доступ к общим ресурсам защищается блокировками (например, threading.Lock),
 чтобы избежать состояний гонки.

 Байткод и GIL
 Каждый поток поочередно получает GIL для выполнения байткода.

 GIL не защищает от race conditions – нужны критические секции.
 КРИТИЧЕСКИЕ СЕКЦИИ – ручное управление доступом к данным.


  --- Сборщик мусора (Garbage Collector, GC) ---
  Задача сборщика мусора — именно поиск циклических ссылок на объекты

  Сборщик мусора не умный и не глупый. Он просто ищет циклы из ссылок и удаляет их.
  Большая часть объектов удаляется ещё до сборщика, так как в Python есть подсчёт ссылок.

 Сборщик мусора автоматически освобождает память, которая больше не используется.
 Он определяет, что память больше не используется, если на объект нет ссылок
 модуль gc - для дополнительных возможностей сборщика мусора
 Сборщик мусора (Garbage Collector, GC)
 import gc, gc.collect() - Принудительный запуск Сборщика мусора
 gc.collect(generation=2)  generation - какое поколение собирать (от 0 до 2)
 gc.set_threshold(...) - Управление частотой вызова Сборщик мусора
 gc.enable(), gc.disable()  - вкл, выкл Сборщик мусора
 gc.freeze() - Заморозить все объекты, отслеживаемые сборщиком мусора; переместите их в постоянное поколение
 и игнорируйте во всех будущих коллекциях.
 gc.unfreeze() - Разморозьте объекты в постоянном поколении и поместите их обратно в самое старое поколение.


 В Основном НЕИЗМЕНЯЕМЫЕ НЕ отслеживаються а ИЗМЕНЯЕМЫЕ отслеживаються
 Можно самому посмотреть кто отслеживается а кто нет:
 import gc
 gc.is_tracked(0)           # -> False    # int не могут находиться в цикле ссылок
 gc.is_tracked("a")         # -> False    # str не могут находиться в цикле ссылок
 gc.is_tracked([])          # -> True     # list может быть в цикле ссылок
 gc.is_tracked(tuple())     # -> False    # пустой tuple не отслеживается
 gc.is_tracked({})          # -> False
 gc.is_tracked({"a": 1})    # -> False
 gc.is_tracked({"a": []})   # -> True
 gc.is_tracked((i for i in range(10)))   # -> True     Generator
 gc.is_tracked(iter([1, 2, 3]))          # -> True     Iterator

 ПОДСЧЕТ ССЫЛОК также является формой сборки мусора(GC).
 В Python, алгоритм подсчета ссылок является фундаментальным и НЕ может быть отключен, тогда как GC опционален и может быть отключен.

 Поскольку сборщик дополняет подсчет ссылок, уже используемый в Python, вы можете отключить сборщик,        <-----
 если уверены, что ваша программа не создает циклы ссылок.

 --- Подсчет ссылок ---
 Подсчет ссылок Этот метод отслеживает количество ссылок, указывающих на каждый объект.
 Когда счетчик ссылок для объекта достигает нуля, что означает их отсутствие,
 объект считается “мусором” (ненужным), он уничтожается сборщиком мусора и память освобождается

 Для всех объектов в программе Python ведется подсчет ссылок. Счетчик ссылок на объект увеличивается всякий раз,
 когда ссылка на объект записывается в новую переменную или когда объект помещается в контейнер, такой как список,
 кортеж или словарь

 Счетчик ссылок (отслеживает создание/удаление обьектов) - ПОТОКОНЕБЕЗОПАСЕН не умеет работать чтобы сразу
 2 потока создавали или удаляли обьекты

 sys.getrefcount()  - функция показывает сколько ссылок ссылается на обьект. Возвращаемое значение обычно (+1) на
 единицу больше, чем вы могли ожидать, поскольку оно включает (временную) ссылку в качестве аргумента getrefcount().
 x = []
 print(sys.getrefcount(x)) # -> 2    +1 за (временную) ссылку в качестве аргумента getrefcount().
 в REPL создаете автоматическую ссылку в переменной  '_'  которая  добавляет +1

 Основной алгоритм сборки мусора, используемый CPython, — подсчет ссылок:
 x = object()
 sys.getrefcount(x) # -> 2 ссылки на обьект +1 из-за getrefcount
 y = x
 sys.getrefcount(x) # -> 3 ссылки на обьект +1 из-за getrefcount
 del y
 # del (y)  # Тоже самое
 sys.getrefcount(x) # -> 2 ссылки на обьект +1 из-за getrefcount
 import gc
 print(gc.collect()) # -> 0  ПОДСЧЕТ ССЫЛОК сработал

 Основная проблема схемы подсчета ссылок заключается в том, что она не обрабатывает циклы ссылок:
 container = []
 container.append(container)
 sys.getrefcount(container)   # -> 3

 Циклические ссылки происходят когда один или более объектов ссылаются на друг друга.
 ЦИКЛ ССЫЛОК - когда ПОДСЧЕТ ССЫЛОК не справляется с удалением обьектов
 Ссылочный цикл просто означает, что один или несколько объектов ссылаются друг на друга


 Основная проблема схемы подсчета ссылок заключается в том, что она не обрабатывает циклы ссылок:
 container = []
 container.append(container)
 sys.getrefcount(container)   # -> 3
 print(container)  # -> [[...]]  # показывает, что структура данных ссылается на себя, избегая бесконечного рекурсивного вывода.

 del container
 import gc
 print(gc.collect()) # -> 1  СБОРЩИК МУСОРА сработал

 Garbage Collector (GC) в Python справится с этой ситуацией. Он может обрабатывать циклические ссылки, такие как
 `container`, содержащий сам себя. Хотя сборка мусора может потребовать немного больше ресурсов из-за необходимости
 отслеживания циклических ссылок, утечек памяти не произойдет, и GC не "ЗАХЛЕБНЕТСЯ".

 # В Python `[...]` используется, чтобы показать циклическую ссылку, поскольку повторяющееся содержание может привести
 # к бесконечному выводу

 --- Слабые и Обычные Ссылки на обьекты ---
 Обычная ссылка увеличивает счетчик ссылок на объект и предотвращает сбор мусора.
 Слабые ссылки - не увеличивают счетчик ссылок. Соответственно, объект на который есть такая ссылка, может быть уничтожен
 Модуль weakref позволяет создавать "слабые" ссылки на объекты.

 PYTHON ССЫЛКИ НА ОБЪЕКТЫ В ПАМЯТИ - Python: strong/weak reference (память, weakref)
 Обычная (сильная) ссылка удерживает объект: пока есть ссылка → объект не удалится.
 Слабая ссылка (weakref) не удерживает: если сильных ссылок нет → объект удалится, а weakref станет None.

 -- Аллокаторы в Python --
 Аллокаторы в Python - это механизмы управления памятью, которые отвечают за выделение и освобождение памяти для объектов.
 Python использует собственные аллокаторы для оптимизации работы с памятью, например:

 - Pymalloc            - аллокатор для НЕбольших объектов, который уменьшает нагрузку на системный аллокатор.

 - Системный аллокатор - используется для больших объектов или когда Pymalloc не подходит.
                         Работает через стандартные вызовы ОС (например, malloc).

 - Аллокатор для больших блоков - Управляет памятью для очень больших объектов.

 Итог: Аллокаторы в Python управляют памятью, чтобы сделать работу программы быстрее и эффективнее.
 -- END Аллокаторы в Python --


 Задачи могут быть:
 CPU-bound - зависит от мощности процессора # расчеты внутри python власть GIL
 IO-bound - зависит от системы ввода/вывода # обращение к файлу, обращение к сайту # GIL не трогает

 threading - IO-bound задачи # GIL не помешает
 asyncio - IO-bound задачи , 1 поток использует Конкурентность (concurrency) по полной
 multiprocessing - любые задачи


 # Потоки ИСПОЛНЯЮТСЯ на уровне ОС а asyncio на уровне Интерпретатора                                 <-----
 ### Потоки — на уровне ОС, asyncio — на уровне приложения (event loop) внутри одного потока.

 В целом — да, идея правильная: threads = OS-level, asyncio = user-level (event loop) внутри процесса.

 Но на собесе я бы сказал так (чуть точнее, чтобы не придрались):
 Потоки: реальные потоки ОС, планируются ядром, но В CPython GIL НЕ даёт потокам параллелить байткод.
 asyncio: обычно один поток ОС, конкурентность делает event loop (кооперативно, в точках await),
 это НЕ “интерпретатор”, а рантайм/библиотека в user-space.

 Если сказать “на уровне интерпретатора” — тебя, скорее всего, поправят, но смысл ты передаёшь верно.


 -- КОНКУРЕНТНОСТЬ vs ПАРАЛЛЕЛИЗМ --

 В случае КОНКУРЕНТНОСТИ несколько задач работают в течении одного промежутка времени, но только одна активна в каждый
 момент.
 В случае ПАРАЛЛЕЛИЗМА несколько задач активно одновременно.

 В случае КОНКУРЕНТНОСТИ мы переключаемся между двумя приложениями.
 В случае ПАРАЛЛЕЛИЗМА мы активно выполняем два приложения одновременно.


 Примеры I/O-bound задач в Python:
 **Загрузка данных из веб-API**:            - ожидание ответа от сервера.
 **Чтение и запись файлов**:                - работа с файлами на диске
 **Взаимодействие с базами данных**:        - выполнение запросов и получение данных.
 **Отправка и получение данных по сети**:   - работа с сокетами и сетевыми протоколами.

  Для I/O-bound задач особенно эффективны многопоточность и асинхронное программирование, так как они позволяют
 выполнять другие задачи, пока происходит ожидание.


 --- Многопоточность (multithreading) ---
 multithreading - многопоточность, подходит для IO-bound задач, использует ОС, страдает от GIL(важно помнить)
 Полезно для ускорения выполнения задач для того, чтобы текущий поток занялся другой задачей
 Любая программа это минимум один процесс и один поток
 Полезно использовать daemon=True (чтобы остальные потоки не висели), Queue(очереди), pool executor,
 НО в любом случае все зависит от программиста!

 Плюсы:
 + просто(сравнительно)
 + быстро
 + не умирает из-за одного(!)

 Минусы:
 - потребление ресурсов(ОС)
 - неуправляемость(старт, приостановка, переключение)
 - проблема потоков(гонка, блокировки)


  Race condition (состояние гонки) - это ситуация, при которой несколько ПОТОКОВ (ИЛИ ПРОЦЕССОВ) одновременно пытаются
  выполнить операции чтения или записи к общим ресурсам без должной синхронизации.

  Состояние гонки могут возникать, когда два потока одновременно обращаются к одному обьекту Python

  Если два потока одновременно увеличивают счетчик ссылок, то может случиться, что счетчик обнулится, хотя обьект еще
  используется.

  Состояние гонки, возникающее, когда два потока одновременно пытаются увеличить счетчик ссылок.


 -- Модуль concurrent.futures --
 concurrent.futures — Запуск параллельных задач, Параллельное выполнения задач в разных процессах или потоках

 - при использовании класса ThreadPoolExecutor, задачи выполняются в потоках;
 - при использовании класса ProcessPoolExecutor, задачи выполняются на ядрах процессора;
 - оба класса реализуют одинаковый API-интерфейс, который определяется абстрактным классом concurrent.futures.Executor,
 поэтому приложения могут переключаться между потоками и процессами с минимальными изменениями;
 - легко интегрируется в модуль asyncio, для запуска блокирующих операции в отдельных потоках/процессах.

 Класс ThreadPoolExecutor() модуля concurrent.futures -  Создает пул потоков для асинхронного выполнения вызовов
 ThreadPoolExecutor(max_workers=None) - использует пул не более max_workers потоков для асинхронного выполнения вызовов.

 Класс ProcessPoolExecutor() модуля concurrent.futures - Создает пул из ядер процессора для асинхронного выполнения вызовов
 ProcessPoolExecutor(max_workers=None) - использует пул не более чем max_workers ядер процессора для асинхронного выполнения вызовов.

 ProcessPoolExecutor в Python использует СЕМАФОР для управления количеством одновременно работающих процессов.
 Семафор ограничивает количество потоков или процессов, которые могут одновременно выполнять определенные операции,
 что помогает предотвратить чрезмерное использование ресурсов системы.

 Семафор — это синхронизирующий объект, который используется для управления доступом к общему ресурсу в многопоточной
 или многопроцессорной среде. Он может быть представлен как счетчик, который указывает, сколько потоков или процессов
 могут одновременно выполнять определенную задачу. Если счетчик достигает нуля, остальные потоки или процессы должны
 ожидать, пока другие освободят ресурсы.


 Многопоточность достигается модулем Threading.
 Это нативные Posix-треды. Такие треды исполняются операционной системой, а не виртуальной машиной.
 from threading import Thread


 -- fork spawn --

 - fork (Linux/macOS): новый процесс создаётся как “копия” текущего — быстрее старт, наследует память/состояние
 (но из-за наследования возможны тонкие баги с потоками/ресурсами).

 - spawn (Windows): новый процесс стартует “с нуля” — медленнее, ничего лишнего не наследует; поэтому нужно,
  чтобы код был импортируемым и был if __name__ == "__main__":.


  -- Модуль joblib --
 Модель joblib использует библиотеку Loky, которая является улучшенной версией concurrent.futures,
 и применяет модуль cloudpickle для сериализации функций, определенных в интерактивной оболочке,
 что позволяет их использовать в многопроцессорных вычислениях.

 from joblib import Parallel, delayed

 def my_function(x):
     return x * x

 # Параллельное выполнение функции с использованием 4 процессов
 results = Parallel(n_jobs=4)(delayed(my_function)(i) for i in range(10))

 print(results)  # -> [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]



 --- Greenlet == Green thread == Зеленые треды ---
 Что такое гринлеты. Общее понятие. Примеры реализаций
 Greenlet == Green thread == Зеленые треды == легковесные треды внутри виртуальной машины. Могут называться корутинами,
 сопроцессами, акторами и т.д. в зависимости от платформы. Операционная система не видит их.
 С точки зрения ОС запущен один процесс виртуальной машины, а что внутри нее – неизвестно.
 Такими тредами управляет сама вируальная машина: порождает, исполняет, согласует доступ к ресурсам.

 Треды в Питоне — это нативные треды или нет
 Да, это (native threads) нативные Posix-совместимые треды, которые исполняются на уровне операционной системы.
 POSIX (англ. Portable Operating System Interface — переносимый интерфейс операционных систем)
 Стандарт POSIX определяет интерфейс операционной системы
 Стандарты, такие как POSIX, позволяют легко перемещаться между различными операционными системами.
 это стандарт, описывающий интерфейс между операционной системой и прикладной программой. Цель создания этого стандарта
 – обеспечение совместимости unix-like операционных систем, а также переносимости программ на уровне исходного кода.

 В чем отличие тредов от мультипроцессинга?
 Главное отличие в разделении памяти. Процессы независимы друг от друга, имеют раздельные адресные пространства,
 идентификаторы, ресурсы. Треды исполняются в совместном адресном порстранстве,
 имеют общий доступ к памяти, переменным, загруженным модулям.


 Многопоточность - имеет смысл использовать только для блокирующего ввода-вывода. GIL не будет мешать.
 Если ЗНАЧИТЕЛЬНАЯ часть библиотеки написаны на C например hashlib, Numpy. - GIL не будет мешать.

 Numexpr — это библиотека Python, которая позволяет эффективно выполнять выражения с использованием NumPy.
 Установка библиотеки NumExpr УСКОРЯЕТ выполнение функции eval в Pandas, но библиотека Pandas не сообщает пользователю
 о необходимости установки NumExpr.                              ВАЖНО ПРО функцию eval          <-----   <-----

 Помните, что Pandas без NumExpr выполняет функцию eval крайне медленно!
 NumExpr оптимизирует вычисления и использует многоядерные процессоры, что значительно ускоряет операции с массивами данных.

 # В этом примере, если NumExpr установлен, выполнение df.eval будет быстрее.                    <-----   <-----
 import pandas as pd
 import numpy as np

 # Создание DataFrame
 df = pd.DataFrame({
     'a': np.random.rand(1000000),
     'b': np.random.rand(1000000)
 })

 # Использование eval без NumExpr
 result_without_numexpr = df.eval('c = a + b')

 # Использование eval с NumExpr
 result_with_numexpr = df.eval('c = a + b')  # NumExpr будет использован автоматически, если установлен


 -- Потоки (multithreading) vs Процессы (multiprocessing) --

 Потоки   - лучше для задач, где много ожидания (сеть, файлы, базы данных).
 Процессы - лучше для тяжелых вычислений (математика, обработка данных).

 Потоки (threads) в операционных системах — это механизм, который позволяет выполнять несколько задач в рамках
 ОДНОГО процесса, разделяя его ресурсы (память, файлы и т. д.).

 Ключевые особенности:

 - Потоки легче процессов, создаются быстрее.
 - Все потоки процесса используют общее адресное пространство.
 - Переключение между потоками требует меньше затрат, чем между процессами.
 - Подходят для I/O-задач, но могут быть неэффективны для CPU-задач из-за GIL (в Python).

 В отличие от процессов (multiprocessing):

 - Процессы изолированы, имеют отдельную память.
 - Подходят для CPU-интенсивных задач (особенно в Python).
 - Создание и управление процессами требует больше ресурсов.

 Потоки - для параллельных I/O-операций в одном процессе, процессы — для истинного параллелизма на многоядерных CPU.


 Критерий	        | Потоки (Threads)	                     | Процессы (Processes)            |
 -------------------|----------------------------------------|---------------------------------|
 Общая память	    | Да (разделяют)	                     | Нет (изолированы)               |
 Скорость создания	| Быстро	                             | Медленнее                       |
 Переключение	    | Дешевле	                             | Дороже                          |
 Параллелизм	    | Псевдопараллелизм (из-за GIL в Python) | Истинный параллелизм            |
 Использование	    | I/O-bound задачи (сеть, диски)	     | CPU-bound задачи (вычисления)   |
 -------------------|----------------------------------------|---------------------------------|

 Дополнение про Python:
 - GIL (Global Interpreter Lock) в CPython мешает потокам выполняться по-настоящему параллельно на многоядерных CPU.
 - Multiprocessing обходит GIL, создавая отдельные интерпретаторы Python для каждого процесса.


  --- Стек вызовов  call stack ---
  — это структура данных, которая управляет вызовами функций во время выполнения программы.
 Если программа пытается разместить на стеке больше данных, чем он может вместить, происходит переполнение стека.
 Когда стек переполняется, он не может больше хранить элементы, и любая попытка добавления нового элемента приведет к ошибке
 на дне стека есть функция module(ИСПОЛНЯЕТ НАШИ ЗАПРОСЫ) 1) когда мы вызываем функцию она попадает в СТЕК.
 2) когда функция завершается она снимается со СТЕКА
 Все функции внутри стека вызова исполняются но только одна функция выполняется реально(которая на верхушке стека)
 а остальные функции ждут соседа сверху пока он исполнится
 СТЕК ВЫЗОВОВ ОТОБРАЖАЕТ ВСЕ ФУНКЦИИ КОТОРЫЕ ИСПОЛНЯЮТСЯ В ДАННЫЙ МОМЕНТ И КТО КОГО ЖДЕТ

 Стек (stack) — это область памяти, в которой программа хранит информацию о вызываемых функциях, их аргументах и
 каждой локальной переменной в функциях. Размер области может меняться по мере работы программы. При вызове функций
 стек увеличивается, а при завершении — уменьшается.

 Стек — это область оперативной памяти, которая создаётся для каждого потока. Он работает в порядке
 LIFO (Last In, First Out), то есть последний добавленный в стек кусок памяти будет первым в очереди на вывод из стека.

 Очереди похожи на стеки, и разница между ними в том, как удаляются элементы.
 stack - принцип «последним пришел — первым ушел», или LIFO
 queue - принцип «первым пришел — первым ушел», или FIFO есть и другие варианты помимо FIFO

 По умолчанию размер стека в Питоне ограничен 1000 вызовов.


 -- Модуль queue, очереди в Python --
 import queue, from queue import Queue

 Модуль queue реализует очереди с несколькими производителями и несколькими потребителями. Это особенно полезно в
 многопоточном программировании, когда необходимо безопасно обмениваться информацией между несколькими потоками.
 Класс Queue в этом модуле реализует всю необходимую семантику блокировки.

 Модуль реализует три типа очереди, которые отличаются только порядком, в котором извлекаются записи:

 1) В очереди FIFO первые добавленные задачи являются первыми извлеченными.  queue.Queue()
 2) В очереди LIFO самая последняя добавленная запись является первой извлеченной (работающей как СТЕК). queue.LifoQueue()
 3) В очереди с приоритетами записи сохраняются отсортированными с использованием модуля heapq(очереди с кучей) и сначала
    извлекается запись с наименьшим значением.  queue.PriorityQueue()

 1) Очередь FIFO - Класс queue.Queue():
 Класс queue.Queue() реализует базовый контейнер типа FIFO - "первым пришел - первым вышел". Элементы добавляются
 к одному концу очереди с помощью метода put(), а удаляются с другого конца с помощью метода get().

 2) Очередь LIFO - Класс queue.LifoQueue():
 В отличие от стандартной реализации очереди FIFO, в queue.LifoQueue() используется порядок
 "последним пришел - первым вышел", который обычно связан со структурой данных стека.

 3) Очередь с приоритетом - Класс queue.PriorityQueue():
 Иногда порядок обработки элементов в очереди должен основываться на характеристиках этих элементов, а не только на
 порядке их создания или добавления в очередь. Например, задания на печать из финансового отдела могут иметь приоритет
 над списком заданий из отдела технической поддержки. Класс модуля queue.PriorityQueue() использует порядок сортировки
 содержимого очереди, чтобы решить, какой элемент получить.


 Внутренне эти три типа очередей используют блокировки для временного блокирования конкурирующих потоков,
 однако они НЕ предназначены для обработки повторного входа в поток.

 Кроме того, модуль реализует простой тип очереди FIFO - queue.SimpleQueue(), специфическая реализация которого
 обеспечивает дополнительные гарантии в обмен на меньшую функциональность.
 Очередь FIFO без отслеживания задач queue.SimpleQueue():
 Класс SimpleQueue() модуля queue представляет собой конструктор для простой неограниченной очереди FIFO.

 -- Класс Lock() модуля threading в Python --
 Синхронизация потоков при помощи блокировок
 Для защиты от одновременного доступа к объекту нескольких потоков используйте объект threading.Lock().

 Методы объекта threading.Lock:
 Lock.acquire() -  устанавливает блокировку
 Lock.release() -  снимает блокировку
 Lock.locked()  -  проверяет состояние блокировки


 -- Lock vs Semaphore --

 Lock (блокировка):
 - Позволяет одному потоку выполнять критическую секцию.
 - Как mutex — владелец один, другие ждут.

 Semaphore (семафор):
 - Ограничивает количество потоков в критической секции.
 - Например, 3 потока могут работать одновременно.

 Коротко:
 Lock — 1 поток, Semaphore — N потоков.

 Когда что использовать?
 - Lock - когда нужно гарантировать эксклюзивный доступ к ресурсу (только один поток)
 - Semaphore - когда нужно разрешить доступ ограниченному числу потоков одновременно


 -- подключение к БД НУЖНО ИСПОЛЬЗОВАТЬ ОЧЕРЕДЬ(queue) --
 запросы добавляются в очередь для обработки в отдельном потоке. Это позволяет избежать блокировок

 ПЛЮСЫ:
 Асинхронная обработка: Запросы могут обрабатываться асинхронно, что позволяет основной логике приложения продолжать
 работу, не дожидаясь завершения операций с базой данных.

 Улучшение производительности: Очередь позволяет разграничить нагрузку на базу данных, что может помочь избежать ее перегрузки.

 Устойчивость к сбоям: В случае сбоя в обработке запроса, его можно повторить, не влияя на остальную работу приложения.

 Гибкость: Легче управлять очередью и изменять детали обработки запросов, такие как количество потоков или алгоритмы обработки.


 --- multiprocessing (многопроцессорность) ---
 multiprocessing - использование нескольких ядер
 Породить процессы с помощью multiprocessing.Pool. Число, которое будет передано в Pool(),
 будет равно числу порожденных процессов
 import multiprocessing, from multiprocessing import Process, Pool

 --- Multiprocessing выдуман чтобы побороть GIL! Как его побороть? - Сделать много GIL каждый из которых независимый ---
 --- Multiprocessing обходит GIL, создавая отдельные процессы, у каждого СВОЙ ИНТЕРПРЕТАТОР И СВОЙ GIL. <---- ВАЖНО
 --- Multiprocessing работает только с обьектами которые поддерживают сериализацию через pickle
 Но не все объекты можно сериализовать (например, файловые дескрипторы, соединения с БД и т. д.).
 В других ЯП: параллельность вычислений достигается с помощью 2-х средств - ПОТОКИ или asyncio. Потому что у них нет GIL
 Multiprocessing - позволяет решать любые задачи (IO-bound или CPU-bound)
 Ускорение любые задачи, распараллеливая их на ядра процессора (лишь до определенного предела, закон Амдала)
 закон Амдала - с какого-то момента бессмысленно добавлять дополнительных рабочих(ЦПУ, доп.ядра) быстрее не станет
 Ускорение не идеально и возможно только до определенного предела, смотрим закон Амдала.
 Создает несколько процессов, у каждого из которых своя память и свой GIL, каждый выполняет свою задачу, взаимодействие
 между ними требует pickle(сериализация, превращение обьектов python в байты и наоборот байтов в обьекты python)
 API принципиально похоже на многопоточность, выгодно использовать Pool, а для взаимодействия между процессами Queue и Pipe
 Pipe - когда нужно 2 процесса между собой подружить чтобы они передавали друг другу данные.

 закон Амдала: ускорение упирается в ту часть работы, которую нельзя распараллелить.
 Закон Амдала: ускорение ограничено частью кода, которую нельзя распараллелить.
 Пример: если 20% работы всегда последовательно, то максимум ускорения ≈ 1 / 0.2 = 5×, хоть 100 ядер.


 Плюсы:
 + реальная параллельность любых задач
 + не умирает из-за одного(!)
 + процессы не зависят друг от друга(у каждого процесса своя память и GIL)
 Минусы:
 - потребление ресурсов (памяти, процессора, времени)
 - необходимость сериализации в pickle
 - проблемы синхронизации (взаимодействие между процессами)


 # конструкция if __name__ == "__main__": критически важна не только для multiprocessing, но и для
 # concurrent.futures.ProcessPoolExecutor, поскольку он тоже использует механизм порождения процессов (аналогично multiprocessing).

 # if __name__ == "__main__": для защиты от рекурсии.


 -- ЦЕЛЬ СЕРИАЛИЗАЦИИ с помощью pickle в multiprocessing --

 ЦЕЛЬ СЕРИАЛИЗАЦИИ с помощью pickle в multiprocessing заключается в том, чтобы преобразовать объекты Python в формат,
 который можно сохранить в файл или передать между процессами. Это необходимо, поскольку каждый процесс в
 multiprocessing имеет своё собственное пространство памяти. Сериализация позволяет копировать данные и передавать их,
 обеспечивая тем самым возможность взаимодействия между процессами.


 --- (ОС vs Python) ---     <----------

 ЧТО БЫСТРЕЕ: СИНХРОННАЯ ИЛИ АСИНХРОННАЯ ЗАПИСЬ В ФАЙЛ?
 ОС = операционная система (Windows / Linux / macOS)

 1) ОС (синхронная/асинхронная запись на диск)   АСИНХРОННАЯ/БУФЕРИЗОВАННАЯ (через кэш ОС) - БЫСТРЕЕ!!!

 - write() без fsync/O_SYNC → асинхронная/буферизованная (через кэш ОС) (без гарантии на диск): ОС кладёт в кэш и сразу возвращает.
 - write() + fsync() или O_SYNC → синхронная (с гарантией на диск): ждёшь, пока запишется на SSD/HDD.

 2) Python (синхронный код vs asyncio):          СИНХРОННЫЙ - БЫСТРЕЕ!!!

 - Синхронный Python write() для ОДНОГО файла часто быстрее (меньше накладных).
 - Асинхронный Python (asyncio/aiofiles) диск не ускоряет — просто не блокирует при многих операциях.


 --- Основы асинхронного программирования в Python ---
 Асинхронное программирование позволяет запускать операции параллельно, НЕ дожидаясь выполнения последовательности.

 Кроме того, в асинхронном коде используются конструкции await и async with,
 которые позволяют приостановить выполнение текущей задачи и переключиться на другую,
 пока выполняется асинхронная операция. Это позволяет эффективно использовать ресурсы процессора и снизить время ожидания.

 --- asyncio ---
 asyncio - Асинхронное выполнение, подходит для IO-bound задач, работает ровно 1 поток - использует Конкурентность по полной
 Плюсы:
 + скорость и экономия времени, вместо x + y + z -> max(x, y, z)
 + управляемость
 + меньше потребление ресурсов (в сравнении с потоками)
 Минусы:
 - "умирает" из-за одного блокирующего вызова (!)
 - event loop НЕ безразмерный, нужно понимать, что корутины не бесплатные

 1) корутина работает как генератор
 2) async - явный флаг, что данная функция является асинхронной (корутиной)
 3) await - явный флаг, что в этом месте функция встает на паузу и дает работать другим, пока ждёт свои данные
 4) event loop - цикл событий, механизм, который отвечает за планирование и запуск корутин. Можно представить как
 список/очередь, из которого в вечном цикле достаются и запускаются корутины
 Частые ошибки:
 - не использование await внутри корутины
 - создание корутины, но использование ее, как функции
 - использование внутри корутин синхронного(блокирующего) кода, в том числе IO


 asyncio.sleep(0) приостанавливает выполнение текущей корутины, позволяя циклу событий выполнить другие задачи.
 Это полезно для перераспределения управления между корутинами без фактической задержки.



 -- Синхронные задачи в EVENT LOOP --

 Если в asyncio попадает синхронная (блокирующая) задача (например, CPU-задача или расчеты), то она блокирует Event Loop,
 пока не завершится. Это значит, что другие асинхронные задачи не смогут выполняться в это время, и весь цикл событий "зависнет".

 Почему так???
 Event Loop блокируется, потому что синхронный код не отдаёт управление.

 Event Loop в asyncio работает в одном потоке и выполняет задачи по очереди. Если попадает синхронная (блокирующая) задача,
 она не отдаёт управление обратно в цикл событий, пока не завершится. В результате другие асинхронные задачи просто НЕ
 получают шанса запуститься, пока она не закончит работу



 -- asyncio.gather vs asyncio.TaskGroup --

 gather – просто запускает корутины параллельно. Если одна упадет, остальные работают дальше.

 TaskGroup (Python 3.11+) – отменяет все задачи, если хоть одна сломалась. Удобнее для сложных сценариев.

 Пример:
 # gather (просто)
 await asyncio.gather(task1(), task2())

 # TaskGroup (надёжнее)
 async with asyncio.TaskGroup() as tg:
     tg.create_task(task1())
     tg.create_task(task2())

 Выбор:

 gather – если не важно падение отдельных задач.
 TaskGroup – если нужна автоматическая отмена при ошибках.


 --- asyncio с многопоточностью ---
 Можно ли совмещать?
 Да, через loop.run_in_executor(), который запускает блокирующий код в отдельном потоке, не ломая асинхронность.

 Это НЕ делает код асинхронным, а лишь избегает блокировки event loop.                        <-----              <-----

 Когда использовать?
 Если есть блокирующий код (например, requests, time.sleep, файловые операции).

 Если нужно параллельно выполнять CPU-bound задачи (но лучше тогда ProcessPoolExecutor).

 Важно:
 Сам asyncio работает в одном потоке, но run_in_executor позволяет вынести тяжелые операции в фоновые потоки.

 Для чистого asyncio лучше использовать асинхронные библиотеки (aiohttp, aiofiles и т. д.).

 Если нужно много потоков — можно передать свой ThreadPoolExecutor:

 with ThreadPoolExecutor(10) as pool:
     await loop.run_in_executor(pool, blocking_task)

 Если хочешь максимальной производительности — везде, где можно, используй чистый asyncio. Потоки — это "костыль" для старого кода


 --- Asyncio с многопоточностью — итоговый ответ ---

 1. Можно ли совмещать asyncio и потоки?
 Да, через:

 await loop.run_in_executor(ThreadPoolExecutor(), blocking_func)

 - Запускает синхронный (блокирующий) код в потоке, не блокируя Event Loop.
 - Не делает код асинхронным — это просто "обёртка" для совместимости.

 2. Когда это использовать?
 Блокирующий I/O (если нет асинхронных аналогов):

 - requests → лучше aiohttp,
 - time.sleep → лучше asyncio.sleep,
 - синхронные файловые операции → лучше aiofiles.

 CPU-bound задачи → но лучше ProcessPoolExecutor (потоки в Python не дают реального ускорения из-за GIL).

 3. Важно!
 - Asyncio работает в одном потоке — потоки лишь помогают не блокировать его.
 - Потоки — не замена asyncio, а временное решение для старого кода.
 - Для максимальной производительности всегда предпочитайте нативные асинхронные библиотеки (aiohttp, aiofiles и т. д.).

 4. Пример

 import asyncio
 from concurrent.futures import ThreadPoolExecutor

 def sync_blocking_operation():
     print("Выполняется в потоке")

 async def main():
     loop = asyncio.get_running_loop()
     with ThreadPoolExecutor(4) as pool:  # Пул из 4 потоков
         await loop.run_in_executor(pool, sync_blocking_operation)

 asyncio.run(main())

 Итог
 Да, можно, но:

 - Потоки — это "костыль" для совместимости со старым кодом.
 - Лучший вариант — чистый asyncio (если есть асинхронные аналоги).
 - Для CPU-bound задач — процессы (ProcessPoolExecutor).


 -- Как работает Event Loop в asyncio --

 Event Loop — Как лифт без маршрута:
 Не едет по порядку (1 → 2 → 3...), а реагирует на события (как кнопки вызова в лифте)

 - Едет по этажам (задачам) циклически, проверяет:
 - Готово? (например, данные пришли или таймер сработал) → выполняет кусок кода.
 - Не готово? (ждёт I/O, sleep и т.д.)                   → пропускает и едет дальше.
 - РАБОТАЕТ пока есть активные задачи

 Где "живут" задачи?
 - В очереди событий (Event Queue), которую Event Loop постоянно проверяет.

 Ключевая фишка:
 - Не блокирует поток на ожидании I/O. Пока одна задача "спит" (await), Event Loop выполняет другие.


 -- Event Loop в потоках и asyncio --

 - Потоки: У каждого потока может быть свой event loop (например, в GUI-фреймворках), но они работают параллельно,
   управляются ОС, подвержены блокировкам.

   Однако стандартные потоки (threading) в Python обычно не используют event loop — они просто выполняют код параллельно
   под управлением ОС.

 - asyncio: Один event loop в главном потоке, кооперативная многозадачность (корутины добровольно отдают контроль),
   нет блокировок ОС.

 asyncio не заменяет потоки для CPU-задач (там нужны процессы/threads).
 Главное отличие: asyncio — это однопоточный асинхронный подход, а потоки — многопоточный с параллельным выполнением.


 - Потоки не используют asyncio-loop, если только не интегрированы специально (например, asyncio.run_coroutine_threadsafe()).
 - Asyncio не даёт настоящего параллелизма (только асинхронность в одном потоке).
 - Для CPU-задач действительно нужны процессы или вынос вычислений в отдельные потоки (с помощью loop.run_in_executor()).


 -- Вытесняющая и Кооперативная многозадачность --

 Вытесняющая и кооперативная многозадачность
 (По книге Мэтью Фаулера "Конкурентность в Python" и в контексте asyncio)

 - Вытесняющая многозадачность   – ОС принудительно переключает потоки (как в threading), даже если задача не завершена.
 - Кооперативная многозадачность – задачи добровольно уступают управление (как в asyncio через await).


 Ключевые различия в asyncio:

 - Asyncio использует кооперативную модель: корутины работают, пока не встретят await, иначе блокируют event loop.
 - Threading (вытесняющая) – переключение управляется ОС, возможны гонки данных, но нет блокировки из-за одной задачи.

 (Фаулер подчёркивает, что asyncio эффективен для I/O-задач, так как переключение происходит только в точках await.)

 - Вытесняющая многозадачность – ОС принудительно переключает задачи, даже если они не завершены.
 - Кооперативная многозадачность – задачи сами передают управление, иначе работают бесконечно.

 (Коротко: в первой ОС решает, во второй – программы.)



 -- uvloop  VS   asyncio event loop --

 1. Скорость
 - uvloop в 2–4 раза быстрее asyncio (использует libuv, как Node.js).
   Для файлового I/O разница меньше (если не используется aiofiles).
 - uvloop быстрее особенно для сетевых операций (TCP/UDP/Unix sockets).
 - asyncio — стандартная реализация, медленнее, но надежнее.

 2. Совместимость
 - uvloop не поддерживает Windows в полной мере (лучше работает на Linux/macOS).
 - asyncio — полностью кроссплатформенный.
 - uvloop почти полностью совместим с asyncio API.

 Просто добавить:

 import uvloop
 uvloop.install()  # заменяет стандартный event loop

 # Лучше использовать явную политику (более надежно)
 import uvloop
 asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

 3. Где использовать?
 - uvloop  — для высоконагруженных сетевых приложений (WebSocket, HTTP-серверы).
 - asyncio — если важна кроссплатформенность и простота отладки.


 Где использовать?

 uvloop — идеален для:

 - Async HTTP-серверов (FastAPI, Sanic, aiohttp)
 - Высоконагруженных WebSocket-серверов
 - Proxy, ботов с тысячами соединений

 asyncio — если:

 - Нужна поддержка Windows
 - Проект небольшой или тестовый
 - Используются специфические event loop политики

 Вывод:
 - Если нужна максимальная скорость — uvloop, если стандартный функционал — asyncio.
 - Если нужна макс. производительность (Linux/macOS) → uvloop
 - Если важна универсальность (Windows, простота) → asyncio



 -- Когда Потоки (threads) могут быть лучше asyncio --

 Потоки лучше asyncio, когда:

 - Много блокирующего I/O (например, requests, файлы, БД без async).
 - CPU-bound задачи (тяжёлые вычисления → ThreadPoolExecutor).
 - Очень много задач (десятки тысяч → event loop тормозит).
 - Код синхронный и переписывать сложно.
 - Библиотеки без async (например, pandas, numpy).

 asyncio лучше, когда:

 - Чистый I/O (веб-запросы, асинхронные БД).
 - Тысячи соединений (потоки жрут память).
 - Код уже async.
 - Комбинируй: asyncio + run_in_executor() для CPU-bound.



 ### ПОТОКИ VS ASYNCIO
 Да — для твоего кейса (5 городов, пройтись по всем и выбрать лучший по эвристике+API) потоковый вариант — самый практичный.

 Почему потоки лучше здесь
 Nominatim-чек — блокирующий I/O. Потоки параллелят запросы без плясок с event loop’ом и без «coroutine was never awaited».
 Мы держим общий RPS через токен-бакет, так что быстро, но без банов.
 Код остаётся синхронным и проще дебагать.


 -- Основные БД и асинхронные драйверы для asyncio --

 Вот основные БД, поддерживающие асинхронные запросы в asyncio:

 PostgreSQL: asyncpg, aiopg
 MySQL: aiomysql
 SQLite: aiosqlite
 MongoDB: motor
 Redis: aioredis (или redis с async-режимом)
 ClickHouse: aioch
 SQLAlchemy (асинхронный режим): sqlalchemy[asyncio]

 Коротко:
 - Драйверы: asyncpg, aiomysql, aiosqlite, motor, aioredis, aioch.
 - Библиотеки: aiopg, SQLAlchemy async.


 -- Блокирующие API --

 Примеры блокирующих API:
 библиотека requests или функция time.sleep

 Вообще, любая функция, которая выполняет ввод-вывод, не-являясь сопрограммой, или занимает процессор длительными
 операциями, может считатся блокирующей.

 библиотека requests - Блокирующая т.е. блокирует поток, в котором выполняется. Поскольку asyncio ОДНОПОТОЧНАЯ,
 requests блокирует цикл событий и не дает ничему выполняться конкурентно.

 Чтобы использовать async c requests  Нужно Указать asyncio задействовать многопоточность с помощью исполнителя ПУЛА  ПОТОКОВ

 Большинство API с которыми мы работаем, в настоящий момент являються БЛОКИРУЮЩИМИ и без ДОРАБОТОК РАБОТАТЬ с asyncio
 НЕ БУДУТ!
 Нужно использовать библиотеки, которые поддерживают СОПРОГРАММЫ и НЕБЛОКИРУЮЩИЕ сокеты.
 Можно использовать aiohttp - в которой используются НЕБЛОКИРУЮЩИЕ сокеты и которая возвращает СОПРОГРАММЫ!


 **Блокирующий сокет** — это сокет, который при выполнении операций останавливает выполнение программы до их завершения.
 Если нет данных для чтения или операция не может быть выполнена, программа "зависает".

 **Неблокирующий сокет** — это сокет, который не останавливает выполнение программы. Если операция не может быть
 выполнена мгновенно (например, нет данных), она завершится неудачей, и программа может продолжать свою работу или
 пытаться повторить операцию позже.

 Таким образом, блокирующий сокет прост в использовании, но может замедлить программу, тогда как неблокирующий позволяет
 более гибко управлять потоком выполнения, но требует более сложной обработки ошибок.


 -- namedtuple, dataclasses и typing.NamedTuple    Построители классов данных  --

 - namedtuple: легковесные объекты с именованными полями.
 - dataclass: классы с автоматическим созданием метода __init__ и другими.
 - typing.NamedTuple: именованные кортежи с поддержкой аннотаций типов.

 Примеры:

 # dataclass                              # typing.NamedTuple                    # namedtuple
 from dataclasses import dataclass        from typing import NamedTuple          from collections import namedtuple

 @dataclass                               class Point(NamedTuple):               Point = namedtuple('Point', ['x', 'y'])
 class Point:                                 x: int
     x: int                                   y: int
     y: int

 p = Point(10, 20)                        p = Point(10, 20)                      p = Point(10, 20)
 Point.__doc__ # Point(x: int, y: int)    print(Point.__doc__)  # Point(x, y)    print(Point.__doc__)  # Point(x, y)
 print(p)  # Point(x=10, y=20)            print(p.x, p.y)  # 10 20               print(p.x, p.y)  # 10 20

 print(Point.__annotations__)               print(Point.__annotations__)                print(Point.__annotations__)
 # {'x': <class 'int'>, 'y': <class 'int'>} # {'x': <class 'int'>, 'y': <class 'int'>}  # {}



 -- Задачи от ВИТАЛИЯ  ПРОСТО ПОСМОТРЕТЬ --

 # 1) Какой объект будет создан в obj?
 obj = {True: 1, 1: 2, 1.0: 3, "1": 4}
 print(obj)  # -> ???



 # ОТВЕТ
 print(obj)  # -> {True: 3, '1': 4}

 # Почему так?
 В Python ключи True, 1 и 1.0 считаются одинаковыми из-за их равенства (True == 1 == 1.0),
 поэтому они перезаписывают друг друга в словаре.

 Порядок перезаписи:

 True: 1 → {True: 1}
 1: 2 → {True: 2} (так как 1 == True)
 1.0: 3 → {True: 3} (так как 1.0 == True)
 "1": 4 → {True: 3, "1": 4} (строка "1" – другой ключ)



 # 2) Что программа выведет на экран?
 def function(list: list = []):
     list.append(1)
     print(list)

 list = []
 function()      # -> ???
 function(list)  # -> ???
 function()      # -> ???
 print(list)     # -> ???



 # ОТВЕТ
 function()      # -> [1]
 function(list)  # -> [1]
 function()      # -> [1, 1]
 print(list)     # -> [1]

 # function() (без аргумента) – использует общий дефолтный список ([] при создании функции). После append(1) он становится [1].

 # function(list) – работает с переданным списком (list = []), меняя его на [1].

 # Снова function() – снова использует дефолтный список (уже [1]), добавляет 1 → [1, 1].

 # print(list) – выводит внешний list ([1]), т.к. дефолтный список функции – это отдельный объект.

 # ИТОГ
 Дефолтный аргумент [] сохраняет состояние между вызовами.

 Внешний list и внутренний дефолтный – разные списки.



 # 3) Что выведет на экран следующий блок кода?
 for index in (10): # -> TypeError: 'int' object is not iterable
     print(index)



 # 4) Что выдаст такое действие ~5 в Python?
 print(~5) # -> ???



 # ОТВЕТ
 print(~5) # -> -6

 # ~5 вернёт -6, потому что побитовая инверсия в Python работает по формуле:
 # ~x = -x - 1
 # ~5 = -5 - 1 = -6

 # Это эквивалентно инверсии всех битов числа 5 в двоичном представлении.



 # 5) Что выведет на экран следующий блок кода?

 print(1 and 2 and 3)  # -> ???




 # ОТВЕТ
 print(1 and 2 and 3)  # -> 3
 # В Python оператор and возвращает последнее истинное значение, если все операнды истинны.



 # 6) Многие валятся на стандартных методах. Проверим?

 set1 = {1, 2, 3}
 set2 = set1.add(4)
 print(set2)  # -> ???



 # ОТВЕТ
 print(set2)  # -> None

 # Он не возвращает новое множество, а возвращает None (как и большинство методов-мутаторов в Python).



 # 7) Попробуем ещё что-нибудь простенькое?

 print(True == 1 > 0)  # -> ???
 print(True == 0 < 1)  # -> ???



 # ОТВЕТ
 print(True == 1 > 0)  # -> True (т.к. True == 1 и 1 > 0)
 print(True == 0 < 1)  # -> False (т.к. True != 0, хоть 0 < 1)

 # Коротко:
 # True == 1 > 0 → (True == 1) and (1 > 0) → True and True  → True
 # True == 0 < 1 → (True == 0) and (0 < 1) → False and True → False



 # 8) Какие значения выдаст следующий код?

 print(all([]))      # -> ???
 print(all([[]]))    # -> ???
 print(all([[[]]]))  # -> ???



 # ОТВЕТ
 print(all([]))      # -> True  (пустой итерируемый)
 print(all([[]]))    # -> False (внутри есть пустой список [])
 print(all([[[]]]))  # -> True  (внутри непустой список [[]])



 # 9) Что выведет данный блок кода?

 a = 123
 b = 123
 print(a == b)  # -> ???
 print(a is b)  # -> ???



 # ОТВЕТ
 print(a == b)  # -> True (значения равны)
 print(a is b)  # -> True (один и тот же объект из-за кэширования)



 # 10) Что выведет данный блок кода?

 a = 1230
 b = 1230
 print(a == b)  # -> ???
 print(a is b)  # -> ???



 # ОТВЕТ
 print(a == b)  # True
 print(a is b)  # -> True # Зависит от контекста выполнения:
                # - Может быть True в одном .py файле или REPL из-за оптимизации
                # - Может быть False в других случаях (особенно для чисел > 256)



 # 11) Что выведет данный блок кода?

 def create_mult():                                 def create_mult():
     return [lambda x: i * x for i in range(3)]         return [lambda x, i=i: i * x for i in range(3)]

 # Все лямбды используют последнее значение i (2)   # Передать i как аргумент по умолчанию (чтобы зафиксировать значение)

 for mult in create_mult():                         for mult in create_mult():
     print(mult(3), end=' ')  # -> 6 6 6                print(mult(3), end=' ')  # -> 0 3 6



 # 12) Что выведет такой блок кода?

 True = False  # -> SyntaxError: cannot assign to True
 if True:
     print("Тrue is Тrue")
 else:
     print("Тrue is False")

 # True = False — запрещено (как и любое присваивание встроенным литералам True, False, None и т.д.).    keyword

 # Проверка на ключевые слова
 import keyword
 print(keyword.kwlist)  # Выведет полный список всех ключевых слов Python.
 print(keyword.iskeyword('None'))  # -> True



 # 13) Что будет в lst?

 lst = [1,5,6]
 for item in [4,3,2]:
     lst.insert(2,item)
 print(lst)  # -> ???



 # ОТВЕТ
 print(lst)  # -> [1, 5, 2, 3, 4, 6]

 # Изначально: [1, 5, 6]

 # Вставляем 4 на позицию 2 → [1, 5, 4, 6]

 # Вставляем 3 на позицию 2 → [1, 5, 3, 4, 6]

 # Вставляем 2 на позицию 2 → [1, 5, 2, 3, 4, 6]



 # 14) Что будет выведено на экран?

 lst = [0, None, 1, [], 2, "", 3, [[]]]
 print(list(filter(None, lst)))  # -> ???



 # ОТВЕТ
 print(list(filter(None, lst)))  # -> [1, 2, 3, [[]]]

 # filter делает bool(item)
 print(bool([[[[[]]]]]))  # -> True (внешний список не пуст)
 print(bool([[]]))        # -> True (аналогично)
 print(bool([]))          # -> False (пустой список)

 # filter(None, lst) оставляет только элементы, для которых bool(item) == True

 # Удалены: 0, None, [], "" (так как они эквивалентны False).

 # Оставлены: 1, 2, 3, [[]] (потому что bool([[]]) == True).


 ### ChatGPT
 bool([[]]) == True, потому что внешний список не пуст.
 all([[]]) == False, потому что внутри него есть элемент [], а bool([]) == False.



 # 15) Вопрос о памяти?

 import sys
 print(sys.getsizeof(1))  # -> ???
 print(sys.getsizeof(0))  # -> ???



 # ОТВЕТ
 print(sys.getsizeof(1))  # -> 28
 print(sys.getsizeof(0))  # -> 28

 # Целое число в Python — это не просто 4 байта (как в C), а объект с накладными расходами.
 # 28 байт — это минимальный размер int, даже для 0 или 1 (из-за структуры PyObject в CPython).

 # Почему так много?

 # Каждый int в Python содержит:
 # - Ссылку на тип (ob_type)
 # - Счётчик ссылок (ob_refcnt)
 # - Само значение (ob_digit)
 # - Выравнивание (для оптимизации доступа).



 # 16) Что будет выведено на экран?

 lst = [1, 2, 3, 4, 5]
 for i in lst:
     print(i, end=' ')
     _ = lst.pop()

 # -> 1 2 3
 # Итерируем по списку и одновременно удаляем его элементы, что приводит к преждевременному завершению цикла после 3 шагов.



 # 17) Что будет выведено на экран?

 print(type((i for i in range(3))))  # -> ???
 print(type([i for i in range(3)]))  # -> ???



 # ОТВЕТ
 print(type((i for i in range(3))))  # -> <class 'generator'>
 print(type([i for i in range(3)]))  # -> <class 'list'>



 # 18) Интересный кейс про датаклассы и наследование. Что произойдёт, если запустим такую штуку?

 from dataclasses import dataclass
 @dataclass
 class Base:
   a: str

 class Child(Base):
   b: str

 Child(b=2, a=1)  # -> ???



 # ОТВЕТ
 Child(b=2, a=1)  # -> TypeError: Base.__init__() got an unexpected keyword argument 'b'

# Код упадёт с ошибкой, потому что Child НЕ dataclass и не генерирует свой __init__.
# Поэтому Child наследует Base.__init__(a), а он не принимает аргумент b.
# => TypeError: Base.__init__() got an unexpected keyword argument 'b'




 # 19) Что произойдёт, если запустить такой код?

 data = {1: 2}
 for key, value in data.items():
     data[key+1] = value+1  # -> RuntimeError: dictionary changed size during iteration

 print(id(data))  # -> 2183511418304  # ОДИНАКОВЫЕ id

 # Как исправить?
 # Итерируйте по копии ключей:
 for key in list(data.keys()):  # Создаём копию списка ключей
     data[key + 1] = data[key] + 1

 print(id(data))  # -> 2183511418304  # ОДИНАКОВЫЕ id
 print(data)      # -> {1: 2, 2: 3}


 # Возникает RuntimeError, потому что нельзя изменять размер словаря во время итерации по нему
 # (это ломает внутренний механизм работы итератора).

 # Python запрещает добавлять/удалять элементы словаря в цикле for — только менять существующие значения.

 # Итератор словаря следит за его размером, и если он меняется — Python вызывает ошибку, чтобы избежать неопределённого поведения.



 # 20) а если такой код запустим?

 data = [1, 2]
 for value in data:
   data.append(value+1)

 print(id(data))  # -> 2832273188416  # ОДИНАКОВЫЕ id

 # Как исправить?
 # Итерируйте по копии списка, если нужно изменять оригинал:
 for value in data.copy():
     data.append(value + 1)
 print(id(data))  # -> 2832273188416  # ОДИНАКОВЫЕ id
 print(data)  # -> [1, 2, 2, 3]


 # Цикл будет выполняться БЕСКОНЕЧНО, потому что список data постоянно увеличивается при каждой итерации,
 # и условие завершения никогда не достигается.

 # В отличие от словарей, Python не запрещает изменять списки во время итерации, но это приводит к логическим ошибкам
 # (например, к бесконечным циклам).                                         <-----      <-----



 # 21) Что получим в таком кейсе?

 print(().__class__.__class__)  # -> ???
 print(().__class__)            # -> ???



 # ОТВЕТ
 print(().__class__.__class__)  # -> <class 'type'>
 print(().__class__)            # -> <class 'tuple'>


 print(42 .__class__)  # -> int (число — экземпляр int),
 print(int.__class__)  # -> type (сам int — экземпляр type).

 # type — это метакласс, который создаёт все классы (включая tuple, int, str и даже сам type).



 # 22) Какой варинат отработает быстрее?

 ...
 # вариант 1
 str = s1 + " " + s2 + " " + s3
 # вариант 2
 str = " ".join([s1, s2, s3])
 # вариант 3
 str = "{} {} {}".format(s1, s2, s3)
 # вариант 4
 str = "%s %s %s" % (s1, s2, s3)
 # вариант 5
 str = f"{s1} {s2} {s3}"


 # Скорость работы вариантов (от быстрого к медленному):

 # 1)  f-строки (вариант 5) – самый быстрый (оптимизированы на этапе компиляции).
 # 2)  .join() (вариант 2) – эффективен для списка строк (особенно если их много).
 # 3) %-форматирование (вариант 4) – быстрее .format(), но медленнее f-строк.
 # 4) .format() (вариант 3) – универсально, но медленнее f-строк и %.
 # 5)  Конкатенация + (вариант 1) – самый медленный, т.к. создаёт промежуточные строки.


 # Когда что использовать?

 # - Лучший выбор: f-строки (быстро, читаемо, современно).
 # - Если строк много: .join() (оптимален для списков/итераций).
 # - Старый код: % или .format() (но f-строки лучше).
 # - Избегать: конкатенацию + (медленно и неэффективно).



 # ЗАМЕРЫ СКОРОСТИ
 import timeit

 s1, s2, s3 = "Hello", "world", "!"

 # Вариант 1: Конкатенация (+)
 t1 = timeit.timeit('s = s1 + " " + s2 + " " + s3', globals=globals(), number=1_000_000)

 # Вариант 2: join()
 t2 = timeit.timeit('s = " ".join([s1, s2, s3])', globals=globals(), number=1_000_000)

 # Вариант 3: .format()
 t3 = timeit.timeit('s = "{} {} {}".format(s1, s2, s3)', globals=globals(), number=1_000_000)

 # Вариант 4: %-форматирование
 t4 = timeit.timeit('s = "%s %s %s" % (s1, s2, s3)', globals=globals(), number=1_000_000)

 # Вариант 5: f-строка
 t5 = timeit.timeit('s = f"{s1} {s2} {s3}"', globals=globals(), number=1_000_000)

 print(f"f-строка: {t5:.6f} сек")          # -> f-строка:         0.173372 сек
 print(f"%-форматирование: {t4:.6f} сек")  # -> %-форматирование: 0.230499 сек
 print(f"join(): {t2:.6f} сек")            # -> join():           0.235247 сек
 print(f"Конкатенация (+): {t1:.6f} сек")  # -> Конкатенация (+): 0.351827 сек
 print(f".format(): {t3:.6f} сек")         # -> .format():        0.496470 сек



 # 23) Что будет выведено на экран?

 a = "hello"
 b = "hello"
 c = a + "1"
 d = b + "1"
 e = a + "1"
 print(a is b, c is d, c is e)  # -> ???



 # ОТВЕТ
 print(a is b, c is d, c is e)  # -> True False False


 # a is b → True: Python кэширует одинаковые строковые литералы (интернирование строк).

 # c is d → False: Динамически созданные строки ("hello1") не кэшируются автоматически.

 # c is e → False: Даже при одинаковом содержимом ("hello1"), строки, созданные через конкатенацию, — разные объекты.


 # a и b указывают на один объект в памяти (оптимизация Python для литералов).
 # c, d, e создаются в runtime и не кэшируются, поэтому is возвращает False.


 # is сравнивает адреса в памяти, а не значения. Для строк используйте ==, если не нужна проверка на идентичность объекта.



 # 24) Какой тип ошибки возникает при попытке запуска данного блока кода?

 def func(x, /, y):  # Всё что ДО '/' — можно передавать ТОЛЬКО позиционно
     return x /  y

 func(x=1, y=0)  # -> ???



 # ОТВЕТ
 func(x=1, y=0)  # -> TypeError: func() got some positional-only arguments passed as keyword arguments: 'x'

 # / в сигнатуре функции запрещает передавать x по имени — только по позиции (func(1, y=0)).

 # Как исправить?
 func(1, y=0)    # x — позиционно, y — по имени
 func(1, 0)      # оба аргумента позиционно



 # 25) Мелкие неочевидные фишечки?

 x, y = 2, 1
 x + y
 print(_)  # -> NameError: name '_' is not defined
 _, x, y = 10, 2, 1
 x + y
 print(_)


 # В REPL _ хранит последнее вычисленное значение (3 после x + y).
 # В скриптах _ — обычная переменная (в вашем примере _ = 10 после распаковки).



 # 26) Разомнёмся? Пятничный квиз?

 lst = [1,2,3,4,5,6]
 print(lst[1:4:-1])  # -> ???



 # ОТВЕТ
print(lst[1:4:-1])  # -> []

# Для отрицательного шага (-1) начальный индекс должен быть больше конечного, иначе Python возвращает []

# При шаге -1 срез идёт влево, поэтому должно быть start > stop.
# В lst[1:4:-1] старт 1, стоп 4 — идти “назад” некуда, поэтому результат [].

print(lst[4:1:-1])  # -> [5, 4, 3]


 # 27) Что напечатает скрипт?

 import copy
 a = [1, [2], [[3]]]
 b = copy.copy(a)
 b.append(4)
 b[1].append(5)
 b[2][0].append(6)
 print(a)  # -> ???



 # ОТВЕТ
 print(a)  # -> [1, [2, 5], [[3, 6]]]


 # Поверхностное копирование (copy.copy) копирует только первый уровень, а вложенные объекты остаются ссылками.

 # Глубокое копирование (deepcopy) рекурсивно копирует всё.

 -- END Задачи от ВИТАЛИЯ  ПРОСТО ПОСМОТРЕТЬ --



 Парадигма ООП (объектно-ориентированного программирования) появилась для:

 - Упрощения сложных систем через разбиение на объекты.
 - Повторного использования кода (наследование, композиция).
 - Инкапсуляции — сокрытия деталей реализации.
 - Моделирования реального мира (объекты = сущности, классы = их типы).

 ООП появилось, чтобы делать код гибким, понятным и переиспользуемым


 --- OOP ---
 ООП основано на «трех китах» - трех важнейших принципах придающих объектам новые свойства.
 Этими принципами являются ИНКАПСУЛЯЦИЯ, НАСЛЕДОВАНИЕ и ПОЛИМОРФИЗМ.

 ### В ПАРАМЕТРАХ УСЛОВИЯ                                                       <-----------
 class _AddressCollector(osmium.SimpleHandler if osmium else object):

 Encapsulation (Инкапсуляция)
 public(публичный) = без _ , __

 _ - protected(защищенный) знак того, что этот атрибут не предназначен для прямого использования.
 Работа обьекта не гарантируется, при использовании таких атрибутов

 from mymod import *     Имена с префиксом _ не будут импортироваться из mymod                      <-----

  __ - private(приватный) под капотом преобразуется в object._Class__attribute (только для случаев когда начинается с __)
 Name MangLing (Преобразование в не явное)
 Явное лучше неявного!!!

 __ private сделано для того чтобы при наследовании НЕ переопределить(изменить) атрибут предка


 # Учтите: преобразование имен не выполняется в таких функциях, как
 # getattr(), hasattr(), setattr() или delattr(), где имя атрибута задается
 # в виде строки. В таких функциях для обращения к атрибуту придется использовать преобразованное имя вида '_Класс__имя'.

 class MyClass:
     def __init__(self):
         self.__private_attr = "Я приватный атрибут"

     def get_private_attr(self):
         return self.__private_attr

 obj = MyClass()

 # Попытка доступа к приватному атрибуту напрямую вызовет ошибку
 # print(obj.__private_attr)  # AttributeError

 # Правильный способ доступа через метод
 print(obj.get_private_attr())  # Вывод: Я приватный атрибут

 # Использование getattr для доступа к приватному атрибуту
 print(getattr(obj, '_MyClass__private_attr'))  # Вывод: Я приватный атрибут


 IS-A является (наследование)
 HAS-A содержит (композиция)

 Правильно исполненная композиция дадут сто очков вперед наследованию.                  <-----


 Итак, в статических языках ИНКАПСУЛЯЦИЯ реализуется более ЖЕСТКО с помощью модификаторов доступа,
 в то время как в Python она основывается на соглашениях и предоставляет более гибкий подход.



 --- В Python разница между private и protected полями и методами заключается в уровне доступа ---

 Protected (защищенные) поля и методы начинаются с одного подчеркивания (_) и предназначены для использования
 внутри класса и его подклассов, но к ним можно получить доступ извне.

 Private (приватные) поля и методы начинаются с двух подчеркиваний (__) и защищены от доступа из подклассов благодаря
 механизму именования, что делает их менее доступными.

 Таким образом, защищенные члены доступны в подклассах, а приватные - нет.                  <------



 --- ЛЮБОЙ обьект в Python наследуется от object ---
 object() не поддерживает атрибуты экземпляра, поскольку он является основой для всех пользовательских классов Python,
 которые должны поддерживать отсутствие атрибута __dict__ при определении слотов .


 Numpy не имеет __dict__ и поэтому НЕ поддерживает произвольные атрибуты .
 Большинство (возможно, все) классов, определенных в C, НЕ имеют словаря для оптимизации.   <-----


 --- Mixin (Миксины, Примеси) ---
 Миксины — это особый вид множественного наследования в Python,
 которые используются для предоставления дополнительной функциональности классам.
 Основная цель миксинов - предоставить какие-то дополнительные методы.


 --- Множественное наследование   Multiple inheritance ---
 Python позволяет использовать концепцию множественного наследования. Это значит,
 что ваш класс может наследовать атрибуты и методы сразу от нескольких родительских классов.
 Это позволяет создавать более сложные иерархии классов, объединяя функциональность из разных источников


 --- MRO (method resolution order) ---
 Аббревиатура MRO – method resolution order (переводится как «порядок разрешения методов»).
 Этот порядок относится не только к поискам методов, но и к прочим атрибутам класса,

 Class.mro()   Class.__mro__
 Поиск идёт по алгоритму С3-линеаризации
 Используется первое обнаруженное вхождение. Такой порядок называется DFLR (Обход вглубину и слева направо).


 -- Diamond problem --
 В Python, наследование ромбовидной формы (или diamond inheritance) возникает, когда класс наследует от двух классов,
 которые, в свою очередь, наследуют от одного общего базового класса. Это может привести к неоднозначности,
 когда необходимо определить, какой метод или атрибут должен быть использован.

 # Пример ромбовидного наследования:
 class A:
     def say_hello(self):
         print("Hello from A")

 class B(A):
     def say_hello(self):
         print("Hello from B")

 class C(A):
     def say_hello(self):
         print("Hello from C")

 class D(B, C):
     pass

 d = D()
 d.say_hello()  # Вывод: Hello from B


 -- Композиция vs Наследование   Composition vs Inheritance  --
 **Композиция** и **наследование** — это два основных подхода к организации кода и связыванию объектов
 в объектно-ориентированном программировании (в том числе в Python).

 ### Наследование:
 - **Определение**: Наследование позволяет создать новый класс на основе существующего. Новый класс (подкласс) наследует
  атрибуты и методы родительского класса (суперкласса).
 - **Преимущества**: Позволяет повторно использовать код и расширять функциональность. Обеспечивает иерархическую структуру классов.
 - **Недостатки**: Может приводить к "жесткой" привязке между классами, что делает код менее гибким и трудным для поддержки.

 **Пример наследования**:
 class Animal:
     def speak(self):
         return "Animal sound"

 class Dog(Animal):
     def speak(self):
         return "Bark"


 ### Композиция:
 - **Определение**: Композиция подразумевает создание классов, которые содержат экземпляры других классов как атрибуты.
  Вместо наследования класс использует функциональность других классов.
 - **Преимущества**: Обеспечивает большую гибкость, позволяет изменять поведение на лету, снижает зависимость между классами.
 - **Недостатки**: Может потребовать больше кода для определения взаимодействий между компонентами.

 **Пример композиции**:
 class Engine:
     def start(self):
         return "Engine starts"

 class Car:
     def __init__(self):
         self.engine = Engine()  # композиция

     def start(self):
         return self.engine.start() + " and Car moves"


 ### Основные различия:
 1. **Структура**: Наследование создает иерархические связи, тогда как композиция использует "составляет" отношения.
 2. **Связь классов**: Наследование подразумевает сильную зависимость, а композиция — более слабую.
 3. **Использование**: Наследование подходит, когда есть "является" (is-a) отношения, а композиция — когда есть "имеет"
    (has-a) отношения.

 Таким образом, выбор между наследованием и композицией зависит от требований вашего проекта и архитектуры вашего кода.

 Агрегация — это слабая связь между объектами, позволяющая им существовать независимо (например, класс "Класс" и "Студент").
 Композиция — это сильная связь, при которой один объект зависит от другого, и при уничтожении контейнера исчезает и его
 содержимое (например, класс "Дом" и "Комната").

 Главная разница в том, что агрегированные объекты могут существовать отдельно, тогда как составные объекты — нет.

 КОМПОЗИЦИЯ является более строгим случаем агрегации, но стоит помнить, что агрегация не всегда подразумевает наличие композиции.
 В общем смысле, можно сказать, что все композиции — это агрегации, но не все агрегации — это композиции.


 Другой способ различать агрегацию и композицию - по сроку их жизни.
 • Если объект составной, то внутренние объекты (части) создаются и удаляются только вместе - это КОМПОЗИЦИЯ.
 • Если объект связанный, то его подобъекты создаются и удаляются независимо друг от друга  - это АГРЕГАЦИЯ


 Агрегация и композиция — это два вида ассоциации между классами в ООП, которые показывают, как один класс связан с другим.

 # Примеры:

 # 1. Агрегация (слабая связь)   #  Агрегация  — объекты живут отдельно, связь слабая.
 class Product:
     def __init__(self, name):
         self.name = name

 class Store:
     def __init__(self):
         self.products = []  # Агрегация: продукты могут существовать без магазина

     def add_product(self, product):
         self.products.append(product)

 # Создаем продукты отдельно
 apple = Product("Apple")
 banana = Product("Banana")

 # Передаем их в магазин
 store = Store()
 store.add_product(apple)
 store.add_product(banana)

 # Даже если магазин удалится, продукты останутся
 del store
 print(apple.name)  # -> Apple     # продукт жив


 # 2. Композиция (сильная связь)    Композиция — объекты зависят от контейнера, связь сильная.
 class Room:
     def __init__(self, name):
         self.name = name

 class House:
     def __init__(self):
         self.rooms = [
             Room("Bedroom"),  # Композиция: комната создается внутри дома
             Room("Kitchen")   # и не существует без него
         ]

 # Дом создается вместе с комнатами
 house = House()
 print(house.rooms[0].name)  # -> Bedroom

 # Если дом удалится, комнаты тоже пропадут
 del house
 # Теперь комнаты недоступны



  --- Polymorphism (Полиморфизм) ---
 Если не вдаваться в теории, то полиморфизм о котором вам нужно знать и о котором спросят на собеседовании
 - это механизм, позволяющий выполнять один и тот же код по-разному.
 Polymorphism (Полиморфизм) в объектно-ориентированном программировании – это возможность обработки разных типов данных,
 т. е.(то есть) принадлежащих к разным классам, с помощью "одной и той же" функции, или метода.
 Ducktyping (утиная типизация) - наличие поведения для использования в полиморфизме

 Пример Полиморфизма!!!
 class Dog:
     def speak(self):
         return "Woof!"

 class Cat:
     def speak(self):
         return "Meow!"

 def animal_sound(animal):
     print(animal.speak())

 dog = Dog()
 cat = Cat()

 animal_sound(dog)  # Вывод: Woof!
 animal_sound(cat)  # Вывод: Meow!

 Полиморфизм в контексте объектно-ориентированного программирования часто связан с принципом подстановки Барбары Лисков
 (Liskov Substitution Principle, LSP).  Полиморфизм также иногда называют принципом подстановки Барбары Лисков   <-----


 Специальный атрибут __slots__ позволяет вам явно указать, какие атрибуты экземпляра вы ожидаете иметь у
 экземпляров вашего объекта, с ожидаемыми результатами:
 1) более быстрый доступ к атрибутам.
 2) экономия места в памяти.

 Экономия места происходит от:
 1) Хранение ссылок на значения в слотах вместо __dict__.
 2) Запрещение создания __dict__ и __weakref__, если родительские классы запрещают их, а вы объявляете __slots__.

 __slots__ — уменьшить объем памяти, занимаемый каждым экземпляром объекта.


 --- Абстрактный класс ---

 Абстрактный класс - это "шаблон" для других классов.

 Зачем нужны АБСТРАКТНЫЕ классы в Python?
 ОНИ ПОЗВОЛЯЮТ ЯВНО УКАЗАТЬ, КАКИЕ МЕТОДЫ ДОЛЖНЫ БЫТЬ РЕАЛИЗОВАНЫ В КЛАССАХ-ПОТОМКАХ.  или NotImplementedError

 Зачем нужен ABC в Python?
 ПОЗВОЛЯЮТ ОПРЕДЕЛИТЬ КЛАСС, УКАЗАВ ПРИ ЭТОМ, КАКИЕ МЕТОДЫ ИЛИ СВОЙСТВА ОБЯЗАТЕЛЬНО ПЕРЕОПРЕДЕЛИТЬ В КЛАССАХ-НАСЛЕДНИКАХ.
 Абстрактный класс - это класс, который НЕ предназначен для создания объектов напрямую.
 NotImplementedError - Исключение, возникающее в случаях, когда наследник класса не переопределил метод, который должен был.

 NotImplemented — спец. значение, которое возвращают из __eq__, __lt__, __add__ и т.п.,
 чтобы Python попробовал обратный метод другого объекта (или дал стандартный результат).

 abc (аббревиатура от Abstract Base Classes) from abc import ABC, abstractmethod
 и вешаем декоратор @abstractmethod на pass функции

 from abc import ABC, abstractmethod
 class Shape(ABC):
     @abstractmethod
     def area(self):
         pass

 # Нельзя создать экземпляр класса Shape:
 s = Shape()  # -> TypeError: Can't instantiate abstract class Shape with abstract method area

 # Нужно реализовать метод area
 class MyClass(Shape):
     pass

 c = MyClass()  # -> TypeError: Can't instantiate abstract class MyClass with abstract method area

 # Всё работает
 class MyClass(Shape):
     def area(self):
         return 1000

 c = MyClass()
 print(c.area())  # -> 1000

 Класс, который наследует абстрактный класс, должен реализовать все его абстрактные методы, иначе он НЕ будет создан.


 --- ABC vs PROTOCOL ---

 # С ABC (обязательное наследование)                # С Protocol (без наследования)
 from abc import ABC, abstractmethod                from typing import Protocol

 class Animal(ABC):                                 class Animal(Protocol):
     @abstractmethod                                    def sound(self) -> str: ...
     def sound(self) -> str:
         pass                                       class Dog:  # Просто имеет нужный метод
                                                        def sound(self) -> str:
 class Dog(Animal):  # Должен наследовать Animal            return "Гав!"
     def sound(self) -> str:
         return "Гав!"                              # Использование
                                                    def make_sound(animal: Animal) -> None:
 # Использование                                        print(animal.sound())
 dog = Dog()
 print(dog.sound())  # Гав!                         make_sound(Dog())  # Гав!

 # ABC: Нужно наследование.
 # Protocol: Проверяет только методы ("утиная типизация").

 # Вывод: Protocol — гибкая замена интерфейсов.


 Protocol похож на интерфейс из других ЯП, он показывает какое поведение должен обеспечивать класс
 Удобен для ситуаций, где мы используем Duck-Typing, где наследование не удобно/ не применимо, а дополнительная
 информация важна и не хочется терять гибкость

 Делает сигнатуру функции и код читаемей
 IDE и библиотеки начинают подсказывать, если что-то не так
 удобнее расширять функциональность



 -- Отличие ОБЫЧНОГО класса от АБСТРАКТНОГО --

 Обычный класс:

 - Можно создавать объекты
 - Методы могут быть реализованы полностью

 Абстрактный класс (ABC):

 - Нельзя создавать объекты
 - Должен содержать хотя бы один @abstractmethod

 Требует реализации всех абстрактных методов в дочерних классах


 Пример:
 from abc import ABC, abstractmethod

 class Animal(ABC):  # Абстрактный
     @abstractmethod
     def make_sound(self): pass

 class Dog(Animal):  # Обычный
     def make_sound(self):
         print("Гав!")

 # Animal()  # Ошибка! Нельзя создать   <-----
 Dog().make_sound()  # OK


 -- Может ли быть обычный метод в АБСТРАКТНОМ классе - ДА! --

 Обычные методы работают как в обычных классах (автоматически наследуются).   <-----

 # В АБСТРАКТНОМ классе может быть обычный метод. Абстрактным является только метод, помеченный как @abstractmethod
 from abc import ABC, abstractmethod

 class Animal(ABC):
     @abstractmethod
     def make_sound(self):
         pass

     def eat(self):  # Обычный метод
         print("Ням-ням")

 class Dog(Animal):
     def make_sound(self):
         print("Гав!")

 dog = Dog()
 dog.make_sound()  # -> Гав!
 dog.eat()         # -> Ням-ням



 # Protocol проверяет типы статически, а не во время выполнения
 from typing import Protocol, runtime_checkable

 @runtime_checkable
 class Eater(Protocol):
     def eat(self) -> None: ...
     def make_sound(self) -> None: ...

 class CorrectCat:
     def eat(self) -> None: print("Eating")
     def make_sound(self) -> None: print("Meow")

 class BrokenCat:  # Не соответствует протоколу
     pass

 correct: Eater = CorrectCat()  # OK
 broken: Eater = BrokenCat()    # Type checker выдаст ошибку

 print(isinstance(CorrectCat(), Eater))  # True
 print(isinstance(BrokenCat(), Eater))   # False

 - Без @runtime_checkable Python не проверяет протоколы во время выполнения
 - Без @abstractmethod или type checker'а можно создать класс без обязательных методов

 Protocol в первую очередь предназначен для статической проверки типов
 Статическая (через mypy): Следит за соответствием интерфейсу.

 Вывод: Protocol сам по себе не предотвращает создание несоответствующих классов - он лишь предоставляет инструменты
 для проверки соответствия, когда вы явно используете аннотации типов или isinstance.


 - ABC + @abstractmethod: Обычные методы работают как в обычных классах.
 - Protocol: Обычные методы не обязательны для реализации, если не объявлены как абстрактные
 (но в Protocol нет явного @abstractmethod, поэтому лучше явно указывать ... или pass для обязательных методов).


 Ключевые различия  ABC VS Protocol:
 Особенность	        ABC                 	        Protocol
 Наследование методов	Да (обычные методы)	            Нет (только проверка интерфейса)
 Обязательные методы	@abstractmethod	                Методы без реализации (...)
 Проверка	            При создании экземпляра	        Через type checker или isinstance или mypy
 Цель	                Жёсткий контроль наследования	Гибкая утиная типизация


 Когда что использовать?

 ABC — если нужно:
 - Запретить создание неполных классов.
 - Переиспользовать код через наследование.


 Protocol — если нужно:
 - Проверить, что класс имеет нужные методы (даже без наследования).
 - Реализовать утиную типизацию.



 -- Интерфейсы в Python --

 Интерфейс — "правила" для класса (какие методы должны быть).

 В Python нет встроенных интерфейсов (как в Java/C#), но их можно имитировать

 В Python нет интерфейсов, но есть 2 способа их сделать:


 # ABC – требует явной реализации методов:
 from abc import ABC, abstractmethod
 class IWriter(ABC):
     @abstractmethod
     def write(self): pass

 class FileWriter(IWriter):
     def write(self): print("Done")  # Обязательно!


 # Protocol – проверяет типы, но не требует наследования:
 from typing import Protocol
 class IReader(Protocol):
     def read(self) -> str: ...

 class DBReader:
     def read(self) -> str: return "Data"  # Совместимо!


 Зачем?
 - Проверка типов (mypy, PyCharm).
 - Чистая архитектура (зависимость от абстракций, а не реализаций).

 Когда что использовать?
 ABC      – если надо жестко заставить реализовать методы.
 Protocol – если важна только "форма" класса (утиная типизация).

 Вывод: Интерфейсы в Python — это договорённость о методах.



 --- Дескрипторы ---
 Дескрипторы - это объекты Python, которые определяют, как другие объекты должны вести себя при доступе к атрибуту.
 Descriptor(Дескриптор) -  Любой объект, определяющий методы __get__(), __set__(), или __delete__(). ТОЧЕЧНЫЙ ПОИСК

 Дескрипторы данных        (data descriptor) - Если объект определяет __set__() или __delete__()
 (Даже если он определяет только __set__() или только __delete__(), он всё равно считается data descriptor.)

 Дескрипторы без данных(non-data descriptor) - которые определяют только __get__()

  Это различие ВАЖНО, потому что оно влияет на порядок поиска атрибутов в Python:                         <-----  <-----
 - Data descriptor имеет приоритет над атрибутами экземпляра (т.е., если в экземпляре есть атрибут с таким же именем,
   Python всё равно обратится к дескриптору).
 - Non-data descriptor не имеет такого приоритета — если в экземпляре есть атрибут с таким же именем, он будет
   использован вместо дескриптора.

 __set_name__ в классе дескриптора это замена метакласса для класса, который имеет экземпляр дескриптора в качестве атрибута .


 # __get__, __set__, __delete__
 # Дескриптор — это то, как реализован тип Python property
 # Функции/методы, связанные методы, property, classmethod и staticmethod все они используют эти специальные методы
 # для управления доступом к ним с помощью ТОЧЕЧНОГО ПОИСКА.

 # Встроенные объектов дескрипторы: classmethod, staticmethod, property, функции в целом
 # свойства с property: Создайте функции для управления получением, установкой и удалением атрибута
 class C:
     def __init__(self):
         self._x = None

     @property
     def x(self):
         '''I'm the 'x' property.'''
         return self._x

     @x.setter
     def x(self, value):
         if not isinstance(value, int):
             raise ValueError
         self._x = value

     @x.deleter
     def x(self):
         del self._x

 c = C()
 c.x = 10
 print(c.x)  # 10


 # Примеры встроенных объектов дескрипторов: classmethod, staticmethod, property, функции в целом      <-----
 def has_descriptor_attrs(obj):
     return set(['__get__', '__set__', '__delete__']).intersection(dir(obj))

 def is_descriptor(obj):
     '''obj can be instance of descriptor or the descriptor class'''
     return bool(has_descriptor_attrs(obj))

 def has_data_descriptor_attrs(obj):
     return set(['__set__', '__delete__']) & set(dir(obj))

 def is_data_descriptor(obj):
     return bool(has_data_descriptor_attrs(obj))



 # Мы можем видеть, что это classmethod и staticmethod и функции в целом есть Non-Data-Descriptors:
 print(is_descriptor(classmethod), is_data_descriptor(classmethod))    # -> True False
 print(is_descriptor(staticmethod), is_data_descriptor(staticmethod))  # -> True False

 # Обычные функция  Тоже Non-Data-Descriptors
 def foo(): pass
 my_func = lambda: 5

 print(is_descriptor(foo), is_data_descriptor(foo))                    # -> True False
 print(is_descriptor(my_func), is_data_descriptor(my_func))            # -> True False

 # Только метод __get__
 print(has_descriptor_attrs(classmethod))   # -> {'__get__'}
 print(has_descriptor_attrs(staticmethod))  # -> {'__get__'}
 # Обычные функции __get__
 print(has_descriptor_attrs(foo))           # -> {'__get__'}
 print(has_descriptor_attrs(my_func))       # -> {'__get__'}



 # Дескриптор данных, @property    Data-Descriptor
 # @property
 print(is_data_descriptor(property))    # -> True
 print(has_descriptor_attrs(property))  # -> {'__get__', '__delete__', '__set__'}

 --- @property ---
 property - это удобный механизм создания геттеров и сеттеров
 У класса (и объекта) есть два основных инструмента взаимодействия — свойства и методы.
 Свойства — это данные, которые лежат внутри объекта.
 Методы — это то, что объект умеет делать или как реагирует на внешние запросы.
 Геттеры и сеттеры являются методами класса, которые используются для получения
 и установки значений атрибутов экземпляра класса, соответственно.

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, value):
        if not value:
            raise AttributeError('Name cant be empty')
        self._name = value

 Возможность установки/получения атрибутов с логикой
 Зарпетить менять атрибут или добавлять новые атрибуты
 __dict__ - это атрибут обьектов в питоне, который хранит состояние
 __setattr__ вызывается при попытке установить атрибут
 @property - это удобный механизм создания геттеров и сеттеров
 в __init__ нужно вызывать setter
 как можно сделать чтобы нельзя было поменять атрибут?
 можно сделать при помощи @property или переопределение __setattr__ просто прописывает условия

 getter еще называют - аксессор,  setter - мутатор


 В Python данные, свойства и методы - это атрибуты класса. Тот факт, что
 метод можно вызывать, не отличает его от других типов атрибутов


 -- Символ ТОЧКИ (.) в ООП --

  - Точка (.) — это синтаксический оператор доступа к атрибуту, а не дескриптор.

  - Но если атрибут является дескриптором (например, property, staticmethod или пользовательским дескриптором),
    то точка автоматически вызовет его методы (__get__, __set__, __delete__).

 # Пример с дескриптором:
 class Descriptor:
     def __get__(self, obj, obj_type):
         return "Вызов __get__!"

 class MyClass:
     attr = Descriptor()  # attr — дескриптор

 obj = MyClass()
 print(obj.attr)  # -> Вызов __get__!   # Точка вызовет Descriptor.__get__!


 # Пример БЕЗ дескриптора:

 class MyClass: pass

 obj = MyClass()
 obj.attr = 10  # Обычный атрибут (дескрипторы не участвуют)
 print(obj.attr)  # -> 10  # Просто чтение (выведет 10)

 Суть:

 - Если атрибут — дескриптор, точка (.) вызывает его методы (__get__/__set__).
 - Если атрибут обычный, точка просто читает/записывает значение.


 РАЗНИЦА между Дескриптором vs @property ???

 Дескриптор - это класс с __get__, __set__ или __delete__, позволяющий гибко управлять доступом к атрибутам других классов.
 @property  - удобный декоратор для создания getter/setter под капотом, упрощающий контроль над отдельными атрибутами.

 - Дескриптор мощнее (можно переиспользовать, сложная логика).
 - @property проще, но привязан к КОНКРЕТНОМУ КЛАССУ.

 Если нужен простой контроль над атрибутом      -> @property.
 Если нужна переиспользуемая или сложная логика -> Дескриптор.


 -- ТОЛЬКО Data-descriptor  ПЕРЕБИВАЕТ obj.__dict__ --
 Data-descriptor (например property) → имеет приоритет над obj.__dict__.                            <-------
 Non-data descriptor (только __get__, например обычная функция-метод) → НЕ перебивает:
 если в obj.__dict__ есть атрибут с тем же именем, он его перекроет.

 data-descriptor (__get__ + __set__/__delete__) ПЕРЕБИВАЕТ obj.__dict__
 non-data descriptor (только __get__) НЕ перебивает → obj.__dict__ может его перекрыть

 ### “Перебивает” obj.__dict__ только data-descriptor (у которого есть __set__ и/или __delete__).   <-------
 ### DATA-DESCRIPTOR - ПЕРЕБИВАЕТ!
 class A:
     @property
     def x(self): return 1

 a = A()
 a.__dict__['x'] = 999
 print(a.x)          # 1  (property перебил)

 ### NON-DATA-DESCRIPTOR - НЕ ПЕРЕБИВАЕТ!
 class A:
     def f(self): return "method"

 a = A()
 a.__dict__['f'] = 999
 print(a.f)       # 999 (obj.__dict__ перебил non-data descriptor)


  --- Встроенные функции vs Магические методы ---

 1. Оптимизация: Встроенные функции в Python часто оптимизированы для производительности.
 2. Накладные расходы: Вызов метода требует дополнительных накладных расходов на доступ к атрибуту объекта,
    что может замедлить выполнение.

 На практике, разница во времени может зависеть от конкретной версии Python и системы, на которой вы выполняете код,
 но в общем случае встроенные функции будут более быстрыми и предпочтительными.

 Для встроенных объектов в CPython встроенные функции (например, len(), str()) работают напрямую с объектами и НЕ
 вызывают соответствующие магические методы. Это позволяет избежать накладных расходов, связанных с вызовом методов,
 что делает их более быстрыми.


 # Сравнение built-in function vs dunder method
 from timeit import timeit

 #  str() vs __str__()
 print(timeit('str(100)'))                                # -> 0.10770369996316731
 print(timeit('100 .__str__()'))                          # -> 0.15946240001358092

 #  iter() vs __iter__()
 print(timeit('iter([100, 1000])', number=1000000))       # -> 0.11073940002825111
 print(timeit('[100, 1000].__iter__()', number=1000000))  # -> 0.1557056000456214


 Для встроенных объектов интерпретатор CPython вообще НЕ вызывает никаких методов: длина просто читается из поля
 C-структуры.  Эффективно работает для встроенных типов как str, list, memoryview и т.п.     <-----  Важно!!!

 # Например функция len()
 # Сравнение tuple, set, dict, list
 #  len() vs __len__()    len()  Намного быстрее работает                                    <-----  Важно!!!

 # РАБОТАЕТ В 2 РАЗА БЫСТРЕЕ  для []
 print(f"{timeit('len([100, 1000])', number=1000000):.6f}")        # -> 0.072438
 print(f"{timeit('[100, 1000].__len__()', number=1000000):.6f}")   # -> 0.145085

 # tuple  ()
 print(f"{timeit('len((100, 1000))', number=1000000):.6f}")        # -> 0.036578
 print(f"{timeit('(100, 1000).__len__()', number=1000000):.6f}")   # -> 0.103023

 # set
 print(f"{timeit('len({100, 1000})', number=1000000):.6f}")        # -> 0.127049
 print(f"{timeit('{100, 1000}.__len__()', number=1000000):.6f}")   # -> 0.183266

 # dict
 print(f"{timeit('len({100: 1000})', number=1000000):.6f}")        # -> 0.117994
 print(f"{timeit('{100: 1000}.__len__()', number=1000000):.6f}")   # -> 0.171075

 # Расширенный пример

 # Измеряем время для встроенной функции str()
 builtin_time = timeit('str(100)', number=1000000)  # Выполняем 1,000,000 раз
 print(f'Встроенная функция str(): {builtin_time:.6f} секунд')  # -> Встроенная функция str(): 0.107998 секунд

 # Измеряем время для дандер-метода
 dunder_time = timeit('100 .__str__()', number=1000000)  # Выполняем 1,000,000 раз
 print(f'Метод __str__(): {dunder_time:.6f} секунд')            # -> Метод __str__():          0.161750 секунд


 -- ОГРАНИЧЕНИЯ! На перегрузку Операторов  --
 ОГРАНИЧЕНИЯ! запрещается перегружать операторы встроенных типов, запрещается создавать новые операторы и перегружать
 операторы is, and, or, not

 Методы __and__, __or__, __is__ и __not__ действительно могут быть определены в вашем классе, но это НЕ означает,
 что перегруженные версии этих операторов будут работать так же, как и у встроенных операторов.                   <-----


 --- Dunder methods (Double Underscores) Магические методы ---

 Магические методы - dunder методы, методы которые начинаются и заканчиваются __
 для самописных классов нужно переопеделить магические методы для нужного поведения и действий

 Метод __new__ вызывается при создании нового экземпляра класса. Он отвечает за выделение памяти под новый объект
  и возвращает этот объект. __new__ является статическим методом и должен возвращать экземпляр класса.
 __init__ отвечает за инициализацию экземпляров класса после их создания.
 __init__ используется для инициализации атрибутов объекта при его создании
 __init__ по умолчанию не ждет аргументов
 __new__() для его создания, а __init__() для его настройки

 self в super().__init__(...) писать НЕ нужно: метод уже “привязан” к текущему объекту, и Python передаёт self автоматически.

 Мини-правило:
 __new__: работает с cls (создаёт объект)
 __init__: работает с self (настраивает уже созданный объект)

 ### РАБОТАЕТ!
 class A:
     def __init__(self, x):
         print("A", x)

 class B(A):
     def __init__(self, x):
         super().__init__(x)          # ✅ self передастся автоматически
         # super().__init__(self, x)  # ❌ TypeError: лишний аргумент

 b = B(10)  # -> A 10


 ### НЕ РАБОТАЕТ!
 class A:
     def __init__(self, x):
         print("A", x)

 class B(A):
     def __init__(self, x):
         # super().__init__(x)      # ✅ self передастся автоматически
         super().__init__(self, x)  # ❌ TypeError: лишний аргумент

 b = B(10)  # -> TypeError: A.__init__() takes 2 positional arguments but 3 were given


 Создание атрибутов экземпляра вне метода __init__ может привести к созданию ненужных атрибутов и увеличению
 потребления памяти, особенно если они не используются.
 Для экономим памяти избегайте создания атрибутов ЭКЗЕМПЛЯРА ВНЕ метода __init__           <-----  Важно!!!

 Добавление атрибута экземпляра после возврата из __init__ заставляет Python создать хэш-таблицу для хранения __dict__
 только для ОДНОГО этого экземпляра это оптимизация сокращает потребление памяти на 10% - 20%.    PEP 412

 В Python, если атрибут экземпляра добавляется после вызова __init__, интерпретатор может оптимизировать использование
 памяти, создавая хэш-таблицу (__dict__) только для этого экземпляра. Это может сократить потребление памяти на 10% - 20%,
 особенно если экземпляр имеет немного атрибутов.

 What the f*ck Python!   # Проект  СПОРНЫЙ МОМЕНТ ПРОВЕРЯЙ САМ!!!                          <-----  Важно!!!
 Небольшой совет, если вы хотите уменьшить объем памяти, занимаемый вашей программой: не удаляйте атрибуты экземпляра
 и обязательно инициализируйте все атрибуты в вашем __init__!

 __repr__ - для програмистов, возвращает строку, по которой видно (и можно воссоздать) состояние обьекта  !r
 __str__ - для людей, возвращает строку    !s
 если не реализован репр и стр, то будет возвращать адрес в памяти (который в Python являет-ся адресом объекта в оперативной памяти).
 __eq__ по умолчанию сравнивает адрес в памяти, в реализации лучше сразу проверить тип
 если определяете метод __eq__ то теряется возможность нахождение hash()
 если методы сравнения не реализованы то падает ошибка
 contains для реализации проверки IN
 bool для самодельных обьектов всегда вернет True, для изменения поведения нужно написать __bool__
 len вернет ошибку если не переопределить метод __len__
 чтобы обьект стал вызываемым (callable) нужно реализовать __call__, иначе ошибка
 __iter__ возвращает обьект итератор, тот кто реализует итер = Итерабл
 __iter__ = должен вернуть обьект который умеет делать __next__
 __next__ должен вернуть следующий обьект из контейнера, кто его реализует = Итератор, for работает до StorIteration
 Итератор = обьект в нём есть данные и он может по вызову обьекта __next__ данные выдавать
 __getitem__ нужен для функционала [] (аналог списка и словаря)
 __setitem__ для присвоения через [], если не реализовать = ошибка
 если в обьекте не реализован __iter__ то для цикла фор будет использован __getitem__ там ожидается падение IndexError


 -- reprlib --                                                                              <-----
 reprlib в Python используется для создания сокращенных представлений объектов, которые могут быть слишком большими
 для стандартного repr()

 import reprlib

 large_list = list(range(1000))

 print(reprlib.repr(large_list))  # -> [0, 1, 2, 3, 4, 5, ...]
 print(repr(large_list))          # -> Будет ОГРОМНЫЙ ВЫВОД!!!

 # По умолчанию 30 символов лимит
 large_str = 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'

 print(reprlib.repr(large_str))  # -> 'aaaaaaaaaaaa...aaaaaaaaaaaaa'
 print(repr(large_str))          # -> 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'

 # ИЗМЕНЯЕМ ЛИМИТЫ
 # Создаем объект Repr    Там ЕЩЕ МНОГО АТРИБУТОВ
 r = reprlib.Repr()
 r.maxlist = 10    # Максимум элементов в списке
 r.maxdict = 5     # Максимум элементов в словаре
 r.maxstring = 10  # Максимум элементов в строке

 # Вывод после ИЗМЕНЕНИЯ  ЛИМИТОВ
 print(r.repr(large_list))       # -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...]
 print(r.repr(large_str))        # -> 'aa...aaa'


 Одной из ключевых функций в reprlib является recursive_repr, которая используется для создания строкового
 представления объектов, которые могут содержать рекурсивные ссылки (например, ссылки на самих себя).
 Это помогает избежать бесконечной рекурсии при вызове repr().

 import reprlib

 class Node:
     def __init__(self, name: str, link: None) -> None:
         self.name = name
         self.link = link

     @reprlib.recursive_repr()                       # Без @reprlib.recursive_repr()  Будет ошибка  RecursionError
     def __str__(self) -> str:
         return f"<Node: {self.name} -> {self.link}>"

 # Создаем рекурсивную структуру данных
 node = Node('root', None)
 node.link = node

 print(node)



 Ах, да... Магические методы не предусматривают работы с async
 Большинство магических методов не предназначены для работы с async def/ await— как правило, вам следует использовать
 только await внутри выделенных асинхронных магических методов — __aiter__, __anext__, __aenter__и __aexit__.


 async и await всего лишь синтаксический сахар вокруг генераторов!                                              <-----

 # До появления ключевых слов async и await в версии Python 3.5 связь между сопрограммами и генераторами были ОЧЕВИДНОЙ.
 # Старый синтаксис с декораторами и генераторами.

 # В таком стиле писать код НЕ СТОИТ! Старый стиль с использованием декораторов и генераторов в asyncio
 # использование `@asyncio.coroutine` более НЕ поддерживается.
 # 3.11 и более поздних версиях действительно была удалена поддержка декоратора `@asyncio.coroutine`

 import asyncio

 Старый стиль/синтаксис                             # Тоже самое Новый стиль/синтаксис  async и await  с версии Python 3.5
 @asyncio.coroutine                                 async def coroutine():
 def coroutine():                                       print('Засыпаю!')
     print('Засыпаю!')                                  await asyncio.sleep(1)
     yield from asyncio.sleep(1)                        print('Проснулась!')
     print('Проснулась!')
                                                    asyncio.run(coroutine())

 asyncio.run(coroutine())  # AttributeError: module 'asyncio' has no attribute 'coroutine'. Did you mean: 'coroutines'?


  ----- ИТОГ S.O.L.I.D -----
 1) ПРИНЦИП ЕДИНСТВЕННОЙ ОТВЕТСТВЕННОСТИ (Single Responsibility Principle):
 1) У класса должна быть всего одна причина для изменения.
 1) Один класс выполнял только одну работу.


 2) ПРИНЦИП ОТКРЫТОСТИ/ЗАКРЫТОСТИ (Open-Closed Principle):
 2) Программные сущности (классы, модули, функции и т.п.) должны быть открыты для расширения, но закрыты для изменений.


 3) ПРИНЦИП ПОДСТАНОВКИ БАРБАРЫ ЛИСКОВ (Liskov Substitution Principle):
 3) подклассы не должны противоречить надклассам
 3) функции, которые используют базовый тип, должны иметь возможность использовать подтипы базового типа, не зная об этом.

 3) Объекты в программе должны быть заменяемы экземплярами их подтипов без ущерба корректности работы программы.
 3) Проще говоря, это значит, что подкласс, дочерний класс должны соответствовать их родительскому классу или супер классу
 3) использовать любой подкласс базового класса, не замечая разницы между ними


 4) ПРИНЦИП РАЗДЕЛЕНИЯ ИНТЕРФЕЙСОВ (Interface Segregation Principle):
 4) Ни один клиент не должен зависеть от методов, которые он не использует.
 4) слишком «толстые» интерфейсы необходимо разделять на более маленькие и специфические
 4) «Программные сущности не должны зависеть от методов, которые они не используют»

 4) «Много интерфейсов, специально предназначенных для клиентов, лучше чем один интерфейс общего назначения»


 5) ПРИНЦИП ИНВЕРСИИ ЗАВИСИМОСТЕЙ (Dependency Inversion Principle):
 5) Модуль высокого уровня не должен зависеть от модулей низкого уровня. И то, и другое должно зависеть от абстракций.
 5) Абстракции не должны зависеть от деталей реализации. Детали реализации должны зависеть от абстракций.

 5) «Зависимость на Абстракциях. Нет зависимостри на что-то конкретное»  зависим от абстракций (Protocol/ABC)

 5) Если ваш код уже реализует принципы 2) открытости/закрытости и 3) подстановки Лисков,
 он уже будет неявно согласован с принципом инверсии зависимостей


 Абстракция — основной способ борьбы со сложностью в программировании. Она позволяет уйти от деталей реализации
 и сосредоточиться на главном. Хороший пример абстракции — функция сортировки списка.
 Не важно, как она устроена, важно, что она делает то, что нам нужно.

 Абстракция гласит что мы должны выделять важные характеристики объекта.



 --- SOLID примеры на Python ---

 Эти принципы помогают создавать гибкий, поддерживаемый и расширяемый код, который легко тестировать и модифицировать.

 1. SRP (Single Responsibility Principle) – Принцип единственной ответственности
 # Класс должен иметь только одну причину для изменения (одну ответственность).

 # Плохо: Класс делает ТРИ вещи

 class User:
     def __init__(self, name: str):
         self.name = name

     def save(self):
         print(f"Сохранение пользователя {self.name} в БД")

     def send_email(self, message: str):
         print(f"Отправка email пользователю {self.name}: {message}")


 # Хорошо: Разделяем на ТРИ класса

 class User:
     def __init__(self, name: str):
         self.name = name

 class UserDB:
     def save(self, user: User):
         print(f"Сохранение пользователя {user.name} в БД")

 class EmailService:
     def send_email(self, user: User, message: str):
         print(f"Отправка email пользователю {user.name}: {message}")

 # ОБЪЯСНЕНИЕ ПЛОХОГО И ХОРОШЕГО ПРИМЕРОВ!!!
 Проблема в плохом примере:

 Класс User делает три разные вещи:
 - Хранит данные пользователя
 - Сохраняет пользователя в БД
 - Отправляет email

 Почему это плохо:

 - Если нужно изменить способ сохранения в БД или отправки email, придется менять класс User
 - Класс становится сложнее тестировать и поддерживать

 Решение:

 Разделяем на три отдельных класса, каждый со своей ответственностью:
 - User         - только данные пользователя
 - UserDB       - работа с базой данных
 - EmailService - отправка email


 2. OCP (Open-Closed Principle) – Принцип открытости/закрытости
 # Классы должны быть открыты для расширения, но закрыты для модификации.

 # Плохо: Добавление нового типа требует изменения метода

 class Discount:
     def __init__(self, customer_type: str):
         self.customer_type = customer_type

     def apply_discount(self, price: float):
         if self.customer_type == "VIP":
             return price * 0.8
         elif self.customer_type == "Regular":
             return price * 0.9

 # Хорошо: Используем абстракцию

 from abc import ABC, abstractmethod

 class Discount(ABC):
     @abstractmethod
     def apply_discount(self, price: float) -> float:
         pass

 class VIPDiscount(Discount):
     def apply_discount(self, price: float) -> float:
         return price * 0.8

 class RegularDiscount(Discount):
     def apply_discount(self, price: float) -> float:
         return price * 0.9

 # ОБЪЯСНЕНИЕ ПЛОХОГО И ХОРОШЕГО ПРИМЕРОВ!!!
 Проблема в плохом примере:

 - При добавлении нового типа скидки (например, для "Premium" пользователей) нужно изменять метод apply_discount
 - Это может привести к ошибкам в существующей логике

 Почему это плохо:

 - Каждое изменение существующего кода несет риск сломать что-то работающее
 - Нарушается принцип "работающий код не трогаем"

 Решение:

 - Создаем абстрактный класс Discount с методом apply_discount
 - Для каждого типа скидки создаем свой класс-наследник
 - Новые типы скидок добавляются созданием новых классов, без изменения существующих


 3. LSP (Liskov Substitution Principle) – Принцип подстановки Барбары Лисков
 # Подклассы должны заменять свои базовые классы без изменения поведения программы.

 # Плохо: Подкласс нарушает поведение родителя

 class Bird:
     def fly(self):
         print("Птица летит")

 class Penguin(Bird):
     def fly(self):
         raise Exception("Пингвины не летают!")

 # Хорошо: Разделяем на разные классы

 class Bird:
     pass

 class FlyingBird(Bird):
     def fly(self):
         print("Птица летит")

 class Penguin(Bird):
     def swim(self):
         print("Пингвин плавает")

 # ОБЪЯСНЕНИЕ ПЛОХОГО И ХОРОШЕГО ПРИМЕРОВ!!!
 Проблема в плохом примере:

 - Класс Penguin наследуется от Bird, но не может летать
 - Это нарушает ожидания: если что-то является птицей, предполагается, что оно может летать

 Почему это плохо:

 - Код, работающий с Bird, может сломаться при получении Penguin
 - Нарушается полиморфизм - основной принцип ООП

 Решение:

 - Разделяем птиц на летающих и нелетающих
 - Penguin наследуется от Bird, но не от FlyingBird
 - Теперь все классы ведут себя предсказуемо


 4. ISP (Interface Segregation Principle) – Принцип разделения интерфейса
 # Клиенты не должны зависеть от методов, которые они не используют.

 # Плохо: Один большой интерфейс

 class Machine(ABC):
     @abstractmethod
     def print(self):
         pass

     @abstractmethod
     def scan(self):
         pass

     @abstractmethod
     def fax(self):
         pass

 class OldPrinter(Machine):
     def print(self):
         print("Печатает")

     def scan(self):
         raise NotImplementedError("Не поддерживается")

     def fax(self):
         raise NotImplementedError("Не поддерживается")

 # Хорошо: Разделяем интерфейсы

 class Printer(ABC):
     @abstractmethod
     def print(self):
         pass

 class Scanner(ABC):
     @abstractmethod
     def scan(self):
         pass

 class SimplePrinter(Printer):
     def print(self):
         print("Печатает")

 # ОБЪЯСНЕНИЕ ПЛОХОГО И ХОРОШЕГО ПРИМЕРОВ!!!
 Проблема в плохом примере:

 - Интерфейс Machine требует реализации всех методов (print, scan, fax)
 - Но старый принтер не умеет сканировать и отправлять факсы

 Почему это плохо:

 - Приходится либо реализовывать ненужные методы с исключениями
 - Либо клиент зависит от методов, которые никогда не будет использовать

 Решение:

 - Разделяем большой интерфейс на маленькие (Printer, Scanner)
 - Класс SimplePrinter реализует только то, что ему нужно (Printer)


 5. DIP (Dependency Inversion Principle) – Принцип инверсии зависимостей
 # Зависимости должны строиться на абстракциях, а не на конкретных реализациях.

 # Плохо: Зависимость от конкретного класса

 class LightBulb:
     def turn_on(self):
         print("Лампочка включена")

 class Switch:
     def __init__(self, bulb: LightBulb):
         self.bulb = bulb

     def operate(self):
         self.bulb.turn_on()

 # Хорошо: Зависим от абстракции

 from abc import ABC, abstractmethod

 class Switchable(ABC):
     @abstractmethod
     def turn_on(self):
         pass

 class LightBulb(Switchable):
     def turn_on(self):
         print("Лампочка включена")

 class Fan(Switchable):
     def turn_on(self):
         print("Вентилятор включен")

 class Switch:
     def __init__(self, device: Switchable):
         self.device = device

     def operate(self):
         self.device.turn_on()

 # ОБЪЯСНЕНИЕ ПЛОХОГО И ХОРОШЕГО ПРИМЕРОВ!!!
 Проблема в плохом примере:

 - Класс Switch зависит напрямую от LightBulb
 - Если мы захотим управлять вентилятором, придется менять класс Switch

 Почему это плохо:

 - Код становится жестко связанным
 - Трудно добавлять новые устройства для управления

 Решение:

 - Создаем интерфейс Switchable с методом turn_on
 - И LightBulb, и Fan реализуют этот интерфейс
 - Switch теперь работает с любым устройством, реализующим Switchable
 - Можно добавлять новые устройства без изменения кода Switch



 --- Метаклассы metaclass ---
 Если коротко, то метаклассы - это классы, которые конструируют другие классы
 Основная цель метаклассов — автоматически изменять класс в момент создания.

 type это встроенный метакласс, который использует Питон, но вы, конечно, можете создать свой.

 Зачем вообще использовать метаклассы?
 Основное применение метаклассов это создание API. Типичный пример — Django ORM.   <----

 Django использует метаклассы для создания моделей базы данных

 Пример Создание своего Метакласса!

 # Таким образом, вы создали метакласс `MyMeta`, который добавляет атрибут `custom_attribute` в класс `MyClass`.

 class MyMeta(type):
     def __new__(cls, name, bases, attrs):
         # Модификация атрибутов класса
         attrs['custom_attribute'] = 'This is a custom attribute'
         attrs['hehe'] = '123'
         return super().__new__(cls, name, bases, attrs)

 class MyClass(metaclass=MyMeta):
     pass

 instance = MyClass()
 print(instance.custom_attribute)  # Вывод: This is a custom attribute
 print(instance.hehe)              # Вывод: 123
 print(instance.__dict__)          # Вывод: {}
 print(MyClass.hehe)               # Вывод: 123
 print(MyClass.custom_attribute)   # Вывод: This is a custom attribute

 # Важно Интересный пример!!!
 print(type(MyClass))              # -> <class '__main__.MyMeta'>
 print(type(instance))             # -> <class '__main__.MyClass'>


 - Как создать класс без слова class?
 Kласс можно создать без использования ключевого слова class, используя типы type :

 MyClass = type('MyClass', (), {'x': 42, 'foo': lambda self: self.x})

 ### types.new_class(name, bases=(), kwds=None, exec_body=None)
 import types

 def body(ns):
     ns["a"] = 1

 A = types.new_class("M", (), {}, body)
 print(A().a)        # 1
 print(A().__dict__) # {}

 # Тоже самое что и выше но с ключевым словом class!
 class MyClass:
    x = 42
    def foo(self):
        return self.x


 # ТОЖЕ САМОЕ НО УКАЗЫВАЕМ  НАСЛЕДОВАНИЕ
 MyClass = type('MyClass', (MySuperClass, MyMixin), {'x': 42, 'foo': lambda self: self.x})

 # Тоже самое что и выше но с ключевым словом class!
 class MyClass(MySuperClass, MyMixin):
     x = 42

     def foo(self):
         return self.x

 # type - это тип всех типов, для которых не указан явно иной метакласс
 print(type(type))    # -> <class 'type'>
 print(type(object))  # -> <class 'type'>
 print(type(list))    # -> <class 'type'>
 print(type(set))     # -> <class 'type'>
 print(type(dict))    # -> <class 'type'>
 print(type(bool))    # -> <class 'type'>
 print(type(int))     # -> <class 'type'>
 print(type(str))     # -> <class 'type'>
 print(type(collections.deque))     # -> <class 'type'>

 class Bar(object): pass

 b = Bar()
 print(type(Bar))     # -> <class 'type'>
 print(type(b))       # -> <class '__main__.Bar'>


 НЕЛЬЗЯ изменять сам type, вы НЕ можете добавлять атрибуты напрямую к встроенному типу type   <-----
 type является частью внутреннего устройства Python и НЕ предназначен для изменения.
 Безопасно изменять только свои собственные метаклассы, созданные на основе type.


 --- Важные модули ---

 from timeit import timeit     # библиотека замера скорости
 from pympler import asizeof   # библиотека замера размера структур
 import dis                    # библиотека работы с байт кодом


 Jupyter-ноутбук — это среда разработки, где сразу можно видеть результат выполнения кода и его отдельных фрагментов.
 в Jupyter-ноутбук можно замерить время ->   %time a = list(range(100))  есть много магических команд  %time, %timeit, %%time


 --- Логирование ---
 Logging — это важный инструмент для отладки и отслеживания работы программы

 Модуль logging
 Этот модуль определяет функции и классы, которые реализуют гибкую систему регистрации событий для приложений и библиотек.


 Graylog - это открытая система для сбора, хранения и анализа логов (логирования) с мощными возможностями поиска и мониторинга.
 Graylog - это open-source-платформа для централизованного сбора, хранения, анализа и мониторинга логов с удобным веб-интерфейсом.

 - Централизованное хранение логов
 - Поиск и анализ в реальном времени
 - Оповещения и мониторинг
 - Поддержка множества источников данных

 Использование Graylog: логи веб-серверов, firewall, базы данных, облачные сервисы.


 -- logging python --
 Модуль logging - это встроенная библиотека Python для записи логов (сообщений о работе программы). Позволяет:

 - Выводить сообщения разных уровней (DEBUG, INFO, WARNING, ERROR, CRITICAL).
 - Настраивать куда писать логи (в файл, консоль, сеть и т.д.).
 - Форматировать сообщения и фильтровать их по важности.

 # Пример logging

 import logging
 logging.warning("Внимание! Что-то пошло не так.")

 Вывод:
 WARNING:root:Внимание! Что-то пошло не так.


  Почему в logging используют % или .format(), а не f-строки?                          <-----    <-----
 - Производительность - f-строки вычисляются сразу, даже если лог не будет выведен
   (например, из-за уровня logging.INFO при logger.setLevel(logging.ERROR)).
 - Гибкость - % и .format() позволяют отложенное форматирование (полезно в логгерах, где сообщение обрабатывается только при нужном уровне).
 - Совместимость - старые версии Python (<3.6) не поддерживают f-строки.

 # Лучший вариант – передавать аргументы напрямую в логгер
 logger.debug("User %s did action %s", user, action)  # Самый эффективный способ

 Вывод: % и .format() - безопаснее для логов, f-строки - для гарантированного вывода.

 Для логов: Лучше % или .format() (или передача аргументов в logger.debug(msg, arg1, arg2)).
 Для гарантированного вывода (print, assert и т. д.): Можно использовать f-строки.

 Основная причина - ленивое (lazy) вычисление.
 Оптимизирует производительность за счёт отложенного форматирования.


 -- Профилирование(profile)   Профилировщики(profilers) --
 Профилирование кода на Python — это важный этап оптимизации, который помогает улучшить производительность программы.
 Bottleneck - Узкое место программы.

 Стандартный профилировщик Python:
 cProfile - расширение C
 profile  - модуль на чистом Python, интерфейс которого имитируется cProfile


 Оптимизация потребления памяти в Python:
 - Модуль memory_profiler - измеряет использование памяти конкретной функцией построчно.
 - Модуль Pympler - много полезных функций включая отслеживание экземпляров классов или выявление утечек памяти.
 - NumPy , Scipy , Pandas , __slots__ , Trie-префиксное дерево
 - Использование генераторов Python и/или модуль mmap - ускорение операций ввода-вывода

 ОПТИМИЗАЦИЯ в Python:
 - Используйте операции с множествами set - Методы set  O(1) без hash-коллизий  O(n) c hash-коллизиями
 - Set - увеличивает скорость выполнения но будет потреблять еще больше оперативной памяти.
 - ОПТИМИЗАЦИЯ ИСПОЛЬЗОВАНИЯ СТРОК: Не используем Конкатенация строк - лучше join или f-строки
 - Интернирование строк, Кэширование чисел, Кэширование результатов functools декораторов
 - more-itertools, itertools, functools, collections, operator, built-in functions, используем and or замыкания,
 - list comprehensions, generator expression, iterators, set comprehensions, dict comprehensions, with
 - timeit(замера скорости) , asizeof(размера структур) , dis(байт код), Логирование logging
 - Профилирование кода - timeit и модуль cProfile (собирает статистику, анализ производительности)
 - multiprocessing (ядра пк), multithreading (потоки) , asyncio (асинхронные операции) , Numba - ускоряет код Python
 - Тестирование Pytest, unittest


 --- Python Snippets ---
 Python Snippets (Сниппет) - это какой-то отрывок кода, который может быть использован повторно.

 Set Operations, Functools, Metaclasses, asyncio, Dataclasses, Decorators with Arguments, Asynchronous Iterators,
 getattr, list/dict/set Comprehensions, genexp, with open, lambda, import, built-in functions, f-string ...


 Наводим на номер строки жмем другой кнопкой мыши и выбираем   Annotate with Git Blame    -  Показать кто делал код


 -- Полезные термины Git --

 Git (Global Information Tracker) - системы контроля версий (слово придумал Линус Торвальдс)

 Remote (удалённый репозиторий) — ссылка на внешний репо (например, origin).
 Clone (клонирование) — скачать репозиторий целиком к себе.
 Push (отправка) — отправить свои коммиты в удалённый репо.
 Status — показывает, что изменено/готово к коммиту.
 Stage / Index (стейджинг) — “буфер”: что попадёт в следующий коммит.
 Add — добавить изменения в stage.
 Diff — разница между версиями файлов/коммитами.
 Log — история коммитов.
 Stash — временно спрятать незакоммиченные изменения.
 Rebase (перебазирование) — перенести коммиты на другой “основание” (переписать историю).
 Cherry-pick — взять один конкретный коммит и применить в другую ветку.
 Reset — откатить ветку/стейдж (может убрать коммиты локально).
 Revert — “отмена коммита” новым коммитом (без переписывания истории).
 Conflict (конфликт) — Git не может сам слить изменения, нужно решить вручную.
 HEAD — указатель на текущий коммит/ветку, на которой вы стоите.
 .gitignore — список файлов, которые Git не отслеживает.


  -- Цвета файлов в проекте git --

 В PyCharm Professional (и других IDE, поддерживающих Git), цвета файлов в проекте обозначают их статус в системе
 контроля версий Git. Вот что они означают:

 - Красный файл - файл не отслеживается Git (untracked). Это новый файл, который еще не был добавлен в репозиторий.

 - Зеленый файл - файл добавлен в Git, но еще не закоммичен (new file). Это новый файл, который был добавлен в индекс
                 (staging area) с помощью git add, но изменения еще не зафиксированы.

 - Синий файл   - файл изменен (modified). Это файл, который уже отслеживается Git, и в него были внесены изменения,
                  но эти изменения еще не добавлены в индекс (staging area).

 - Голубой файл - файл изменен и добавлен в индекс (staged). Это файл, изменения которого были добавлены в индекс с
                  помощью git add, но еще не закоммичены.

 Также могут быть другие цвета:

 - Серый файл - файл игнорируется Git (например, добавлен в .gitignore).

 - Желтый файл - файл находится в конфликте после слияния (merge conflict).

 Синий цвет обычно означает измененный файл, который еще не добавлен в индекс. Если файл добавлен в индекс, он становится голубым.



 --- Какие есть ветки в Git ---

 Вот основные ветки, которые часто используются в Git:

 1. **`master` (или `main`)** - Основная ветка, содержащая стабильную и готовую к производству версию проекта.

 2. **`develop`** - Ветка для разработки, в которую интегрируются новые функции и изменения перед слиянием с
 `master` или `main`.

 3. **`feature/*`** - Ветки для разработки конкретных функций, которые создаются от `develop` и сливаются обратно
 после завершения.

 4. **`bugfix/*`** или **`hotfix/*`** - Ветки для исправления ошибок, которые могут создаваться как от `develop`,
 так и от `master`, в зависимости от ситуации.

 5. **`release/*`** - Ветки, предназначенные для подготовки к выпуску новой версии, где проводятся финальные тесты и
 фиксации изменений перед слиянием с `master`.

 Эти ветки помогают организовать процесс разработки и управления версиями в проекте.



 --- В чем разница между git rebase и git merge? ---

 merge  - Добавляет коммит мержа и сохраняет историю коммитов
 rebase - Применяет все коммиты на целевую ветку и удаляет историю коммитов

 Git Merge - Объединение веток
 - Команда `git merge` объединяет две ветки, создавая новый коммит, который включает изменения из обеих веток.
 В результате появляется так называемый "мердж-коммит".

 Git Rebase - Перемещение коммитов     "перезаписывает" историю.

 **Merge:** Используйте `merge`, когда хотите сохранить всю историю изменений, включая ветвление и слияния. Это особенно
 полезно в больших проектах с несколькими разработчиками.
 **Rebase:** Используйте `rebase`, чтобы иметь более чистую и понятную историю, особенно при работе с локальными
 ветками перед отправкой (push) на удалённый репозиторий.


 -- git stash --
 Команда git stash позволяет на время «сдать в архив» (или отложить) изменения, сделанные в рабочей копии,
 чтобы вы могли применить их позже.
 Откладывание изменений полезно, если вам необходимо переключить контекст и вы пока не готовы к созданию коммита.

 `git stash` — это команда в системе контроля версий Git, которая позволяет временно сохранить текущие изменения в
 рабочем каталоге и индексе (stage), чтобы вы могли вернуться к чистому состоянию репозитория. Это может быть полезно,
 если вы хотите переключиться на другую ветку или выполнить какую-либо другую задачу, не теряя незавершенные изменения.

 Когда вы выполняете `git stash`, Git делает следующее:

 1. Сохраняет ваши изменения (как отслеживаемые, так и неотслеживаемые файлы) в специальном хранилище (stash).
 2. Возвращает ваш рабочий каталог и индекс к состоянию последнего коммита.

 После этого вы можете переключаться на другие ветки, выполнять коммиты или любые другие действия, не беспокоясь о том,
 что потеряете ваши изменения.

 Для восстановления ваших изменений из стека вы можете использовать команду `git stash pop` или `git stash apply`.
 Первая команда удаляет сохранённые изменения из стека после их применения, а вторая — применяет изменения, оставляя их в стеке.

 git stash apply "stash@{3}"     - Выбор нужного stash  "stash@{1}", "stash@{2}", "stash@{3}"
 git stash apply                 - Выбор ПОСЛЕДНЕГО stash
 git stash -m "Имя"              - Задать ИМЯ (описания) стеша.  Без push ТОЖЕ Работает             <------
 git stash push -m "Имя"         - Задать ИМЯ (описания) стеша.                                     <------

 Некоторые часто используемые подкоманды:

 - `git stash list` — показывает список всех сохранённых изменений.
 - `git stash show` — отображает изменения, сохранённые в последнем хранилище (по умолчанию).
 - `git stash drop` — удаляет указанный элемент из стека.
 - `git stash clear` — очищает весь стек сохранённых изменений.

 Таким образом, `git stash` предоставляет удобный способ временно "отложить" изменения, чтобы вы могли продолжить
 работу с чистым состоянием репозитория.


 Посмотреть изменения файла - Нажимаем на файлик и выбираем Show diff

 -- cherry-picking --
 В GitLab (и Git) **cherry-picking** — это процесс выбора отдельных коммитов из одной ветки и применения их в другую ветку.
 Это позволяет перенести конкретные изменения или исправления без слияния всей ветки. Обычно используется для быстрого
 исправления ошибок или выборочного внедрения функций.

 Cherry-picking — это выборочное применение изменений из одной ветки репозитория в другую.
 1. Найдите нужный коммит:

    git log

 2. Перейдите на целевую ветку:

    git checkout target-branch

 3. Примените коммит:

    git cherry-pick <commit-hash>

 Это добавит изменения из указанного коммита в текущую ветку.


 -- Как создать ветку в Git --
 git branch имя_ветки

 Например:
 git branch feature/new-feature

 **Переключитесь на эту ветку**
 git checkout имя_ветки             git switch имя_ветки     # Лучше использовать switch  для переключения между ветками

 **Создание новой ветки и переключение на нее**
 git checkout -b имя_ветки

 **Просмотреть все ветки** в репозитории:
 git branch

 **Удалить ветку** (если требуется):
 git branch -d имя_ветки

 ** Принудительное удаление ветки.
 git branch -D имя_ветки



 -- git branch  vs git tag --

 В branch — коммитить можно.
 В tag    — коммитить нельзя.

 branch — для работы с изменениями (движется).
 tag    — для отметки версий (не двигается).

 Git branch — это подвижный указатель на коммит, используемый для разработки и слияния изменений.
 Git tag    — это статичная метка, обычно отмечающая конкретную версию (например, релиз).



 -- Pull request --

 Pull request (PR) — это функция в системах управления версиями, таких как Git, которая позволяет разработчикам
 сообщать о том, что они внесли изменения в код и предлагают интегрировать эти изменения в основную ветку проекта.
 Pull request обычно создаётся, когда разработчик завершает работу над какой-либо задачей в отдельной ветке и хочет,
 чтобы другие участники проекта просмотрели, обсудили и, при необходимости, внесли изменения, прежде чем эти изменения
 будут объединены (или «вмержены») в основную ветку (например, `main` или `master`).

 Процесс работы с pull request обычно включает следующие шаги:

 1. **Создание ветки**: Разработчик создаёт новую ветку для выполнения своей работы.
 2. **Внесение изменений**: Проделываются необходимые изменения в коде.
 3. **Коммит и пуш**: Изменения коммитятся в локальной ветке и отправляются (push) на удалённый репозиторий.
 4. **Создание Pull request**: Через интерфейс удалённого репозитория (например, GitHub, GitLab, Bitbucket и др.)
 создаётся pull request на слияние созданной ветки с основной.
 5. **Код-ревью**: Другие разработчики или участники проекта могут просмотреть изменения, оставить комментарии,
 предложить улучшения или задать вопросы.
 6. **Слияние**: После обсуждения и одобрения изменений pull request может быть объединён с основной веткой.

 Pull request не только упрощает процесс интеграции кода, но и способствует улучшению качества кода благодаря код-ревью
 и обсуждениям среди команды.

 Pull request (PR) — это запрос на слияние изменений из одной ветки кода в другую в системе контроля версий Git.
 Он позволяет разработчикам предложить свои изменения, получить отзывы от команды и обсудить их, прежде чем объединить
 с основной веткой проекта. PR включает описание изменений, обсуждения и возможность проведения код-ревью.

 **Создание Pull Request**:
 - Перейдите в ваш репозиторий на GitHub (или другой платформе).
 - В разделе "Pull requests" нажмите на кнопку "New pull request".
 - Выберите вашу ветку (`feature/my-feature`) и основную ветку (`main`).
 - Заполните заголовок и описание PR, объясняя изменения и их цель.
 - Нажмите на кнопку "Create pull request".


 -- Команды GIT --
 Основные команды для работы с состояниями файлов

 1. git status: Показывает текущее состояние репозитория, включая изменения, которые были сделаны, но еще не
    зафиксированы, и файлы, готовые к коммиту.

 2. git add: Добавляет изменения в указанных файлах в индекс (staging area) для последующего коммита.

 3. git commit: Фиксирует изменения в локальном репозитории с соответствующим сообщением о коммите.

 4. git diff: Показывает различия между файлами: изменения в рабочем каталоге, разницу между индексом и
    последним коммитом или между двумя коммитами.

 5. git reset: Отменяет изменения в индексе или в рабочем каталоге; может быть использована с разными уровнями
    "жесткости" (например, --soft, --mixed, --hard) в зависимости от того, что нужно сбросить.


 -- РАЗРЕШИТЬ КОНФЛИКТЫ В GIT --

 # Переключитесь на основную ветку и обновите её:
 git checkout main
 git pull origin main

 # Переключитесь обратно на вашу ветку:
 git checkout имя_ветки

 # Начните процесс rebase:
 git rebase main

 # Будет окошно с редактированием
 Shift + :  и вводим  wq и нажимаем Enter

 # Решите конфликты:  Выбираем какие изменения нужно оставить   <<<  >>>

 git rebase --continue

 # Запушите изменения в удалённый репозиторий:  ПОСЛЕ rebase  делаем  git push --force
 git push origin имя_ветки --force    или    git push --force   или   git push -ff

 при разрешении конфликтов слияния (merge conflicts) F7 используется для пошагового перехода между конфликтующими изменениями
 F7         - перейти к следующему конфликту.
 Shift + F7 – перейти к предыдущему конфликту.


 -- ПОДТЯНУТЬ ИЗМЕНЕНИЯ --
 1) git stash
 2) git checkout main
 3) git pull origin main
 4) git checkout имя_ветки
 5) git rebase main
 6) git status
 7) git stash apply


 -- amend --
 git commit --amend — перезаписывает последний коммит: меняет сообщение и/или состав файлов, создавая новый коммит
 вместо старого (переписывает историю).


 -- git fetch --

 git fetch - это команда, которая загружает последние изменения из удалённого репозитория (новые ветки, коммиты, теги),
 но не сливает их с вашей текущей рабочей версией.  "Скачай изменения с сервера, но пока ничего не меняй в своём коде"

 # Добавить ветку  # Загружает конкретную ветку release/01.003.00 из удалённого репозитория origin.
 git fetch origin release/01.003.00

 # Если вам нужно просто обновить данные о всех удалённых ветках, проще использовать
 git fetch --all


 -- Сделать Pull Request --
 Переходим в папку проекта через cd
 1) git checkout -b имя_ветки
 2) git push   или   Команда git push --set-upstream origin имя_ветки
 3) Переходим по ссылке на Pull Request

 Команда git push --set-upstream origin имя_ветки используется для отправки локальной ветки имя_ветки на
 удалённый репозиторий origin и установки её в качестве "отслеживаемой" ветки.
 Это означает, что в дальнейшем вы сможете использовать просто git push и git pull без указания имени ветки,
 и Git будет знать, что работать нужно с этой веткой на удалённом репозитории.


 Команда git rebase -i HEAD~4 используется в Git для интерактивного перебазирования (interactive rebasing)
 последних 4 коммитов, находящихся в истории вашей текущей ветки.

 Можно засквошить перед показом ПРа (чтобы не править много конфликтов)  <----


 -- squash коммитов --

 если вдруг ошибаемся  - МОЖНО Редактировать(удаляем) скрытый файл в папке  .git          <-----
 git bash  (команды linux)     СЛЭШИ в ОС Linux  //   в Windows \\

 переходим в нужную директорию  	   -  cd c://W_Project/ae-lcp           в Windows   cd C:\W_Project\ae-lcp
 убедимся что находимся в нужной ветку -  git status
 посмотреть сколько коммитов    	   -  git log
 если надо прячем в стэш измененные файлы    	   -  git stash
 сколько коммитов нужно склеить        -  git rebase HEAD~2  (2 - это число коммитов которые нужно убрать) (НЕинтерактивный) ребейз.

 сколько коммитов нужно склеить        -  git rebase -i HEAD~2  (2 - это число коммитов которые нужно убрать) интерактивный ребейз.

 войти в режим вставки (insert mode)   -  i (удалять можно через кнопку delete)            Если ваш редактор это Vim
 # меняем все pick на squash  КРОМЕ ПЕРВОГО
 для выхода из режима редактирования   -  Ctrl + C
 для сохранения изменений              -  Shift + :     Вводим wq и нажимаем Enter    wq! - игнорировать любые предупреждения
 # затем снова режим редактирования откроется там удаляем все коммиты кроме первого
 комменты # можно оставить удаляем всё кроме первого без комментов  (Редактируем текст сверху)
 если ошибся                           -  git rebase --abort
 затираем историю   				   -  git push --force   или команда git push -ff

 git rebase HEAD~2      - НЕ даёт вам возможности изменять, переименовывать или удалять отдельные коммиты.
 git rebase -i HEAD~2   - Вы получаете текстовое меню, где можете изменять порядок коммитов, объединять их
 (с помощью команды squash или fixup), изменять сообщения коммитов, а также удалять коммиты. Это более гибкий подход,
 позволяющий вам управлять историей коммитов более детально.


 Git Bash, который эмулирует среду Unix, команды и синтаксис отличаются от командной строки Windows

 В Linux и Git Bash используются прямые слэши /.

 В Windows используются обратные слэши \.



 -- Отменить commit --
 Сверху Pycharm выбираем Git -> VCS Operations -> Show History -> Другой кнопкой мышки -> Revert commit   или Alt + 9
 (Файлы должны быть в git stash)

 Git Stash: Перед отменой коммита вам не обязательно помещать файлы в git stash, если вы просто хотите отменить
 последний коммит и оставить изменения в рабочем каталоге. Однако, если вы хотите сохранить текущие изменения,
 которые не были добавлены в коммит, тогда использование git stash может быть хорошей идеей.

 Шаги для отмены коммита:

 В верхнем меню PyCharm выберите Git -> VCS Operations Popup (можно также использовать комбинацию клавиш Alt + 9).
 Далее выберите Show History, чтобы открыть историю коммитов.
 Найдите нужный коммит в списке, щелкните по нему правой кнопкой мыши.
 Выберите Revert Commit, чтобы отменить изменения, внесенные в этом коммите.
 Результат: После выполнения этих шагов изменения, сделанные в отмененном коммите,
 будут добавлены обратно в ваш рабочий каталог, и вы сможете их отредактировать или создать новый коммит.

 Если у вас есть какие-то изменения в рабочем каталоге, которые вы не хотите потерять, используйте git stash перед
 отменой коммита. Если вы хотите просто отменить коммит, можно обойтись и без этого шага.


 -- Отменить commit через Терминал --

 Проверьте статус репозитория (необязательно, но полезно):
 git status

 Отмените последний коммит: Чтобы отменить последний коммит, оставив изменения в рабочем каталоге, используйте следующую команду:
 git reset --soft HEAD~1

 Если вы хотите удалить коммит и убрать изменения из рабочего каталога, используйте:
 git reset --hard HEAD~1

 (Опционально) Использование git stash: Если вы хотите сохранить текущие изменения перед отменой коммита,
 вы можете использовать команду git stash:
 git stash

 Затем выполните команду для отмены коммита:
 git reset --soft HEAD~1

 После этого вы можете вернуть изменения из stash:
 git stash pop


 -- Добавить файлы в git через терминал --
 git status и выбираем файлы которые нужно закоммитить (они будут красным цветом)
 git add src/repository/blocking.py    (Добавляем все файлы которые нужно закоммитить)
 git commit -m "Имя коммитам"


 -- pre-commit --
 pre-commit — это инструмент для автоматизации проверок и форматирования кода перед коммитом в систему контроля версий.
 Он позволяет запускать скрипты и утилиты, чтобы предотвратить добавление некачественного кода в репозиторий.

 pre-commit run --all-files  - выполняет все настроенные хуки для всех файлов в репозитории, независимо от того,
                               были ли они изменены или нет.

 pre-commit                  - без дополнительных аргументов обычно выполняет хуки только для изменённых файлов,
                               которые были добавлены в индекс (то есть те, которые готовы к коммиту).

 git add -u добавляет изменения в уже отслеживаемых файлах (изменения и удаление), но игнорирует новые неотслеживаемые файлы.
 -u или --update указывает Git добавить изменения только для файлов, которые уже находятся в индексе
 (т.е. были добавлены ранее с помощью git add).

 Таким образом, git add -u полезен, когда вы хотите зафиксировать изменения в уже отслеживаемых файлах,
 не добавляя при этом новые файлы.

 Порядок действий, делаем две команды одна за одной пока всё не пройдёт
 git add -u
 pre-commit


 Для pre-commit добавьте файл .pre-commit-config.yaml (хуки перед коммитом).
 ruff (линтер)
 mypy (типы)
 pytest (тесты)
 black (форматирование)

 pre-commit автоматически проверяет код перед коммитом.
 Теперь перед каждым коммитом будут запускаться проверки.


 -- Клонировать проект git clone --
 Переходим в нужную директорию через cd
 пишем в терминале команду:  git clone ссылка_на_проект


  -- Редактор Vim (ТЕКСТОВЫЙ РЕДАКТОР) --

 Основные команды:
 dd - удалить текущую строку.
 i - войти в режим вставки (insert mode).
 d30 - удалить 30 строк, начиная с текущей (вводится как d30 + Enter). Вместо 30 можно указать любое другое число.
 Сохранение и выход из редактора
 :w - сохранить файл.
 :q - выйти из Vim (если не было изменений).
 :q! - выйти из Vim без сохранения изменений.
 :wq - сохранить файл и выйти из Vim.
 :wq! - сохранить файл и выйти из Vim.  игнорировать любые предупреждения


 Редактирование текста    Выход из режима РЕДАКТИРОВАНИЯ: Esc или Ctrl + C.  Pycharm Pro
 i - вставить текст перед курсором.
 a - вставить текст после курсора.
 o - вставить новую строку после текущей строки и перейти в режим вставки.
 yy - скопировать текущую строку.
 p - вставить скопированный или вырезанный текст после курсора.
 u - отменить последнее действие.
 Ctrl + r - повторить последнее отмененное действие.


 Дополнительные команды
 h - показать справку с командами редактора (в режиме команд).
 x - удалить текущий символ (не строку).
 j - перейти к следующей строке.
 k - перейти к предыдущей строке.
 gg - перейти к первой строке.
 G - перейти к последней строке.
 :set number - включить отображение номеров строк.
 :set nonumber - отключить отображение номеров строк.
 /текст - найти строку (поиск по тексту, после / вводите текст для поиска).
 c - изменить текущую строку (входит в режим вставки).
 P - вставить строку перед текущей.

 Все эти команды относятся к редактору Vim, который является мощным инструментом для редактирования текста в Unix-подобных системах.

 git bash
 squash - в режиме редактирования (i) pick меняем на squash
 squash - сжатие или объединение данных


 --- Модуль functools предназначен для функций высшего порядка: функций, которые действуют или возвращают другие функции.
 В общем, любой вызываемый объект может рассматриваться как функция для целей этого модуля.

 Кэширование результатов представлено тремя функциями (можно использовать в качестве декоратора)
 - functools.lru_cache(),- functools.cache(),- functools.cached_property().

 @functools.total_ordering - Автоматическая реализация операторов сравнения.
 functools.cmp_to_key(func) - преобразует функцию сравнения старого стиля в ключевую функцию сравнения.
 functools.partial(func, /, *args, **keywords) - Заморозить часть аргументов вызываемой функции.
 functools.reduce(function, iterable[, initializer]) - берет итерируемый объект и уменьшает (или складывает) все его значения в одно.
 @functools.singledispatch - Декоратор @singledispatch модуля functools создает из обычной функции - универсальную
 функцию одиночной диспетчеризации.
 class functools.singledispatchmethod(func) - Декоратор singledispatchmethod() модуля functools создает из обычного
 метода класса - универсальный метод одиночной диспетчеризации.

 @functools.wraps(wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES) - Сохранение имени и строк документации
 декорированной функции.   Заменить атрибуты декоратора на атрибуты исходной функции.

 Модуль multipledispatch и его декоратор @dispatch ведут себя очень похоже на @singledispatchmethod.
 Единственная разница заключается в том что он может принимать в качестве аргументов несколько типов


 --- Модуль itertools в Python, эффективные итераторы для циклов   - сборник полезных итераторов ---

 Но если itertools вам уже не хватает, то вэлкам: more-itertools.
 Тут есть chunked, spy, first, one, only, unique_everseen и прочие прелести. Осторожно, с этой дряни невозможно слезть.


 --- Бесконечные итераторы   Infinite iterators ---

 Iterator       Arguments         Results                            Example

 count()        [start[, step]]   start, start+step, start+2*step, … count(10) → 10 11 12 13 14 ...

 cycle()        p                 p0, p1, … plast, p0, p1, …         cycle('ABCD') → A B C D A B C D ...

 repeat()       elem [,n]         elem, elem, elem, … endlessly      repeat(10, 3) → 10 10 10
                                  or up to n times


 --- Конечные итераторы    Iterators terminating on the shortest input sequence ---
 --- Итераторы, оканчивающиеся на самой короткой входной последовательности ---


 Iterator       Arguments          Results                             Example

 accumulate()   p [,func]          p0, p0+p1, p0+p1+p2, …              accumulate([1,2,3,4,5]) → 1 3 6 10 15

 batched()      p, n               (p0, p1, …, p_n-1), …               batched('ABCDEFG', n=3) → ABC DEF G

 chain()        p, q, …            p0, p1, … plast, q0, q1, …          chain('ABC', 'DEF') → A B C D E F

 chain.from_iterable()  iterable   p0, p1, … plast, q0, q1, …          chain.from_iterable(['ABC', 'DEF']) → A B C D E F

 compress()     data, selectors    (d[0] if s[0]), (d[1] if s[1]), …   compress('ABCDEF', [1,0,1,0,1,1]) → A C E F

 dropwhile()    predicate, seq     seq[n], seq[n+1], starting when     dropwhile(lambda x: x<5, [1,4,6,3,8]) → 6 3 8
                                   predicate fails

 filterfalse()  predicate, seq     elements of seq where               filterfalse(lambda x: x<5, [1,4,6,3,8]) → 6 8
                                   predicate(elem) fails

 groupby()      iterable[, key]    sub-iterators grouped by value     [k for k, g in groupby('AAAABBBCCDAABBB')] → A B C D A B
                                   of key(v)                          [list(g) for k, g in groupby('AAAABBBCCD')] → AAAA BBB CC D

 islice()       seq, [start,]      elements from seq[start:stop:step] islice('ABCDEFG', 2, None) → C D E F G
                stop [, step]

 pairwise()     iterable           (p[0], p[1]), (p[1], p[2])         pairwise('ABCDEFG') → AB BC CD DE EF FG

 starmap()     func, seq           func(*seq[0]), func(*seq[1]), …    starmap(pow, [(2,5), (3,2), (10,3)]) → 32 9 1000

 takewhile()   predicate, seq      seq[0], seq[1],                    takewhile(lambda x: x<5, [1,4,6,3,8]) → 1 4
                                   until predicate fails

 tee()         it, n               it1, it2, … itn splits       [list(i) for i in tee([1, 2, 3], 2)] →[[1, 2, 3], [1, 2, 3]]
                                   one iterator into n

 zip_longest() p, q, …             (p[0], q[0]), (p[1], q[1]), …  zip_longest('ABCD', 'xy', fillvalue='-') → Ax By C- D-



 --- Комбинаторные итераторы   Combinatoric iterators ---

 Iterator       Arguments          Results                             Example

 product()      p, q, … [repeat=1] cartesian product, equivalent      product('ABCD', repeat=2)
                                   to a nested for-loop               AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DD

 permutations() p[, r]             r-length tuples, all possible      permutations('ABCD', 2)
                                   orderings, no repeated elements    AB AC AD BA BC BD CA CB CD DA DB DC

 combinations() p, r               r-length tuples, in sorted order,  combinations('ABCD', 2)
                                   no repeated elements               AB AC AD BC BD CD

 combinations_with_replacement() p, r     r-length tuples, in sorted order,     combinations_with_replacement('ABCD', 2)
                                          with repeated elements                AA AB AC AD BB BC BD CC CD DD



 --- Модуль operator в Python  Функциональный интерфейс для встроенных операторов ---

 - Общий интерфейс поиска атрибутов и элементов модуля operator в Python -

 - Поиск атрибутов или элементов последовательностей/итераций -

 Одной из самых необычных особенностей модуля operator является концепция геттеров. Это вызываемые объекты, созданные
 во время выполнения для извлечения атрибутов объектов или содержимого из последовательностей.

 Геттеры особенно полезны при работе с итераторами или последовательностями генераторов, где они предназначены для
 быстрого извлечения полей аргументов из функций для map(), sorted(), itertools.groupby() или других функций, которые
 ожидают аргумент переданной функции.

 --- Полезный пример ---
 from operator import attrgetter, itemgetter

 class Cat:
     def __init__(self, name, age):
         self.name = name
         self.age = age

     def __repr__(self):
         return f'Cat {self.name}, age is {self.age}'

 if __name__ == '__main__':
     ints = list(range(20))
     print(list(map(lambda x: x ** 2, filter(lambda x: x % 2 == 0, ints))))
     print([i**2 for i in range(20) if i % 2 == 0])
     a_dict = {'a': 3, 'b': 2, 'd': 1, 'c': 4}
     print(sorted(a_dict.items(), key=lambda x: x[0]))
     print(sorted(a_dict.items(), key=itemgetter(1)))
     cats = [Cat('Tom', 3), Cat('Angela', 4)]
     print(sorted(cats, key=lambda x: x.age))
     print(sorted(cats, key=attrgetter('age')))
     print(sorted(cats, key=attrgetter('name')))


 --- Модуль collections ---
 # OrderedDict нужен для действий со словарем где необходим порядок элементов, например
 # сравнение с учетом порядка, перестановки элементов с сохранением порядка. Платим памятью!!!

 # ChainMap нужен для логического обьединения словарей для поиска информации, но при изменениях меняется первый словарь

 # Counter нужен для подсчета элементов в последовательности, работает только с hashable
 from collections import Counter

 print(Counter([1, 2]))          # -> Counter({1: 1, 2: 1})
 print(Counter([1, 2, [1, 2]]))  # -> TypeError: unhashable type: 'list'

 # defaultdict нужен для создания словаря по умолчанию. Значение подставляется при обращении к несуществующему ключу

 # deque(двунаправленная очередь) потокобезопасна, быстро оперирует с обеими сторонами

 # namedtuple нужен для создания структуры данных, нечто среднее между стандартными типами и самописными классом.
 # Неизменяемый, позволяет обращаться по имени атрибута, позволяет использовать индексы.


 # МОЖНО ПЕРЕДЕЛАТЬ В СТАНДАРТНЫЕ СТРУКТУРЫ ДАННЫХ PYTHON!  collections
 from collections import deque, Counter

 a_deque = deque([1, 2, 3])
 print(a_deque)         # -> deque([1, 2, 3])
 print(list(a_deque))   # -> [1, 2, 3]

 b_counter = Counter([1, 2, 3])
 print(b_counter)         # -> Counter({1: 1, 2: 1, 3: 1})
 print(dict(b_counter))   # -> {1: 1, 2: 1, 3: 1}


 Например, циклы for, списковые включения и генераторы - все это ФАСАДЫ более сложного протокола Итератор.
 Реализация defaultdict - это фасад, который не учитывает исключительные ситуации, когда ключа нет в словаре.

 # Циклы/генераторы - это простой интерфейс для сложного механизма итераторов (__iter__/__next__).
 # Пример: for x in [1,2,3] скрывает вызовы iter() и next().

 # defaultdict - упрощает работу с отсутствующими ключами (автоматически создаёт значения), но:

 # - Не обрабатывает случаи, когда фабрика не может создать значение
 # - Не заменяет полноценную обработку исключений try/except

 # Суть: Оба примера - это "удобные обёртки" над сложной логикой, которые экономят код.  Паттерн - ФАСАД   <----


  --- Паттерны ---
 Паттерны или ШАБЛОНЫ разработки - это общие способы решения частых задач и проблем

 Паттерны в Python – это шаблоны для решения задач, которые часто встречаются в практике программиста.
 Паттерны — это типовые решение для типовых задач.
 Существует три основные разновидности паттернов:
 1) Архитектурные паттерны, 2) Паттерны проектирования, 3) Идиомы

 ПАТТЕРНЫ ПРОЕКТИРОВАНИЯ:
 1) Порождающие паттерны — отвечают за процесс создания объектов.
 2) Структурные паттерны — определяют отношения между объектами, облегчая их взаимодействие
 3) Поведенческие паттерны — определяют способы коммуникации между объектами

  Книга о разработке и алгоритмах. Банда Четырех Паттерны Проектирования.
  Авторы: Гамма Эрих, Хелм Ричард, Джонсон Роберт, Влиссидес Джон

 Большинство паттернов проектирования основаны на двух базовых принципах ООП: КОМПОЗИЦИИ и НАСЛЕДОВАНИИ   <-----

 Порождающие паттерны:

 1)  Абстрактная фабрика (Abstract factory):
 создает семейства связанных объектов, не привязываясь к конкретным классам создаваемых объектов.


 2)  Строитель (Builder):
 От абстрактной фабрики отличается тем, что делает акцент на пошаговом конструировании объекта.
 Строитель возвращает объект на последнем шаге, тогда как абстрактная фабрика возвращает объект немедленно.

 Используется для пошагового создания сложных обьектов


 3)  Фабричный метод (Factory method):
 Создает объекты без указания точного класса, но оставляет подклассам решение о том, какой класс инстанцировать.

 Инстанцирование — создание экземпляра класса

 4) Прототип (Prototype):
 Создает новые экземпляры объекта путем клонирования прототипа: Пример Модуль copy


 5) Одиночка (Singleton):
 У класса есть только один экземпляр : Пример Модуль — это файл с расширением .py

 С помощью паттерна одиночка могут быть реализованы многие паттерны (АБСТРАКТНАЯ ФАБРИКА, СТРОИТЕЛЬ, ПРОТОТИП).


Структурные паттерны:

 1) Адаптер (Adapter):
  позволяет объектам с несовместимыми интерфейсами работать вместе


 2) Мост (Bridge):
  Отделяет абстракцию от ее реализации так, чтобы и то и другое можно было изменять независимо.


 3) Компоновщик (Composite):
  упрощает создание иерархий объектов:  Python это не особенно нужно, так как для этой цели хватает словарей

  Описывает группу объектов, которая рассматривается как один экземпляр.


 4) Декоратор (Decorator):
 Динамически Добавляет дополнительную функциональность, не изменяя класс.


 5) Фасад (Facade):
 Фасад надстраивает простой интерфейс поверх сложного

  Фасад надстраивает простой интерфейс поверх сложного,
  Адаптер надстраивает унифицированный интерфейс над каким-то другим (необязательно сложным).

 Унифицировать - приведение к единообразию, к единому виду
 Унифицированный интерфейс упрощает поддержку версионности(способность иметь версии), что облегчает последующую эволюцию продукта.

 6) Приспособленец (Flyweight):
 для уменьшения затрат памяти при работе с большим количеством мелких объектов:  Пример - Менеджер памяти Python


 7) Заместитель (Proxy, Surrogate)
 позволяет создать объект-замену для другого объекта: Пример - Mock(макет-пустышка)


 Поведенческие Паттерны :

 1) Цепочка ответственности (Chain of responsobility):
  Цель Разрешить передачу запроса по цепочке получателей до тех пор, пока он(запрос) не будет обработан.


 2) Команда (Command):
  превращает запросы в объекты, позволяя передавать их как аргументы при вызове методов, Пример: Django HttpRequest (без метода выполнения)
  ставить запросы в очередь, логировать их, а также поддерживать отмену операций


 3) Интерпретатор (Interpreter):
 Для заданного языка определяет представление его грамматики,
 а также интерпретатор предложений этого языка.


 4) Итератор (Iterator):
 позволяет последовательно обойти элементы коллекции, не раскрывая внутренних деталей реализации.


 5) Посредник (Mediator):
 Инкапсулирует взаимодействие набора объектов которые ничего не знают друг о друге


 6) Хранитель (Memento):
 Предоставляет возможность восстановить объект в предыдущее состояние: Пример: модуль json, модуль pickle
 Позволяет вернуть предыдущее состояние объекта


 7) Наблюдатель (Observer):
 Определяет зависимость типа "один ко многим" между объектами таким образом,
 что при изменении состояния одного объекта все зависящие от него оповещаются об этом
 и автоматически обновляются

 # Пример: Django Signals(Сигналы), Flask Signals(Сигналы)

 8) Состояние (State):
 создания объектов, поведение которых изменяется при изменении состояния


 9) Стратегия (Strategy):
 Позволяет выбрать алгоритм во время выполнения.


 10) Шаблонный метод (Template method):
 --- CBV  -  Class-Based Views --- базовый класс - оставив реализацию некоторых шагов подклассам.  Django CBV


 11) Посетитель (Visitor):
 когда нужно применить функцию к каждому элементу коллекции: Пример - функция map()



 -- DAO (Data Access Object) --
 DAO (Data Access Object) в Python — это паттерн проектирования, который предоставляет абстрактный интерфейс для работы
 с данными, отделяя бизнес-логику приложения от деталей работы с хранилищем (базой данных, файлами, API и т. д.).

 Основные цели DAO:

 - Инкапсуляция логики доступа к данным.
 - Упрощение замены источника данных (например, переход с SQLite на PostgreSQL).
 - Уменьшение дублирования кода, связанного с операциями CRUD (Create, Read, Update, Delete).



 --- REGEXP ---

 x(?=y) находит x, только если за x следует y             # Positive Lookahead
 x(?!y) находит x, только если за x НЕ следует y          # Negative Lookahead
 (?<=y)x находит x, только если перед x следует y         # Positive Lookbehind
 (?<!y)x находит x, только если перед x НЕ следует y      # Negative Lookbehind

 Модуль re - это просто модуль расширения на языке C, включенный в Python   Шаблоны обрабатываются как строки   <-----


 Специальные символы:
 '.'              - любой символ кроме новой строки;  кроме переноса строки \n
 '^'              - началу строки;   Добавляем в НАЧАЛЕ строки
 '$'              - конец строки;    Добавляем в КОНЦЕ строки
 '*'              - Предыдущий символ 0 или более повторений;   сокращенная форма: {0,}    Жадный
 '+'              - Предыдущий символ 1 или более повторений;   сокращенная форма: {1,}    Жадный
 '?'              - Предыдущий символ 0 или 1 повторений;       сокращенная форма: {0,1}
 '*?', '+?','??'  - ограничение жадности;
 '*+', '++', '?+' - притяжательные квантификаторы, (новое в Python 3.11);
 '{m}'            - m повторений;                                             Нет смысла ставить '?'  Жадный/НЕ Жадный
 '{m,n}'          - как можно больше повторений в промежутке от m до n ;    Жадный
 '{m,n}?'         - как можно меньше повторений в промежутке от m до n;  НЕ Жадный/Ленивый
 '{m,n}+'         - притяжательная версия квантификатора выше, (новое в Python 3.11);
 '\'              - экранирование специальных символов;
 '[]'             - символьный класс;           Символьный класс [] - описывает только 1 символ
 '|'              - или  or;        если A совпало, то B не будет проверяться,  оператор ' | ' никогда не бывает жадным.
 '(...)'          - группа с захватом;


 Квантификаторы '*', '+' и '?' являются жадными. Они пытаются захватить как можно больше текста для анализа.
 Жадный      - означает что квантификатор находит самые 'ДЛИННЫЕ' последовательности     'ДЛИННЫЕ'       <-----
 НЕ Жадный ? - означает что квантификатор находит самые 'КОРОТКИЕ' последовательности    'КОРОТКИЕ'      <-----


 Расширения регулярных выражений:
 '(?aiLmsux)'                         - установка флагов регулярного выражения;
 '(?aiLmsux-imsx:...)'                - установка и удаление флагов;
 '(?>...)'                            - атомарная группа, (новое в Python 3.11);
 '(?:...)'                            - группа без захвата;
 '(?P<name>...)'                      - именованная группа;
 '(?P=name)'                          - обратная ссылка на именованную группу;
 '(?#...)'                            - комментарий;
 '(?=...)'                            - опережающая позитивная проверка;
 '(?!...)'                            - опережающая негативная проверка;
 '(?<=...)'                           - позитивная ретроспективная проверка;
 '(?<!...)'                           - негативная ретроспективная проверка;
 '(?(id/name)yes-pattern|no-pattern)' - стараться соответствовать yes-pattern;

  Вот полный список метасимволов: '.', '^', '$', '*', '+', '?', '{', '}', '[', ']', '\', '|', '(', ')'

 Для часто используемых символьных классов существуют краткие обозначения.                                     <-----
 Специальные последовательности:                              Эквивалентно символьному классу []:
 '\number' - соответствие группы с тем же номером;
 '\A'      - только с начало строки;
 '\b'      - пустая строка (начало или конец слова);                                   инверсия - это противоположность
 '\B'      - пустая строка (НЕ начало или НЕ конец слова);                                 '\B' инверсия '\b'
 '\d'      - любая десятичная цифра;                          [0-9]
 '\D'      - НЕ десятичная цифра;                             [^0-9]               [^\d]   '\D' инверсия '\d'
 '\s'      - пробельный символ;                               [ \t\n\r\f\v]
 '\S'      - НЕ пробельный символ;                            [^ \t\n\r\f\v]       [^\s]   '\S' инверсия '\s'
 '\w'      - буквенно-цифровые(str.isalnum()) и _             [A-Za-zА-Яа-я0-9_]
 '\W'      - НЕ буквенно-цифровые(str.isalnum()) и _          [^A-Za-zА-Яа-я0-9_]  [^\w]   '\W' инверсия '\w'
 '\Z'      - только с конец строки;


 fr'' rf'' - Нет разницы
 2 префикса сразу. fr-строка:  fr'test'       fr'' rf''  для escape в регулярках не работает квантификатор  {}
 2 префикса сразу. rf-строка:  rf'test'       fr'' rf''  для escape в регулярках не работает квантификатор  {}
 Решение!!!     - Используем двойные {{ }} и всё будет работать
 Можно использовать двойные фигурные скобки при f-string & r-raw, т.е. это будет как \w{,2}     re.sub(fr'{a}\w{{,2}}'

 Пример fr'' rf''  Используем двойные {{ }} и всё будет работать

 re - Могут принимать в качестве аргумента  str  или bytes                                  <----   ВАЖНО!!!

 # rb  Для поиска в b''  БАЙТОВЫХ СТРОКАХ                                                   <----   ВАЖНО!!!
 text = '111'
 text_rb = b'111'
 print(re.findall(r'1', text))      # -> ['1', '1', '1']
 print(re.findall(rb'1', text_rb))  # -> [b'1', b'1', b'1']         # rb''



  --- Для чего вообще АРХИТЕКТУРА? ---

  Архитектура нужна, чтобы:

 - Писать код, который легко поддерживать.
 - Быстро и надёжно тестировать.
 - Масштабировать проект без боли.
 - Избегать "хаоса" в больших проектах.


  Коротко об архитектуре в программировании (Python)
  Зачем?
 - Читаемость – Хорошая архитектура делает код интуитивно понятным даже для новых разработчиков.
 - Масштабируемость – Позволяет добавлять фичи без переписывания всей системы.
 - Тестируемость – Изолированные модули позволяют писать юнит-тесты и интеграционные тесты без сложных моков.
 - Гибкость – Можно заменять библиотеки, фреймворки и даже БД с минимальными изменениями.
 - Производительность – Хорошая архитектура не всегда = высокая скорость работы, но помогает выявлять и исправлять узкие места.


 Примеры архитектурных подходов:

 - MVC (Model-View-Controller) – Классика (Django, Flask).
 - Сервис-слой – Бизнес-логика в отдельных сервисах (полезно в FastAPI).
 - Clean Architecture / Hexagonal – Независимость от фреймворков и БД.
 - DDD (Domain-Driven Design) – Акцент на бизнес-логику.
 - CQRS – Разделение чтения и записи данных (полезно в микросервисах).




  --- Архитектура и System Design ---

 Архитектура — это общий план или структура системы, определяющая её компоненты и их взаимодействие. Это более
 высокоуровневый подход, который описывает, как система будет организована.

 System Design — это более детализированный процесс, включающий проектирование конкретных компонентов,
 их функциональности и взаимодействия. Это этап, на котором разрабатываются технические решения и спецификации.

 Вкратце, архитектура — это "что" и "почему", а системный дизайн — это "как".                       <------


 Архитектура приложения на Python — это структура, которая определяет, как компоненты приложения взаимодействуют друг с другом.

 Основные архитектурные стили включают:

 - Монолитная архитектура: Все компоненты интегрированы в одно приложение. Простота в разработке и развертывании, но сложность масштабирования.

 - Микросервисная архитектура: Приложение разделено на независимые сервисы, которые общаются между собой через API.
  Обеспечивает гибкость и масштабируемость, но требует сложного управления.

 - MVC (Model-View-Controller): Разделяет приложение на три компонента. Модель управляет данными, представление отвечает
   за пользовательский интерфейс, а контроллер обрабатывает ввод пользователя.

 - Событийно-ориентированная архитектура: Компоненты взаимодействуют через события. Хорошо подходит для распределенных систем.


 Вопросы по Архитектуре и System Design:

 - Каковы основные требования к архитектуре приложения?

 Масштабируемость, производительность, безопасность, поддерживаемость, простота развертывания.


 - Как обеспечить безопасность в веб-приложении на Python?

 Использование HTTPS, защита от CSRF и XSS, аутентификация и авторизация, регулярные обновления зависимостей.


 - Как выбрать между монолитом и микросервисами?

 Монолит лучше для небольших проектов. Микросервисы подходят для крупных, сложных систем с высокими требованиями к масштабируемости.


 - Что такое API и какую роль он играет в архитектуре?

 API (интерфейс программирования приложений) позволяет различным компонентам системы взаимодействовать друг с другом.
 Ключевой элемент в микросервисной архитектуре.

 API (Application Programming Interface) — интерфейс для запросов к сервису. Обычно используют HTTPS и часто требуют аутентификацию пользователя.
 HTTPS (HyperText Transfer Protocol Secure) — HTTP поверх шифрования TLS.
 SSL (Secure Sockets Layer) — старое название технологии; сейчас используется TLS (Transport Layer Security).
 TLS/SSL шифрует и защищает данные между клиентом и сервером (от перехвата/подмены).
 CLI (Command Line Interface) - интерфейс командной строки

 - Как обрабатывать ошибки в приложении?

 Использование логирования, централизованного управления ошибками, обработчиков исключений.


 - Каковы лучшие практики для проектирования базы данных?

 Нормализация, индексация, использование транзакций, выбор подходящей схемы данных.


 - Как обеспечить высокую доступность приложения?

 Использование кластеризации, резервирования, балансировки нагрузки, репликации данных.

 Репликация данных — это процесс копирования и синхронизации данных между разными местами для обеспечения их доступности,
 надежности и повышения производительности. Это важно для резервного копирования и масштабируемости систем.
 Репликация = автоматическое копирование данных между узлами для надежности и скорости.
 Суть: Создаются точные копии данных, которые обновляются в реальном времени или с небольшой задержкой.

 Репликация - это процесс автоматического поддержания идентичных копий данных на нескольких узлах (серверах),
 соединённых сетью, с целью обеспечения отказоустойчивости, распределённой нагрузки или снижения задержек.

 Репликация — это хранение копий одной базы данных на нескольких серверах.
 Частый вариант: master–slave (primary–replica)
 Master (ведущий) принимает запись (insert/update/delete).
 Slave (ведомые/реплики) получают копии данных и обслуживают чтение.
 Зачем это нужно: чтобы ускорить работу (чтение распределяется по репликам), выдерживать больше пользователей и
 часто — повысить отказоустойчивость.


 -- Шардирование vs Репликация --

 Определения (шардирование = разделение, репликация = копирование)

 Шардирование - горизонтальное разделение данных между серверами (шардами), где каждый шард хранит только часть данных.
 Репликация   - копирование одних и тех же данных на несколько серверов (реплик) для повышения отказоустойчивости.

 | Шардирование	                       |  Репликация                             |
 +-------------------------------------+-----------------------------------------+
 | Данные разделяются между серверами  | Данные копируются на несколько серверов |
 | Увеличивает ёмкость	               | Увеличивает доступность                 |
 | Сложные запросы	                   | Простые запросы                         |
 +-------------------------------------+-----------------------------------------+

 Основная цель:
 Шардирование - масштабирование записи (для масштабирования записи и хранения больших данных)
 Репликация   - отказоустойчивость (для отказоустойчивости и масштабирования чтения)

 Когда использовать?
 - Шардирование: когда данные не помещаются на один сервер или нужна высокая скорость записи.
 - Репликация: когда критична бесперебойная работа (например, финансовая система).

 Примеры:
 - Шардирование: крупные соцсети (Facebook, TikTok), где данные пользователей распределены.
 - Репликация: банковские транзакции (копии на нескольких серверах для защиты от сбоев).



 --- Схема архитектуры приложения с Celery ---

 Точно отражает стандартный workflow работы Celery с веб-приложением (Flask/Django/FastAPI).


 +---------------------+       Запрос от клиента для выполнения фоновой задачи
 |    Веб-сервер      |       (Flask/Django/FastApi)
 | (Flask/Django/FastApi)
 +----------+----------+
            |
            |     Отправка задачи в очередь
            v
 +----------+----------+
 |      Брокер        |       Очередь задач (RabbitMQ/Redis)
 | (RabbitMQ/Redis)   |
 +----------+----------+
            |
            |     Получение задачи из очереди
            v
 +----------+----------+
 |    Celery Worker    |       Обработка фоновых задач
 |  (Обработка задач)  |
 +----------+----------+
            |
            |     Сохранение результата в БД
            v
 +---------------------+
 |    База данных      |       Хранение данных и результатов
 | (PostgreSQL/MySQL)  |
 +---------------------+


 # Сверхкраткое объяснение логики:

 - Клиент → Веб-сервер (HTTP-запрос).

 - Сервер → Брокер (ставит задачу в очередь).

 - Celery Worker забирает задачу → выполняет → сохраняет результат в БД.

 - Клиент может проверить статус через сервер.


 Короткие ответы на возможные вопросы:
 Зачем Celery, а не потоки/async?
 → Celery работает в отдельных процессах, не блокирует сервер, масштабируется на несколько машин.

 2. Как клиент узнает результат?
 → Опрос /task-status/<id> или WebSocket.  Точно  (polling или WebSocket)

 3. Что если Worker упадет?
 → Задача останется в очереди (перезапустится, если есть retry).

 4. Как избежать потери задач?
 → Подтверждение (ACK) в RabbitMQ или Redis Persistence.

 ACK (Подтверждение) — это механизм, при котором:
 Worker сообщает брокеру (RabbitMQ/Redis), что задача успешно выполнена. Только после этого задача удаляется из очереди.

 5. Где хранятся результаты?
 → В БД (PostgreSQL/MySQL) или кеше (Redis).


 --- Монолитное приложение и Микросервисная архитектура — это два подхода к разработке программного обеспечения. ---

 Монолитное приложение - Это единое, целостное приложение, в котором все компоненты и функциональности интегрированы
 и развиваются вместе. Это единый код, который управляет всеми бизнес-функциями и интерфейсами.

 - **Преимущества**:
 - Простота разработки и тестирования.
 - Меньше накладных расходов на взаимодействие между компонентами.
 - **Недостатки**:
 - Сложность масштабирования: при увеличении нагрузки нужно копировать всю систему.
 - Трудности с обновлениями и поддержкой, изменения в одном месте могут влиять на все приложение.


 Микросервисы - Это архитектурный стиль, в котором приложение разбивается на мелкие, независимые сервисы, каждое из
 которых выполняет  свою задачу и может быть разработано и развернуто отдельно/самостоятельно .

 **Преимущества**:
 - Гибкость в использовании разных технологий для отдельных сервисов.
 - Упрощенное масштабирование: можно отдельно масштабировать только нужные сервисы.
 - Лучшая устойчивость: сбой в одном сервисе не приводит к сбоям в целом приложении.
 - **Недостатки**:
 - Увеличенная сложность в управлении и взаимодействии между сервисами.
 - Необходимость в более сложной инфраструктуре для развертывания и мониторинга.


 -- Монолит vs Микросервисы --

 Преимущества МОНОЛИТА:
 - Простота разработки и деплоя   - один код, одна база, меньше зависимостей.
 - Высокая производительность     - нет накладных расходов на межсервисное взаимодействие (RPC, REST, JSON-сериализация).
 - Легче тестировать и отлаживать - всё в одном месте.

 Минусы МОНОЛИТА:
 - Масштабируемость    - тяжело расти, приходится масштабировать весь сервис, даже если нагрузка только на одну часть.
 - Гибкость технологий - сложно внедрять новые технологии или языки.
 - Надёжность          - падение одной функции может уронить всю систему.


 Преимущества МИКРОСЕРВИСОВ:
 - Гибкость и независимость - каждый сервис на своём стеке, можно обновлять отдельно.
 - Масштабируемость         - можно масштабировать только нужные сервисы.
 - Отказоустойчивость       - падение одного сервиса не убивает всю систему.

 Минусы МИКРОСЕРВИСОВ:
 - Сложность              - оркестрация, мониторинг, логирование, дебаг (Kubernetes, Istio, Grafana и т. д.).
 - Накладные расходы      - да, JSON-сериализация, сетевые задержки, RPC-вызовы съедают время
   (особенно если сервисов много и они часто общаются).
 - Согласованность данных - сложно поддерживать транзакции (нужен Saga, Outbox и т. д.).

 RPC (Remote Procedure Call) – это вызов функции или метода на удалённом сервисе, как если бы он был локальным.


 Если микросервисов много и они активно обмениваются данными, сериализация/десериализация JSON
 (или даже Protobuf/GRPC) добавляет задержки.
 gRPC – это современный RPC-фреймворк от Google для быстрого обмена данными между сервисами.

 Решение:
 - Оптимизировать API (меньше данных, batch-запросы).
 - Использовать binary-форматы (Protobuf, Avro).
 - Кэшировать данные (Redis).
 - Объединять сервисы, если они слишком часто общаются.


 МОНОЛИТ      - быстрее и проще, но плохо масштабируется.
 МИКРОСЕРВИСЫ - гибкие и масштабируемые, но сложные и медленнее из-за сетевых накладных расходов.



 -- Вертикальное масштабирование (Монолит)  Горизонтальное масштабирование (Микросервисы) --

 Монолит чаще масштабируется ВЕРТИКАЛЬНО (мощнее сервер).
 Суть: Увеличение ресурсов одной машины (CPU, RAM, GPU, диск).

 Микросервисы масштабируются ГОРИЗОНТАЛЬНО (больше серверов).
 Суть: Добавление новых машин/контейнеров в кластер.

 Вертикально = больше ресурсов на 1 машину.
 Горизонтально = больше машин.


 Вертикальное масштабирование (Монолит)

 Что значит:

 - Увеличение мощности одного сервиса (CPU, RAM, диск).
 - Пример: У вас есть сервер с 4 ядрами → апгрейдим до 16 ядер.

 Как связано с монолитом:

 - Монолит обычно масштабируется вертикально, потому что:
   - Легче запустить один мощный сервер, чем разбивать логику на части.
   - Нет сетевых накладных расходов между компонентами.

 Минусы:
 - Есть физический предел (максимальная мощность сервера).
 - Если упадёт сервер – упадёт весь сервис.



 Горизонтальное масштабирование (Микросервисы)

 Что значит:

 - Добавление новых экземпляров сервиса (а не увеличение мощности одного).
 - Пример: Было 2 сервера → стало 10 серверов.

 Как связано с микросервисами:

 - Каждый микросервис можно масштабировать отдельно.
 - Можно добавлять инстансы только для нагруженных частей системы.

 Плюсы:
 - Нет предела масштабирования (можно добавлять тысячи серверов).
 - Отказоустойчивость (если один сервер упадёт, другие продолжат работать).

 Минусы:
 - Нужна балансировка нагрузки (Nginx, Kubernetes).
 - Сложнее управлять (сеть, задержки, согласованность данных).


 Дополнительные реальные кейсы:

 Вертикальное:
 - Увеличение CPU для монолитного СУБД (PostgreSQL/Oracle)
 - Апгрейд GPU-сервера для ML-инференса

 Горизонтальное:
 - Шардирование MongoDB по кластерам
 - Автомасштабирование воркеров Celery/RabbitMQ



 ---  Основные паттерны МИКРОСЕРВИСОВ ---

 - API Gateway – единая точка входа для клиентов.
 - Service Discovery – автоматическое нахождение сервисов (Eureka, Consul).
 - Circuit Breaker – защита от сбоев (Hystrix, Resilience4j).
 - Fallback — запасной ответ/маршрут при отказе (кэш, дефолт, деградация). Обычно идёт в связке с Circuit Breaker + Timeout (часто и Retry)
 - Event-Driven – обмен сообщениями (Kafka, RabbitMQ).
 - Database per Service – у каждого сервиса своя БД.
 - Saga – управление распределёнными транзакциями. (распределённая операция + компенсации.)
 - Outbox — “БД + событие” атомарно (публикация через воркер).
 - CQRS – разделение чтения и записи.
 - Retry & Timeout – повтор запросов при ошибках.
 - Bulkhead – изоляция ресурсов.
 - Health Check – проверка работоспособности.
 - Sidecar – вспомогательный процесс (например, Istio).
 - Blue-Green / Canary – безопасное развертывание.


 -- SOA vs Микросервисы --

 SOA (Service-Oriented Architecture) - это архитектурный подход, где приложение строится из loosely coupled
 (слабо связанных) сервисов, которые общаются через стандартные протоколы (чаще SOAP/WS-*).

 Разница с микросервисами:

 SOA - сервисы крупнее, часто общая БД, акцент на интеграцию через ESB (Enterprise Service Bus).
 Микросервисы - сервисы мельче, автономны (своя БД), легковесные протоколы (HTTP/REST, gRPC), упор на независимое развертывание.

 Микросервисы - эволюция SOA, но без тяжелых стандартов и ESB.

 Да, микросервисы можно считать эволюцией SOA, но с упором на:

 - Полную независимость сервисов (включая БД).
 - Отказ от тяжелых стандартов (SOAP/ESB) в пользу легковесных решений.
 - DevOps-культуру (независимое развертывание, контейнеризация).

 SOAP (Simple Object Access Protocol) - протокол обмена XML-сообщениями для веб-сервисов со строгой структурой и
 поддержкой WS-* стандартов (безопасность, транзакции).

 ESB (Enterprise Service Bus) - шина интеграции в SOA, которая централизует взаимодействие сервисов
 (маршрутизация, трансформация данных, мониторинг).


 --- Clean Architecture (чистая архитектура) ---
 Чистое разделение, чтобы "главное" не зависело от "деталей".

 Clean Architecture (чистая архитектура) — это подход к проектированию ПО, при котором код делится на слойки с
 четкими правилами зависимостей:

 - Независимость от фреймворов, UI и БД
 - Бизнес-логика в центре (самый важный слой)
 - Внешние компоненты (БД, API) зависят от ядра, а не наоборот

 Основные принципы:

 - Разделение ответственности (каждый слой — своя роль)
 - Тестируемость (ядро можно тестировать без UI/БД)
 - Гибкость (легко менять внешние компоненты)

 Пример слоёв (от центра к краям):

 - Entities (бизнес-сущности)
 - Use Cases (правила бизнес-логики)
 - Adapters (преобразование данных для внешнего мира)
 - UI/DB/API (внешние системы)

 Главное: зависимости направлены только внутрь, к ядру.



 Clean Architecture — это:

 Слои:
 - Ядро (Entities, Use Cases)
 - Адаптеры (преобразуют данные)
 - Внешнее (UI/БД/API)

 Правила:
 - Зависимости → внутрь (ядро не знает о внешнем мире)
 - Бизнес-логика независима от фреймворков/БД
 - Легко тестировать и менять внешние компоненты

 Суть: Чистое разделение, чтобы "главное" не зависело от "деталей".
  --- END Clean Architecture (чистая архитектура) ---


 -- ZooKeeper --

 ZooKeeper - это распределённый сервис для координации и управления конфигурацией в распределённых системах.
 ZooKeeper - Быстрое хранилище мелких данных в памяти с сохранением на диск и репликацией
 ZooKeeper - Распределённая key-value база для координации, с гарантированной согласованностью и высокой скоростью

 Ключевые особенности:

 Хранит данные в виде дерева (как файловая система).
 Обеспечивает высокую доступность и согласованность.
 Используется для синхронизации, конфигурации, лидер-выборов (например, в Hadoop, Kafka).

 Очень кратко: "Координатор для распределённых систем".

 ZooKeeper действительно предназначен для хранения небольших объемов данных (например, конфигураций, состояний, метаданных)
 которые:

 - Помещаются в оперативной памяти для быстрого доступа.
 - Дублируются на диск для надежности и восстановления после сбоев.
 - Реплицируются между узлами кластера для отказоустойчивости.

 Примеры использования:
 Apache Kafka (хранение метаданных брокеров, offset’ов).
 Hadoop (координация сервисов HDFS, YARN).
 Микросервисы (конфигурация, service discovery).

 ZooKeeper обычно относят к CP — он выбирает Consistency + Partition Tolerance, а при сетевом разделении жертвует
 ДОСТУПНОСТЬЮ (узлы без кворума не смогут корректно обслуживать запросы, особенно записи).


 -- Алгоритмы ограничения трафика (rate limiting) или Алгоритмы ограничения ЧАСТОТЫ ЗАПРОСОВ --

 Алгоритмы ограничения ЧАСТОТЫ ЗАПРОСОВ (rate limiting) — это способы ограничивать число запросов/скорость от пользователя,
 сервиса или IP за единицу времени, чтобы защитить систему от перегрузки, злоупотреблений и обеспечить стабильную работу.

 Основные виды:
 - Token bucket (маркерная корзина) — ограничивает среднюю скорость, разрешает короткие всплески.
 - Leaky bucket (дырявое ведро) — выпускает запросы равномерно, сглаживает трафик (очередь/сброс лишнего).
 - Fixed window counter (фиксированное окно) — считает запросы в интервале (например, за минуту); просто, но есть проблема на границе окна.
 - Sliding window log (журнал скользящего окна) — хранит времена запросов за последние T секунд; точно, но дорого.
 - Sliding window counter (счётчик скользящего окна) — приближённый вариант скользящего окна; дешевле, почти точно.


 -- Процесс загрузки видео (как на YouTube) --

 - Пользователь загружает видео → балансировщик → API-серверы.
 - Видео сохраняется в BLOB-хранилище (BLOB — Binary Large Object: хранение больших двоичных файлов, например видео).
 - Видео перекодируют в разные форматы/качества → кладут в хранилище перекодированного видео.
 - Метаданные пишутся в БД и кэш.
 - После перекодирования событие через очередь обновляет метаданные/кэш.
 - Просмотр идёт через CDN (Content Delivery Network: сеть серверов по миру, кэширует и отдаёт видео с ближайшего узла быстрее).


 -- Модель OSI (Open Systems Interconnection) — 7 уровней --

 - L1 Физический (Physical) — передача битов по среде (сигналы, кабель/радио).
 - L2 Канальный (Data Link) — кадры, MAC-адреса, доступ к среде, контроль ошибок на линии.
 - L3 Сетевой (Network) — пакеты, IP-адреса, маршрутизация между сетями.
 - L4 Транспортный (Transport) — доставка “конец-конец”: порты, надёжность/порядок (TCP) или без гарантий (UDP).
 - L5 Сеансовый (Session) — управление сеансом (установить/поддерживать/завершить); часто функции на практике в приложениях/протоколах.
 - L6 Представления (Presentation) — формат данных: кодировки, сериализация, сжатие, шифрование/дешифрование (часто через TLS).
 - L7 Прикладной (Application) — протоколы приложений (HTTP, DNS, SMTP и т.д.).


 -- Балансировщик нагрузки (Load Balancer) --

 Балансировщик нагрузки (Load Balancer) — компонент, который распределяет входящий трафик между несколькими серверами,
 чтобы было быстрее и надёжнее (нет перегрузки одного узла).

 Уровни балансировки :

 L4 (транспортный уровень, TCP/UDP) — балансирует по IP/порту, “не смотрит” в HTTP-содержимое; быстрее, проще.
 L7 (прикладной уровень, HTTP/HTTPS) — понимает запрос (URL, заголовки, cookies) и может маршрутизировать “умно”
 (например, /api на один пул, /images на другой).

 Липкая сессия (sticky session / session affinity) — режим балансировщика, когда все запросы одного пользователя
 направляются на один и тот же сервер (по cookie/IP и т.п.), чтобы не терять состояние сессии на этом сервере.



 -- Сериализация в JSON (формирование HTTP-ответа) во Flask, Django, FastAPI --

 Зачем: чтобы сервер вернул данные клиенту в JSON (обычно для API), с правильным заголовком Content-Type: application/json.

 - Flask: jsonify(...) — превращает данные в JSON и делает HTTP-ответ.
 - Django: JsonResponse(data) — то же самое: JSON-ответ.
 - FastAPI: обычно return dict — FastAPI сам преобразует в JSON; при необходимости — JSONResponse(...).


 --- Middleware во Flask, Django, FastAPI ---

 Middleware — это промежуточный слой, который обрабатывает запросы и ответы перед тем, как они достигнут основного кода или клиента.
 Middleware — прослойка между запросом и ответом для аутентификации, логирования, CORS и т.д.
 Middleware могут изменять запрос/ответ (например, добавлять заголовки)

 Для чего:
 - Аутентификация (проверка токенов, сессий)
 - Логирование (запись действий)
 - Обработка CORS (доступ между доменами)
 - Сжатие данных (Gzip, Brotli)
 - Парсинг данных (JSON, формы)

 Примеры:
 Flask: @app.before_request, Flask-JWT, Flask-CORS
   - @app.before_request — это декоратор, а не middleware в классическом понимании, но выполняет схожую роль.
   - Настоящие middleware во Flask — это WSGI-прослойки (напр., ProxyFix).
 Django: SessionMiddleware, CsrfViewMiddleware, AuthenticationMiddleware
   - Middleware работает как цепочка обработчиков (request → middleware → view → middleware → response).
 FastAPI: CORSMiddleware, HTTPSRedirectMiddleware, кастомные middleware через app.middleware("http")
   - Middleware работает на уровне ASGI/HTTP, а не только HTTP (как в Flask/Django).

 в FastAPI кастомные классы (особенно чистые ASGI-классы) обычно работают быстрее, чем декораторы @app.middleware,
 потому что:
 - Меньше обёрток и вызовов.
 - Лучше оптимизированы для ASGI.
 - Меньше накладных расходов на динамические проверки.
 - Но разница заметна только в высоконагруженных приложениях. Для большинства случаев @app.middleware вполне достаточно,
   так как он проще в использовании.


 Коротко: Middleware помогает добавлять общую логику для всех запросов/ответов без дублирования кода.

 Middleware — это промежуточный слой, который:
  - Обрабатывает запросы и ответы до/после основного кода.
  - Может изменять их (например, добавлять заголовки).
  - Позволяет реализовать общую логику без дублирования кода.


 -- Где в Django бизнес-логика ??? --
 В Django бизнес-логика обычно располагается:

 - В моделях (models.py) – основное место для логики, связанной с данными.
 - В сервисных слоях (отдельные файлы, например services.py) – для сложной логики, не привязанной к моделям.
 - В формах (forms.py) – валидация и обработка данных форм.
 - В представлениях (views.py) – минимально, только обработка HTTP-запросов.

 Идеально: максимум логики в моделях и сервисах, минимум во views.


 --- Django ---

 Django по умолчанию — однопоточный (синхронный), но может работать с многопоточностью и асинхронностью (с ограничениями).
 Django поддерживает ASGI (с версии 3.0) для асинхронных запросов:
 Но: Не все компоненты Django асинхронны (ORM, кэши — работают синхронно).
 Django даёт асинхронность, но с оговорками — это гибридный фреймворк, а не полноценный async (как FastAPI)
 Ядро Django остаётся синхронным

 # Главные ограничения асинхронности в Django
 # Компонент	Асинхронная поддержка?
 # ORM	        ❌ Нет (только синхронные запросы)
 # Кэши	        ❌ Нет (например, cache.get() блокирует)
 # Admin-панель	❌ Только синхронный код
 # Сессии	    ❌ Синхронные


 -- raw запросы --
 people = Person.objects.raw("SELECT id, name FROM hello_person")

 Пример raw запроса:

 class Person(models.Model):
     first_name = models.CharField(...)
     last_name = models.CharField(...)
     birth_date = models.DateField(...)
 You could then execute custom SQL like so:

 for p in Person.objects.raw("SELECT * FROM myapp_person"):
     print(p)

 # John Smith
 # Jane Jones

 # Сопоставление полей запроса с полями модели
 name_map = {"first": "first_name", "last": "last_name", "bd": "birth_date", "pk": "id"}
 Person.objects.raw("SELECT * FROM some_other_table", translations=name_map)



 Django использует метаклассы для создания моделей базы данных

 Django во многом работает через метаклассы.
 Поэтому когда Django конструирует ваш класс, она делает это с помощью своего метакласса.
 Чтобы при конструировании ей знать какие-то параметры вашего класса, ну, например модель или поля в вашем случае,
 она ищет в вашем классе класс с названием Meta.

 select_related(key) - 'ЖАДНАЯ' загрузка связанных данных по внешнему ключу key, который имеет тип ForeignKey, OneToOneField
 уменьшение количество запросов к базе данных

 'ЖАДНАЯ' загрузка - загрузка сразу всех данных
 'ЛЕНИВАЯ' загрузка - происходит в момент обращения к тому или иному атрибуту

 prefetch_related(key) - 'ЖАДНАЯ' загрузка связанных данных по внешнему ключу key, который имеет тип ManyToManyField
 уменьшение количество запросов к базе данных


 -- select_related  vs  prefetch_related --

 select_related   - Использует JOIN в SQL-запросе, чтобы получить связанные объекты за один запрос
 prefetch_related - Делает ОТДЕЛЬНЫЕ ЗАПРОСЫ для связанных объектов, а затем объединяет их в Python


 Короткие примеры SQL для select_related и prefetch_related:

 1. select_related (один JOIN-запрос)

 Пример Django:

 Book.objects.select_related('author').get(id=1)


 SQL:

 SELECT book.*, author.*
 FROM book
 JOIN author ON book.author_id = author.id
 WHERE book.id = 1;

 → Один запрос с JOIN, данные автора сразу в результате.



 2. prefetch_related (отдельные запросы + объединение в Python)

 Пример Django:

 Publisher.objects.prefetch_related('books').all()


 SQL:

 Сначала запрос для издателей:

 SELECT * FROM publisher;

 Затем запрос для книг этих издателей (через промежуточную таблицу publisher_books):

 SELECT book.*
 FROM book
 JOIN publisher_books ON book.id = publisher_books.book_id
 WHERE publisher_books.publisher_id IN (1, 2, 3, ...);

 → Два запроса, связывание происходит в Python.



 -- Транзакции/transaction в django --

 Поведение транзакции по умолчанию в Django

 По умолчанию Django работает в режиме автоматической фиксации. Каждый запрос немедленно фиксируется в базе данных,
  если транзакция не активна.

 Django автоматически использует транзакции или точки сохранения, чтобы гарантировать целостность операций ORM,
  которые требуют нескольких запросов, особенно запросов delete () и update () .

 TestCase Класс Django также включает каждый тест в транзакцию из соображений производительности.


 -- Привязка транзакций к HTTP-запросам --

 Распространенный способ обработки транзакций в Интернете - заключить каждый запрос в транзакцию. Установите
  ATOMIC_REQUESTS значение True в конфигурации каждой базы данных, для которой вы хотите включить это поведение.

 Это работает вот так. Перед вызовом функции просмотра Django запускает транзакцию. Если ответ получен без проблем,
 Django фиксирует транзакцию. Если представление вызывает исключение, Django откатывает транзакцию.

 Вы можете выполнять частичные транзакции, используя точки сохранения в коде представления, обычно с помощью atomic()
 диспетчера контекста. Однако в конце просмотра все изменения или ни одно из них не будут зафиксированы.

 Когда ATOMIC_REQUESTS он включен, по-прежнему можно запретить выполнение представлений в транзакции.

 non_atomic_requests(using=None)
 Этот декоратор нейтрализует эффект ATOMIC_REQUESTS для данного вида:

 from django.db import transaction

 @transaction.non_atomic_requests
 def my_view(request):
     do_stuff()

 @transaction.non_atomic_requests(using='other')
 def my_other_view(request):
     do_stuff_on_the_other_database()

 Он работает только в том случае, если он применяется к самому представлению.


 -- Явное управление транзакциями --

 atomic(using=None, savepoint=True, durable=False)

 Атомарность - определяющее свойство транзакций базы данных. atomic позволяет нам создать блок кода, в котором
 гарантируется атомарность базы данных. Если блок кода успешно завершен, изменения фиксируются в базе данных.
 Если есть исключение, изменения отменяются.

 atomic блоки могут быть вложенными. В этом случае, когда внутренний блок завершается успешно, его эффекты все еще могут
 быть отменены, если во внешнем блоке позже возникнет исключение.

 Иногда полезно убедиться, что atomic блок всегда является самым внешним atomic блоком, гарантируя, что любые изменения
 базы данных будут зафиксированы при выходе из блока без ошибок. Это называется долговечностью и достигается за счет
 схватывания durable=True. Если atomic блок вложен в другой, возникает ошибка RuntimeError.


 atomic можно использовать как декоратор :

 from django.db import transaction

 @transaction.atomic
 def viewfunc(request):
     # This code executes inside a transaction.
     do_stuff()

 и как менеджер контекста :

 from django.db import transaction

 def viewfunc(request):
     # This code executes in autocommit mode (Django's default).
     do_stuff()

     with transaction.atomic():
         # This code executes inside a transaction.
         do_more_stuff()

 Заключение atomic в блок try / except позволяет естественным образом обрабатывать ошибки целостности:

 from django.db import IntegrityError, transaction

 @transaction.atomic
 def viewfunc(request):
     create_parent()

     try:
         with transaction.atomic():
             generate_relationships()
     except IntegrityError:
         handle_exception()

     add_children()


 -- Автокоммит --

 Почему Django использует автокоммит?

 В стандартах SQL каждый запрос SQL запускает транзакцию, если она еще не активна. Затем такие транзакции должны быть
 явно зафиксированы или отменены.

 Это не всегда удобно для разработчиков приложений. Чтобы решить эту проблему, в большинстве баз данных предусмотрен
 режим автоматической фиксации. Когда автоматическая фиксация включена и транзакция не активна, каждый запрос SQL
 помещается в свою собственную транзакцию. Другими словами, каждый такой запрос не только запускает транзакцию,
 но транзакция также автоматически фиксируется или откатывается, в зависимости от того, был ли запрос успешным.

 PEP 249 , спецификация API базы данных Python v2.0, требует, чтобы функция автоматической фиксации была изначально
 отключена. Django отменяет это значение по умолчанию и включает автоматическую фиксацию.

 Чтобы этого избежать, вы можете отключить управление транзакциями , но это НЕ рекомендуется.    <-----

 Django предоставляет API в django.db.transaction модуле для управления состоянием автоматической фиксации каждого
 соединения с базой данных.

 get_autocommit(using=None)
 set_autocommit(autocommit, using=None)
 Эти функции принимают using аргумент, который должен быть именем базы данных. Если он не указан, Django использует
 "default" базу данных.

 Автокоммит изначально включен. Если вы выключите его, вы обязаны восстановить его.

 Вы должны убедиться, что ни одна транзакция не активна, обычно с помощью символа a commit() или a rollback(),
 прежде чем снова включить автоматическую фиксацию.

 Django откажется выключать автоматическую фиксацию, когда atomic() блок активен, потому что это нарушит атомарность.


 -- Отключение управления транзакциями --
 Вы можете управление транзакциями полностью отключить Джанго для данной базы данных, установив AUTOCOMMIT для False
 в его конфигурации. Если вы это сделаете, Django не включит автокоммит и не будет выполнять никаких коммитов.
 Вы получите обычное поведение базовой библиотеки базы данных.


 -- Выполнение действий после коммита --

 Иногда вам нужно выполнить действие, связанное с текущей транзакцией базы данных, но только в том случае, если транзакция
 успешно зафиксирована. Примеры могут включать задачу Celery , уведомление по электронной почте или аннулирование кеша.

 Django предоставляет on_commit() функцию для регистрации функций обратного вызова, которые должны выполняться после
 успешной фиксации транзакции:

 on_commit(func, using=None, robust=False)
 Передайте любую функцию (не принимающую аргументов) в on_commit():

 from django.db import transaction

 def do_something():
     pass  # send a mail, invalidate a cache, fire off a Celery task, etc.

 transaction.on_commit(do_something)
 Вы также можете заключить свою функцию в лямбду:

 transaction.on_commit(lambda: some_celery_task.delay('arg1'))

 Обратным вызовам не будут переданы никакие аргументы, но вы можете связать их с помощью functools.partial():

 from functools import partial

 for user in users:
     transaction.on_commit(partial(send_invite_email, user=user))


 -- Транзакции --
 Транзакция - это атомарный набор запросов к базе данных. Даже если ваша программа выйдет из строя, база данных
 гарантирует, что будут применены либо все изменения, либо ни одно из них.

 Django не предоставляет API для запуска транзакции. Ожидаемый способ начать транзакцию - отключить автоматическую
 фиксацию с помощью set_autocommit().

 После того, как вы участвуете в транзакции, вы можете либо применить изменения, которые вы выполняли до этого момента
 commit(), либо отменить их с помощью rollback(). Эти функции определены в django.db.transaction.

 commit(using=None)
 rollback(using=None)

 Эти функции принимают using аргумент, который должен быть именем базы данных. Если он не указан, Django использует
 "default" базу данных.

 Django откажется выполнять фиксацию или откат, когда atomic() блок активен, потому что это нарушит атомарность.


 -- Точки сохранения --

 Точка сохранения - это маркер внутри транзакции, который позволяет вам откатить часть транзакции, а не полную транзакцию.

 Точки сохранения управляются тремя функциями в django.db.transaction:

 savepoint(using=None)
 Создает новую точку сохранения. Это отмечает точку транзакции, которая, как известно, находится в «хорошем» состоянии.
 Возвращает идентификатор точки сохранения ( sid).

 savepoint_commit(sid, using=None)
 Освобождает точку сохранения sid. Изменения, внесенные с момента создания точки сохранения, становятся частью транзакции.

 savepoint_rollback(sid, using=None)
 Откатывает транзакцию до точки сохранения sid.

 Эти функции ничего не делают, если точки сохранения не поддерживаются или если база данных находится в режиме автоматической фиксации.


 Кроме того, есть служебная функция:

 clean_savepoints(using=None)
 Сбрасывает счетчик, используемый для генерации уникальных идентификаторов точек сохранения.

 -- END Транзакции/transaction в django --


 -- Расшифровка В терминале python -m venv venv --
 команда python -m venv venv создает новое виртуальное окружение в папке с именем venv в текущем каталоге.
 Команда python -m venv venv используется для создания виртуального окружения в Python.

 - python: запускает интерпретатор Python.

 - -m: указывает на запуск модуля как скрипта.

 - venv: модуль для создания виртуальных окружений.

 - venv (второй раз): имя папки, в которой будет создано виртуальное окружение.
 В результате выполнения этой команды создается новая папка с именем venv, содержащая изолированное окружение для установки пакетов.

 -- END Расшифровка В терминале python -m venv venv --


    -- командная строка cmd   Terminal commands --
 dir - Отображение файлов и папок в текущем каталоге
 cd - Изменить каталог, если нажать Tab будет автодополнение
 cd .. - переход на 1 папку назад
 cd ..\.. - переход на 2 папки назад
 cd \ - Переход в корневую директорию
 cd . - Переход в текущую директорию (никаких изменений)
 mkdir - создание папки

 cls - используется для очистки экрана Windows (CMD, PowerShell) (cls работает, но это алиас для Clear-Host)
 clear - используется для очистки экрана Linux/macOS (Terminal, Bash, Zsh и др.)

 python --version           - Посмотреть версию Python
 python -V                  - Тоже самое сокращенная команда
 python3.12 -m venv venv    - Установка python и запуск локального окружения
 python -m venv venv        - Установка python и запуск локального окружения  БЕЗ ВЕРСИИ


 rm -rf venv                - Удалить виртуальное окружение

 Ctrl + Z выйти из терминала или   exit()

 Активировать venv  (Виртуальное окружение)        .\venv\Scripts\activate       # venv\Scripts\activate   # Тоже самое
 ДЕ-Активировать venv  (Виртуальное окружение)     .\venv\Scripts\deactivate     # Или deactivate
 Чтобы деактивировать активное виртуальное окружение, просто введите команду deactivate в командной строке.   <-----

 conda deactivate  # Для анакоды деактивировать

 source venv/bin/activate - Активировать venv  (Виртуальное окружение)  в Linux
 .\venv\Scripts\activate  - Активировать venv  (Виртуальное окружение)  в Windows

 $env:Path -split ';'     - разбивает строку переменной окружения Path на массив строк, используя символ ; в качестве разделителя.

 # Команда используется в PowerShell для добавления пути к установленному Python
 [Environment]::SetEnvironmentVariable("Path", $Env:Path + ";C:\Program Files\Python312", [EnvironmentVariableTarget]::User)

 cd папка\папка\папка   -  Быстрый переход за одну операцию
 cd "имя имя"  - если между именами пробел
 cd "Абсолютный путь" - Сразу перейдем туда куда нужно
 Название_Диска: - чтобы сменить диск  - Примеры C:  D:  Только название диска и двоеточие в терминале(командной строке)

 echo $PYTHONPATH - Команда echo $PYTHONPATH используется в Unix-подобных операционных системах (включая Linux)
 для отображения значения переменной окружения PYTHONPATH.
 - Если переменная PYTHONPATH не установлена, вывод будет пустым
 - Если переменная окружения PYTHONPATH установлена, вы увидите путь или несколько путей, разделенных двоеточиями.
   Например: /home/user/my_python_libs


 --- ЗАМЕТКИ РАБОТА --
 # ТЕРМИНАЛ ПОСМОТРЕТЬ ИСТОРИЮ КОМАНДА
 history

 # Проверить существование директории /home
 ls -la /home

 # Или посмотреть содержимое корневой директории
 ls -la /

 # Посмотреть время обновления файла. По умолчанию ls -l показывает mtime.
 ls -l filename

 # Подробная информация
 stat filename

 # -lah — это три флага для команды ls, просто написанные вместе
 ### означает: показать все файлы/папки, подробно и с удобными размерами.
 ls -lah /root/.bittensor/wallets/ck_dg_12412_5

 ### можно использовать комбинации по-отдельности
 ls -l
 ls -a
 ls -lh

 Расшифровка:

 -l (long format)
 Показывает подробный список файлов:

 - права доступа
 - владелец, группа
 - размер
 - дата/время
 - имя файла/папки

 -a (all)
 Показывает все файлы, в том числе скрытые (которые начинаются с .), например .bittensor, .ssh, .config и т.д.

 -h (human-readable)
 Делает размеры удобными для чтения:

 вместо 4096 → 4.0K
 вместо 1048576 → 1.0M
 и т.п.

 ### СКОЛЬКО ВЕСИТ ПАПКА
 du -sh /root/.bittensor/miners

 ### СКОЛЬКО СВОБОДНО МЕСТА НА ДИСКЕ
 df -h

 --- END ЗАМЕТКИ РАБОТА --


 10 самых частых Linux команд:                                    <--------

 1) ls – список файлов и папок
      ls -l – подробный вывод
      ls -a – показать скрытые файлы

 2) cd <dir> – перейти в директорию
      cd .. – на уровень выше
      cd ~ – в домашнюю папку

 3) pwd – текущий путь
 4) mkdir <dir> – создать папку

 5) rm <file> – удалить файл
      rm -r <dir> – удалить папку рекурсивно
      rm -f <file> – принудительное удаление

 6) cp <source> <dest> – копировать файл/папку
      cp -r <dir> <dest> – копировать папку

 7) mv <source> <dest> – переместить или переименовать

 8) cat <file> – вывести содержимое файла

 9) grep "text" <file> – поиск текста в файле

 10) chmod <permissions> <file> – изменить права доступа
      chmod +x <file> – сделать файл исполняемым


 Команды для Unix/Linux/macOS:
 sudo -v    -  Проверить наличие прав sudo
 groups     -  Посмотреть группы пользователя
 sudo -l    -  Проверить доступные команды sudo
 sudo -s    -  Запустить оболочку с правами root


 Команды для Unix/Linux/macOS:

 1. **`ls`**     - Показать список файлов и папок в текущем каталоге.
 2. **`cd`**     - Перейти в указанный каталог. Пример: `cd /path/to/directory`.
 3. **`pwd`**    - Показать текущий рабочий каталог.
 4. **`mkdir`**  - Создать новый каталог. Пример: `mkdir new_directory`.
 5. **`rm`**     - Удалить файл или каталог. Пример: `rm file.txt` (для удаления каталога используйте `rm -r directory_name`).
 6. **`cp`**     - Копировать файлы или каталоги. Пример: `cp source.txt destination.txt`.
 7. **`mv`**     - Переместить или переименовать файл или каталог. Пример: `mv old_name.txt new_name.txt`.
 8. **`touch`**  - Создать новый пустой файл или обновить дату и время существующего файла.
 9. **`cat`**    - Показать содержимое файла. Пример: `cat file.txt`.
 10. **`grep`**  - Поиск строк в файлах, содержащих заданный шаблон. Пример: `grep 'search_term' file.txt`.  GREP <-----

 grep(global regular expression print) - При этом с ее помощью можно не просто находить куски текста, но и с высокой
 эффективностью фильтровать вывод другой команды.

 11. **`chmod`** - Изменить права доступа к файлу или каталогу. Пример: `chmod +x script.sh`.
 12. **`man`**   - Показать руководство по использованию команды. Пример: `man ls`.


 ls -a           - Отображения списка файлов и директорий в текущем каталоге включая скрытые файлы
 Команда `ls -a` в Unix-подобных системах используется для отображения списка файлов и директорий в текущем каталоге,
 включая скрытые файлы, имя которых начинается с точки (`.`). Флаг `-a` означает "all" (все).


 **`uname`**       - Показывает информацию о системе. Этот вариант выводит полную информацию о системе,
 включая имя хоста, версию ядра и тип операционной системы.

 **`lsb_release`** - Показывает информацию о дистрибутиве (если доступно).

 **Bash** (Bourne Again SHell) — это командный интерпретатор и оболочка для UNIX и UNIX-подобных операционных систем,
 таких как Linux и macOS. Он является улучшенной версией оригинальной оболочки Bourne shell (sh) и включает множество
 дополнительных функций  Аналогом Bash в Windows является **PowerShell**

 bash
     uname -a        - Показывает информацию о системе.

 bash
     lsb_release -a  - Показывает информацию о дистрибутиве (если доступно).


 При вводе пароля в терминале Linux сочетание `Shift + Insert` вставляет содержимое буфера обмена. ИЛИ  ПКМ
 Однако в большинстве случаев символы не отображаются на экране.         Shift + Insert ИЛИ  ПКМ


 ### Команды для Windows:

 1. **`dir`**    - Показать список файлов и папок в текущем каталоге.
 2. **`cd`**     - Перейти в указанный каталог. Пример: `cd C:\path\to\directory`.
 3. **`mkdir`**  - Создать новый каталог. Пример: `mkdir new_directory`.
 4. **`del`**    - Удалить файл. Пример: `del file.txt`.
 5. **`copy`**   - Копировать файлы. Пример: `copy source.txt destination.txt`.
 6. **`move`**   - Переместить или переименовать файл. Пример: `move old_name.txt new_name.txt`.
 7. **`type`**   - Показать содержимое текстового файла. Пример: `type file.txt`.
 8. **`find`**   - Поиск строк в файлах. Пример: `find "search_term" file.txt`.
 9. **`attrib`** - Изменить атрибуты файла. Пример: `attrib +r file.txt` (для установки файла как только для чтения).
 10. **`help`**  - Показать доступные команды и их короткие описания.

 **`systeminfo`** - Показывает подробную информацию о системе.

 Аналогом Bash в Windows является **PowerShell**

 Чтобы открыть PowerShell в Windows, выполните следующие шаги:

 ### Способ 1: Поиск в меню

 1. Нажмите **Windows** на клавиатуре или щелкните на кнопку **Пуск**.
 2. Введите **PowerShell** в строке поиска.
 3. Выберите **Windows PowerShell** из результатов.

 ### Способ 2: Сочетание клавиш

 1. Нажмите **Win + X** на клавиатуре.
 2. Выберите **Windows PowerShell** или **Windows PowerShell (Администратор)** в открывшемся меню.

 ### Способ 3: Выполнение команды

 1. Нажмите **Win + R** для открытия окна "Выполнить".
 2. Введите `powershell` и нажмите **Enter**.

 Чтобы выйти обратно в командную строку вводим команду     exit


 Эти методы позволяют быстро открыть PowerShell для выполнения команд и сценариев.

 Эти команды являются базовыми, и существуют более продвинутые команды и параметры. Выбор команд зависит от конкретной
 задачи, которую вы хотите выполнить


 curl - `curl` — это командная строка и библиотека для передачи данных с использованием различных протоколов,
 таких как HTTP, HTTPS, FTP и многих других. Он позволяет отправлять запросы к серверам и получать ответы,
 что делает его полезным инструментом для разработчиков и системных администраторов.

 `curl` — мощный инструмент для работы с сетевыми запросами. Он поддерживает множество опций и позволяет
 взаимодействовать с API, загружать и отправлять данные, а также осуществлять диагностику сетевых проблем.

 ### Полезные опции

 - `-o` или `--output`: сохранять вывод в файл.
 - `-O` или `--remote-name`: сохранять файл с оригинальным именем.
 - `-I` или `--head`: запрашивать только заголовки.
 - `-H`: добавлять дополнительные заголовки.
 - `-d`: отправлять данные в теле запроса (обычно используется с POST).
 - `-X`: указывать метод запроса (GET, POST, PUT, DELETE и т.д.).
 - `-v` или `--verbose`: выводить подробную информацию о запросе и ответе.

 # Простой пример POST-запроса
 # По умолчанию curl использует метод POST, когда вы указываете данные с помощью флага -d.
 curl -H "Accept: application/json" -d '{"key":"value"}' https://api.example.com/data           # Тоже самое

  # Объяснение запроса
 -H "Content-Type: application/json" указывает, что вы отправляете данные в формате JSON.

 -d '{"key":"value"}' содержит данные, которые вы хотите отправить на сервер.

 # явно указываем метод POST с помощью флага -X POST.
 curl -X POST -H "Accept: application/json" -d '{"key":"value"}' https://api.example.com/data   # Тоже самое

 Лучше указывать оба:
 - Accept – говорит серверу, какой формат ожидается в ответе.
 - Content-Type – говорит серверу, какой формат вы отправляете.
 curl -H "Content-Type: application/json" -H "Accept: application/json" -d '{"key":"value"}' https://api.example.com/data

 # GET-запрос   убрать флаг -X POST и флаг -d, так как GET-запросы не содержат тела запроса.
 curl -H "Accept: application/json" "https://api.example.com/data?key=value"


 - **cURL** лучше подходит для разработчиков, которые предпочитают работать в терминале и нуждаются в автоматизации.
 - **Postman** удобнее для визуального тестирования API и разработчиков, которые хотят более простой и наглядный интерфейс.

 Swagger - Использует CURL  <-----                                                                           <-----
 при запуске ручки swagger - есть Request URL - это его url
 так же снизу у классов можно посмотреть одязательные поля * В Swagger UI обязательные поля помечаются звёздочкой (*)


 -- Заголовки (headers) в HTTP-запросах содержат метаданные для сервера --

 Кратко: headers задают формат, авторизацию и другие параметры запроса.

 Общие
 - User-Agent    – информация о клиенте (браузер, программа).
 - Accept        – типы данных, которые клиент понимает (например, application/json).
 - Content-Type  – тип отправляемых данных (например, application/json).
 - Authorization – токены или ключи доступа (например, Bearer токен).

 Для работы с API
 - Accept-Language – предпочитаемый язык ответа.
 - X-API-Key – API-ключ.

 Управление кешем
 - Cache-Control – как кешировать ответ (no-cache, max-age=3600).

 -- END Заголовки (headers) в HTTP-запросах содержат метаданные для сервера --


 GRUB (Grand Unified Bootloader) — это загрузчик операционных систем, используемый в Linux и Unix-подобных системах.
 Он позволяет запускать разные операционные системы на одном компьютере, предоставляет меню для выбора ОС при загрузке
 и обеспечивает конфигурацию параметров загрузки.

 GRUB (также известный как GNU GRUB или GNU Grand Unified Bootloader) — загрузчик и менеджер загрузки для
 Linux и других ОС на базе Unix .


 папка migrations: предназначена для хранения миграций - скриптов,
 которые позволяют синхронизировать структуру базы данных с определением моделей


 # **Прямое выполнение миграций через код**
 Если вам нужно выполнить миграции программно, вы можете использовать следующий код в Python

 from django.core.management import call_command

 call_command('migrate', 'app_name')  #  Замените `'app_name'` на имя вашего приложения

 Имейте в виду, что `migrate` остается основным способом применения миграций и рекомендуется использовать его для
 адекватного управления базой данных.

 - Создайте каталог `management/commands` внутри приложения, если его еще нет.
 - Создайте файл, например, `run_migrations.py`:

 python
      # myapp/management/commands/run_migrations.py

      from django.core.management import BaseCommand, call_command

      class Command(BaseCommand):
          help = 'Выполнить миграции для приложения'

          def handle(self, *args, **kwargs):
              call_command('migrate', 'app_name')  # Замените 'app_name' на имя вашего приложения

  Запускайте команду в терминале:
  python manage.py run_migrations

 Не забывайте, что выполнять миграции программно — это менее распространенная практика, и делайте это осознанно,
 чтобы избежать непредвиденных проблем с базой данных.


 -- Создание миграции вручную --

 Чтобы создать миграции вручную через файл миграции `initial` в Django, выполните следующие шаги:

 1. **Создайте файл миграции вручную**:
 - В каталоге вашей модели приложения создайте директорию `migrations`, если она еще не существует.
 - Создайте файл миграции, назовите его, например, `0001_initial.py`.

 2. **Напишите код миграции**:
 - Откройте созданный файл и добавьте следующий код:


    from django.db import migrations, models

    class Migration(migrations.Migration):

        initial = True

        operations = [
            migrations.CreateModel(
                name='YourModelName',
                fields=[
                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                    ('field_name', models.CharField(max_length=100)),
                    # добавьте другие поля вашего модели
                ],
            ),
        ]

 - Замените `YourModelName` на название вашей модели и добавьте нужные поля.

 3. **Примените миграцию**:
 - Запустите команду, чтобы применить миграцию:

 bash
    python manage.py migrate

 Это создаст соответствующую таблицу в базе данных на основе вашей модели.

 ### Примечания:
 - Убедитесь, что вы правильно указали все поля и их типы в файле миграции.
 - Файл миграций должен иметь уникальный номер (например, `0001`, `0002` и т.д.) для правильного управления миграциями.


 MIDDLEWARE - это промежуточный слой между запросом и ответом в Django

 Django Debug Toolbar - Помогает оптимизировать проект

 ОСНОВНОЕ ОТЛИЧИЕ между Swagger и Postman заключается в том, что Swagger сконцентрирован на проектировании
 и документировании API, в то время как Postman — на тестировании и создании запросов.

 Insomnia: более простой и минималистичный интерфейс, ориентирован на разработчиков

 Insomnia и Postman — это инструменты для тестирования и разработки API.
 Insomnia проще и легче в использовании, Postman предлагает более богатый функционал и возможности для командной работы.


 CSRF-токен помогает удостовериться, что запрос исходит от законного пользователя.
 CSRF-токен (Cross-Site Request Forgery) используется для защиты веб-приложений от атак

 CSRF токен — это случайное значение, генерируемое веб-приложением и связываемое с текущей сессией пользователя
 CSRF токен - нужен для защиты межсайтовых атак
 Django требует CSRF токен при всех POST - запросах

  -- HTTP --
 HTTP сообщения - это обмен данными между сервером и клиентом
 Http запрос, Http ответ между Клиентом и Сервером

 Полный цикл HTTP-запроса включает следующие шаги:

 1. Клиент (обычно браузер) формирует HTTP-запрос.
 2. Запрос отправляется на сервер через Интернет.
 3. Сервер принимает запрос и обрабатывает его, выполняя необходимые операции (например, извлечение данных из базы).
 4. Сервер формирует ответ в формате HTTP, включая статус (успешно, ошибка и т.д.) и, при необходимости, данные
   (например, HTML-страницу).
 5. Ответ отправляется обратно клиенту по тому же соединению.
 6. Клиент получает ответ и обрабатывает его (например, отображает страницу).

 Этот цикл обеспечивает взаимодействие между клиентом и сервером для передачи данных в веб-приложениях.

 СТАТУСЫ HTTP запросов:

 100-199: ИНФОРМАЦИОННЫЕ ответы.
 200-299: УСПЕШНАЯ обработка запроса.
 300-399: ПЕРЕНАПРАВЛЕНИЕ запроса.
 400-499: ОШИБКИ КЛИЕНТА.
 500-599: ОШИБКИ СЕРВЕРА.


 САМЫЕ ПОПУЛЯРНЫЕ СТАТУСЫ:

 100-199: Информационные
 - 100 Continue - Сервер готов к приёму данных.

 200-299: Успешные
 - 200 OK         - Успешный запрос.
 - 201 Created    - Ресурс создан.
 - 202 Accepted   - Запрос принят, но ещё не обработан (асинхронные операции).
 - 204 No Content - Нет содержимого для отправки.

 300-399: Перенаправления
 - 301 Moved Permanently  - Ресурс перемещён навсегда.
 - 302 Found              - Временное перенаправление.
 - 304 Not Modified       - Контент не изменился (кеширование).
 - 307 Temporary Redirect - Аналог 302, но с гарантией сохранения метода запроса (POST не превратится в GET).

 400-499: Ошибки клиента
 - 400 Bad Request          - Неправильный синтаксис запроса (битый JSON, неверный Content-Type).
 - 401 Unauthorized         - Требуется авторизация.
 - 403 Forbidden            - Доступ запрещён.
 - 404 Not Found            - Ресурс не найден (самая известная ошибка).
 - 405 Method Not Allowed   - Метод не поддерживается (например, POST вместо GET).
 - 408 Request Timeout      - Сервер не дождался полного запроса.
 - 409 Conflict             - Сервер не может выполнить запрос из-за конфликта данных.
 - 422 Unprocessable Entity - Если запрос корректен, но данные не проходят валидацию.
 - 429 Too Many Requests    - Слишком много запросов.

 500-599: Ошибки сервера
 - 500 Internal Server Error - Внутренняя ошибка сервера.
 - 501 Not Implemented       - Сервер не поддерживает функциональность.
 - 502 Bad Gateway           - Проблема с прокси-сервером.
 - 503 Service Unavailable   - Сервис временно недоступен.
 - 504 Gateway Timeout       - Таймаут соединения.
 - 507 Insufficient Storage  - Не хватает места для обработки запроса.


  Пинг может влиять на скорость запроса   <-----


 -- Методы HTTP-запросов --
 GET – получить объект или список объектов
 HEAD – получить метаданные объекта.  Запрашивает ресурс так же, как и метод GET, но без тела ответа.
 POST – создать объект
 PUT – обновить существующий объект
 PATCH – частично обновить существующий объект
 DELETE – удалить объект
 TRACE - выполняет вызов возвращаемого тестового сообщения с ресурса
 OPTIONS - используется для описания параметров соединения с ресурсом
 CONNECT - устанавливает "туннель" к серверу, определённому по ресурсу


 ИДЕМПОТЕНТНЫЕ (GET, HEAD, PUT, DELETE, OPTIONS, TRACE) — при повторном выполнении результаты будут ожидаемо ОДИНАКОВЫМИ
 НЕИДЕМПОТЕНТНЫЕ (POST, PATCH)                          — при повторном выполнении результаты будут РАЗНЫМИ


 Таким образом, ИДЕМПОТЕНТНЫЕ запросы приводят к одному и тому же состоянию системы НЕЗАВИСИМО от количества вызовов,
 а НЕИДЕМПОТЕНТНЫЕ могут изменять состояние при каждом вызове.



 -- Почему плохо использовать GET вместо POST? --

 Потому что:
 - Ограниченная длина URL.
 - Данные видны в URL (небезопасно).
 - Нарушает семантику HTTP (GET — для получения, POST — для отправки).
 - Риск кэширования и случайных повторных запросов.
 - Уязвимость к CSRF.

 Можно ли отправить данные в GET-запросе?
 Да, но с ограничениями:

 - Данные передаются в URL как параметры (после ?):
 https://example.com/api?param1=value1&param2=value2

 - Нельзя отправить файлы или сложные структуры (как в POST с multipart/form-data).

 - Не подходит для конфиденциальных данных.

 GET — для запросов без изменений (поиск, фильтрация).
 POST — для отправки данных, изменений на сервере.



 Фреймворк Django реализует АРХИТЕКТУРНЫЙ паттерн Model-View-Template или сокращенно MVT,
 который по факту является модификацией распростаненного в веб-программировании паттерна MVC (Model-View-Controller).

 Model View Template(MVT) (MVT – модель, представление и шаблон) - АРХИТЕКТУРНЫЙ ПАТТЕРН

 Основные элементы паттерна:
 URL dispatcher: при получение запроса на основании запрошенного адреса URL определяет, какой ресурс должен обрабатывать данный запрос

 View: получает запрос, обрабатывает его и отправляет в ответ пользователю некоторый ответ.
 Если для обработки запроса необходимо обращение к модели и базе данных, то View взаимодействует с ними.
 Для создания ответа может применять Template или шаблоны. В архитектуре MVC этому компоненту соответствуют контроллеры (но не представления).

 Model: описывает данные, используемые в приложении. Отдельные классы, как правило, соответствуют таблицам в базе данных.

 Template: представляет логику представления в виде сгенерированной разметки html.
 В MVC этому компоненту соответствует View, то есть представления.

 QuerySet, по сути, — список объектов заданной модели. QuerySet позволяет читать данные из базы данных,
 фильтровать и изменять их порядок


 -- Модель OSI (Open Systems Interconnection) — 7 уровней --

 - L1 Физический (Physical) — передача битов по среде (сигналы, кабель/радио).
 - L2 Канальный (Data Link) — кадры, MAC-адреса, доступ к среде, контроль ошибок на линии.
 - L3 Сетевой (Network) — пакеты, IP-адреса, маршрутизация между сетями.
 - L4 Транспортный (Transport) — доставка “конец-конец”: порты, надёжность/порядок (TCP) или без гарантий (UDP).
 - L5 Сеансовый (Session) — управление сеансом (установить/поддерживать/завершить); часто функции на практике в приложениях/протоколах.
 - L6 Представления (Presentation) — формат данных: кодировки, сериализация, сжатие, шифрование/дешифрование (часто через TLS).
 - L7 Прикладной (Application) — протоколы приложений (HTTP, DNS, SMTP и т.д.).

 # КОРОТКО
 L1 биты → L2 кадры/MAC → L3 пакеты/IP → L4 сегменты/порты → L7 протоколы приложений (HTTP/DNS/SMTP).

  Существует ДВА основных типа сокетов:
 -- TCP-сокеты (Transmission Control Protocol) — обеспечивают надежное соединение между двумя устройствами. Они обычно
 используются в веб-серверах и клиентах, почтовых серверах и других приложениях, где требуется надежность и порядок доставки данных.

 TCP — соединение-ориентированный: держит состояние (установление, номера последовательности, ACK, окна, ретрансляции),
 что требует больше ресурсов.

 -- UDP-сокеты (User Datagram Protocol) — обеспечивают передачу данных без установления соединения. Они обычно
 используются в видео и аудио потоковых приложениях, играх и других приложениях, где скорость важнее, чем надежность.

 UDP — протокол без установления соединения (connectionless): нет состояния соединения
 (handshake, подтверждений, контроля порядка и т.п.), поэтому меньше накладных расходов и проще обработка.

 Нюанс: UDP всё равно может иметь “состояние” на уровне приложения/ОС (например, таблицы NAT/файрвола),
 но в самом протоколе состояния соединения нет.


 Сокет - низкоуровневая обстракция отправки и получения данных по сети. Именно с помощью ее производится обмен данными
 между клиектами и серверами. Сокеты поддерживают две основные операции: отправку и получение байтов.


 Вот краткое описание ключевых интернет-архитектур, технологий, протоколов и форматов:

 1. **TCP/IP**: Протоколы, обеспечивающие передачу данных по сети. TCP (Transmission Control Protocol)
  отвечает за надежную доставку, а IP (Internet Protocol) — за маршрутизацию данных.

 2. **HTTP**: Протокол передачи данных, используемый для обмена информацией между веб-серверами и клиентами (браузерами).
  Он лежит в основе работы Всемирной паутины.

 3. **AJAX**: Технология для асинхронной загрузки данных на веб-страницах, позволяющая обновлять интерфейс без перезагрузки страницы.

 4. **HTML**: Язык разметки для создания и структурирования веб-страниц, который описывает содержимое и структуру документов.

 5. **CSS**: Язык стилей, используемый для оформления HTML-документов. Он управляет внешним видом элементов веб-страниц,
  включая цвета, шрифты и макеты.

 6. **XML**: Язык разметки, используемый для структурированного хранения и передачи данных с возможностью определения
  собственных тегов.

 7. **JSON**: Формат обмена данными, основанный на JavaScript, который является более легковесной альтернативой
  XML для передачи структурированных данных.

 8. **WSDL**: Язык описания веб-служб, который определяет интерфейс и методы, доступные в веб-службах, позволяя
  программам взаимодействовать друг с другом.

 9. **REST**: Архитектурный стиль для разработки веб-сервисов, использующий стандартные HTTP-методы
  (GET, POST, PUT, DELETE) для взаимодействия с ресурсами, которые представляются в виде URL.

 Эти технологии и протоколы создают основу для работы интернета и веб-приложений.


 -- Основные причины внедрения NoSQL --

 - Гибкость схемы – поддержка неструктурированных и полуструктурированных данных.
 - Масштабируемость – горизонтальное масштабирование для больших нагрузок.
 - Высокая производительность – оптимизация для быстрых операций чтения/записи.
 - Распределённые системы – лучшее управление большими объёмами данных в облаке.
 - Разнообразие моделей данных – ключ-значение, документы, графы, колоночные хранилища.

 Коротко: NoSQL решает проблемы масштабируемости, скорости и гибкости, которых НЕ хватает в реляционных СУБД.

  -- END Основные причины внедрения NoSQL --


 В чем разница между РЕЛЯЦИОННЫМИ и НЕРЕЛЯЦИОННЫМИ базами данных?                        <------

 В Реляционныx БД - Данные Хранятся в виде СВЯЗНЫХ ТАБЛИЦ   Внешние ключи FOREIGN KEY    <------

 Реляционные и НЕреляционные базы данных – два способа хранения данных для приложений.
 Реляционная база данных (или база данных SQL) хранит данные в табличном формате со строками и столбцами.
 Столбцы содержат атрибуты данных, а строки – значения данных.

 НЕреляционные базы данных (или базы данных NoSQL) используют различные модели данных для доступа к данным и управления ими.


 Нормализация баз данных — это процесс организации данных в базах данных с целью уменьшения избыточности и зависимости.
 Основная цель нормализации — разделение данных на таблицы так, чтобы каждая таблица содержала информацию о конкретном
 объекте и обеспечивала целостность данных.

 Нормализация баз данных — это процесс организации данных в базе таким образом, чтобы избежать избыточности и снизить
 вероятность ошибок. Простыми словами, это упорядочивание информации так, чтобы каждая деталь хранилась только в одном месте.

 Когда база данных организована таким образом, что НЕ содержит повторяющейся информации, говорят, что она НОРМАЛИЗОВАНА.
 Процесс преобразования базы данных с дубликатами в базу данных без таковых называется НОРМАЛИЗАЦИЕЙ.   <-----

 Основные цели нормализации:
 - Устранение дублирующихся данных.
 - Обеспечение целостности данных.
 - Облегчение обновления и управления данными.

 В результате нормализации достигается более эффективное использование ресурсов и упрощается работа с данными.

 Нормализация   — это процесс организации данных в БД для уменьшения избыточности и улучшения целостности.
                  Делится на нормальные формы (1NF, 2NF, 3NF и т. д.).

 Кратко о нормальных формах:

 1NF – данные атомарны (никаких списков в ячейках).          1NF — данные атомарны, нет массивов.
 2NF – все поля зависят от всего ключа (не от части).        2NF — нет частичных зависимостей от составного ключа.
 3NF – поля зависят только от ключа, а не от других полей.   3NF — нет зависимостей между неключевыми полями.
 Дальше (BCNF, 4NF, 5NF) редко нужны.  BCNF/4NF/5NF — нужны для сложных случаев (многозначных зависимостей, соединений)
 1NF → 2NF → 3NF = меньше дублирования, больше целостности.

 Денормализация — намеренное добавление избыточности для ускорения запросов, обычно за счёт снижения целостности данных.

 Нормализация   = минимизация дублирования, акцент на целостность.
 Денормализация = допуск дублирования для скорости чтения.

 Нормализация — для целостности и удобства обновлений:
 - уменьшает дублирование
 - уменьшает риск аномалий (update/insert/delete)
 - обычно увеличивает число таблиц и JOIN’ов → чтение иногда медленнее, запись/обновления проще и безопаснее

 Денормализация — для скорости чтения и простых запросов:
 - уменьшает JOIN’ы → чтение часто быстрее
 - увеличивает дублирование и размер данных
 - усложняет записи/обновления (надо обновлять несколько мест) и повышает риск рассинхронизации данных

 СУБД - Система Управления Базами Данных
 Реляционные базы данных - SQL       - Все данные хранятся в виде Таблиц
 Строго структурированные (Есть привязки к типу данных)

 НЕ Реляционные базы данных - NOSQL  - Данные могут хранится различным способом
 НЕТ Строгой Структуры (НЕТ привязки к типу данных)
  - ключ-значение
  - графовые
  - документно-ориентированные
  - колоночные

 Реляционные (SQL):      Oracle, PostgreSQL, MariaDB, MySQL, SQLite (Из коробки)
 НЕ Реляционные (NOSQL): MongoDB(документо-ориентированная), Elasticsearch(документо-ориентированная), Redis(ключ-значение)
                         ClickHouse(колоночные) -   поддерживает язык запросов на основе SQL немного модифицированный

 Обычно NoSQL-БД не поддерживают JOIN в том виде, как реляционные (SQL),
 потому что они оптимизируются под денормализацию и горизонтальное масштабирование.


  -- Базовые атаки --

 DoS/DDoS атаки - цель состоит в том, чтобы перегрузить сервер. Часто путем отправки большого количества запросов.
 DoS — Denial of Service (отказ в обслуживании)
 DDoS — Distributed Denial of Service (распределённый отказ в обслуживании)

 DoS — атака, которая перегружает сервис запросами и делает его недоступным, обычно с одного источника.
 DDoS — то же самое, но с множества источников одновременно (часто ботнет), поэтому её сложнее блокировать.


 SQL-инъекции — это один из самых распространенных методов атак на базы данных.
 SQL-инъекция (SQLi) - это уязвимость веб-безопасности, которая позволяет злоумышленнику вмешиваться в запросы,
 которое приложение делает к своей базе данных. Как правило, это позволяет просматривать данные,
 которые он обычно не может получить

 Способы избежать SQL-инъекция (SQLi):

 - Проверка пользовательского ввода
  Всегда проверяйте и фильтруйте пользовательский ввод, включая символы, которые могут быть потенциально опасными
 (например, кавычки, точки с запятой и т. д.).

 - Ограничение прав доступа
   НЕ давайте учетной записи, используемой веб-приложением, права на изменение структуры базы данных.

 - Логи и Мониторинг
   Ведите логи всех запросов к вашей базе данных и следите за необычной активностью, что позволит быстро обнаружить потенциальные атаки.

 - Использование ORM (Object-Relational Mapping)
   SQLAlchemy (Python), автоматически генерируют SQL-запросы и уменьшают риск инъекций.
   Предотвращение SQLi в Django

  Django ORM повсеместно использует параметризованные операторы, поэтому он очень устойчив к SQLi. Таким образом,
  если вы используете ORM для выполнения запросов к базе данных, вы можете быть уверены, что ваше приложение в безопасности.

 - Экранирование данных — это процесс обработки пользовательского ввода в таком виде, чтобы вредоносные символы НЕ
  интерпретировались как части SQL-команды. Это обеспечивает безопасность, предотвращая возможность SQL-инъекций.

  Пример:
  import sqlite3
  connection = sqlite3.connect('example.db')
  cursor = connection.cursor()
  input_data = "O'Reilly"  # Пользовательский ввод
  escaped_input = input_data.replace("'", "''")  # Экранирование
  query = f"SELECT * FROM authors WHERE name = '{escaped_input}'"

 - Подготовленные запросы SQL (prepared statements) — это механизм работы с базами данных, который позволяет заранее
  компилировать SQL-запросы с параметрами.
  В Python: Используйте библиотеки, такие как `sqlite3` или `SQLAlchemy`, которые поддерживают подготавливаемые запросы.

 Django был разработан таким образом, чтобы быть устойчивым к SQL-инъекциям (и другим распространенным веб-уязвимостям).
 Большинство распространенных вариантов использования Django будут автоматически защищены, поэтому уязвимости SQLi
 в реальных приложениях Django, к счастью, встречаются редко.

 Bento автоматически проверяет код Django на наличие шаблонов SQL-инъекций.   <-----


 Что такое ОРМ?
 Прежде всего, давайте заложим основу. Объектно-реляционные отображения (ORM) — это уровень абстракции,
 который позволяет разработчикам взаимодействовать с базами данных с помощью объектов Python вместо написания сырых SQL-запросов


 --- SQLAlchemy  синхронная ORM ---
 интересный аспект SQLAlchemy: хотя он позиционируется как синхронная ORM, под капотом он использует greenlet для
 псевдо-асинхронного выполнения операций ввода-вывода
 SQLAlchemy — синхронная ORM, но может работать псевдо-асинхронно благодаря greenlet (легковесным корутинам).


 Сравнение с AsyncIO

 - SQLAlchemy + greenlet → псевдо-асинхронность (подходит для Gevent, синхронных фреймворков).
 - SQLAlchemy 2.0 + asyncpg → настоящая асинхронность (для asyncio, FastAPI и т. д.).

 SQLAlchemy даёт синхронный API, но под капотом может вести себя почти как асинхронный благодаря greenlet.


 SQLAlchemy использует greenlet для неблокирующего I/O в синхронном коде, эмулируя асинхронность.
 Но это не настоящий asyncio — просто оптимизация под капотом.

 Для настоящей асинхронности нужен SQLAlchemy 2.0 + asyncpg/aiomysql с поддержкой asyncio.


 Разница:

 Greenlet → "псевдо-асинхронность" (совместима с Gevent, Flask, Django).

 Asyncio → настоящая асинхронность (FastAPI, Quart и т. д.).

 SQLAlchemy даёт синхронный API, но внутри может переключать контекст (greenlet), чтобы уменьшить простои.


--- Запросы у SQLAlchemy ---

 SQLAlchemy не выполняет запросы сразу, но и не поддерживает бесконечное откладывание, как Django.
 Ленивость есть, но контролируемая – запрос выполняется только при явном указании.

 В SQLAlchemy все запросы по умолчанию "жадные" (eager) – они выполняются сразу при вызове, а не откладываются на потом.

 Почему в SQLAlchemy нет ленивых запросов?
 Явность лучше неявности – SQLAlchemy предпочитает чёткое выполнение запросов, а не магические отложенные вызовы
  (как, например, в Django ORM с QuerySet).

 Контроль над транзакциями – если запрос выполняется сразу, проще управлять сессиями и избегать проблем с LazyLoading
 вне контекста БД.


 SQLAlchemy не поддерживает ленивые запросы в стиле Django, но предлагает:

 select() – для отложенного выполнения.

 lazy="dynamic" – для ленивой загрузки связей.

 Сессии – контроль над временем выполнения запроса.

 В Django ORM QuerySet можно модифицировать цепочкой вызовов (filter(), exclude()), а запрос выполнится только при
 итерации или вызове .all().

 В SQLAlchemy запрос формируется сразу, но выполняется только при execute() – это ближе к "отложенному выполнению",
 чем к истинной ленивости.

 SQLAlchemy даёт контролируемую ленивость – запрос не выполняется сразу, но и не откладывается "навсегда".   <-----


 -- Lazy Load vs Eager Load: Django и FastAPI (SQLAlchemy) --

 Lazy Load (ленивая загрузка) - Загружает связанные данные только при явном обращении к ним. Может вызывать N+1 проблему.
 Eager Load (жадная загрузка) - Загружает связанные данные сразу, используя JOIN или дополнительные запросы.
                                Оптимизирует производительность, избегая N+1.

 Lazy Load:
 - Плюсы: Не загружает лишние данные, если они не нужны.
 - Минусы: Риск N+1, если обращаться к связанным объектам в цикле.

 Eager Load:
 - Плюсы: Избегает N+1, загружает всё за 1-2 запроса.
 - Минусы: Может загружать лишние данные, если связи не используются.


 ##  FastAPI  SQLAlchemy   Django ORM                                   <--------------------
 ##  joinedload()          == select_related()
 ##  selectinload()        == prefetch_related()

 Django ORM
 Lazy Load (ленивая загрузка) (по умолчанию) -  Запросы к БД выполняются только при обращении к связанным объектам.

 book = Book.objects.get(id=1)  # Запрос только для книги
 author = book.author  # Дополнительный запрос для автора


 Eager Load (жадная загрузка) - Использует select_related (для ForeignKey, OneToOne) или prefetch_related (для ManyToMany).

 book = Book.objects.select_related('author').get(id=1)  # Один запрос с JOIN
 author = book.author  # Без доп. запроса


 FastAPI (с SQLAlchemy)
 Lazy Load:
 По умолчанию в SQLAlchemy (lazy=True).  Если НЕ указать joinedload/selectinload, будет N+1 проблема.

 book = await db.query(Book).get(1)  # Запрос только для книги
 author = book.author  # Доп. запрос (если lazy=True)

 Eager Load:
 Использует joinedload или selectinload.

 from sqlalchemy.orm import selectinload
 book = (await db.query(Book).options(selectinload(Book.author)).get(1)
 author = book.author  # Без доп. запроса


 joinedload (через JOIN) Лучше для:

 - One-to-One / Many-to-One (например, загрузка автора книги).
 - Когда нужно минимум запросов (1 запрос с JOIN).
 - Если связанных данных не слишком много (иначе дублирование в результате).

 Минусы:

 - Может возвращать избыточные данные (дубли при множественных JOIN).
 - Неэффективно для Many-to-Many (раздувает результат).


 selectinload (отдельный запрос) Лучше для:

 - Many-to-Many (например, теги книги).
 - Когда JOIN приводит к сильному раздуванию данных.
 - Если нужно загрузить большие списки без дублирования.

 Минусы:

 - Делает 2+ запроса (но без N+1).
 - Медленнее, если связь очень простая (лучше joinedload).


 ВАЖНО!!!
 joinedload (JOIN)   - One-to-One, Many-to-One (если данных немного).
 selectinload (отдельный IN-запрос) - Many-to-Many, большие списки, избегание дублей. Для Many-to-Many/больших данных.


 Итог:
 Lazy - меньше данных сразу, но возможны N+1 запросы.
 Eager - больше данных за один запрос, но оптимизирует нагрузку.


 -- Курсор SQLAlchemy --
 Курсор автоматически управляется SQLAlchemy. Эти методы позволяют НЕ загружать все данные в память сразу.
 Для работы с большим количеством записей в SQLAlchemy через курсор можно использовать yield_per() или stream_scalars()


  -- SQLAlchemy vs Django ORM --
 Django ORM обладает теми же свойствами: более-менее типичные запросы на ней можно сделать очень легко.
 А вот со сложными запросами Django ORM не справится.

 SQLAlchemy наоборот: позволяет создавать очень сложные запросы, но в среднем формирование запроса будет сложнее.


 необходимо Django ORM, если:
 Вы создаете новое веб-приложение с помощью Django.
 Вы отдаете предпочтение быстрой разработке и читаемости кода.
 У вас нет требований к базе данных.

 понадобится SQLAlchemy, если:
 Вам нужна максимальная гибкость для сложного взаимодействия с базами данных.
 Вы работаете со схемой базы данных.
 Вам требуется детальная оптимизация производительности.
 Вы используете веб-фреймворк, отличный от Django, или создаете отдельный проект.

 -- END SQLAlchemy vs Django ORM --

 ORM Django позволяет разработчикам взаимодействовать с базами данных с помощью кода Python

 ORM
 Таблица - Это Класс
 Записи в таблице - Это Экземпляры класса

 Фильтры запросов  Lookups
 # https://docs.djangoproject.com/en/5.0/ref/models/querysets/

 Movies.objects.filter(budget=1000)                    ==   фильтр на равенство поля
 Movies.objects.filter(budget__gt=1000)                 >   фильтр на поле больше значения (great then)
 Movies.objects.filter(budget__lt=1000)                 <   фильтр на поле меньше значения
 Movies.objects.filter(budget__gte=1000)               >=   фильтр на поле больше либо равно значения
 Movies.objects.filter(budget__lte=1000)               <=   фильтр на поле меньше либо равно значения
 Movies.objects.exclude(budget=1000)                   !=   фильтр на поле не равно значению
 Movies.objects.filter(year__isnull=True)                   фильтр на поле пустое (False - не пустое)
 Movies.objects.filter(year__isnull=True, name=’Avatar’)    фильтр на два поля
 Movies.objects.exclude(budget=1000).filter(name=’Avatar’)  фильтр на два поля
 Movies.objects.filter(name__contains=’Avatar’)             поле содержит значение, чувствителен к регистру
 Movies.objects.filter(name__icontains=’Avatar’)            поле содержит значение, НЕ чувствителен к регистру
 Movies.objects.filter(name__startswith=’a’)                поле начинается с “a”
 Movies.objects.filter(name__endswith=’a’)                  поле заканчивается на “a”
 Movies.objects.filter(id__in=[3,5,6])                     выбираются все значения из списка



 ORM (Object-Relation Mapping) – общее название для фреймворков или библиотек,
 позволяющих автоматически связать базу данных с кодом

 Запросы в Django ORM ЛЕНИВЫЕ и они не отправляются до тех пор,
 пока их не запустят (в англоязычных текстах это называют evaluate)

  В Django ORM запросы по умолчанию ЛЕНИВЫЕ. Это означает, что они не выполняются до тех пор, пока не потребуется
 получение данных. Чтобы применить запрос и получить данные, вы можете использовать следующие методы:

 **Конвертация в другие структуры данных**:
 **`list()`**: Преобразует `QuerySet` в список и выполняет запрос.   ** list() или tuple() или dict() или другие **:
 results = list(MyModel.objects.all())


 **Использование `next()` или `list()`**: Если вам нужно только одно значение, вы можете использовать `next()`:
 queryset = MyModel.objects.all()
 first_item = next(iter(queryset))  # Возвращает первый элемент, выполняя запрос


 **`len()`**: Получает количество объектов в `QuerySet` и выполняет запрос.
 count = len(MyModel.objects.all())


 **`for` циклы**: Итерирование по `QuerySet` также выполняет запрос.
 for obj in MyModel.objects.all():
        print(obj)


 **`get()`**: Получает единственный объект и выполняет запрос.
 obj = MyModel.objects.get(id=1)


 **Методы `get()` и `filter()`**:
- Вызов `get()` возвращает конкретный объект, тогда как `filter()` возвращает QuerySet, который будет выполнен позже.

 single_object = MyModel.objects.get(id=1)                  # Выполняет SQL-запрос
 filtered_objects = MyModel.objects.filter(name='example')  # Запрос выполняется при дальнейшей обработке


 **`first()` и `last()`**: Получает первый или последний объект и выполняет запрос.
 first_obj = MyModel.objects.first()
 last_obj = MyModel.objects.last()


 **`exists()`**: Проверяет наличие объектов и выполняет запрос.
 exists = MyModel.objects.filter(condition).exists()


 **`count()`**: Возвращает количество объектов в `QuerySet` и выполняет запрос.
 count = MyModel.objects.all().count()


 **`aggregate()` и `annotate()`**: Эти методы возвращают агрегированные данные и также выполняют запрос.
 from django.db.models import Count

 result = MyModel.objects.aggregate(Count('field_name'))

 # Подсчитываем общее количество объектов модели MyModel
 result = MyModel.objects.aggregate(total_count=Count('field_name'))

 print(result)  # Вывод: {'total_count': 42} (пример результата)


 from django.db.models import Count
 from myapp.models import MyModel, RelatedModel

 Получаем все объекты MyModel с подсчитанным количеством связанных объектов из RelatedModel
 result = MyModel.objects.annotate(related_count=Count('relatedmodel'))

 for obj in result:
     print(f'{obj.pk}: {obj.related_count}')  # Выводит идентификатор объекта и количество связанных объектов

 # Получаем общее количество объектов модели и аннотируем каждую запись с суммой значений определенного поля
 result_aggregate = MyModel.objects.aggregate(total_count=Count('id'), total_sum=Sum('field_name'))
 result_annotate = MyModel.objects.annotate(field_name_sum=Sum('field_name'))

 print(result_aggregate)  # Выводит агрегированные результаты, например: {'total_count': 42, 'total_sum': 1000}
 for obj in result_annotate:
     print(f'{obj.pk}: {obj.field_name_sum}')

 - `aggregate()` возвращает итоговые значения по всему набору данных,
 - `annotate()` добавляет агрегированные значения к каждому объекту в queryset.


 **`values()` и `values_list()`**: Эти методы возвращают список словарей или кортежей соответственно, выполняя запрос.
 queryset = MyModel.objects.values('id', 'name')           # Выполняет запрос и возвращает словари с указанными полями  {'id': ..., 'name': ...}
 queryset = MyModel.objects.values_list('id', 'name')      # Возвращает список кортежей              (id, name)
 queryset = MyModel.objects.values_list('name', flat=True) # результаты в виде списков   flat=True   ['Ann', 'Bob']


 **Срезы [ ]**: Использование срезов для получения определенного количества объектов.
 first_five = queryset[:5]  # Выполняет запрос и возвращает первые пять объектов

 Использование одного из этих методов заставит Django выполнить запрос к базе данных и вернуть результаты.

 CRUD - Create(создание), Read(чтение), Update(обновление), Delete(удаление)


 __Class__ = имя класса в модели
 __Class__.objects.all()
 __Class__.objects.create(title='')
 __Class__.objects.filter(title='Best')
 __Class__.objects.filter(pk__gte=2) # __gt, __lt, __lte, __contains   <--- Lookup
 __Class__.objects.get(id=1) # получить строго одну запись
 __Class__.objects.order_by("blog__name") # сортировка по выбранному полю
 __Class__.objects.order_by("-blog__name") # обратная сортировка
 __Class__.objects.order_by("blog__name").desc()) # обратная сортировка
 __Class__.objects.filter(pk__lte=4).update('') # update - применяется к QuerySet, не работает с одной записью
 __Class__.objects.filter(cat_id=1).count() # количество записей
 __Class__.objects.get(pk=3).posts.exists() # существуют записи или нет # posts = 'название модели'_set


  Можно использовать регулярные выражения

 # Очень полезные методы   __regex  - чувствительное к регистру       __iregex - НЕчувствительное к регистру
 # https://docs.djangoproject.com/en/5.0/ref/models/querysets/#regex
 Синтаксис регулярных выражений соответствует синтаксису используемой базы данных.

 # Должны быть выведены первые 3 строки в зависимости от установленного вами порядка.
 Model.objects.filter(adv_images__regex=r'^\d\.')[:3]
 Model.objects.filter(adv_images__regex=r'^\d\.')[:3]


 Entry.objects.get(title__regex=r"^(An?|The) +")

 # Регулярные выражения в SQL
 SQL equivalents:
 SELECT ... WHERE title REGEXP BINARY '^(An?|The) +'; -- MySQL

 SELECT ... WHERE REGEXP_LIKE(title, '^(An?|The) +', 'c'); -- Oracle

 SELECT ... WHERE title ~ '^(An?|The) +'; -- PostgreSQL

 SELECT ... WHERE title REGEXP '^(An?|The) +'; -- SQLite
 Рекомендуется использовать необработанные строки (например, r'foo'вместо 'foo') для передачи синтаксиса регулярных выражений.

 Примечание Еще зависит от БД

 Единственное условие: LIKE быстрее
 Несколько условий: REGEXP быстрее

 # Единственное условие
 SELECT * FROM comments WHERE text LIKE '%\\\\n%'; 🚀 Faster
 SELECT * FROM comments WHERE text REGEXP '\\\\n'; 🐢 Slower

 # Множественные условия
 SELECT * FROM comments
 WHERE text LIKE '%\\\\r\\\n%'
 OR text LIKE '%\\\\n%'
 OR text LIKE '%\\\\r%'; 🐢 Slower

 SELECT * FROM comments
 WHERE text REGEXP '((\\\\r\\\\n)|(\\\\(n|r)))'; 🚀 Faster

 Заключение:
 Используется LIKE для запросов с одним условием и REGEXP для запросов с несколькими условиями.   <----  ВАЖНО

 Фреймворк Djnago имеет три специальных класса для организации связей:

 ForeignKey – для связей Many to One (Один ко многим) (поля отношений)
 ManyToManyField – для связей Many to Many (Многие ко многим)
 OneToOneField – для связей One to One (Один к одному)


 --- Field types Типы полей ---
 classes:

 BooleanField  - Поле истина/ложь. Значение по умолчанию BooleanField равно None
 CharField     - Строковое поле, для строк малого и большого размера.
 TextField     - Большое текстовое поле.
 DateField     - Дата, представленная в Python экземпляром datetime.date
 DateTimeField - Дата и время, представленные в Python экземпляром datetime.datetime.
 TimeField     - Время, представленное в Python экземпляром datetime.time.
 EmailField    - Проверяет является ли значение действительным адресом электронной почты, используя EmailValidator
 FileField     - Поле для загрузки файла.
 ImageField    - Наследуется от FileField, но также проверяет, что загруженный объект является допустимым изображением.
 JSONField     - Поле для хранения данных в кодировке JSON.
 SlugField     - Slug - газетный термин. Слаг - это короткая метка для чего-либо, содержащая только буквы, цифры,
                 подчеркивания или дефисы. Они обычно используются в URL.
 URLField      - CharField для URL, проверяется валидатором URLValidator.
 IntegerField  - Целое число. Значения от -2147483648 до 2147483647  использует MinValueValidator и MaxValueValidator
 PositiveBigIntegerField, PositiveIntegerField, PositiveSmallIntegerField, SmallIntegerField - Классы чисел


 -- Поля моделей в Django являются ДЕСКРИПТОРАМИ --

 Поля моделей в Django являются дескрипторами. Они определяют, как данные хранятся и обрабатываются в базе данных. <----

 from django.db import models

 class Book(models.Model):
     title = models.CharField(max_length=100)    # Дескриптор для поля строкового типа      <----
     published_date = models.DateField()         # Дескриптор для поля даты                 <----


  --- ПРОБЛЕМА N+1 запроса ---

 ПРОБЛЕМА N+1 запроса — это НЕ эффективный способ обращения к базе данных, когда приложение генерирует запрос на каждый вызов объекта.
 Эта проблема обычно возникает, когда мы получаем список данных из базы данных без использования ленивой или
 жадной загрузки (lazy load, eager load)

 ПРОБЛЕМА N+1 запросов, когда каждая возвращенная строка из сотен строк вызывает дополнительный запрос.
 Проблема N + 1 возникает, когда фреймворк доступа к данным выполняет N дополнительных SQL‑запросов для получения
 тех же данных, которые можно получить при выполнении одного SQL‑запроса.

 Чтобы РЕШИТЬ ПРОБЛЕМА N+1 запросов:  select_related, prefetch_related, annotate и подзапросы   <-----

 select_related(key) - 'ЖАДНАЯ' загрузка связанных данных по внешнему ключу key, который имеет тип ForeignKey, OneToOneField
 уменьшение количество запросов к базе данных

 prefetch_related(key) - 'ЖАДНАЯ' загрузка связанных данных по внешнему ключу key, который имеет тип ManyToManyField
 уменьшение количество запросов к базе данных

 Запросы N+1 происходят из-за способа, которым ORM Django обрабатывает связанные объекты по умолчанию.



 --- Django ORM vs FastApi SQLAlchemy  ПРОБЛЕМА N+1 запроса ---

 Django ORM:
 select_related() (FK/OneToOne) + prefetch_related() (M2M/OneToMany) + при необходимости annotate()/Subquery.

 FastAPI (обычно SQLAlchemy/SQLModel):
 N+1 решается eager loading в ORM: joinedload() (как select_related) и чаще selectinload()/subqueryload() (как prefetch_related),
 либо писать запрос сразу с JOIN/агрегациями и отдавать DTO.


 joinedload()   == select_related()
 selectinload() == prefetch_related()

 Нюанс: в SQLAlchemy selectinload() можно применять и к FK тоже, но по смыслу пары соответствуют именно так.

 ### ПРИМЕРЫ!

 ### Django ORM

 # FK / OneToOne → select_related("fk_field")
 Book.objects.select_related("author")


 # ManyToMany / OneToMany (reverse FK) → prefetch_related("m2m_or_reverse")
 Book.objects.prefetch_related("tags")
 Author.objects.prefetch_related("books")


 ### FastAPI (обычно SQLAlchemy / SQLModel)

 # FK / OneToOne → joinedload() или selectinload()
 from sqlalchemy import select
 from sqlalchemy.orm import joinedload

 stmt = select(Book).options(joinedload(Book.author))
 books = session.scalars(stmt).all()


 # ManyToMany / OneToMany → чаще selectinload()
 from sqlalchemy.orm import selectinload

 stmt = select(Book).options(selectinload(Book.tags))
 books = session.scalars(stmt).all()

 ⚡ Примечание: если Book.tags — ManyToMany (relationship через промежуточную(secondary)-таблицу),
 то selectinload(Book.tags) — корректный eager loading (аналог prefetch_related).



 -- Агрегация и агрегирующие функции --

 Для того чтобы получить уже агрегированные данные, нужно воспользоваться методом .aggregate(), вызвав его у имеющегося
 менеджера или QuerySet. Этот метод принимает в качестве параметров так называемые агрегирующие функции. Функций этих
 достаточно много, но все они используются примерно одинаково, поэтому рассмотрим для примера функцию Avg:

 from django.db.models import Avg

 # Получение средней цены среди всех книг магазина
 Book.objects.aggregate(Avg('price'))
 # {'price__avg': 34.35}

 # Можно задать имя ключа результирующего словаря явно
 Book.objects.aggregate(average_price=Avg('price'))
 # {'average_price': 34.35}

 Если аргументы указываются как позиционные, то имена для ключей генерирует Django ORM на основе имени поля и имени
 агрегирующей функции. Аргументов можно указать сразу несколько и генерируемые имена не дадут запутаться:

 from django.db.models import Avg, Max, Min

 Book.objects.aggregate(Avg('price'), Max('price'), Min('price'))
 # {'price__avg': 34.35, 'price__max': Decimal('81.20'), 'price__min': Decimal('12.99')}

 Как можно заметить, каждый запрос на агрегацию возвращает не сами книги, а только итоговый результат. Таким образом со
 стороны Python никаких промежуточных объектов создавать не приходится!

 Вернёмся к учебному проекту, который моделирует платформу для ведения блогов. Никаких "цен" в этом проекте нет,
 но задачи для анализа найдутся. Предположим, что нужно для каждой записи в блоге некоторого автора узнать количество
 комментариев. Агрегация на первый взгляд не подходит: сами посты тоже нужны. Можно решить задачу "в лоб", написав:

 author = User.objects.get(id=1)
 posts = [(p, p.postcomment_set.count()) for p in author.post_set.all()]

 Такое решение имеет своё собственное название – "N+1 запросов" – поскольку будет выполнен один запрос N постов,
 а затем N запросов комментариев к каждому. Легко представить, насколько это неэффективно.

 Для того чтобы для каждой возвращаемой сущности вычислить некоторое значение в рамках одного запроса, Django ORM
 предоставляет механизм аннотирования.


 Процесс, при котором к каждому объекту из выборки применяется агрегирующая функция, назвается аннотированием.


  -- Аннотирование и дубликаты в выдаче --

 Например, .aggregate(Count('postcomment')) подсчитает количество всех комментариев, а .annotate(Count('postcomment'))
 даст количество комментариев к каждому посту. Так выглядит подсчёт количества тегов, которыми помечен каждый пост:

 posts = Post.objects.annotate(Count('tags'))
 posts[0].tags__count  # только тут финализируется запрос!
 # SELECT "blog_post"."id",
 #        ...
 #        COUNT("blog_post_tags"."tag_id") AS "tags__count"
 #   FROM "blog_post"
 #   LEFT OUTER JOIN "blog_post_tags"
 #     ON ("blog_post"."id" = "blog_post_tags"."post_id")
 #  GROUP BY ...
 #  LIMIT 1

 # Execution time: 0.000563s [Database: default]

 # => 2

 Здесь новый атрибут получил имя "tags__count", но имя можно было указать вручную, как и в случае обычной агрегации.


 Если вы уже имеете некоторый опыт в SQL, вы можете задаться вопросом: а не добавляет ли OUTER JOIN, который можно
 заметить в примере выше, в выборку дублирующиеся элементы, если присовокупляемые сущности соотносятся с текущей как
 "многие к одному"? Добавляет! Более того, агрегация в таких случаях даёт неверные результаты, так как учитывает и
 повторяющиеся строки. И тем больше дублей вы увидите, чем больше разных связей "многие к одному" задействуете
 (и даже одну и ту же, но несколько раз).

 Увы, в общем виде эту проблему не решить. Но конкретно агрегирующая функция Count имеет опцию distinct=True,
 которая убирает дублирование, пока вы используете только этот вид аннотаций и каждый Count используете с distinct=True.


 -- Агрегация аннотированных значений --
 Аннотирование позволяет добавить вычислимые данные к каждому элементу запроса, а это значит, что можно выполнить
 итоговую агрегацию с использованием этих значений! Получение среднего количества тегов среди всех постов будет выглядеть так:

 Post.objects.annotate(Count('tags')).aggregate(Avg('tags__count'))
 # SELECT AVG("tags__count")
 #   FROM (
 #         SELECT COUNT("blog_post_tags"."tag_id") AS "tags__count"
 #           FROM "blog_post"
 #           LEFT OUTER JOIN "blog_post_tags"
 #             ON ("blog_post"."id" = "blog_post_tags"."post_id")
 #          GROUP BY "blog_post"."id"
 #        ) subquery

 # Execution time: 0.000361s [Database: default]

 # => {'tags__count__avg': 1.5}


 Функция annotate() в Django — это мощный инструмент, который позволяет разработчикам добавлять дополнительные данные
 в свои наборы запросов, не прибегая к использованию чистого SQL

 annotate() - метод позволяет создавать новые вычисляемые поля для нашей выборки
 lst = Husband.objects.all().annotate(is_married=Value(True))

 annotate() - Позволяет Создавать дополнительное поле в Запросе к примеру в SQL. Новые поля на основе Имеющихся.
 Нужно использовать класс F            Пример выше.    annotate() - Позволяет Создавать Новые дополнительные вычисляемые поля

 Для того чтобы получить уже агрегированные данные, нужно воспользоваться методом .aggregate()
 Агрегирующие функции: Count, Sum, Avg, Max, Min. Метод values()
 from django.db.models import Count, Sum, Avg, Max, Min

# Задача на ORM
 class City(models.Model):
    name = models.CharField()


 class Person(models.Model):
     name = models.CharField()
     city = models.ForeignKey(City)

 Вывести список людей и городов где они живут?
 Вывести всех людей, живущих в городе N
 Вывести 5 городов с наибольшим населением, упорядочив по убыванию.

 # Ответ:
 Вот пример определения моделей с учетом этих деталей:
 from django.db import models

 class City(models.Model):
     name = models.CharField(max_length=255)

     def __str__(self):
         return self.name


 class Person(models.Model):
     name = models.CharField(max_length=255)
     city = models.ForeignKey(City, on_delete=models.CASCADE, related_name='people')

     def __str__(self):
         return self.name

 # Теперь перейдем к выполнению задач:
 1. Вывести список людей и городов, где они живут:

 people_with_cities = Person.objects.select_related('city').values('name', 'city__name')
 for person in people_with_cities:
     print(f'Человек: {person["name"]}, Город: {person["city__name"]}')


 2. Вывести всех людей, живущих в городе N:
 city_name = 'N'  # укажите название города
 people_in_city_n = Person.objects.filter(city__name=city_name)

 for person in people_in_city_n:
     print(f'Человек: {person.name}')

 3. Вывести 5 городов с наибольшим населением, упорядочив по убыванию.
 Для этого нам нужно будет добавить поле для хранения количества людей в каждом городе. Однако, чтобы подсчитать
 это количество динамически, мы можем использовать аннотирование с `Count`.

 from django.db.models import Count

 top_cities = City.objects.annotate(population=Count('people')).order_by('-population')[:5]

 for city in top_cities:
     print(f'Город: {city.name}, Население: {city.population}')



 --- CBV  -  Class-Based Views ---
 CBV могут наследоваться от множества классов и миксинов,
 но все CBV берут начало от класса View.


 from django.views.generic.edit import UpdateView # можно импортировать так
 from django.views.generic import UpdateView      # можно импортировать так тоже

 View         - Базовый класс представлений
 TemplateView - он служит для обработки шаблонов и отправки результата пользователю
 RedirectView - Осуществляет перенаправление на любой GET запрос.


 ListView   - Страница, представляющая список объектов
 DetailView - Базовое представление для отображения одного объекта.

 Редактирование просмотров:
 FormView   - Базовое представление для отображения формы
 CreateView - Служит для добавления новых записей в БД через форму
 UpdateView - Служит для изменения существующих записей в БД через форму
 DeleteView - Служит для удаления существующих записей в БД через форму



 LoginView                 - класс представления для авторизации пользователей
 LogoutView                - класс представления для выхода пользователя из системы
 AuthenticationForm        - класс формы обработки аутентификации пользователя
 LoginRequiredMixin        - Миксин для проверки аутентификации пользователя, требует, чтобы пользователь был авторизован.
 UserCreationForm          - Класс для регистрации пользователей
 PasswordChangeView        - для обработки формы изменения пароля
 PasswordChangeDoneView    - для отображения результата успешного изменения
 PasswordResetView         - Позволяет пользователю сбросить свой пароль,
 PasswordResetDoneView     - после того, как пользователю была отправлена по электронной почте ссылка для сброса пароля
 PasswordResetConfirmView  - Представлена форма для ввода нового пароля.
 PasswordResetCompleteView - Позволяет пользователю сбросить свой пароль,
 PasswordResetConfirmView  - Представляет представление, которое информирует пользователя об успешном изменении пароля
 PermissionRequiredMixin   - как и permission_required декоратор, проверяет, имеет ли пользователь, обращающийся к
                             представлению, все заданные разрешения.


 Paginator                 - Он выполняет всю тяжелую работу по фактическому разбиению объекта QuerySet на Page объекты



 CRM-система – это сервис для автоматизации бизнес-процессов

  Что такое сигналы? Зачем нужны? Назовите основные
 Сигналы – это события в экосистеме Джанго. С помощью сигналов подсистемы оповещают приложение о том, что случилось.
 Чтобы читать сигналы, программист регистрирет обработчики сигналов. Сигналы распространяются синхронно. Это значит,
 подписав на один сигнал сотню обработчиков, мы увеличим время, необходимое на отдачу ответа.

 Основные сигналы это начало запроса и его окончание, перед сохранением модели и после, обращение к базе данных. <-----

 Django Signals - Чтобы принять сигнал, зарегистрируйте функцию приемник с помощью метода Signal.connect().
 Функция-приемник вызывается при отправке сигнала. Все функции-приемники сигнала вызываются по очереди,
 в том порядке, в котором они были зарегистрированы
 Сигналы распространяются синхронно.

 В Django built-in signals позволяет пользовательскому коду получать уведомления об определенных действиях.

 В Django сигналы позволяют вам связывать определенные действия с событиями, происходящими в модели, например,
 сохранением или удалением объектов. Это полезно для выполнения дополнительных задач, таких как отправка уведомлений
 или обновление связанных данных.

 ### Как использовать сигналы:
 1. **Импортируйте необходимые модули:** Вам нужно импортировать `signals` и `receiver` из `django.db.models.signals`.

 2. **Создайте обработчик сигнала:** Напишите функцию, которая будет выполнена при срабатывании сигнала.

 3. **Подпишитесь на сигнал:** Используйте декоратор `@receiver` для связывания сигнала с обработчиком.

 ### Пример

 Допустим, у вас есть модель `MyModel`, и вы хотите выполнить действие при ее сохранении.

 from django.db import models
 from django.db.models.signals import post_save
 from django.dispatch import receiver

 class MyModel(models.Model):
     name = models.CharField(max_length=100)

 @receiver(post_save, sender=MyModel)
 def mymodel_saved(sender, instance, created, **kwargs):
     if created:
         print(f'Объект {instance.name} был создан!')
     else:
         print(f'Объект {instance.name} был обновлён!')

 # Пример использования:
 # Создание нового объекта:
 new_object = MyModel.objects.create(name='Пример')

 # Обновление существующего объекта:
 new_object.name = 'Обновлённый пример'
 new_object.save()

 - **Сигналы** позволяют автоматически выполнять действия при возникновении событий в моделях.
 - **Пример** показывает, как обрабатывать сигналы `post_save` для создания и обновления объектов.



 -- СОЛЬ в криптографии --
 Соль - случайные данные, добавляемые перед хешированием. Концепция соли в криптографии.

 Зачем?
  - Без соли одинаковые коды дают одинаковые хеши → легко взломать.
  - Соль делает хеши уникальными, даже если коды одинаковые.

 Как?
 - Генерируем случайную строку (salt = secrets.token_hex(16))
 - Смешиваем с кодом: хэш = sha256(код + salt)
 - Храним и хеш, и соль (соль можно открыто).

 # Пример:
 import secrets

 code = "123456"
 salt = secrets.token_hex(16)  # Уникальная соль
 hash = hashlib.sha256((code + salt).encode()).hexdigest()

 Итог:
 Соль + хеш = защита от перебора и радужных таблиц.
 Всегда используйте для паролей, кодов подтверждения!


 Рекомендации для production-кода:

 Используйте специализированные функции вместо чистого SHA-256:

 # Более безопасная альтернатива (PBKDF2):
 from hashlib import pbkdf2_hmac
 salt = secrets.token_bytes(16)
 hash = pbkdf2_hmac('sha256', code.encode(), salt, 100000).hex()
 Для паролей лучше применять:


 # Оптимальный вариант (bcrypt):
 import bcrypt
 salt = bcrypt.gensalt()
 hash = bcrypt.hashpw(code.encode(), salt)


 -- JWT токен VS ОБЫЧНЫЙ токен --

 JWT (JSON Web Token) - это токен в формате JSON, который содержит данные (claims) и подпись для проверки подлинности.

 Разница между JWT и обычным токеном (например, случайной строкой):

 - Структура - JWT состоит из 3 частей (header, payload, signature), а обычный токен часто просто случайная строка.
 - Данные внутри - JWT хранит информацию (например, ID пользователя, срок действия),
   а обычный токен - НЕТ (нужно запрашивать данные из БД).
 - Проверка - JWT проверяется по подписи, а обычный токен проверяется в базе данных.


 Дополнительные уточнения
 JWT НЕ всегда лучше обычного токена:
 - JWT нельзя отозвать (если не использовать чёрные списки или короткий срок жизни).
 - Обычные токены проще инвалидировать (удалить из БД).

 JWT может быть большим: Если хранить много данных, то токен увеличивается (влияет на передачу в заголовках).

 Безопасность:
 - JWT уязвим, если подпись слабая (например, использование HS256 с простым секретом).
 - Обычные токены уязвимы к утечке базы данных.

 Коротко:
 JWT           = самодостаточный (данные внутри), не требует запросов к БД.
 Обычный токен = просто ключ, нужно проверять в базе.


 -- АВТОРИЗАЦИЯ В Flask --

 В Flask авторизацию можно реализовать разными способами:

 - Flask-Login        – для сессионной аутентификации (куки, сессии).
 - Flask-JWT-Extended – для аутентификации через JWT (JSON Web Tokens).
 - Flask-Security     – расширение, объединяющее Flask-Login и другие функции (роли, сброс пароля).

 - Flask также поддерживает OAuth2 через библиотеки, такие как Authlib или Flask-Dance.
 В Flask сессионная аутентификация (Flask-Login) хранит состояние на сервере, а JWT (Flask-JWT-Extended) – stateless.


 -- АВТОРИЗАЦИЯ В FastAPI --

 FastAPI предлагает встроенную поддержку OAuth2 (с Password Flow, Bearer tokens) и JWT через:

 - OAuth2PasswordBearer – стандартный способ аутентификации.
 - FastAPI              - JWT-Auth или PyJWT – для работы с JWT.
 - FastAPI-Security     – дополнительные инструменты безопасности.

 FastAPI активно использует Pydantic для валидации данных и Starlette для middleware,
 что делает реализацию авторизации гибкой и производительной.

 В FastAPI чаще используют JWT, так как он лучше сочетается с асинхронностью и микросервисами.


 -- АВТОРИЗАЦИЯ В Django --
 В Django авторизация реализуется с помощью встроенного модуля django.contrib.auth. Основные шаги:

 Модели: Django предоставляет готовые модели User и Group.

 URLs: Подключите стандартные URL-адреса авторизации:

 urlpatterns = [
     path('accounts/', include('django.contrib.auth.urls')),
 ]

 - Views: Используйте готовые представления для входа (LoginView), выхода (LogoutView) и т.д.
 - Шаблоны: Создайте шаблоны в templates/registration/ (например, login.html).
 - Декораторы: Защитите представления с помощью @login_required.


 Пример входа:

 from django.contrib.auth import authenticate, login

 user = authenticate(request, username='user', password='pass')
 if user is not None:
     login(request, user)


 Пример проверки аутентификации в шаблоне:

 html
 {% if user.is_authenticated %}
     Привет, {{ user.username }}!
 {% endif %}



 КОРОТКО:

 - Модели – Используется встроенная User (или AbstractUser для кастомизации).
 - URLs – Подключение стандартных путей (login/, logout/, password_reset/ и др.).
 - Views – Готовые классы (LoginView, LogoutView).
 - Шаблоны – Лежат в templates/registration/ (например, login.html).
 - Декораторы – @login_required для ограничения доступа.
 - Проверка в шаблоне – {% if user.is_authenticated %}.

 Django Allauth – это аналог?
 Нет, django-allauth – это расширение стандартной аутентификации Django, а не аналог.
 django-allauth – отличное решение, но это надстройка, а не замена.

 -- END АВТОРИЗАЦИЯ В Django --


 Django-allauth — Библиотека авторизации:
 помогает реализовать функции регистрации, авторизации и управления учётными записями

 ipython  - для удобства в терминале
 django-extensions - набор инструментов, которые помогут вам в вашей повседневной работе. manage.py shell_plus --print-sql

 django-ckeditor - Для редактирования в Админке

 pillow - Для работы с изображениями

 Crispi-forms - для работы с формами. Позволят вам управлять поведением рендеринга ваших форм

 WebSocket — это протокол связи, который обеспечивает ПОСТОЯННОЕ соединение между клиентом и сервером для обмена данными
 в реальном времени. Он позволяет отправлять и получать сообщения в обоих направлениях с низкой задержкой, что удобно
 для приложений, требующих мгновенной передачи данных, таких как чаты, онлайн-игры и финансовые приложения.

 WebSocket работает по следующему принципу:

 1. Установление соединения: Клиент (например, веб-браузер) инициирует подключение к серверу WebSocket путем отправки
  HTTP-запроса с заголовком Upgrade. Это запрос сообщает серверу, что клиент хочет переключиться на протокол WebSocket.

 2. Подтверждение соединения: Сервер, приняв запрос, отвечает с кодом 101 (Switching Protocols), что означает успешное
  переключение на протокол WebSocket. После этого устанавливается постоянное соединение.

 3. Двусторонняя связь: В отличие от традиционных HTTP-запросов, где связь осуществляется только от клиента к серверу,
  WebSocket позволяет обмениваться данными в обоих направлениях одновременно. Это означает, что сервер может отправлять
  сообщения клиенту без предварительного запроса.

 4. Закрытие соединения: Когда соединение больше не требуется, оно может быть закрыто одной из сторон
  (клиентом или сервером), что освобождает ресурсы.

 Этот процесс обеспечивает эффективный обмен данными в реальном времени с минимальной задержкой.


  **Django** не поддерживает WebSocket "из коробки", но есть способы реализации этого протокола с помощью дополнительной
 библиотеки **Django Channels**.

 Создать файл  consumers.py

 Пример:
 from channels.generic.websocket import AsyncWebsocketConsumer
 import json

 class MyConsumer(AsyncWebsocketConsumer):
     async def connect(self):
         await self.accept()

     async def disconnect(self, close_code):
         pass

     async def receive(self, text_data):
         data = json.loads(text_data)
         await self.send(text_data=json.dumps({
             'message': data['message']
         }))


 -- Брокер сообщений --

 Брокер сообщений (также известный как очередь сообщений). По существу, это своего рода база данных,
 оптимизированная для обработки потоков сообщений.

 Он работает как сервер, а инициаторы и потребители подключаются к нему как клиенты. Инициаторы пишут сообщения брокеру,
 а потребители получают сообщения, читая их у него.

 Брокер сообщений (RabbitMQ, Kafka и др.) — это специализированная "база" для временного хранения и передачи сообщений,
 оптимизированная под потоки данных. Но в отличие от классических СУБД (PostgreSQL, MySQL), он заточен на скорость и
 маршрутизацию, а НЕ на хранение и сложные запросы.

 Коротко:
 Брокер = потоковая обработка, очереди, временное хранение.
 СУБД   = постоянное хранение, запросы, транзакции.


 -- ЛЯМБДА-АРХИТЕКТУРА --

 Лямбда-архитектура — подход к обработке данных, где используются два потока:

 - Batch-слой — считает точные результаты, но с задержкой.
 - Speed/Stream-слой — даёт быстрые (часто временные) результаты почти сразу.
 - Serving-слой — хранит/объединяет результаты и отвечает на запросы.

 Параллельное выполнение быстрого неточного пайплайна с медленным точным пайплайном называется лямбда-архитектурой. <----

 Брокер сообщений (Kafka, RabbitMQ) может быть частью stream-слоя для передачи событий, но лямбда-архитектура шире, чем просто брокер.


 -- КАППА-АРХИТЕКТУРА --

 Каппа-архитектура — упрощённая альтернатива лямбде, где нет отдельного batch-слоя: есть один поток (stream),
 а “пакетную” обработку делают переигрыванием (replay) событий из лога (например, Kafka).

 - Stream-слой — единственный слой обработки (real-time + перерасчёты).
 - Log/Storage (Kafka и т.п.) — хранит события, позволяет перечитывать их заново.
 - Serving-слой — отдаёт результаты (витрины, базы, кэши).

 Коротко: лямбда = batch + stream, каппа = только stream + replay.



 -- EDA (Event-Driven Architecture) СОБЫТИЙНАЯ АРХИТЕКТУРА --

 EDA (Event-Driven Architecture) — событийная архитектура, где сервисы обмениваются событиями (“что-то произошло”)
 через брокер/шину, а другие сервисы на них подписываются и реагируют, без жёстких прямых вызовов.

 Можно ещё совсем коротко уточнить: события обычно идут по схеме publisher → broker → subscribers.  <-----

 Порождение событий и CDC — примеры EDA (событийной архитектуры).
 - Порождение событий (event emission / domain events): сервис при факте “что-то произошло” (создан заказ, оплата прошла)
   публикует событие в брокер/шину, а другие сервисы подписываются и реагируют.
 - CDC (Change Data Capture): изменения в базе данных (INSERT/UPDATE/DELETE) считываются и превращаются в события,
   которые отправляются в брокер (часто Kafka), и дальше потребители их обрабатывают.

 Оба подхода соответствуют EDA, потому что взаимодействие строится через события, а не прямые вызовы сервисов.


 --- Celery ---
 Celery - это асинхронная очередь задач, используемая для распределенной обработки сообщений.
 Он позволяет выполнять задачи в фоновом режиме, не загружая основной поток выполнения

 Celery — это программа, которая отслеживает задачи (tasks), которые необходимо выполнить,
 и в которой есть набор обработчиков (workers), которые будут выполнять эти задачи.
 Основной смысл в том, что она (программа) может выполнять несколько задач параллельно
 и что она не блокирует поставщиков (producers) этих самых задач.

 Celery на самом деле не хранит все эти задачи в памяти, брокер сообщений хранит задачи.

 Чтобы получать задачи от вашей программы и отправлять результаты в серверную часть, Celery требуется брокер сообщений
 для связи. Redis и RabbitMQ — это два брокера сообщений, которые разработчики часто используют вместе с Celery.

 Брокер сообщений - это архитектурный паттерн в распределённых системах, где элементы системы общаются через посредника
 Брокер упрощает работу веб-сервисов, отвечая за пересылку сообщений и все связанные задачи.


 Есть два основных алгоритма передачи сообщений в очередь:
 1) Есть Producer, который направляет сообщения в брокер, и есть Consumer, который их получает.
 2) Есть Publisher, который закидывает сообщения в очередь, и есть n-ое количество Subscriber'ов, которые их обрабатывают в дальнейшем.

 Паттерны обмена информации:
 --- 1) Request-Response (Запрос-Ответ);
 --- 2) One-Way (Односторонний) или Fire and Forget (Отправил и забыл);
 --- 3) Publish-Subscribe (Публикация-Подписка) или сокращённо Pub-Sub;
 --- 4) Point-to-Point (Точка-Точка).

 1) В интернете Request-Response — это работа HTTP: клиент выполняет запрос на сервер, ждёт и получает ответ.
 2) One-Way, когда приложение по сети отправляет UDP (User Datagram Protocol) пакет на сервер без ожидания ответа
 3), 4) Publish-Subscribe, Point-to-Point - связаны с брокерами сообщений,
 В паттернах Pub-Sub и Point-to-Point происходит асинхронное общение через посредника.

 Брокер сообщений — это программная система полностью или частично реализующая паттерны Pub-Sub и Point-to-Point.
 Взаимодействие программ через брокер упрощает процесс разработки. Нет необходимости в каждом сервисе реализовывать
 механизмы доставки, маршрутизации, хранения сообщений — всем этим занимается «посредник». Общение через «посредника»
 помогает навести порядок и внести ясность в потоки данных, а значит, упростить разработку и снизить вероятность появление ошибок.

 -- At most once (Не более одного раза):
 Самый простой вариант — отправка в стиле Fire and Forget (Отправил и забыл).
 Большая часть сообщений доходит до получателя, но часть теряется из-за сбоев.

 -- At least once (Хотя бы один раз):
 Чтобы все данные достигли цели, могут предприниматься повторные отправки. Хотя бы одна попытка будет успешной.
 В таком случае сообщения не теряются, но могут дублироваться.

 Обычно реализуется через механизм подтверждений (ACK, acknowledgment). Сообщение повторяется, если не получено
 подтверждение о доставке. Возможны дубли, если подтверждение потерялось или не было отправлено из-за сбоя.

 -- Exactly once (Строго один раз):
 Самый труднодостижимый вариант — максимальная гарантия доставки. Сообщения никогда не теряются и не дублируются,
 каждое доставляется ровно один раз.


 Краткое резюме:
 At most once (Не более одного раза)
 - Сообщение отправляется один раз и больше не повторяется.
 - Возможны потери, но нет дублирования.
 - Пример: UDP, простые веб-хуки без повторов.

 At least once (Хотя бы один раз)
 - Сообщение повторяется, пока не будет подтверждения (ACK).
 - Нет потерь, но возможны дубли.
 - Пример: TCP (хотя TCP ближе к exactly once в некоторых сценариях), Kafka с подтверждениями.

 Exactly once (Строго один раз)
 - Сообщение доставляется ровно один раз, без потерь и дублей.
 - Требует сложных механизмов (идиомпотентность, транзакции, дедупликация).
 - Пример: Kafka Transactions, RabbitMQ с идиомпотентными обработчиками.

 Дедупликация - это процесс удаления дубликатов данных, чтобы хранить только уникальные копии, экономя место и ресурсы.

 Очередь и Топик:
 Брокер, реализующий шаблон Point-to-Point, ассоциируется с термином Queue (Очередь). Сообщения отправителя попадают в
 очередь, получатель извлекает сообщения из очереди. После извлечения сообщение становится больше никому не доступными.
 Данные в очереди хранятся, пока они не будут прочитаны или не истечёт срок их действия.

 В Pub-Sub ассоциируется с темой, топиком (Topic). Сообщения попадают в топик. Система распределяет каждое сообщение
 между всеми подписчиками топика (Broadcast, вещание). Сообщения могут храниться в топике, до тех пор, пока это
 необходимо для распространения данных между всеми подписчиками.


 Изначально Celery НЕ предназначен для работы с Kafka, поэтому вам может понадобиться настроить много дополнительных
 вещей, чтобы все работало гладко.

 Некоторые функции Celery, такие как механизм приоритетов или задержки, могут НЕ работать или требовать дополнительных настроек в Kafka.


 -- Как работает CELERY --
 Celery по умолчанию использует процессы, но можно переключить на потоки (-P threads) или асинхронные пулы
 (eventlet/gevent) для специфичных задач.                                                                     <-----

 - Celery по умолчанию использует процессы (prefork pool).
 - Можно переключить на потоки: -P threads (но GIL может мешать в CPU-задачах).
 - Для I/O-задач лучше eventlet/gevent (асинхронные пулы).


 -- Различие между Rabbit vs Kafka --

 Разница между RabbitMQ и Kafka значительная, хотя оба являются системами обмена сообщениями, но с разными архитектурами и подходами.
 RabbitMQ использует push-модель: брокер сам отправляет сообщения подписчикам.

 - Плюсы: низкая задержка, простота для потребителей.
 - Минусы: риск перегрузить медленных потребителей.

 Kafka использует pull-модель: потребители сами запрашивают данные.

 - Плюсы: контроль скорости, балансировка нагрузки.
 - Минусы: небольшая задержка из-за опросов.


 RabbitMQ лучше для задач с очередями и гарантированной доставкой.
 Kafka — для потоковой обработки, больших данных и долгосрочного хранения событий.

 Интегрировать их можно через промежуточные сервисы или коннекторы.


 Kafka    — PULL: потребители сами запрашивают сообщения, когда готовы их обработать.
 RabbitMQ — PUSH: брокер сам отправляет сообщения потребителям, как только они поступают.

 Ключевые отличия:

 Характеристика	    RabbitMQ (PUSH)	                Kafka (PULL)
 Модель доставки	Брокер "толкает" сообщения	    Потребители "тянут" сообщения
 Задержка	        Минимальная (мгновенная)	    Небольшая (из-за опросов)
 Нагрузка	        Риск перегрузить потребителя	Потребитель контролирует скорость
 Использование	    Очереди, RPC, задачи	        Потоки данных, аналитика, логи

 RabbitMQ — для быстрых задач с очередями.
 Kafka    — для больших данных и потоковой обработки.


 -- Брокеры Сообщений --

 -- Особенности Rabbit MQ --  Удаляет сообщение после доставки его получателю.

 Принцип «Умный брокер, тупой потребитель» по отношению к RabbitMQ означает, что брокер берёт на себя много
 дополнительных действий. Например, следит за прочитанными сообщениями и удаляет их из очереди. Или сам организует
 процесс распределения сообщений между подписчиками.

 Для Kafka принцип «Тупой брокер, умный потребитель» означает, что, в отличие от RabbitMQ, он не занимается контролем и
 распределением сообщений. Потребители сами опрашивают брокер и решают, какие сообщения им читать, брокер только хранит данные.

  Apache Kafka:
 - когда нужно обработать большой объем данных, которые очень быстро генерируются   нет большого потока данных
 - при реализации транзакционных или конвейерных систем
 - при построении событийно-ориентированной архитектуры
 - идеально подходит в тех случаях, где требуется персистентность.

 RabbitMQ:
 - когда нужно обработать большой объем данных, которые очень быстро генерируются	нет большого потока данных
 - важна гибкость маршрутизации сообщений внутри системы
 - важен факт доставки сообщений

 Redis:
 - необходима обработка больших объемов данных
 - не требуется персистентность
 - необходима высокая скорость доставки сообщений


 Redis в основном хранит данные в оперативной памяти (RAM), что обеспечивает высокую производительность.
 Однако он также поддерживает сохранение данных на диск для обеспечения долговечности (persistence).


 Персистентность в программировании означает способность состояния существовать дольше, чем процесс, создавший его.
 персистентность - возможность долговременного хранения состояния
 Персистентные структуры данных (англ. persistent data structures) — это структуры данных, которые при внесении в них
 изменений сохраняют доступ ко всем своим предыдущим состояниям.

 Уровни персистентности:
 - Частичная — к каждой версии можно делать запросы, но изменять можно только последнюю.
 - Полная — можно делать запросы к любой версии и менять любую версию.
 - Конфлюэнтная — помимо этого можно объединять две структуры данных в одну (например, сливать вместе кучи или деревья поиска).
 - Функциональная — структуру можно реализовать на чистом функциональном языке: для любой переменной значение может
   быть присвоено только один раз и изменять значения переменных нельзя.


 -- Брокеры B-Tree/Hash vs. Журналы (логи) --

 Традиционные брокеры сообщений используют ХЭШ-ТАБЛИЦЫ и B-деревья для управления уведомлениями и фильтрации,
 но эти структуры требуют обслуживания и памяти для быстрой работы. В то же время журналы (логи) обеспечивают операции
 за O(1) - запись и чтение всегда быстрые, независимо от того, хранятся данные на диске или в памяти.
 Это делает их более эффективными для некоторых сценариев.

 Суть:
 Индексы (хэши, B-деревья) — гибкие, но затратные. Журналы — простые и быстрые, но менее гибкие в поиске.
 Журналы (например, commit log, WAL, LSM-дерево) работают по принципу дописывания (append-only).

 Когда что использовать?
 Критерий	            Индексы (B-деревья, хэши)	         Журналы (логи)
 Запись	                O(log N) (B-дерево), O(1)* (хэш)     O(1) (append-only)
 Чтение (по ключу)      O(log N) (B-дерево), O(1) (хэш)	     O(N) (без индексов), (или O(1) с индексами)
 Гибкость запросов      Высокая (диапазоны, JOIN)            Низкая (нужны доп. структуры)
 Память	                Требует больше (индексы в RAM)	     Экономит (можно хранить на диске)
 Использование	        RabbitMQ (B-деревья)	             Kafka (лог)

 *Хэш-таблицы дают O(1) на вставку/поиск, но могут требовать рехеширования.

 Выбор:

 Индексы - если нужен быстрый поиск и гибкие запросы.
 Журналы - если важна скорость записи и эффективное хранение.

 Журналы лучше, если важна скорость записи и масштабируемость (Kafka, WAL).
 Индексы лучше, если нужен быстрый поиск и гибкие запросы (БД, очереди).


 Kafka
 - Использует лог + sparse-индексы (чтобы быстро находить позицию сообщения).
 - Чтение по ключу не O(1), а O(log M) (где M — число сегментов, т.к. ищется по индексу).

 LSM-деревья (RocksDB, Cassandra)
 - Запись: O(1) (в memtable).
 - Чтение: O(log N) (из-за многоуровневого поиска).
 - Используют журнал (WAL) для durability и SSTable с индексами для поиска.

 RabbitMQ
 - Для очередей использует B-деревья (быстрый доступ к голове/хвосту).
 - Для маршрутизации (exchange → queue) — хэш-таблицы (O(1) для прямых совпадений).


 Оптимизации:

 Kafka: лог + sparse-индексы → чтение за O(log M) (по сегментам).
 LSM-деревья: запись в memtable (O(1)), чтение через многоуровневый поиск (O(log N)).
 RabbitMQ: B-деревья для очередей, хэши для маршрутизации.


 -- Kafka vs БД --

 Kafka - «База данных наизнанку»   потому что работает с потоками событий, а не статичными данными.
 Kafka - это НЕ классическая база данных, а система для обработки данных в реальном времени. Хотя в ней можно
 хранить данные и делать запросы (как в БД), её главная цель — непрерывный поток событий, а не пакетная обработка.

  KSQL — это язык запросов для Apache Kafka, который позволяет обрабатывать потоки данных в реальном времени
 (как SQL, но для потоковых данных).  KSQL = "SQL для Kafka" (фильтрация, агрегация, соединение потоков).

 Сравнение:
 Традиционная БД - хранит данные и отвечает на запросы.
 Kafka - обрабатывает потоки данных на лету, обновляя результаты в реальном времени. Обрабатывает потоки данных непрерывно.



 --- Где хранятся данные в Redis? ---

 В оперативной памяти (RAM)
 - Основное хранилище, обеспечивающее высокую скорость доступа.
 - Все операции (GET, SET и др.) выполняются в памяти.
 - Если Redis перезапустится без persistence, данные будут потеряны.

 На диске (опционально, для persistence)
 - Redis поддерживает два механизма сохранения данных на диск:
   - RDB (Snapshotting) – периодическое сохранение снимка данных в бинарный файл (dump.rdb).
   - AOF (Append Only File) – запись каждой операции изменения в лог-файл (appendonly.aof).
 После перезапуска Redis может восстановить данные из этих файлов.

 Без persistence при перезапуске данные теряются.


 Коротко о хранении данных в Redis:
 - Основное хранилище – оперативная память (RAM), обеспечивающая высокую скорость доступа.

 - Persistence (сохранение на диск) – опционально, два механизма:
   - RDB – периодические бинарные снимки (snapshots) в файл dump.rdb.
   - AOF – лог операций (append-only), записывается в файл appendonly.aof.

 - Без persistence – при перезапуске Redis данные теряются.


 -- Кэш Redis --
 Кэш Redis - это высокопроизводительное хранилище данных в памяти, используемое для ускорения доступа к часто запрашиваемым данным.

 Ключевые особенности:
 - Хранение в RAM – быстрый доступ, но volatile (данные могут быть потеряны при перезагрузке).
 - Поддержка структур данных – строки, списки, хеши, множества и др.
 - TTL (время жизни) – автоматическое удаление данных по истечении срока.
 - Репликация и кластеризация – высокая доступность и масштабируемость.
 - Использование: кэширование, сессии, очереди.

 Пример команды: SET key value EX 60 (сохранить значение на 60 секунд).
 Redis - мощный инструмент для снижения нагрузки на БД и ускорения приложений.


 --- Тестирование unittest в Django ---

 В Django тесты определяются в специальном модуле tests.py внутри приложения.
 В файлике test пишем тесты

 # Пример
 from django.test import TestCase

 class TestHoroscope(TestCase):

    # Тест на статут кода
    def test_index(self):
        response = self.client.get('/horoscope/')  # client - Замена Браузера
        self.assertEqual(response.status_code, 200)

    # Тест на статут кода и перенаправление
    def test_libra_redirect(self):
        response = self.client.get('/7')
        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/horoscope/libra/')

 # Затем в терминале пишем команду
 python manage.py test название_приложения   Или через   Edit Configurations Django tests   target == Приложение или все

 Типы проверок в классе TestCase
 assertEqual(a, b)          a == b
 assertNotEqual(a, b)       a != b
 assertTrue(x)              bool(x) is True
 assertFalse(x)             bool(x) is False
 assertIs(a, b)             a is b
 assertIsNot(a, b)          a is not b
 assertIsNone(x)            x is None
 assertIsNotNone(x)         x is not None
 assertIn(a, b)             a in b
 assertNotIn(a, b)          a not in b
 assertIsInstance(a, b)     isinstance(a, b)
 assertNotIsInstance(a, b)  not isinstance(a, b)


 Django предназначен для создания полнофункциональных веб-приложений,
 в то время как DRF специализируется на создании RESTful API.


 -- GraphQL ГрафКюЭль :) (альтернатива REST) --

 GraphQL — это язык запросов для API (альтернатива REST), который позволяет:

 - Запрашивать только нужные данные (без перегрузки лишней информацией).
 - Получать несколько ресурсов за один запрос (в отличие от REST, где нужно делать много запросов).
 - Самостоятельно определять структуру ответа (клиент решает, какие поля ему нужны).

 GraphQL — это:
 - Язык запросов для API (как умный REST).
 - Позволяет запрашивать только нужные данные одним запросом.
 - Клиент сам выбирает поля, а сервер возвращает ровно то, что просили.

 Коротко: REST 2.0 — гибкий, экономичный, без перегрузки.


 -- Что такое REST --
 в веб-программировании — это архитектурный стиль, который используется для создания веб-сервисов.

 **REST (Representational State Transfer)**:
 - Это архитектурный стиль для проектирования сетевых приложений. REST использует стандартные HTTP-методы
  (GET, POST, PUT, DELETE и т.д.) для взаимодействия между клиентом и сервером.
 - RESTful API (интерфейсы программирования приложений) соответствуют принципам REST и обычно обмениваются данными в
  формате JSON или XML.
 - Основные принципы REST:
 - **Статус представление**: Клиент и сервер взаимодействуют путем обмена состояниями ресурса (например, данные о пользователе).
 - **Безсостояние**: Каждый запрос от клиента к серверу должен содержать всю информацию, необходимую для его обработки,
   и сервер не хранит состояние клиента.
 - **Кешируемость**: Ответы должны указывать, могут ли они быть закешированы для повышения производительности.
 - **Уровень абстракции**: Каждый ресурс в REST может быть доступен по уникальному URI (Uniform Resource Identifier).


 --- Что такое RESTful ??? ---

 RESTful - это архитектурный стиль для создания веб-сервисов, основанный на принципах REST (Representational State Transfer).

 Основные черты:
 - Использует HTTP-методы (GET, POST, PUT, DELETE).
 - Данные передаются в форматах JSON/XML.
 - Статус ресурса определяется URI (например, /users/1).
 - Без состояния (каждый запрос независим).

 Пример RESTful API:
 http
 - GET    /users     – получить список пользователей
 - POST   /users     – создать пользователя
 - GET    /users/1   – получить пользователя с id=1
 - PUT    /users/1   – обновить пользователя
 - DELETE /users/1   – удалить пользователя
 Это простой, масштабируемый и стандартизированный способ обмена данными между клиентом и сервером


 ДОПОЛНИТЕЛЬНО!!!
 REST (Representational State Transfer) – это не протокол, а набор архитектурных принципов, предложенных Роем Филдингом в 2000 году.
  - Ключевые принципы RESTful-сервисов:
  - Единообразие интерфейса (URI + HTTP-методы).
  - Кэшируемость (ответы могут кэшироваться).
  - Клиент-серверная архитектура (разделение ответственности).
  - Отсутствие состояния (stateless) – сервер не хранит данные о сессии клиента.
  - Слоистая система (возможность проксирования и балансировки нагрузки).

 Популярные форматы данных:
 - JSON (наиболее распространён, лёгкий и читаемый).
 - XML (реже, но используется в legacy-системах).

 Примеры HTTP-методов и их семантика:
 - GET – запрос данных (без изменения состояния сервера).
 - POST – создание ресурса (идемпотентность не гарантируется).
 - PUT – полное обновление ресурса (идемпотентный).
 - PATCH – частичное обновление.
 - DELETE – удаление.

 Статус-коды HTTP тоже важны в REST:
 - 200 OK – успешный запрос.
 - 201 Created – ресурс создан.
 - 400 Bad Request – ошибка клиента.
 - 404 Not Found – ресурс не существует.
 - 500 Internal Server Error – ошибка сервера.

 Ваш пример с /users – это классический CRUD (Create, Read, Update, Delete) через HTTP, что полностью соответствует RESTful-подходу.

 Что НЕ является RESTful?
 - Использование только GET/POST для всех операций.
 - Передача действий в URI (например, /users/delete/1 вместо DELETE /users/1).
 - Хранение состояния сессии на сервере (нарушение stateless).

 Для наглядности можно сравнить RESTful с GraphQL (альтернативный подход, где клиент сам запрашивает нужные поля)
 или gRPC (бинарный протокол от Google).


 -- REST vs RESTful --
 REST    - это концепция (набор принципов)
 RESTful - её практическая реализация (например, API, которое следует этим принципам).

 REST    = теория (что такое "хорошее API"?).
 RESTful = практика (API, которое соответствует этой теории).


 -- Что в rest значить Path и Query? --
 В REST (Representational State Transfer) API, `path` и `query` являются частями URL, которые используются для
 определения ресурсов и параметров запроса. Давайте подробнее рассмотрим каждую из них.

 1) Path (путь)
 `path` — это то, что идет после доменного имени и определяет конкретный ресурс или коллекцию ресурсов. Обычно он
 используется для идентификации ресурса, с которым вы взаимодействуете.

 Пример URL:
 https://api.example.com/users/123
 В этом URL `users/123` — это Path (путь). Он обозначает, что вы хотите получить информацию о пользователе с ID 123.

 2) Query (параметры запроса)
 query` — это часть URL, которая начинается со знака вопроса `?` и используется для передачи дополнительных параметров,
 которые могут уточнить или изменить запрос. Параметры в запросе могут быть ключами и значениями, разделенными знаком `&`.

 Пример URL:
 https://api.example.com/users?age=25&active=true

 В этом URL `age=25&active=true` — это параметры запроса. Они могут использоваться для фильтрации или настройки данных,
 которые вы хотите получить. В данном случае, это запрос списка пользователей, которые активны и имеют возраст 25 лет.

 ### Примеры использования

 **Path без query**:
 Это запрос на получение информации о продукте с ID 456.
 GET https://api.example.com/products/456


 **Path с query**:
 Это запрос на получение списка продуктов в категории "электроника", отсортированных по цене.
 GET https://api.example.com/products?category=electronics&sort=price

 Таким образом, `path` используется для навигации к определенному ресурсу, а `query` для передачи параметров, которые
 могут изменить или уточнить запрос.



--- DRF  Django REST Framework ---
 Фреймворк, работающий со стандартными моделями Django для создания гибкого и мощного API для проекта.

 Установка  pip install djangorestframework

 DRF облегчает взаимодействие между сервером и клиентами, позволяя передавать данные в формате JSON
 и выполнять различные операции, такие как чтение, запись и удаление данных. Он также обеспечивает безопасность,
 авторизацию и управление правами доступа.

 Архитектура DRF:

 Сериализатор (Serializer): преобразует информацию, хранящуюся в базе данных и определенную с помощью моделей Django,
 в формат JSON, который легко и эффективно передаётся через API.

 Представление (View, ViewSet): определяет функции (чтение, создание, обновление, удаление), которые будут доступны через API.

 - View — базовый класс-представление Django: один URL → один обработчик (обычно get/post/...).
 - ViewSet — из Django REST Framework: набор действий для ресурса (list/retrieve/create/update/destroy) и
   обычно роутится через Router, который сам создаёт несколько URL.

 Маршрутизатор (Router): определяет URL-адреса, которые будут предоставлять доступ к каждому представлению.


 Как работает Serializer в Django REST Framework?
 Serializer преобразует информацию, хранящуюся в базе данных и определенную с помощью моделей Django, в формат,
  который легко и эффективно передается через API - JSON.

 Наиболее распространенной формой, которую принимает сериализатор DRF, является тот, который привязан непосредственно к модели Django:

 class ThingSerializer(serializers.ModelSerializer):
     class Meta:
         model = Thing
         fields = (‘name’, )

 Если взять за пример Serializer, то можно посмотреть на код джанги:

@six.add_metaclass(SerializerMetaclass)
class Serializer(BaseSerializer):
  ...
SerializerMetaclass - это тот самый метакласс, который конструирует класс ModelForm.


 За что отвечает Meta в сериализаторе?
 В классе Meta сериализатора можно задать модель по которой будет создан сериализатор, поля, которые будут включены
 (или exclude для исключения), list_serializer_class, например для того чтобы задать специфическую валидацию списков и тд.


 Throttling в Django REST Framework (DRF) — это механизм, который ограничивает количество запросов, которые клиент
 может сделать к API в заданный период времени. Это помогает предотвратить злоупотребления, защитить сервер от
 перегрузок и обеспечить справедливое распределение ресурсов между пользователями.

 В DRF есть несколько встроенных классов для реализации throttling, таких как:

 1. **AnonRateThrottle** — ограничивает анонимных пользователей.
 2. **UserRateThrottle** — ограничивает зарегистрированных пользователей.
 3. **ScopedRateThrottle** — позволяет устанавливать разные лимиты для различных частей API.

 Настройки throttling можно задать в файле конфигурации Django или в представлениях (views). Это делает его гибким и
 настраиваемым под нужды вашего приложения.


 Какая разница между аутентификацией и авторизацией?
 - Сначала определяют имя (логин или номер) – идентификация
 - Затем проверяют пароль (ключ или отпечаток пальца) – аутентификация
 - И в конце предоставляют доступ – авторизации


 -- GeoDjango --
 GeoDjango — это расширение Django для работы с геоданными. Позволяет хранить, обрабатывать и отображать географическую
 информацию (координаты, полигоны и т.д.) в веб-приложениях.

 Использует PostGIS (для PostgreSQL) и другие пространственные СУБД.

 Примеры: картографические сервисы, геолокация, анализ территорий.

 Коротко: «Django + ГИС (геоинформационные системы)».



 -- nginx --
 Nginx – это веб сервер. Он хранит файлы сайта и направляет их по запросу на компьютер или мобильное устройство.
 То есть он нужен для быстрого отображения интернет-страничек.
 Его основная задача заключается в обработке статичного контента.
 Сервер Nginx выполняет две функции:
 - Принимает, обрабатывает и отправляет запросы клиентам
 - Играет роль прокси-сервера

 Сайты в интернете работают на веб-серверах, которые обрабатывают запросы пользователей и отвечают на них.
 Сегодня один из самых популярных веб-серверов — Nginx
 NGINX (Engine X, или «Энджин-икс») — это программное обеспечение с открытым исходным кодом для создания веб-серверов.
 Оно принимает запрос клиента, например браузера, обрабатывает его и возвращает ответ.

 --- Типа Советы ---

 -- Рефакторить и замерять скорость при помощи  @funcy.log_durations. - там на сайте еще много чего

 Ну, например, select_related на самом деле делает JOIN. А prefetch_related не делает. Не всегда синтаксис django ORM
 выдерживает реальности SQL, и появляются всякие странные вещи типа OuterRef, F, Q, и иже с ними. GROUP BY вообще замаскирован.

 Если вы хотите выполнить агрегацию, вы можете использовать функции агрегации ORM :

 from django.db.models import Count
 result = (Members.objects
     .values('designation')
     .annotate(dcount=Count('designation'))
     .order_by()
)


 -- Docker  Kubernetes (K8s)--

  Docker     - Изолирует приложение и упаковывает все в контейнер
  Kubernetes - Инструмент для оркестровки и управления деплоем контейнеров на нескольких машинах паралельно

 Docker — это платформа, которая позволяет упаковать в контейнер приложение со всем окружением и зависимостями,
 а затем доставить и запустить его в целевой системе.

 Приложение, упакованное в контейнер, изолируется от операционной системы и других приложений.

 Контейнеры позволяют разработчикам упаковать приложение со всеми его зависимостями и развернуть как единое целое.

 Контейнеризация — это способ упаковки приложения и всех его зависимостей в один образ, который запускается
 в изолированной среде, не влияющей на основную операционную систему.

 Виртуализация в Docker реализуется на уровне ОС. Виртуальная среда запускается прямо из ядра основной
 операционной системы и использует её ресурсы.


 Чем виртуализация отличается от контейнеризации?
 Контейнеры и виртуальные машины — это разные способы виртуализации. Только виртуалка реализует её на уровне железа,
 а Docker — на уровне операционной системы.

 Виртуальная машина функционирует как отдельный компьютер с собственным оборудованием и операционной системой.

 Когда использовать Kubernetes, а когда достаточно Docker-контейнеров?

 Если у вас мало контейнеров и они все работают на одном узле, вам будет достаточно простых Docker-контейнеров.
 Kubernetes нужен, когда у вас много контейнеров и узлов, которыми нужно управлять. Также Kubernetes подходит,
 если вам нужна распределенная отказоустойчивая система.

 Volumes в Docker — это механизм для сохранения данных контейнеров.


 10 самых частых Docker команд:                                    <--------

 1) docker ps – список запущенных контейнеров
      docker ps -a – показать все контейнеры (включая остановленные)

 2) docker images – список образов

 3) docker run <image> – запустить контейнер из образа
      docker run -d <image> – запуск в фоновом режиме (демон)
      docker run -p 8080:80 <image> – проброс портов

 4) docker stop <container> – остановить контейнер

 5) docker rm <container> – удалить контейнер
      docker rm -f <container> – принудительное удаление

 6) docker rmi <image> – удалить образ

 7) docker exec -it <container> bash – войти в контейнер (интерактивный режим)

 8) docker logs <container> – просмотр логов контейнера

 9) docker build -t <name> . – собрать образ из Dockerfile

 10) docker-compose up   – запуск контейнеров через docker-compose.yml
     docker-compose down – остановить и удалить контейнеры из docker-compose.yml


 docker logs — просмотреть логи контейнера


 1. Просмотр логов в реальном времени
 bash
 docker logs -f <имя_контейнера_или_ID>
 Флаг -f (или --follow) позволяет следить за логами в реальном времени (как tail -f).

 2. Просмотр последних N строк
 bash
 docker logs --tail=100 <имя_контейнера_или_ID>
 Выведет последние 100 строк логов.

 3. Просмотр логов с временными метками
 bash
 docker logs -t <имя_контейнера_или_ID>
 Флаг -t (или --timestamps) добавляет временные метки к каждой строке.



 Docker host — это операционная система, на которую устанавливают Docker и на которой он работает.
 Docker daemon — служба, которая управляет Docker-объектами: сетями, хранилищами, образами и контейнерами.
 Docker client — консольный клиент, при помощи которого пользователи взаимодействуют с Docker daemon и отправляют
 ему команды, создают контейнеры и управляют ими.
 Docker image — это неизменяемый образ, из которого разворачивается контейнер.
 Docker container — развёрнутое и запущенное приложение.
 Docker Registry — репозиторий, в котором хранятся образы.
 Dockerfile — файл-инструкция для сборки образа.
 Docker Compose — инструмент для управления несколькими контейнерами. Он позволяет создавать контейнеры и задавать их конфигурацию.
 Docker Desktop — GUI-клиент, который распространяется по GPL. Бесплатная версия работает на Windows, macOS,
 а с недавних пор и на Linux. Это очень удобный клиент, который отображает все сущности Docker и позволяет запустить
 однонодовый Kubernetes для компьютера.


 Podman — это инструмент для управления контейнерами, который позволяет создавать, запускать и управлять контейнерами и
 подами без необходимости в демоне, как в Docker. Он обеспечивает совместимость с Docker CLI и позволяет работать
 с контейнерами от имени обычного пользователя, что повышает безопасность.


 Podman и Docker — это инструменты для управления контейнерами, но имеют несколько ключевых отличий:

 Архитектура:

 Docker использует клиент-серверную архитектуру, где демон (Docker daemon) управляет контейнерами.
 Podman работает без демона и использует архитектуру "без демона", что делает его более безопасным и простым в
 использовании в некоторых сценариях.

 Пользовательские права:

 Docker обычно требует привилегированных прав для работы с демоном.
 Podman может работать от имени обычного пользователя без необходимости в повышенных привилегиях.
 Совместимость:

 Podman поддерживает команды Docker CLI, что облегчает переход с Docker на Podman.
 Поддержка подов:

 Podman имеет встроенную поддержку для управления подами (группами контейнеров), что делает его более подходящим для
 работы с многоконтейнерными приложениями.

 Эти отличия делают каждый инструмент более подходящим для различных сценариев использования.


 Консенсус в распределённых системах - это механизм, позволяющий узлам (нодам) прийти к согласию относительно некоторого
 значения или состояния системы, даже в условиях сбоев (например, отказов узлов или сетевых задержек).
 В современных распределённых системах (например, Kubernetes, распределённые базы данных, блокчейны) консенсус играет ключевую роль.
 Ваше определение корректно: консенсус действительно обеспечивает согласованность между узлами по какому-либо вопросу.

 Согласованность - Это свойство системы, означающее, что все узлы видят одни и те же данные в один и тот же момент времени
 (или с определёнными гарантиями).

 Согласованность - формальное соответствие правилам или стандартам.
 Консенсус - общее согласие по ключевым вопросам, достигнутое через обсуждение.
 консенсус - это один из способов достижения согласованности.

 Коротко: согласованность — это соответствие, консенсус — договорённость.


 Docker - Изолирует приложение и упаковывает все в контейнер
 Kubernetes - Инструмент для оркестровки и управления деплоем контейнеров на нескольких машинах паралельно
 K8s - Сокращенное название Kubernetes      В сокращении “K8S” цифра 8 - это восемь букв между K и S.

 Ansible — это инструмент/фреймворк для автоматизации управления серверами и сетью (настройка, обновления, развёртывание)
 с помощью декларативных playbook’ов (YAML).

 YAML (YAML Ain’t Markup Language) легко конвертируется в JSON, потому что JSON — это подмножество YAML 1.2.
 В Python YAML легко парсится в типы dict/list/str/int и обратно с помощью библиотек (например, PyYAML).


 --- BI  (Business Intelligence, Бизнес-Аналитика) ---
 ETL (Extract, Transform, Load) — это процесс, который используется для интеграции данных из разных источников в единую
 систему, обычно в системы бизнес-аналитики (BI)

 1. **Extract (Извлечение)**: Собираются данные из различных источников, таких как базы данных, файлы, API и другие
 системы. Это может включать как структурированные, так и неструктурированные данные.

 2. **Transform (Преобразование)**: Данные обрабатываются и преобразуются в нужный формат. Этот этап может включать
 фильтрацию, агрегацию, очистку и интеграцию данных, чтобы обеспечить их качество и целостность.

 3. **Load (Загрузка)**: Преобразованные данные загружаются в целевую систему хранения, такую как дата-warehouse
 (хранилище данных) или BI-платформу, где они могут быть использованы для анализа и отчетности.

 MicroStrategy — BI Система - это мощная платформа для бизнес-аналитики (BI), которая предоставляет инструменты для
 анализа данных, визуализации и создания отчетов.

 Datalens (или Яндекс.Даталеннс) — это инструмент для бизнес-аналитики и визуализации данных, разработанный Яндексом.
 Он позволяет пользователям анализировать и представлять свои данные в удобном и наглядном формате.


 -- Grafana/Графана --

 Grafana - это платформа для визуализации и анализа данных, часто используемая с базами данных, такими как Prometheus, и другими.

 Grafana — дашборды и алерты для метрик/временных рядов (Prometheus, InfluxDB и т.п.).
 Kibana — поиск/визуализация логов и событий в Elastic (Elasticsearch).

 Prometheus — система мониторинга и алёртинга: собирает и хранит метрики (временные ряды) и по ним строит графики/оповещения.
 PromQL (Prometheus Query Language) — язык запросов Prometheus для поиска и вычислений по метрикам (фильтрация, агрегации, rate и т.п.).

 Если запросы SELECT стали выполняться дольше, проверь в Grafana:

 - Задержки панелей – посмотри на время загрузки графиков (внизу панели).
 - Логи запросов – открой встроенный инспектор (F12 → Network или кнопка "Inspect" в Grafana).
 - Оптимизация запросов – проверь сложность SQL/PromQL, используй лимиты, индексы.
 - Нагрузка на БД – посмотри метрики самой БД (CPU, память, диски).
 - Кеширование – включи кеш в Grafana или настрой его в источнике данных.

 Кратко: смотри логи, оптимизируй запросы, проверь нагрузку на БД.
 - Проверить Query Inspector в Grafana.
 - Использовать EXPLAIN для SQL-запросов.
 - Мониторить не только Grafana, но и саму БД.
 - Настроить Time ranges (слишком большой интервал может замедлять запросы).


 -- Проектирование базы данных (БД) --

 Проектирование БД - это процесс создания структуры базы данных для эффективного хранения, управления и обработки данных.

 Проектирование БД — это баланс между:

 - Нормализацией (минимизация дублирования) и производительностью (денормализация, индексы).
 - Гибкостью (под будущие изменения) и оптимизацией (под текущие запросы).


 Основные этапы проектирования БД:
 1) Анализ требований
 - Определение целей БД.
 - Выявление сущностей, их атрибутов и взаимосвязей.

 2) Концептуальная модель (ER-диаграмма)
 - Моделирование сущностей, атрибутов и связей между ними.
 - Пример:
   - Сущность "Пользователь" (атрибуты: id, имя, email).
   - Сущность "Заказ" (атрибуты: id, дата, сумма).
   - Связь: "Один ко многим" (1 пользователь → N заказов).

 3) Логическая модель
 - Преобразование ER-диаграммы в схему БД.
 - Определение таблиц, столбцов, первичных и внешних ключей.

 4) Нормализация
 - Устранение избыточности данных (обычно до 3НФ).
 - Дополнение: Иногда применяют денормализацию для ускорения запросов (например, в аналитических БД).

 5) Физическая модель
 - Реализация БД в конкретной СУБД (PostgreSQL, MySQL и др.).
 - Выбор типов данных, настройка индексов, партиционирование.
 - Дополнение:
   - Индексы ускоряют поиск, но замедляют вставку/обновление.
   - Важно учитывать безопасность (права доступа, шифрование).
   - Партиционирование (по диапазонам, спискам, хешу).
   - Репликация и шардирование для распределенных БД.
   - Оптимизация запросов через EXPLAIN ANALYZE в PostgreSQL.

 6) Тестирование и доработка
 - Проверка корректности данных и производительности.
 - Оптимизация запросов, индексов, структуры таблиц.

 7) Безопасность
 - SQL-инъекции и параметризованные запросы. - Экранирование


 Основные принципы проектирования БД:
 - Минимизация дублирования данных (нормализация).
 - Обеспечение целостности (первичные и внешние ключи).
 - Удобство запросов (индексы, структура таблиц).
 - Масштабируемость (гибкость для будущих изменений).


 Пример реализации в SQL

 -- Таблица "Пользователи"
 CREATE TABLE Users (
     id INT PRIMARY KEY,
     имя VARCHAR(100) NOT NULL,
     email VARCHAR(100) UNIQUE NOT NULL
 );

 -- Таблица "Заказы" (связь один-ко-многим)
 CREATE TABLE Orders (
     id INT PRIMARY KEY,
     user_id INT,
     дата DATE NOT NULL,
     сумма DECIMAL(10, 2) NOT NULL,
     FOREIGN KEY (user_id) REFERENCES Users(id)
 );

 -- Создание индекса для ускорения поиска по email
 CREATE INDEX idx_user_email ON Users(email);


 Итог!
 Правильное проектирование БД обеспечивает:
 - Эффективное хранение данных (минимум дублирования).
 - Быстрые запросы (индексы, нормализация/денормализация).
 - Надежность (целостность, безопасность).
 - Гибкость (масштабируемость под будущие изменения).



 -- Какие есть ОПТИМИЗАЦИИ в SQL КРОМЕ индексов  --

 1. Партиционирование
 - Разделение таблицы на физические части (партиции) по ключу (например, по дате или диапазону значений).
 - Улучшает производительность при работе с большими таблицами, особенно при операциях чтения.

 2. Кэширование запросов
 - Некоторые СУБД (MySQL, PostgreSQL, Oracle) кэшируют результаты запросов.
   - В PostgreSQL кэш запросов (plan cache) работает автоматически, но нет встроенного кэша результатов.
 - Внешние кэши (Redis, Memcached) тоже помогают ускорить повторяющиеся запросы.

 3. Оптимизация JOIN
 - Правильный порядок таблиц (меньшие таблицы сначала, если нет фильтрации).
 - Использование INNER JOIN вместо OUTER JOIN, где возможно.
 - Применение HASH JOIN, MERGE JOIN или NESTED LOOPS в зависимости от данных.

 4. Денормализация
 - Сознательное дублирование данных для ускорения чтения.
 - Пример: хранение имени пользователя в заказе, чтобы не делать JOIN с users.

 JOIN часто медленнее, чем чтение из одной таблицы, потому что нужно сопоставлять строки и иногда делать много I/O.
 Поэтому в системах с очень частыми чтениями иногда делают денормализацию, чтобы читать данные из одной таблицы/документа без JOIN.

 Но это не “всегда”: при индексах и хорошей схеме JOIN может быть быстрым, а денормализация увеличивает дублирование данных
 и усложняет обновления/согласованность.


 5. Временные таблицы и CTE (Common Table Expressions)
 - Временные таблицы (CREATE TEMP TABLE) помогают разбивать сложные запросы.
 - CTE (WITH ... AS) улучшают читаемость и иногда оптимизируют выполнение.

 6. Оптимизация запросов
 - Избегание SELECT * – выбирать только нужные столбцы.
 - Замена LIKE '%text%' на полнотекстовый поиск (если возможно).
 - Минимизация подзапросов, замена их на JOIN или временные таблицы.
 - Использование EXISTS вместо IN для больших наборов данных.
 - Избегание NOT IN (лучше NOT EXISTS или LEFT JOIN ... WHERE NULL).
 - Использование UNION ALL вместо UNION, если дубликаты не нужны.

 - Оптимизация подзапросов:
 Замена коррелированных подзапросов на JOIN.

 Пример:

 -- Плохо (для каждой строки выполняется подзапрос)
 SELECT * FROM orders WHERE customer_id IN (SELECT id FROM customers WHERE active = 1);

 -- Лучше
 SELECT orders.* FROM orders JOIN customers ON orders.customer_id = customers.id
 WHERE customers.active = 1;

 7. Настройка СУБД
 - Оптимизация размера буферного кэша (shared_buffers в PostgreSQL, innodb_buffer_pool_size в MySQL).
 - Настройка параллельного выполнения запросов (max_parallel_workers).
 - Оптимизация параметров сортировки и работы с временными таблицами.

 8. Материализованные представления
 - Предварительно рассчитанные данные, которые периодически обновляются.
 - Пример: CREATE MATERIALIZED VIEW ... REFRESH ... в PostgreSQL.

 9. Сжатие данных
 - Встроенное сжатие в СУБД (например, TABLESPACE в Oracle, COMPRESS в MySQL).
 - Уменьшает место на диске и ускоряет чтение (но может замедлить запись).

 10. Векторизация и пакетная обработка
 - Некоторые СУБД (ClickHouse, современные версии PostgreSQL) поддерживают векторизацию.
 - Пакетная вставка (INSERT ... VALUES (...), (...), ...) вместо одиночных запросов.

 Дополнительные методы:
 Шардинг                          - горизонтальное разделение данных между серверами.
 Оптимизация типов данных         - выбор подходящих типов (INT вместо VARCHAR для чисел).
 Использование частичных индексов - индексы только для нужных строк (WHERE condition).
 Оптимизация транзакций           - меньшие транзакции, правильный уровень изоляции.

 EXPLAIN ANALYZE - это команда в SQL, которая показывает план выполнения запроса и реальное время его работы
 EXPLAIN ANALYZE - главный инструмент для поиска и устранения «тормозов» в SQL.



 -- EXPLAIN ANALYZE - команда SQL для анализа запросов (в PostgreSQL, в MySQL - EXPLAIN FORMAT=JSON/TREE) --

 EXPLAIN ANALYZE — это команда SQL, которая показывает:

 1) Как выполняется запрос
  - Использует ли индекс (Index Scan) или сканирует всю таблицу (Seq Scan).
  - Где тратится больше всего времени (JOIN, сортировка и т.д.).

 2) Помогает оптимизировать
  - Если видите Seq Scan на большой таблице — возможно, нужен индекс.
  - Если Index Scan медленный — индекс может быть неэффективен.

 3) Показывает время и затраты
 - Сколько строк обработано (rows=...).
 - Сколько времени занял каждый этап.

 Пример:

 EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';
 - С индексом: Быстрый Index Scan (0.05 мс).
 - Без индекса: Медленный Seq Scan (0.2 мс + проверка всех строк).

 Когда использовать?
 - Для поиска и исправления медленных запросов.
 - Перед добавлением индексов.

 Итог: Инструмент для поиска и устранения "тормозов" в SQL.


 -- EXPLAIN vs EXPLAIN ANALYZE  в PostgreSQL --

 EXPLAIN         - показывает план выполнения запроса без его реального выполнения.
 EXPLAIN ANALYZE - выполняет запрос и показывает фактическое время выполнения, а также дополнительные метрики
 (например, количество строк, затраченное время).

 EXPLAIN
 - Только показывает план выполнения запроса, который оптимизатор PostgreSQL собирается использовать.
 - Не выполняет запрос на самом деле.
 - Полезен для предварительного анализа и оптимизации запросов без их реального выполнения.

 EXPLAIN ANALYZE
 Выполняет запрос и собирает статистику.
 Показывает:
 - План выполнения (как EXPLAIN).
 - Фактическое время выполнения каждого шага.
 - Количество обработанных строк.
 - Другие метрики (например, использование буферов, если добавить BUFFERS).
 - Полезен для точного анализа производительности, но изменяет данные (если запрос модифицирующий).

 Для INSERT/UPDATE/DELETE используйте EXPLAIN ANALYZE с осторожностью, так как он выполнит изменения!

 EXPLAIN         - только план.
 EXPLAIN ANALYZE - план + реальные данные выполнения.

 # Примеры

 -- Только план
 EXPLAIN SELECT * FROM users WHERE id = 1;

 -- План + фактические метрики выполнения
 EXPLAIN ANALYZE SELECT * FROM users WHERE id = 1;


 -- SQL --

 Напишите запрос, который выводит информацию о версии MySQL сервера.
 SELECT VERSION();

 Напишите запрос, который выводит список баз данных на сервере
 SHOW DATABASES;

 Какие таблицы есть в  БД.
 SHOW TABLES;

 Выведите информацию о структуре таблицы statistics.
 USE название_схемы;
 SHOW COLUMNS FROM statistics;

 - **DATABASE** — это более широкий контейнер с данными.
 - **SCHEMA** — это способ организации данных внутри базы данных.


 -- Алгебра реляционных операций --

 Алгебра реляционных операций - это формальная система операций над отношениями (таблицами) в реляционных базах данных.
 Реляционная алгебра - это процедурный язык запросов, который работает с конечными отношениями
 Реляционная алгебра НЕ может выразить бесконечные последовательности или рекурсию, так как она оперирует конечными
 отношениями и не поддерживает рекурсивные операции. Для этого нужны более мощные языки (например, Datalog или SQL с рекурсивными CTE).

 Сравнение операций РЕЛЯЦИОННОЙ АЛГЕБРЫ с SQL-операциями:

 Выбор (σ) - WHERE                                       - Фильтрация строк по условию.
 SELECT * FROM Студенты WHERE возраст > 30;

 Проекция (π)  SELECT                                    - Выбор определённых столбцов.
 SELECT имя, фамилия FROM Студенты;

 Объединение (∪) - UNION                                 - Комбинация строк из двух отношений.
 SELECT * FROM Студенты1
 UNION
 SELECT * FROM Студенты2;

 Разность (−) EXCEPT (или NOT IN, LEFT JOIN + IS NULL)   - Строки из первого отношения, отсутствующие во втором.
 SELECT * FROM Студенты1
 EXCEPT
 SELECT * FROM Студенты2;

 Декартово произведение (×) - CROSS JOIN                 - Все возможные комбинации строк.
 SELECT * FROM Студенты CROSS JOIN Предметы;

 Соединение (⋈) - JOIN (чаще INNER JOIN или другие виды) - Комбинация строк по условию.
 SELECT Студенты.имя, Оценки.предмет
 FROM Студенты
 JOIN Оценки ON Студенты.id = Оценки.студент_id;

 Переименование (ρ) - AS (алиасы)                        - Для изменения схемы отношения (имен атрибутов)
 -- Переименование столбца
 SELECT имя AS student_name FROM Студенты;

 -- Переименование таблицы
 SELECT s.имя
 FROM Студенты AS s;

 -- Переименование результата вычисления
 SELECT AVG(оценка) AS average_grade FROM Оценки;

 Тета-соединение (⋈θ) - INNER JOIN с условием в ON      - Соединение с произвольным условием θ (>, <, ≥, ≤, ≠)
 SELECT * FROM A JOIN B ON A.attr1 > B.attr2

 Натуральное соединение (⋈) - NATURAL JOIN (но редко используется) - Автоматически соединяет таблицы по одноимённым атрибутам
 SELECT * FROM Студенты NATURAL JOIN Оценки

 Пример ОБЩИЙ:
 π_имя(σ_возраст>30(Студенты)) - имена студентов старше 30 лет.   SELECT имя FROM Студенты WHERE возраст > 30;

 Это основа для работы с SQL и реляционными БД. Это действительно база SQL и реляционных БД!


 -- Тета-соединение (θ-соединение) --

 В реляционных базах данных
 Это соединение двух таблиц по условию θ (например, =, >, <, ≠).
 Пример: A ⋈_{A.x > B.y} B - строки, где x из таблицы A больше y из B.

 В математике (теория множеств)
 Обобщённое декартово произведение с условием θ.
 A θ B = {(a,b) | a ∈ A, b ∈ B, условие θ(a,b) выполняется}.

 Кратко: В БД - аналог SQL JOIN ON с произвольным условием, в математике — фильтрованное декартово произведение.


 -- Мультимножества (Multisets) в СУБД --

 Мультимножество (multiset) в контексте СУБД (Систем Управления Базами Данных) - это неупорядоченная коллекция
 элементов, в которой ДОПУСКАЮТСЯ повторяющиеся значения.

 Это коллекция, где элементы могут повторяться (в отличие от обычного множества). Порядок элементов не важен (как и в множествах).

 Как мультимножества реализуются в СУБД?
 В реляционных базах данных (PostgreSQL, Oracle, MySQL и др.) нет отдельного типа MULTISET, но концепция используется НЕЯВНО:

 1) Таблицы без ограничений уникальности
 Если в таблице нет PRIMARY KEY или UNIQUE-ограничений, она может содержать дубликаты строк - это и есть мультимножество.

 Пример:
 CREATE TABLE items (name text); -- Допускает дубликаты
 INSERT INTO items VALUES ('apple'), ('apple'), ('banana');

 2) Операции, сохраняющие дубликаты
 UNION ALL объединяет результаты с дубликатами (в отличие от UNION, который их удаляет).
 SELECT * FROM table без DISTINCT возвращает все строки, включая повторы.

 3) Агрегация и анализ
 GROUP BY и агрегатные функции (COUNT, SUM и др.) часто применяются для обработки мультимножеств.
 Пример подсчета повторений:
 SELECT name, COUNT(*) FROM items GROUP BY name;

 Ключевые отличия от МНОЖЕСТВ:
 Характеристика	            Множество (Set)	         Мультимножество (Multiset)
 Уникальность элементов	    Да (нет дубликатов)	     Нет (дубликаты разрешены)
 Порядок элементов	        Не важен	             Не важен
 Пример в SQL	            UNION, DISTINCT	         UNION ALL, таблицы без PK

 в PostgreSQL и других СУБД концепция мультимножеств (multisets) используется, хотя явного типа данных MULTISET
 (как в некоторых NoSQL или специализированных СУБД) НЕТ.

 В PostgreSQL и других реляционных СУБД мультимножества используются неявно (через дубликаты в таблицах и UNION ALL).
 Явного типа MULTISET обычно нет, но его можно имитировать массивами или JSON.
 В некоторых СУБД (Oracle, Db2) есть более продвинутая поддержка.

 Мультимножества в реляционных СУБД (PostgreSQL, Oracle, MySQL и др.):

 - Нет явного типа данных MULTISET, но концепция используется неявно:
   - Таблицы (особенно без PRIMARY KEY/UNIQUE-ограничений) могут содержать дубликаты строк — это и есть мультимножества.
   - Операции вроде UNION ALL сохраняют дубликаты, работая как мультимножества.
   - GROUP BY и агрегатные функции (например, COUNT) часто применяются для анализа мультимножеств.

 в SQL дубликаты строк в таблицах - это и есть "НЕЯВНЫЕ" мультимножества
 DISTINCT убирает дубликаты, превращая мультимножество в множество

 Вывод: Мультимножества - это баланс между гибкостью, производительностью и соответствием реальным данным.


 -- Множество (Set) vs Мультимножество (Multiset)  vs Список (List)в SQL --

 В SQL множество - это неупорядоченный результат запроса (например, из SELECT),
 а список - это упорядоченная последовательность, которую можно получить, применив ORDER BY.
 SQL работает с мультимножествами, если явно не указано DISTINCT.
 Мультимножество (Multiset) - Это действительно "родной" режим работы SQL для большинства операций, если явно не указано DISTINCT.

 1) По умолчанию SQL оперирует мультимножествами (коллекциями, где элементы могут повторяться, а порядок не определён).

 Например:
 SELECT name FROM users;  -- Может вернуть {"Alice", "Bob", "Alice"}
 Здесь "Alice" встречается дважды — это мультимножество.

 2) DISTINCT преобразует мультимножество в множество (уникальные элементы):

 SELECT DISTINCT name FROM users;  -- Вернёт {"Alice", "Bob"} (без дубликатов)

 3) ORDER BY не влияет на "множественность", а только задаёт порядок:

 SELECT name FROM users ORDER BY name;  -- Упорядоченное мультимножество
 SELECT DISTINCT name FROM users ORDER BY name;  -- Упорядоченное множество

 Примеры:
 -- Множество (нет порядка)
 SELECT name FROM users;

 -- Список (упорядоченный по name)
 SELECT name FROM users ORDER BY name;

 Когда будет список? Только после применения ORDER BY, иначе это множество без гарантированного порядка.

 1) Множество (Set) vs Мультимножество (Multiset) в SQL:
 - По умолчанию SQL действительно работает с мультимножествами (повторяющиеся элементы разрешены, порядок не гарантирован).
 - Настоящее "множество" (без дубликатов) получается только при использовании DISTINCT.

 2) Список (List) в SQL:
 - В SQL нет понятия "списка" как такового. Результат запроса — это всегда таблица (или табличное выражение).
 - ORDER BY действительно добавляет порядок к результату, но важно понимать, что:
 - На уровне реляционной алгебры ORDER BY не является частью "чистого" запроса
 - Вне контекста ORDER BY порядок строк не определён (даже если кажется, что он есть)

 3) Уточнение терминологии:
 "Множество" в SQL — это скорее теоретическое понятие. На практике даже DISTINCT + ORDER BY даёт упорядоченную таблицу,
 а НЕ список в классическом смысле.
 SQL стандарты говорят о "табличных выражениях", а не о множествах/списках.


 Примеры для закрепления:
 -- Мультимножество (дубликаты + неупорядочено)
 SELECT department_id FROM employees;

 -- Множество (уникальные значения, неупорядочено)
 SELECT DISTINCT department_id FROM employees;

 -- Упорядоченное мультимножество
 SELECT department_id FROM employees ORDER BY department_id;

 -- Упорядоченное множество
 SELECT DISTINCT department_id FROM employees ORDER BY department_id;


 Главное помнить, что:
 - Уникальность (множество) обеспечивает DISTINCT
 - Порядок (аналог списка) обеспечивает ORDER BY
 - Без этих операторов вы работаете с неупорядоченным мультимножеством

 Формально правильнее было бы сказать:
 - Без ORDER BY: неупорядоченное мультимножество строк
 - С ORDER BY: упорядоченное мультимножество строк
 - С DISTINCT: неупорядоченное множество уникальных строк
 - С DISTINCT + ORDER BY: упорядоченное множество уникальных строк

 ПРОСТО в SQL предпочитают использовать термины "таблица" или "набор строк",
 а "множество" и "список" - это скорее аналогии из программирования.


 -- "ОДИН ко МНОГИМ" и "МНОГИЕ к ОДНОМУ" --

 "один ко многим" и "многие к одному" - это две стороны одной медали, но в зависимости от контекста используют разные формулировки.
 отношение "один-к-одному" (1:1) действительно является частным случаем отношения "многие-к-одному" (N:1).

 В реляционных базах данных связь один-к-одному (1:1) является частным случаем связи многие-к-одному (N:1),
 где максимальное количество связанных записей ограничено одной.
 требование поддержки связи N:1 включает в себя и возможность 1:1.

 Технически в реляционных БД связь 1:1 реализуется как N:1 + UNIQUE CONSTRAINT на внешний ключ.

 Связь один-к-одному (1:1) является частным случаем связи многие-к-одному (N:1), где внешний ключ дополнительно
 ограничен уникальностью (UNIQUE)


 -- SQL vs ODL vs E/R --

 SQL                              - язык запросов для работы с реляционными БД (SELECT, INSERT и т.д.).
 ODL (Object Definition Language) - язык описания структуры объектно-ориентированных БД (классы, свойства, связи).
 ODL                              - это аналог DDL (из SQL), но для объектных БД.
 E/R (Entity-Relationship)        - метод визуального проектирования БД через сущности и связи, не зависит от типа БД.
 E/R                              - это абстрактная модель, которая затем может быть реализована как в SQL, так и в ODL.

 Основное отличие:
 SQL - для запросов, ODL - для определения объектной БД, E/R - универсальная методика

 Основное отличие: SQL - это язык запросов для работы с уже созданной БД, ODL - язык определения структуры объектной БД,
 а E/R - это методология проектирования, которая может быть реализована как в реляционных, так и в объектных БД.
 E/R - это НЕ язык, а методология (визуальная нотация), тогда как SQL и ODL – конкретные языки.


 -- Ключевые слова SQL - НЕчувствительны к регистру --
 Ключевые слова SQL (SELECT, FROM, WHERE и др.) нечувствительны к регистру в большинстве СУБД
 (MySQL, PostgreSQL, SQLite, MS SQL Server* и др.).

 Пример Все эти запросы работают одинаково:
 SELECT * FROM table;
 select * from table;
 SeLeCt * FrOm table;

 Строки в кавычках ('Текст') всегда чувствительны к регистру:
 WHERE name = 'Ivan';  -- Разные запросы
 WHERE name = 'IVAN';  -- Разные запросы

 Рекомендуется придерживаться единого стиля (например, писать ключевые слова в ВЕРХНЕМ регистре,
 а имена таблиц/столбцов – в НИЖНЕМ с подчеркиванием).


 -- WHERE (ВХЕРЕ) vs HAVING в SQL --

 WHERE

 - Фильтрует строки до группировки (GROUP BY).
 - Работает с отдельными записями.
 - Нельзя использовать с агрегатными функциями (SUM, COUNT и т.д.).

 HAVING

 - Фильтрует результаты после группировки (GROUP BY).
 - Работает с группами.
 - Может использовать агрегатные функции (SUM, AVG и др.).


 Пример:
 - WHERE: фильтр до группировки (возраст > 30)
 SELECT department, COUNT(*)
 FROM employees
 WHERE age > 30
 GROUP BY department;

 - HAVING: фильтр после группировки (отделы с > 5 сотрудников)
 SELECT department, COUNT(*)
 FROM employees
 GROUP BY department
 HAVING COUNT(*) > 5;


 - Коротко:
 WHERE — фильтр строк, HAVING — фильтр групп.
 WHERE → до GROUP BY, HAVING → после.
 HAVING — для агрегатов, WHERE — нет.

 - Можно сделать ещё короче:

 WHERE  — фильтрует сырые строки (до группировки).
 HAVING — фильтрует агрегированные данные (после GROUP BY).


 Главное:
 WHERE  — для условий над полями.
 HAVING — для условий над группами (с COUNT, SUM и др.).

 -- END WHERE (ВХЕРЕ) vs HAVING в SQL --


 --- Удаление в SQL ---

 -- TRUNCATE VS DELETE --

 TRUNCATE
 - Удаляет все строки таблицы быстро (не записывает в журнал удаление каждой строки).
 - Нельзя использовать с WHERE (удаляет всё).
 - Сбрасывает счетчик автоинкремента (если есть).
 - Нельзя откатить (ROLLBACK в некоторых СУБД).

 DELETE
 - Удаляет строки медленнее (записывает каждое удаление в журнал).
 - Можно использовать с WHERE (удаление выборочно).
 - Не сбрасывает автоинкремент.
 - Можно откатить (ROLLBACK).

 Короче:
 TRUNCATE — быстрое очищение всей таблицы.
 DELETE   — гибкое удаление строк с возможностью отката.

 Когда что использовать?
 TRUNCATE — когда нужно быстро очистить всю таблицу без возможности отката.
 DELETE   — когда нужно выборочно удалять строки или если нужна возможность отката.

 Пример
 -- TRUNCATE: быстрое удаление всех данных
 TRUNCATE TABLE employees;

 -- DELETE: удаление с условием + можно откатить
 DELETE FROM employees WHERE department = 'HR';


 TRUNCATE

 Работает быстрее, потому что это DDL-операция (Data Definition Language), а не DML (Data Manipulation Language).
 Освобождает место на диске (в отличие от DELETE, который может оставлять его для отката).
 В некоторых СУБД (например, PostgreSQL) TRUNCATE можно откатить, если выполнен внутри транзакции (BEGIN; TRUNCATE ...; ROLLBACK;).
 Требует более высоких привилегий (обычно DROP на таблицу).

 DELETE

 Можно использовать с RETURNING (в PostgreSQL) для получения удаленных данных.
 Активирует триггеры ON DELETE (TRUNCATE — нет).

 # ПРИМЕРЫ SQL
 -- 1. Очистка всей таблицы
 TRUNCATE TABLE users;        -- Быстро, без отката (в большинстве СУБД)
 DELETE FROM users;           -- Медленнее, но можно откатить

 -- 2. Удаление с условием (только DELETE)
 DELETE FROM orders WHERE created_at < '2020-01-01';

 -- 3. Автоинкремент
 TRUNCATE TABLE products;     -- Счётчик сбросится (начнёт с 1)
 DELETE FROM products;        -- Счётчик сохраняется (продолжит с последнего id)

 -- 4. Транзакции (PostgreSQL)
 BEGIN; TRUNCATE TABLE logs; ROLLBACK;  -- Откат возможен только в транзакции
 BEGIN; DELETE FROM logs; ROLLBACK;     -- DELETE всегда можно откатить

 -- 5. Возврат удалённых данных (PostgreSQL)
 DELETE FROM employees
 WHERE salary > 100000
 RETURNING id, name;  -- TRUNCATE так не умеет

 -- 6. Триггеры
 DELETE FROM accounts;    -- Вызовет триггеры ON DELETE
 TRUNCATE TABLE accounts; -- Триггеры НЕ сработают



 -- DROP TABLE vs DROP SCHEMA --

 DROP TABLE  — уничтожает таблицу (структура + данные).     Нужно удалить таблицу
 DROP SCHEMA — удаляет всю схему (таблицы, функции и др.).  Нужно удалить всю базу/схему


 -- DROP TABLE — удаляет таблицу и все её данные.
 DROP TABLE employees;  -- Таблица `employees` удалена полностью

 -- DROP SCHEMA — удаляет схему (пространство с таблицами, представлениями и др. объектами).
 DROP SCHEMA hr CASCADE;  -- Удаляет схему `hr` и все её объекты


 DROP TABLE

 - Можно добавить CASCADE, чтобы автоматически удалить зависимости (например, внешние ключи).
 - В некоторых СУБД (MySQL) есть DROP TABLE IF EXISTS, чтобы избежать ошибки, если таблицы нет.

 DROP SCHEMA

 - Без CASCADE выдаст ошибку, если в схеме есть объекты.
 - В PostgreSQL и MySQL можно использовать DROP SCHEMA IF EXISTS.



 -- При удалении в SQL строк НЕ стало меньше --

 При удалении в SQL строк не стало меньше — возможно, данные помечены как удалённые, но физически ещё хранятся
 (например, из-за транзакций или MVCC в PostgreSQL). MVCC = версионность строк для изоляции транзакций без блокировок.
 MVCC (Multi-Version Concurrency Control) - это механизм в СУБД для работы с данными без блокировок.
 MVCC именно в том, чтобы избежать read/write блокировок.

 Time to Live (TTL) — срок жизни данных, после которого они автоматически удаляются.

 Вакуум (VACUUM) в SQL — процесс очистки "мёртвых" строк (удалённых или устаревших) и освобождения места (особенно в PostgreSQL).

 Кратко:
 Удалённые строки могут временно оставаться в таблице (MVCC).
 TTL — автоматическое удаление по времени.
 VACUUM — очистка мусора в БД.


 - Удаление строк в SQL
 Да, при обычном DELETE строки могут оставаться в таблице физически (особенно в PostgreSQL из-за MVCC).
 Они помечаются как "мёртвые", но не удаляются сразу.

 - TTL (Time To Live)
 Это верно, но TTL — это не стандартная SQL-функция. Он есть в некоторых СУБД (например, в ClickHouse, Redis),
 но в PostgreSQL его нет "из коробки". Аналог можно реализовать вручную (например, через задания cron или триггеры).

 - VACUUM в PostgreSQL
 Абсолютно верно. VACUUM (особенно VACUUM FULL) освобождает место, занимаемое "мёртвыми" строками. Автовакуум
 (autovacuum) делает это автоматически.

 - Дополнительно
 В других СУБД (например, MySQL InnoDB) есть схожие механизмы (например, очистка через механизм undo-логов),
 но там нет команды VACUUM.




 -- Constraint (ограничение) в SQL --

 Constraint (ограничение) в SQL — это правило, которое ограничивает данные в таблице для сохранения их целостности.

 Основные типы:
 - PRIMARY KEY — уникальный идентификатор (не NULL).
 - FOREIGN KEY — связь с другой таблицей.
 - UNIQUE — все значения в столбце уникальны.
 - NOT NULL — запрет на пустые значения.
 - CHECK — проверка условия (например, age > 0).

 Пример:
 CREATE TABLE users (
     id INT PRIMARY KEY,
     email VARCHAR(255) UNIQUE NOT NULL,
     age INT CHECK (age >= 18)
 );
 Зачем: чтобы данные были корректными и непротиворечивыми.

 Какие CONSTRAINT создают индексы автоматически?
 - PRIMARY KEY
 - UNIQUE

 -- END Constraint (ограничение) в SQL --


 -- Блокировки (Locks) vs Deadlock в SQL --

 Блокировки (Locks) - Механизм контроля доступа к данным при параллельных транзакциях.

 Типы:

 - Shared (S) – чтение (много транзакций).
 - Exclusive (X) – запись (только одна).
 - Update (U) – подготовка к изменению.
 - Intent – намерение заблокировать данные.

 Deadlock - Взаимная блокировка: две транзакции ждут ресурсы друг друга, создавая тупик.

 Пример:

 Транзакция A блокирует строку 1, B – строку 2.
 A хочет строку 2, B – строку 1.
 Обе зависают.


 Как “решают” блокировки  <-----

- Избегают долгих транзакций: меньше держишь locks → меньше конфликтов.
- Индексы + точные WHERE: чтобы блокировать меньше строк (не сканировать таблицу).
- Правильный уровень изоляции: где можно — ниже (READ COMMITTED вместо SERIALIZABLE).
- MVCC/снимки: чтения часто идут без блокировок (зависит от СУБД).
- Пессимистические/оптимистические подходы: SELECT ... FOR UPDATE (пессимистично) или проверка версии/таймстампа (оптимистично).

 Как решают deadlock    ---->  Deadlock обычно решает сама СУБД

- Детект: СУБД строит граф ожиданий, находит цикл → убивает одну транзакцию (victim) и откатывает.
- Таймаут: если слишком долго ждёт → ошибка/откат.
- Профилактика (в коде)
  - Одинаковый порядок захвата ресурсов (всегда сначала строка 1, потом 2).
  - Короткие транзакции и меньше “ручных” блокировок.
  - Retry: при deadlock-ошибке обычно делают повтор транзакции (с backoff).

 Твоя задача в приложении: поймать ошибку deadlock и сделать retry (и по возможности уменьшать шанс дедлока:
 один порядок блокировок, короткие транзакции, индексы, меньше “долгих” lock’ов).


 -- Deadlock в SQL --

 Deadlock в SQL — это ситуация, когда две или более транзакции блокируют друг друга, бесконечно ожидая освобождения ресурсов.
 Deadlock в SQL — взаимоблокировка транзакций, когда каждая ждёт ресурс, занятый другой.

 Проще:
- Транзакция A ждёт данные, которые заблокировала транзакция B.
- Транзакция B ждёт данные, которые заблокировала транзакция A.
- Результат: взаимный клин, обе транзакции «зависают».

 Как решается:
 - СУБД автоматически обнаруживает deadlock и аварийно завершает одну из транзакций (жертва).
 - Приложение должно повторить отменённую транзакцию.

 Пример:
 - Транзакция 1
 BEGIN;
 UPDATE accounts SET balance = balance - 100 WHERE id = 1; -- Блокирует запись 1
 UPDATE accounts SET balance = balance + 100 WHERE id = 2; -- Ждёт, пока транзакция 2 освободит запись 2

 - Транзакция 2 (параллельно)
 BEGIN;
 UPDATE accounts SET balance = balance - 50 WHERE id = 2; -- Блокирует запись 2
 UPDATE accounts SET balance = balance + 50 WHERE id = 1; -- Ждёт, пока транзакция 1 освободит запись 1
 -- DEADLOCK! СУБД убивает одну из транзакций.

 Профилактика Deadlock:
 - Запросы в одинаковом порядке (например, всегда id=1 → id=2).
 - Короткие транзакции.
 - Использование NOWAIT или таймаутов.

 -- END Deadlock в SQL --


 -- Уровни изоляции в SQL --

 Уровни изоляции в SQL определяют, насколько транзакции "видят" изменения друг друга,
 балансируя между согласованностью данных и производительностью

 Уровни изоляции контролируют видимость изменений между параллельными транзакциями.

 Уровни изоляции в SQL определяют, как транзакции взаимодействуют с параллельными транзакциями,
 предотвращая проблемы (грязное чтение, неповторяемое чтение, фантомы).

 4 уровня изоляции (от низкого к высокому):

 - Read Uncommitted – Видны даже незафиксированные изменения других транзакций (грязное чтение).
 - Read Committed – Только зафиксированные данные (стандарт в многих СУБД).
 - Repeatable Read – Гарантирует, что прочитанные данные не изменятся в рамках транзакции.
 - Serializable – Полная изоляция, как если бы транзакции выполнялись строго по очереди.

 Чем выше уровень, тем строже изоляция, но ниже производительность.
 чем выше уровень, тем больше накладных расходов (блокировки, проверки конфликтов), но в современных СУБД
 (например, PostgreSQL) даже Serializable может работать достаточно быстро благодаря оптимизациям.

 -- END Уровни изоляции в SQL --


 При сравнении NULL с ЛЮБЫМ другим значением, включая 0, результат всегда будет NULL, а не TRUE или FALSE.   <-----

 В SQL значение NULL представляет собой отсутствие данных или неопределенное значение.

 При сравнении NULL с ЛЮБЫМ другим значением, включая 0, результат всегда будет NULL, а не TRUE или FALSE.
 Это значит, что NULL не может быть ни больше, ни меньше, ни равно чему-либо другому.

 Таким образом, в SQL можно сказать, что:

 0 > NULL: результат будет NULL (неопределено)
 0 < NULL: результат будет NULL (неопределено)
 0 = NULL: результат будет NULL (неопределено)
 Если вам нужно проверить, является ли значение NULL, вы можете использовать оператор IS NULL.

 в SQL операции с NULL обычно возвращают NULL. Это связано с тем, что NULL представляет отсутствующее или
 неизвестное значение, и любая операция с ним также становится неопределённой.

 Примеры в SQL Операции с NULL возвращают NULL (кроме специальных функций):
 Умножение:
 SELECT NULL * 0;  -- Результат: NULL

 Сложение:
 SELECT 0 + NULL;  -- Результат: NULL

 Другие арифметические операции:
 SELECT NULL - 5;   -- NULL
 SELECT 10 / NULL;  -- NULL
 SELECT NULL % 2;   -- NULL (в некоторых СУБД)

 Логические операции:
 SELECT NULL AND TRUE;   -- NULL (не FALSE!)
 SELECT NULL OR FALSE;   -- NULL
 SELECT NOT NULL;        -- NULL

 Сравнения:
 SELECT NULL = NULL;  -- NULL (не TRUE!)
 SELECT NULL <> 5;    -- NULL

 Исключения:
 Функции, обрабатывающие NULL:
 Некоторые функции, например COALESCE, NULLIF или IS NULL, специально предназначены для работы с NULL.

 SELECT COALESCE(NULL, 0);  -- 0 (возвращает первое не-NULL значение)
 SELECT NULLIF(5, 5);       -- NULL (если значения равны)
 SELECT IS NULL(NULL);      -- TRUE (проверка на NULL)

 Оператор IS [NOT] NULL:
 Единственный безопасный способ проверить NULL:

 SELECT NULL IS NULL;   -- TRUE
 SELECT 0 IS NOT NULL;  -- TRUE

 ИНТЕРЕСНЫЕ ПРИМЕРЫ
 Запрос с условием WHERE NULL = NULL не вернёт ни одной строки потому что: NULL = NULL оценивается как NULL (не TRUE!).
 SELECT NULL = NULL AS result;  -- Вернёт NULL, а не TRUE!
 SELECT NULL IS NULL;  -- Вернёт TRUE

 Почти все операции с NULL возвращают NULL, если только не используются специальные функции или операторы для
 обработки NULL. Это часть стандарта SQL (логика трёхзначной логики: TRUE, FALSE, NULL).


 Разница между UNION и UNION ALL заключается в том, что UNION удаляет дубликаты из результата,
 а UNION ALL сохраняет все строки, включая дубликаты.


 -- При JOIN в SQL, если для INT поля не найдено соответствие, результат зависит от типа соединения --

 - Для LEFT/RIGHT/FULL JOIN INT поле будет NULL, если соответствие не найдено.
 - Для INNER JOIN такие строки не попадут в результат.


 Примеры:

  -- Данные
 CREATE TABLE A(id INT);
 INSERT INTO A VALUES (1), (2);

 CREATE TABLE B(id INT, num INT);
 INSERT INTO B VALUES (1, 100);

 -- Запрос LEFT JOIN                                  -- Запрос INNER JOIN
 SELECT A.id, B.num                                   SELECT A.id, B.num
 FROM A                                               FROM A
 LEFT JOIN B ON A.id = B.id;                          JOIN B ON A.id = B.id;

 #  LEFT JOIN (NULL при отсутствии совпадений)        # INNER JOIN (только совпадения)
 Результат:                                           Результат:
 1 | 100                                              1 | 100 (id=2 не выведен)
 2 | NULL (нет совпадения → num=NULL)
 RIGHT JOIN и FULL JOIN (работают аналогично LEFT JOIN).

 # Разница одной строкой:

 INNER JOIN = только пересечение (как строгий фильтр).
 LEFT JOIN = все из левой таблицы + NULL справа при отсутствии совпадений.

 Аналогично работают RIGHT и FULL JOIN.

 # Более Краткий пример
 -- LEFT JOIN → NULL при отсутствии совпадений
 SELECT A.id, B.num FROM A LEFT JOIN B ON A.id = B.id;
 -- Результат: 1|100, 2|NULL

 -- INNER JOIN → только совпадения
 SELECT A.id, B.num FROM A JOIN B ON A.id = B.id;
 -- Результат: 1|100

 -- END При JOIN в SQL, если для INT поля не найдено соответствие, результат зависит от типа соединения --



 -- Self JOIN в PostgreSQL --

 -- Пример: найти сотрудников с одинаковой зарплатой
 SELECT
     a.id,
     a.name,
     a.salary,
     b.id as colleague_id,
     b.name as colleague_name
 FROM employees a
 JOIN employees b ON a.salary = b.salary AND a.id != b.id;


 Ключевые моменты:

 - Таблица соединяется сама с собой под разными алиасами (a, b).
 - Условие a.id != b.id исключает совпадение строки с самой собой.
 - Можно использовать LEFT JOIN, если нужно сохранить все строки из первой таблицы.

 Где использовать: иерархии (например, сотрудник → менеджер), поиск дубликатов, связи внутри одной таблицы.

 -- END Self JOIN в PostgreSQL --



 -- DEFAULT в SQL - значение по умолчанию для столбца --

 DEFAULT в SQL — значение по умолчанию для столбца, если данные не указаны явно.

 # Пример:                                                                # Минимальный пример:
 CREATE TABLE products (                                                  CREATE TABLE t (
    id INT PRIMARY KEY,                                                       id INT,
    price DECIMAL DEFAULT 0.00,  -- Цена по умолчанию 0                       flag BOOLEAN DEFAULT FALSE
    in_stock BOOLEAN DEFAULT TRUE  -- Наличие по умолчанию TRUE           );
);

 -- END DEFAULT в SQL - значение по умолчанию для столбца --



 Таблицы — это основа хранения данных в SQL, а представления — инструмент для извлечения конкретной информации.

 В SQL представление(VIEW) — это виртуальная таблица, основанная на SELECT операторе.   CREATE VIEW НАЗВАНИЕ AS SELECT ...
 Примеры:
 CREATE VIEW top_rated_movies           CREATE VIEW NamesView
 AS                                     AS
 SELECT id,                             SELECT pl.vcName, count(*)
       title,                           FROM tbPeoples pl
       release_year,                    GROUP BY vcName
       genre,
       rating
 FROM movies
 WHERE rating = 9;


 Виртуальная таблица - Таблица созданная на основе источника посредством выполнения запроса.
 Итак, вьюшка – это просто запрос на языке SQL, который выбирает данные, а в базе данных она выглядит как таблица
 и работа с ней происходит также.

 Чем View отличается от таблицы?
 View — виртуальная (логическая) таблица, представляющая собой поименованный запрос (синоним к запросу), который будет
 подставлен как подзапрос при использовании представления. В отличие от обычных таблиц реляционных баз данных,
 представление не является самостоятельной частью набора данных, хранящегося в базе.

 Таблица содержит данные, Представление(View) — это просто оператор SELECT, который был сохранен в базе данных

 Таблица — это сущность базы данных, которая хранит данные в виде строк и столбцов.
 Представление — это виртуальная таблица, используемая для просмотра или манипулирования некоторыми частями таблицы .

 Представление(View) является результатом SQL-запроса и представляет собой виртуальную таблицу, тогда как таблица состоит из
 строк и столбцов, которые хранят информацию о любом объекте и используются для извлечения этих данных по мере необходимости.


 -- Что такое ИНДЕКСЫ в SQL и как их использовать --

 По сути, индекс — это сбалансированное двоичное дерево поиска. Каждая строка в таблице соответствует узлу в дереве.

 Производительность: Проблема в том, что, когда новая запись вставляется в таблицу или удаляется из нее, приходится
 обновлять все индексы, чтобы отразить это изменение. Если индексов много, то обновление, вставка или удаление строк
 могут стать в вычислительном плане дорогостоящими операциями (ВСПОМНИМ ПРО БАЛАНСИРОВКУ ДЕРЕВА).

 Более того, индексы занимают ограниченное дисковое пространство.

 - Ответ 1:

 Если в кратце, то индекс, это поле по которому оптимизирован(ускорен) поиск.                            <-----

 Поскольку индекс занимает место, то индексировать нужно только те поля, по которым происходит выборка.  <-----

 Допустим есть таблица.

 CREATE TABLE MyGuests (
     id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,
     firstname VARCHAR(30) NOT NULL,
     lastname VARCHAR(30) NOT NULL,
     email VARCHAR(50),
     reg_date TIMESTAMP
 )

 id - уже индекс

 Допустим вам нужен поиск по имени (firstname).

 SELECT * FROM MyGuests WHERE firstname = "Вася"

 тогда есть смысл добавить индекс по данному полю.

 CREATE INDEX firstname_index ON MyGuests (firstname) USING BTREE;    # Создание Индекса

 Будет созданна "карта" которая позволет легко находить записи в оригинальном списке.


 - Ответ 2:

 Вкратце, индексы создаются для повышения производительности поиска данных. Таблицы могут иметь огромное количество строк,
 которые хранятся в произвольном порядке. Без индекса поиск нужных строк идёт по порядку (последовательно), что на
 больших объемах данных отнимает много времени.

 Индекс - обычно один или несколько столбцов таблицы и указателей на соответствующие строки таблицы, позволяет искать
 строки, удовлетворяющие критерию поиска. Ускорение работы с использованием индексов достигается в первую очередь за
 счёт того, что индекс имеет структуру, оптимизированную под поиск — например, в MySQL b-дерева. Индекс лучше
 использовать на тех столбцах таблицы, на которые вы чаще всего будете накладывать условия через where column_name = ...

 Индекс создаётся по правилу:

 create index название_индекса
 on название_таблицы (название_столбца)

 Например, у вас таблица называется test, где хранятся данные по городам России с улицами вида Город, Улица, Дом.Понятно,
 что строк в таблице при таком раскладе будет много. Если вы часто делаете выборку по определенному городу, например:

 select *
 from test
 where city = 'Омск'

 то, чтобы этот запрос отработал быстрее обычного, следует добавить индекс по вышеуказанному правилу:

 create index city_index
 on test (city)

 Тогда тот же самый запрос

 select *
 from test
 where city = 'Омск'

 отработает гораздо быстрее, если столбец city будет проиндексирован.


 Внешний ключ - это столбец для соединения с другими таблицами.
 Индекс - столбец, по которому можно задать условие и тогда запрос отработает гораздо быстрее.


 -- Ответ 3:
 На пальцах можно объяснить так:

 Когда Вы создаёте таблицу, добавляете в неё данные, то таблица разрастается и она выглядит как просто последовательный
 список, упорядоченный по тому как в неё данные добавлялись.

 Когда данных мало, список маленький и все запросы к ней выполняются, почти, незаметно. Но когда количество записей в
 таблице начинает переваливать за миллион (в разных случаях по разному, но как пример миллион), то у Вас поиск уже идёт
 не так быстро и с добавлением всё новых и новых записей - ещё медленнее.

 Это связано с тем, что когда Вы ищите какую-то запись, то просматриваются все записи, пока не дойдут до нужной.

 Когда Вам это окончательно надоедает и Вы хотите что-нибудь сделать, то к Вам на помощь приходят индексы.

 Индекс создаётся по какому-то определённому полю (можно по нескольким) по которому, обычно, выполняется поиск.
 Когда Вы создаёте индекс, то MySql (и любая другая БД) обходит все записи в таблице и строит дерево
 (скорее всего B-дерево или разновидность), в котором ключами выступает выбранное поле, а содержимым ссылки на записи в таблице.

 И когда Вы делаете очередной свой select запрос по таблице, по полю для которого создали индекс MySql
 (и любая другая БД) знает что у неё есть индекс, по которому пройтись будет быстрее, нежели перебирать все записи и
 Ваш запрос будет направлен этому индексу и записи, удовлетворяющие условию, будут найдены гораздо быстрее, так как
 поиск по построенному дереву будет гораздо быстрее, нежели простой перебор всех записей.


 B-Tree индекс дает скорость выборки порядка O(log n): Логарифмическая сложность
 hash в среднем обеспечивают скорость выборки O(1):  Константная сложность.   если происходит множество коллизий  линейную O(n).

 Таким образом:
- B-Tree обеспечивает логарифмическую сложность O(log n) при поиске.
- Хэш-индексы в среднем обеспечивают постоянную сложность O(1) для поиска, но могут достигать до O(n) в худшем случае при наличии коллизий.

 Важно также отметить, что хэш-индексы НЕ поддерживают диапазонный поиск, в отличие от B-Tree.

 В реальной жизни hash и B-Tree применяются совместно, то есть для вычисления значений B-Tree индекса все равно применяются хэши.

 ### Сравнение   B-Tree vs Hash:

 - **Производительность**:
  B-Tree лучше для операций, требующих диапазонного поиска, тогда как хэш-индексы оптимальны для точных совпадений.

 - **Структурные особенности**:
  B-Tree — это сбалансированная структура, которая остается упорядоченной, в то время как хэш-индексы НЕ упорядочены.

 - **Гибкость**:
  B-Tree более универсален и подходит для разнообразных типов запросов, в то время как хэш-индексы более узкоспециализированы.

 В зависимости от конкретных задач и типов запросов, СУБД может использовать один или оба типа индексов.


 -- Вывод: Когда что использовать? --

 | Критерий	           | Без индекса	    | С индексом            |
 |---------------------|--------------------|-----------|-----------|
 | Скорость	           | Медленно (O(n))	| Быстро (O(log n))     |
 | Подходит для	       | Маленькие таблицы	| Большие таблицы       |
 | Поддержка операций  | Любые условия	    | Только индексируемые  |
 | Нагрузка на БД	   | Высокая	        | Низкая                |

 Рекомендация:

 - Всегда добавляйте индексы на часто используемые столбцы (WHERE, JOIN, ORDER BY).
 - Не индексируйте редко используемые столбцы или маленькие таблицы — это тратит ресурсы.
 - Индексы занимают место и замедляют вставку/обновление.
 - Проверяйте ПЛАН ЗАПРОСА (EXPLAIN ANALYZE), чтобы убедиться, что индекс используется.

  EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';
  # Вывод должен содержать Index Scan (если индекс есть) или Seq Scan (если нет).


 -- ПОЧЕМУ ИНДЕКСЫ замедляют операции вставки (INSERT) и обновления (UPDATE) --

 Индексы замедляют INSERT и UPDATE в PostgreSQL, потому что:

 - INSERT - при добавлении новой записи нужно обновлять все индексы таблицы.
 - UPDATE - если изменяются индексируемые поля, PostgreSQL должен пересчитать соответствующие индексы.

 Чем больше индексов, тем больше накладных расходов на их поддержку.

 - Для INSERT - каждая новая запись требует добавления соответствующих записей во все индексы таблицы.
 - Для UPDATE - если меняется значение в индексируемом поле, PostgreSQL должен удалить старую запись из индекса и добавить новую.

 Чем больше индексов -> тем больше операций записи -> тем выше нагрузка.



 -- Индексы в PostgreSQL --

 1. По умолчанию - B-tree
 CREATE INDEX idx_name ON table(column);  -- B-tree (автоматически)

 Применение B-tree:
 - Сравнения: =, >, <, BETWEEN, IN
 - Сортировка: ORDER BY
 - Поиск по префиксу: LIKE 'prefix%' (но не LIKE '%abc') (Уточнение: LIKE '%suffix' НЕ использует B-tree индекс)

 Сложность:
 - O(log n) для всех операций (поиск/вставка/удаление)

 2. Специальные типы индексов (требуют USING):

 | Тип    | Оптимальное применение        | Сложность | Ключевые ограничения            |
 |--------|-------------------------------|-----------|---------------------------------|
 | Hash   | Точные совпадения (=)         | O(1)      | Не поддерживает сортировку, >/< |
 | GIN    | JSONB, массивы, полнотекст    | O(log n)  | Больше места, чем B-tree        |
 | GiST   | Геоданные, сложные типы       | Зависит   | Медленнее B-tree для простых запросов |
 | BRIN   | Упорядоченные большие таблицы | O(1)      | Менее точен, чем B-tree         |

 Примеры создания:
 CREATE INDEX idx_hash ON table USING hash(column);  -- Для быстрого =
 CREATE INDEX idx_gin ON table USING gin(jsonb_column);  -- Для JSONB

 Итоговые рекомендации:
 1. B-tree - стандартный выбор (покрывает 90% случаев)
 2. GIN - для JSONB/массивов (индексирует элементы внутри)
 3. BRIN - для временных рядов (где данные упорядочены по времени)
 4. Hash - только для узких случаев (поиск по =)
 5. GiST - для геоданных и специализированных расширений


 -- Обычный индекс (INDEX) VS  Уникальный индекс (UNIQUE INDEX) --

 Индексы в SQL — это специальные структуры данных для ускорения поиска и сортировки в таблицах.

 1. Обычный индекс (INDEX)
 - Ускоряет поиск по столбцу.
 - Допускает дубликаты и NULL.

 Пример:
 CREATE INDEX idx_name ON users(name);  -- Ускоряет поиск по имени

 2. Уникальный индекс (UNIQUE INDEX)
 Запрещает дубликаты значений (как PRIMARY KEY, но может содержать NULL).


 Пример:

 CREATE UNIQUE INDEX idx_email ON users(email);  -- email должен быть уникальным

 Разница между   INDEX и UNIQUE INDEX


 | Характеристика	  | INDEX          | UNIQUE INDEX
 |--------------------|----------------|--------------------------------------|
 | Дубликаты          | Разрешены      | Запрещены                            |
 | NULL-значения      | Разрешены      | Один NULL обычно разрешён*           |
 | Скорость поиска    | Ускоряет       | Ускоряет + контролирует уникальность |


 *Зависит от СУБД: в PostgreSQL несколько NULL разрешены, в MySQL — один, если столбец не NOT NULL.

 Когда использовать?
 - INDEX — для часто фильтруемых столбцов (поиск по имени).
 - UNIQUE INDEX — для столбцов, где важна уникальность (email, телефон).


 Примеры запросов:

 - Создание индекса
 CREATE INDEX idx_age ON users(age);

 - Создание уникального индекса
 CREATE UNIQUE INDEX idx_phone ON users(phone);

 - Удаление индекса
 DROP INDEX idx_name ON users;

 Важно:
 - Индексы ускоряют SELECT, но замедляют INSERT/UPDATE/DELETE (т.к. нужно обновлять индекс).
 - Составные индексы (по нескольким столбцам) работают только для условий, где используются первые столбцы индекса.

 Коротко:
 - INDEX        — просто ускоритель.
 - UNIQUE INDEX — ускоритель + защита от дублей.

 Супер-кратко:
 - INDEX        — ускоряет поиск, разрешает дубли и NULL.
 - UNIQUE INDEX — ускоряет + запрещает дубли (но обычно разрешает несколько NULL).

 -- END Обычный индекс (INDEX) VS  Уникальный индекс (UNIQUE INDEX) --



 -- Можно ли много индексов? МНОЖЕСТВЕННЫЕ ИНДЕКСЫ в SQL --

 Пример создания нескольких индексов:

 CREATE INDEX idx_customer_name ON customers(name);
 CREATE INDEX idx_order_date ON orders(order_date);
 CREATE INDEX idx_product_category ON products(category_id, price);


 Да, можно создавать много индексов в SQL, но с оговорками:

 Плюсы множественных индексов:

 - Ускоряют поиск и сортировку по разным полям
 - Оптимизируют JOIN-операции
 - Улучшают производительность сложных запросов

 Минусы:

 - Замедляют INSERT/UPDATE/DELETE (каждое изменение данных требует обновления индексов)
 - Занимают дополнительное место на диске
 - Могут избыточно дублировать друг друга

 Рекомендации:

 - Создавайте индексы только для часто используемых в WHERE/JOIN/ORDER BY полей
 - Комбинируйте поля в составных индексах вместо создания отдельных
 - Избегайте индексов для редко используемых запросов
 - Удаляйте неиспользуемые индексы



 -- Кластерные индексы vs Обычные индексы --

 - Кластерные индексы лучше подходят для операций с диапазонами
 - НЕкластерные индексы — для выборок по конкретным значениям.


| Аспект                 |Кластерные индексы                            | Некластерные индексы
|------------------------|----------------------------------------------|------------------------------------------------
| **Структура**          | Определяет физический порядок хранения строк.| Создает отдельную структуру с указателями на строки.
| **Количество**         | Только один на таблицу.                      | Много на таблицу.
| **Производительность** | Быстрый для диапазонных запросов.            | Быстрый для точных запросов.
| **Обновления**         | Более затратные при изменениях данных.       | Менее затратные операции при изменениях.

 Кластерные индексы играют важную роль в оптимизации производительности базы данных за счет упорядочивания данных.


 В PostgreSQL “кластерный” = это CLUSTER (физически переупорядочить таблицу по индексу).
 По умолчанию все индексы обычные (b-tree), “clustered index” как в SQL Server НЕТ.

 Если хочешь “кластерность” в Postgres (РЕДКО нужно)
 CLUSTER orders USING idx_orders_created_at;

 -- END Что такое ИНДЕКСЫ в SQL и как их использовать --



 -- Что такое Транзакции в SQL Атомарное действие --

 Транзакция, если по-простому - это совокупность неких действий, причем такая, что либо все эти действия
 выполняются успешно, либо ни одно не выполняется вообще.

 Соответственно, транзакция в sql - это последовательность операторов, которая либо выполняется целиком, либо целиком
 же откатывается. (почитайте про ключевые слова TRANSACTION, COMMIT, ROLLBACK)

 TCL (Transaction Control Language)
 BEGIN TRANSACTION    — начало транзакции.
 COMMIT TRANSACTION   — изменение команд транзакции.
 ROLLBACK TRANSACTION — отказ в транзакции.
 SAVE TRANSACTION     — формирование промежуточной точки сохранения внутри

 Транзакции в SQL — это набор операций, которые выполняются как единое целое. Они обеспечивают атомарность,
 консистентность, изолированность и долговечность (ACID-принципы). Это значит, что либо все операции в транзакции
 выполняются успешно, либо, в случае ошибки, все изменения отменяются.

 ### Пример транзакции:

 Предположим, у нас есть две таблицы: `Accounts` (для учета балансов пользователей) и `Transactions` (для учета сделанных транзакций).

 #### 1. Создание таблиц

 CREATE TABLE Accounts (
     AccountID INT PRIMARY KEY,
     Balance DECIMAL(10, 2)
 );

 CREATE TABLE Transactions (
     TransactionID INT PRIMARY KEY AUTO_INCREMENT,
     FromAccount INT,
     ToAccount INT,
     Amount DECIMAL(10, 2),
     TransactionDate DATETIME DEFAULT CURRENT_TIMESTAMP
 );


 #### 2. Пример транзакции

 Предположим, мы хотим перевести 100 единиц денег от одного аккаунта к другому:

 BEGIN;

 UPDATE Accounts
 SET Balance = Balance - 100
 WHERE AccountID = 1;

 UPDATE Accounts
 SET Balance = Balance + 100
 WHERE AccountID = 2;

 INSERT INTO Transactions (FromAccount, ToAccount, Amount)
 VALUES (1, 2, 100);

 COMMIT;


 ### Объяснение:

 1. **BEGIN**  - Начинает транзакцию.
 2. **UPDATE** - Первое обновление уменьшает баланс первого аккаунта.
 3. **UPDATE** - Второе обновление увеличивает баланс второго аккаунта.
 4. **INSERT** - Записывает информацию о транзакции в таблицу `Transactions`.
 5. **COMMIT** - Подтверждает все изменения, сделанные в рамках транзакции.

 Если на каком-то этапе возникает ошибка (например, недостаточно средств на первом аккаунте), можно выполнить `ROLLBACK`,
 чтобы отменить все изменения, сделанные в транзакции.

 -- END Что такое Транзакции в SQL Атомарное действие --


 -- Грязные операции чтения (dirty reads) в транзакциях --

 Грязные операции чтения (dirty reads) в транзакциях - это чтение незафиксированных изменений из параллельной транзакции,
 которая может быть откачена.

 Кратко:
 - Чтение незавершённых данных.
 - Возникает при низком уровне изоляции (например, READ UNCOMMITTED).
 - Может привести к некорректным результатам.

 Пример:

 -- Транзакция 1 (не завершена)
 UPDATE users SET balance = 100 WHERE id = 1;

 -- Транзакция 2 (читает незафиксированные данные)
 SELECT balance FROM users WHERE id = 1; -- Вернёт 100, даже если Транзакция 1 откатится.

 Итог: Опасны, так как используют потенциально невалидные данные.



 -- ACID-принципы --
 ACID — это набор свойств, которые гарантируют надежность транзакций в системах управления базами данных (СУБД).

 ACID расшифровывается как:

 1. **Atomicity (Атомарность)**: Транзакция выполняется полностью или не выполняется вообще. Частичные изменения не допустимы.

 2. **Consistency (Согласованность)**: Транзакция переводит базу данных из одного согласованного состояния в другое.
    Это означает, что все бизнес-правила и ограничения базы данных должны соблюдаться. Ограничения (например, UNIQUE, FOREIGN KEY)

 3. **Isolation (Изолированность)**: Одновременно выполняющиеся транзакции не влияют друг на друга.
    Результаты транзакции не доступны другим до её завершения. Уровни изоляции определяют, насколько строго транзакции изолированы.

 4. **Durability (Надежность/Долговечность)**: После завершения транзакции её изменения сохраняются в базе данных, даже в случае сбоя системы.

 Эти свойства помогают обеспечить надежность и целостность данных в реляционных базах данных.


 # КОРОТКО:
 ACID — свойства надежных транзакций в БД:

 Атомарность — либо всё, либо ничего.
 Пример: Перевод денег: если списание прошло, а зачисление нет — операция отменится.

 Согласованность — данные всегда корректны.
 Пример: Невозможно записать отрицательный баланс, если есть ограничение balance >= 0.

 Изолированность — транзакции не мешают друг другу.
 Пример: Пока один пользователь не завершит редактирование записи, другой не увидит её изменения.

 Долговечность — изменения сохраняются даже при сбое.
 Пример: После подтверждения платежа он останется в БД, даже если сервер упадёт.

 Короче: «Либо всё, либо ничего; только по правилам; без помех; навсегда».    <-----    <-----


 # Хорошее определение
 Atomicity (атомарность)       - все операции в транзакции выполняются или ни одна.
 Consistency (согласованность) - данные остаются валидными после транзакции.
 Isolation (изолированность)   - параллельные транзакции не мешают друг другу.
 Durability (долговечность)    - после commit изменения сохраняются даже при сбое.


 Какие уровни изоляции транзакций вы знаете?

 Read Uncommitted - возможны "грязные" чтения.
 Read Committed   - только committed данные (стандарт в PostgreSQL).
 Repeatable Read  - защита от "неповторяемого чтения".
 Serializable     - полная изоляция (как последовательное выполнение).


 -- CAP теорема --
 CAP теорема (или теорема CAP) — это концепция, которая описывает баланс между тремя важными свойствами распределённых
  вычислений в системах хранения данных:

 1. **Consistency (Согласованность)**: Все узлы системы видят одни и те же данные в одно и то же время. Это значит,
  что после успешного завершения операции все пользователи будут видеть обновленные данные.

 2. **Availability (Доступность)**: Каждый запрос к системе получает ответ, даже если некоторые узлы недоступны.
  Система всегда отвечает, но не обязательно корректно.

 3. **Partition Tolerance (Устойчивость к разделению)**: Система продолжает функционировать даже в случае сетевых
  разделений, когда узлы не могут обмениваться данными.

 Согласно теореме CAP, в любой распределенной системе можно одновременно гарантировать только два из трёх свойств:

 - Если вы стремитесь к **согласованности** и **доступности**, у вас может возникнуть проблема при сетевом разделении (Partition).
 - Если вы стремитесь к **доступности** и **устойчивости к разделению**, вы можете потерять **согласованность**.
 - Если вы пытаетесь обеспечить **согласованность** и **устойчивость к разделению**, вы можете столкнуться с проблемами
   в **доступности**.

 Таким образом, теорема CAP помогает разработчикам и архитекторам систем понимать компромиссы, которые необходимо
 учитывать при проектировании распределенных систем.


 -- PACELC — расширение CAP --

 Идея: показывает компромисс в двух ситуациях.

 P (Partition) — при разделении сети система предпочитает A (Availability) или C (Consistency).
 ELSE — без разделения сети система всё равно выбирает: L (Latency, задержка) или C (Consistency).


 -- В ClickHouse НЕТ ACID И ТРАНЗАКЦИЙ ??? --

 В ClickHouse действительно нет полноценной поддержки ACID (Atomicity, Consistency, Isolation, Durability)
 и классических транзакций, как в реляционных СУБД (PostgreSQL, MySQL и т. д.).

 В ClickHouse нет полноценной поддержки ACID и транзакций в классическом понимании (как в PostgreSQL или Oracle).

 Коротко:

 Нет транзакций — нельзя откатить группу запросов.
 Нет атомарности на уровне запросов, изменяющих данные (кроме вставок в одну партицию).
 Изоляция ограничена (данные видны сразу после вставки).


 Когда ClickHouse подходит, а когда нет?
 Подходит:

 Аналитика, логгирование, big data (быстрое чтение, агрегация).
 Данные, которые можно пересчитать или где возможна консистентность в конечном счёте.

 Не подходит:

 Традиционные OLTP (банковские транзакции, инвентаризация).
 Системы, требующие строгой согласованности и изоляции.

 Альтернативы для ACID в ClickHouse

 Использовать PostgreSQL + ClickHouse (PostgreSQL для транзакций, ClickHouse для аналитики).
 ClickHouse + Kafka (буферизация и обработка событий).

 ClickHouse не ACID-совместим в классическом смысле, но предоставляет некоторые гарантии атомарности и durability
 в рамках своей архитектуры. Если нужны транзакции — лучше выбрать другую СУБД или комбинировать решения.


 Пример для альтернатив:
 -- В PostgreSQL (OLTP):
 BEGIN;
   UPDATE accounts SET balance = balance - 100 WHERE user_id = 1;
   INSERT INTO transactions VALUES (1, -100, now());
 COMMIT;

 -- В ClickHouse (аналитика):
 INSERT INTO analytics.events (event_date, user_id, action)
 VALUES ('2023-10-01', 42, 'click'); -- Атомарно, но без отката

 -- END В ClickHouse НЕТ ACID И ТРАНЗАКЦИЙ ??? --



 -- Many-to-Many НЕТ в PostgreSQL ??? --

 Реализуется через ПРОМЕЖУТОЧНУЮ ТАБЛИЦУ (junction/associative table), которая содержит внешние ключи на обе связанные таблицы.

 M2M в PostgreSQL ЕСТЬ, но требует явного создания связующей таблицы.
 PostgreSQL (как и другие SQL-СУБД) поддерживает M2M через промежуточные таблицы.
 Возможно, путаница с NoSQL БД (например, MongoDB), где M2M реализуется иначе (вложенными массивами).


 Many-to-Many (M2M) в PostgreSQL
 Как работает:

 - Связь между двумя таблицами через промежуточную таблицу (junction table).
 - Например: Студенты ↔ Курсы (студент может быть на многих курсах, курс может иметь многих студентов).
 - Промежуточная таблица содержит внешние ключи (FK) на обе таблицы.

 M2M существует, но реализуется через промежуточную таблицу (junction/associative table).
 Эта таблица содержит внешние ключи (FK) на обе связанные таблицы.
 Это стандартный подход в реляционных БД (PostgreSQL, MySQL, SQL Server и др.).


 # Пример: Студенты и Курсы

 -- Основные таблицы
 CREATE TABLE students (
     student_id SERIAL PRIMARY KEY,
     name TEXT
 );

 CREATE TABLE courses (
     course_id SERIAL PRIMARY KEY,
     title TEXT
 );

 -- Промежуточная таблица для M2M
 CREATE TABLE student_courses (
     student_id INT REFERENCES students(student_id),
     course_id INT REFERENCES courses(course_id),
     PRIMARY KEY (student_id, course_id)  -- Составной первичный ключ
 );

 -- END Many-to-Many нет в PostgreSQL ??? --


 Оконные функции SQL:
 Оконная функция вычисляет значение по набору данных, связанных с текущей строкой, то есть данные из одной группы,
 если используется Partition by.


 `PARTITION BY` в SQL — это предложение, используемое в оконных функциях для разделения набора строк на более мелкие
 подгруппы (партиции) перед применением функции.

 Оконные функции в SQL — это функции, которые позволяют выполнять вычисления по наборам строк, связанным с текущей
 строкой, не сводя их в одну строку, как это делает, например, агрегатная функция. Они "окружают" строки и позволяют
 выполнять аналитику, сохраняя при этом все строки в результирующем наборе. Оконные функции особенно полезны для
 вычисления скользящих средних, рангов, сумм и т.д.


 Пример:
 SELECT
     id,
     amount,
     sale_date,
     SUM(amount) OVER (ORDER BY sale_date) AS cumulative_sum
 FROM
     sales;

 ### Объяснение:

 - `SUM(amount)`: Это оконная функция, которая вычисляет сумму по столбцу `amount`.
 - `OVER (ORDER BY sale_date)`: Это определяет "окно" для функции, указывая, что суммы должны быть рассчитаны в порядке дат.



 -- RANK, DENSE_RANK и ROW_NUMBER  оконные функции --

 RANK, DENSE_RANK и ROW_NUMBER — это оконные функции в SQL для нумерации строк в результате запроса
 Все они работают с OVER (ORDER BY ...) и могут включать PARTITION BY для группировки.
 Эти функции могут быть медленными на больших таблицах, особенно с PARTITION BY.

 Пример RANK, DENSE_RANK и ROW_NUMBER:

 SELECT
    name,
    RANK() OVER (ORDER BY score DESC) AS rank,
    DENSE_RANK() OVER (ORDER BY score DESC) AS dense_rank,
    ROW_NUMBER() OVER (ORDER BY score DESC) AS row_num
 FROM students;


 # Сравнение в таблице для наглядности

 | Функция    | Поведение при равенстве | Нумерация (пример: 100, 90, 90, 80) |
 |------------|-------------------------|-------------------------------------|
 | RANK       | Пропускает номер        | 1, 2, 2, 4                          |
 | DENSE_RANK | Не пропускает           | 1, 2, 2, 3                          |
 | ROW_NUMBER | Всегда уникальная       | 1, 2, 3, 4                          |



 RANK в SQL — это оконная функция, которая назначает ранг каждой строке в результате запроса с возможностью пропусков.

 Как работает:
 Присваивает номер позиции (1, 2, 3...) в зависимости от сортировки.

 Если значения одинаковые — даёт им одинаковый ранг, пропуская следующие номера.

 Пример 1:                                          Пример с PARTITION BY
                                                    -- Ранжирование по отделам
 SELECT                                             SELECT
     name,                                              department,
     score,                                             name,
     RANK() OVER (ORDER BY score DESC) AS rank          salary,
 FROM students;                                         RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS dept_rank
                                                    FROM employees;
 Результат Пример 1:

 | name  | score | rank |
 |-------|-------|------|
 | Иван  | 100   | 1    |
 | Мария | 90    | 2    |
 | Алекс | 90    | 2    |  -- Пропуск 3
 | Петр  | 80    | 4    |

 Отличия от других функций:

 DENSE_RANK — без пропусков (в примере выше Алекс получил бы 3).
 ROW_NUMBER — всегда уникальные номера (даже при одинаковых значениях).

 Примечание:
 - "NULL-значения считаются равными друг другу"

 Где использовать: рейтинги, топ-N запросы, аналитика.

 -- END RANK, DENSE_RANK и ROW_NUMBER  оконные функции --



 DDL (Data Definition Language)
 CREATE  — создание нового объекта в существующей базе.
 ALTER   — изменение существующего объекта.
 DROP    — удаление объекта из базы.

 DML (Data Manipulation Language)
 SELECT — позволяет выбрать данные в соответствии с необходимым условием.
 INSERT — осуществляют добавление новых данных.
 UPDATE — производит замену существующих данных.
 DELETE — удаление информации.

 DCL (Data Control Language)
 GRANT  — предоставляет доступ к объекту.
 REVOKE — аннулирует выданное ранее разрешение на доступ.
 DENY   — запрет, который прекращает действие разрешения.

 TCL (Transaction Control Language)
 BEGIN TRANSACTION    — начало транзакции.
 COMMIT TRANSACTION   — изменение команд транзакции.
 ROLLBACK TRANSACTION — отказ в транзакции.
 SAVE TRANSACTION     — формирование промежуточной точки сохранения внутри


 # Как начать работу с SQL
 SELECT    — выбор данных.
 FROM      — источник информации, откуда брать данные.
 JOIN      — добавление таблиц.
 WHERE     — при каком условии.
 GROUP BY  — сформируй группу данных по заданному признаку.
 ORDER BY  — сортировка данных по нужному признаку.
 LIMIT     — количество результатов.
 ;         — конец предложения


 WITH позволяет дать блоку подзапроса имя/псевдоним, на которое можно ссылаться в нескольких местах основного SQL-запроса.
 Имя, присвоенное подзапросу, обрабатывается так, как если бы оно было встроенным представлением или таблицей.
 SQL оператор WITH по сути является заменой обычному подзапросу.

 Cинтаксис Oracle PL/SQL WITH с одним подзапросом:
 WITH query_name AS (SELECT expressions FROM table_A)

 SELECT column_list

   FROM query_name [,table_name]

 [WHERE conditions]

 # Выбор уникальных элементов столбца  DISTINCT                                                                 <-----
 На самом деле это тяжёлая операция - группировка и сортировку в себя включает, на маленьких объёмах это незаметно,
 а вот на больших его лучше не использовать


 # *Агрегирующие функции (иногда их ещё называют агрегатными) обрабатывают набор строк для подсчета и возвращают одно
 обобщенное значение: SUM, MAX, MIN, COUNT, AVG

 COUNT(*) —  подсчитывает  все записи, относящиеся к группе, в том числе и со значением NULL;
 COUNT(имя_столбца) — возвращает количество записей конкретного столбца (только NOT NULL), относящихся к группе.


 --- JOINS ---
 В pandas это - pandas.DataFrame.join   pandas.DataFrame.merge

 <join_type> ::=
    [ { INNER | { { LEFT | RIGHT | FULL } [ OUTER ] } } [ <join_hint> ] ]       # [] - НЕобязательное
    JOIN

 Ключевое слово OUTER отмечено как необязательное (заключено в квадратные скобки). В этом конкретном случае OUTER
 не имеет значения, указываете вы его или нет. Обратите внимание, что хотя другие элементы предложения join также
 отмечены как необязательные, их исключение будет иметь значение.

 Например, вся часть type в JOIN предложении является необязательной, в этом случае по умолчанию будет, INNER
 если вы просто укажете JOIN. Другими словами, это допустимо:

 SELECT *
 FROM A JOIN B ON A.X = B.Y

 Вот список эквивалентных синтаксисов:
 A LEFT JOIN B            A LEFT OUTER JOIN B
 A RIGHT JOIN B           A RIGHT OUTER JOIN B
 A FULL JOIN B            A FULL OUTER JOIN B
 A INNER JOIN B           A JOIN B


 На верхнем уровне в основном существуют 3 типа соединений:

 1.INNER JOIN извлекает данные, если они присутствуют в обеих таблицах.

 2.OUTER JOINs бывают 3 типов:
    1.LEFT OUTER JOIN  - извлекает данные, если они присутствуют в левой таблице.
    2.RIGHT OUTER JOIN - извлекает данные, если они присутствуют в нужной таблице.
    3.FULL OUTER JOIN  - извлекает данные, если они присутствуют в любой из двух таблиц.
 3. CROSS JOIN, как следует из названия, делает n раз m пар, которые соединяют все со всем. Это похоже на то, как мы
 просто перечисляем таблицы для соединения (в предложении FROM оператора SELECT), используя запятые для их разделения.

 Следует отметить следующее:
 - Если вы просто упомянули JOIN, то по умолчанию это INNER JOIN.
 - Соединение OUTER должно быть LEFT| RIGHT| FULL; вы не можете просто сказать OUTER JOIN.
 - Вы можете опустить OUTER ключевое слово и просто сказать LEFT JOIN или RIGHT JOIN или FULL JOIN.  <-----



  --- Pandas vs SQL ---

 # ! Абстрактный код
 # новая переменная
 sorted_df = df.sort_values("col1")
 # перезапись исходного `DataFrame`
 df = df.sort_values("col1")

 -- Операция SQL SELECT --

 tips = pd.read_csv(url)

 SELECT total_bill, tip, smoker, time FROM tips;       tips[["total_bill", "tip", "smoker", "time"]]

 # Вызов DataFrame без списка имен столбцов отобразит все столбцы (аналогично * в SQL).
 SELECT * FROM tips;                                   df

 # В SQL можно добавить вычисляемый столбец:
 SELECT *, tip/total_bill as tip_rate FROM tips;       tips.assign(tip_rate=tips["tip"] / tips["total_bill"])


 -- Операция SQL WHERE --

 SELECT * FROM tips WHERE time = 'Dinner';                  tips[tips['time'] == 'Dinner']
 SELECT * FROM tips WHERE time = 'Dinner' AND tip > 5.00;   tips[(tips['time'] == 'Dinner') & (tips['tip'] > 5.00)]
 SELECT * FROM tips WHERE size >= 5 OR total_bill > 45;     tips[(tips['size'] >= 5) | (tips['total_bill'] > 45)]

 SELECT * FROM tips WHERE 'day'  IN ('Sun', 'Sat') and sex='Female'
 # Pandas
 tips[(tips['day'].isin(['Sun', 'Sat'])) & (tips['sex'] == 'Female')]


 frame = pd.DataFrame(
    {"col1": ["A", "B", np.nan, "C", "D"], "col2": ["F", np.nan, "G", "H", "I"]}
 )

 # Проверка NULL выполняется с помощью методов .notna() и .isna().
 SELECT * FROM frame WHERE col2 IS NULL;                    frame[frame['col2'].isna()]
 SELECT * FROM frame WHERE col1 IS NOT NULL;                frame[frame['col2'].notna()]


 -- Операция SQL GROUP BY --

 SELECT sex, count(*) FROM tips GROUP BY sex;               tips.groupby("sex").size()
                                                            tips.groupby("sex")["total_bill"].count() # Альтернатива

 Обратите внимание, что в коде с pandas используется .size(), а не .count(). Это связано с тем, что метод .count()
 применяет функцию к КАЖДОМУ столбцу, возвращая количество записей NOT NULL в КАЖДОМ столбце.

 tips.groupby("sex").count()

 SELECT day, AVG(tip), COUNT(*) FROM tips GROUP BY day;     tips.groupby('day').agg({'tip': 'mean', 'day': 'size'})


 SELECT smoker, day, COUNT(*), AVG(tip) FROM tips GROUP BY smoker, day;

 # Pandas
 tips.groupby(["smoker", "day"]).agg({"tip": ["size", "mean"]})


 -- INNER JOIN --

 SELECT *                     # Pandas                      Метод DataFrame.merge() по умолчанию выполняет INNER JOIN.
 FROM df1                     pd.merge(df1, df2, on="key")
 INNER JOIN df2
   ON df1.key = df2.key;


 merge() также предлагает параметры для случаев, когда вы хотите объединить столбец одного DataFrame с индексом другого DataFrame.
 indexed_df2 = df2.set_index("key")
 pd.merge(df1, indexed_df2, left_on="key", right_index=True)


 -- LEFT OUTER JOIN --

 SELECT *                      # Pandas
 FROM df1                      pd.merge(df1, df2, on="key", how="left")
 LEFT OUTER JOIN df2
   ON df1.key = df2.key;


 -- RIGHT JOIN --

 SELECT *                      # Pandas
 FROM df1                      pd.merge(df1, df2, on="key", how="right")
 RIGHT OUTER JOIN df2
   ON df1.key = df2.key;


 -- FULL JOIN --

 SELECT *                       # Pandas
 FROM df1                       pd.merge(df1, df2, on="key", how="outer")
 FULL OUTER JOIN df2
   ON df1.key = df2.key;


 --- Модуль pandas, анализ данных в Python ---


 Kaggle - это международная платформа, на которой проводятся соревнования по анализу данных.

 Медиана — это число, которое является серединой множества чисел
 Например, медианой для чисел 2, 3, 3, 5, 7 и 10 будет 4.

 from numpy import median

 res = numpy.array([1, 2, 3, 5])
 print(median(res))                  # -> 2.5
 print(median([1, 2, 3, 5, 10]))     # -> 3.0  # Если чётное количество чисел берет число по середине
 print(median([1, 2, 3, 5, 9, 10]))  # -> 4.0  # Если НЕ чётное количество чисел берет 2 числа из середины (a+b)/2

 Медиана В статистике: Нужно чтобы данные были Отсортированные по возрастанию
 Медиана(median) — значение признака, которое делит УПОРЯДОЧЕННОЕ множество данных ПОПОЛАМ.

 Мода — это число, наиболее часто встречающееся в данном наборе чисел.


 ЭКСТРАПОЛЯЦИЯ и ИНТЕРПОЛЯЦИЯ — это методы прогнозирования и оценки значений функций или данных.

 1. **Интерполяция** - это процесс оценки значений функции на основе известных данных в пределах заданного диапазона.
 Например, если у вас есть набор точек, интерполяция позволяет определить значения между этими точками.

 2. **Экстраполяция** - это метод предсказания значений функции за пределами известных данных. Это более рискованная
 операция, так как предполагает, что тенденции, наблюдаемые в имеющихся данных, будут продолжаться и за их пределами.

 В итоге, ИНТЕРПОЛЯЦИЯ работает внутри диапазона данных, а ЭКСТРАПОЛЯЦИЯ — за его пределами.


 -- Разработка модели машинного обучения строится из стандартных этапов, я их кратко опишу ниже. --

 0. Сбор данных и оценка их качества в свете требований к модели.

 1. Предобработка данных: удаление дубликатов и пропусков; преобразование категориальных переменных в числовые (One-Hot
 Encoding, Label Encoding); нормализация числовых переменных; в Вашем случае – добавление/интерполяция данных для дат,
 когда данные отсутствуют.

 2. Разделение данных на обучающую, валидационную и тестовую выборки (например, 70%/15%/15%).

 3. Исследование тенденций в данных (в Вашем случае, например, есть ли сезонность продаж и др.). Это влияет на то,
  что в разработку модели добавляются дополнительные признаки (features): дни недели, месяцы, годы,
  лаги — это некоторые предыдущие значения временного ряда.

 4. Выбор модели, в Вашем случае для задачи регрессии (например, линейная регрессия, случайный лес, градиентный бустинг).

 5. Обучение модели на обучающей выборке, используя выбранные алгоритмы и параметры.

 6. Валидация модели на валидационной выборке, используя метрики, такие как MSE (среднеквадратичная ошибка) или MAE (средняя
 абсолютная ошибка). RMSE (корень среднеквадратичной ошибки) , MAPE (средняя абсолютная процентная ошибка)
 SMAPE (Symmetric mean absolute percentage error) - Чувствительность к нулевым значениям и очень маленьким значениям

 7. Тюнинг модели – настройка гипер-параметров модели для улучшения качества предсказания.

 8. Тестирование модели на тестовой выборке для финальной оценки качества модели.

 9. Деплой модели в продакшн-среде для использования в реальных условиях


 MSE, RMSE, MAE, MAPE, SMAPE - ИЗМЕРЯЮТ ОШИБКИ МЕЖДУ ПРЕДСКАЗАННЫМИ ЗНАЧЕНИЯМИ МОДЕЛИ И ФАКТИЧЕСКИМИ ЗНАЧЕНИЯМИ.


 -- Метрики для регрессии --

 **Mean Squared Error (MSE)**: Среднеквадратичная ошибка; измеряет среднее значение квадратов ошибок между
 предсказанными и фактическими значениями.

 **Root Mean Squared Error (RMSE)**: Квадратный корень из MSE; простое интерпретируемое значение, которое показывает,
 в среднем, насколько предсказания отклоняются от фактических значений.

 **Mean Absolute Error (MAE)**: Средняя абсолютная ошибка; измеряет среднее значение абсолютных ошибок между
 предсказанными и фактическими значениями.

 **R-squared (коэффициент детерминации)**: Показывает, какую долю дисперсии зависимой переменной объясняет модель;
 значение от 0 до 1, где 1 указывает на идеальное соответствие.


 -- Метрики для классификации --

 **Accuracy (точность)**: Доля правильно предсказанных объектов к общему числу объектов.

 **Precision (точность)**: Доля истинных положительных результатов к общему числу предсказанных положительных результатов.

 **Recall (полнота)**: Доля истинных положительных результатов к общему числу фактических положительных результатов.

 **F1-score**: Гармоническое среднее между Precision и Recall, полезно при наличии дисбаланса классов.

 **ROC-AUC (площадь под кривой ROC)**: Измеряет качество бинарного классификатора; выше 0.5 считается хорошим.



 DataSet обычно представляет собой файл с таблицей в формате JSON или CSV.       CSV - значения, разделенные запятыми
 Dataset – это обработанная и структурированная информация в табличном виде.

 SciPy — это библиотека для языка Python, построена поверх NumPy, но для более глубоких и сложных научных вычислений
 Matplotlib – это библиотека для визуализации данных на Python.  - для создания графиков и диаграмм
 Seaborn - это библиотека для визуализации данных на Python. Построена поверх Matplotlib - для создания статистических графиков

 Библиотека Pandas построена на базе NumPy.
 Numeric Python (NumPy) - Она ускоряет работу с многомерными массивами и матрицами, а также позволяет вычислять
 много высокоуровневых математических функций при работе с массивами данных.

 matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]            # -> Матрица через списки(list) в Python

 import numpy as np
 matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  # -> Матрица через numpy

  Две основные структуры данных, pandas.Series (1-мерная) и pandas.DataFrame (2-мерная)

 pandas.DataFrame - двумерный массив.  представляет собой двумерную помеченную структуру данных со столбцами
 потенциально разных типов.
 pandas.Series - одномерный массив.    это одномерный помеченный массив, способный хранить данные любого типа
 Каждый pandas.DataFrame и pandas.Series имеют индекс pandas.Index, который представляет собой метки строк данных.

 import pandas as pd

 df_filtered = df[df['column'] == 'condition']

 # только на больших ДФ .query() работает медленнее.
 df_filtered = df.query('column == "condition"')

# Предположим, что game_events — это ваш DataFrame
 game_events = pd.read_csv('путь_к_вашему_файлу.csv')  # Загрузите данные, если это необходимо

 Разные функции pandas              # Так тоже можно
 game_events['revenue'].count()     game_events.revenue.count()
 game_events['revenue'].sum()       game_events.revenue.sum()
 game_events['revenue'].mean()      game_events.revenue.mean()
 game_events['revenue'].max()       game_events.revenue.max()
 game_events['revenue'].min()       game_events.revenue.min()
 game_events['revenue'].median()    game_events.revenue.median()
 game_events['revenue'].fillna(1)   game_events.revenue..fillna(0)    - Замена ПРОПУЩЕННЫХ или НЕОПРЕДЕЛЕННЫХ значений


 # Получить столбец
 df['type']
 game_events['type']

 # Только уникальные
 game_events['user_id'].unique()   #  ['7f0344f8' '00aa49ac' 'f5ef9841' '13d17d67']  сами уникальные элементы
 game_events['user_id'].nunique()  #  4                                              количество уникальных элементов


 # Как посмотреть количество значений `Null` (или `NaN`)
 Пример!!!
 data = {'A': [1, 2, None], 'B': [None, 5, 6], 'C': [7, 8, 9]}
 df = pd.DataFrame(data)

 # Смотрим на количество Null значений в каждом столбце
 null_counts = df.isnull().sum()
 print(null_counts)


 # Фильтрация
 count_events = game_events[game_events['user_id'] == 'f5ef9841']['event_name'].count()

 # Тоже самое но SQL
 SELECT COUNT(event_name) AS count_events
 FROM game_events
 WHERE user_id = 'f5ef9841'


 # Фильтрация с датой
 count_events = game_events[game_events['event_date'] == '2021-01-15']['event_name'].count()

 # Выберите только те строки, где внутриигровые покупки пользователей больше или равны числу 7.49.
 game_events[game_events['revenue'] >= 7.49]


 # Получение/извлечение значений Series/DataFrame

 DataFrame.loc[]    - доступ к срезу данных DataFrame по индексным меткам;
 Series.loc[]       - доступ к срезу данных Series по индексным меткам;
 DataFrame.iloc[]   - доступ к срезу данных DataFrame по позиции;          # целочисленная индексация
 Series.iloc[]      - доступ к срезу данных Series по позиции;             # целочисленная индексация

 df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],
 index=['cobra', 'viper', 'sidewinder'],
 columns=['max_speed', 'shield'])

 print(df) # Вывод ниже

 #             max_speed  shield
 # cobra               1       2
 # viper               4       5
 # sidewinder          7       8

 # Одно значение loc[]                      # Несколько значений loc[[]]
 print(df.loc['cobra']) # Вывод ниже        print(df.loc[['cobra', 'viper']])

 # max_speed    1                           #        max_speed  shield
 # shield       2                           # cobra          1       2
 # Name: cobra, dtype: int64                # viper          4       5

 # Интересные примеры loc
 df.loc[df['shield'] > 6]
 df.loc[df['shield'] > 6, ['max_speed']]
 df.loc[lambda df: df['shield'] == 8]
 df.loc['cobra':'viper', 'max_speed']
 df.loc[['viper', 'sidewinder'], ['shield']] = 50
 df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]
 df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]
 df.loc['cobra'] = 10

 # iloc
 df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],
 columns=['max_speed', 'shield'])


 # Одно значение iloc[]                     # Несколько значений iloc[[]]
 print(df.iloc[1])                          print(df.iloc[[1, 2]])

 # max_speed    4                           #    max_speed  shield
 # shield       5                           # 1          4       5
 # Name: 1, dtype: int64                    # 2          7       8

 # Интересные примеры iloc
 df.iloc[[True, False, True]]
 df.iloc[lambda x: x.index % 2 == 0]
 df.iloc[:3]                            # что в отличие от обычных срезов python, включены как начало, так и конец.


 # Применить функцию с столбцу                                                              <-----
 df = pd.DataFrame({'hehe': ['Y', 'N']})

 print(df)  # Вывод ниже

 #   hehe
 # 0    Y
 # 1    N


 def split_it(year):
     return re.findall('Y', year)
                                                  # Тоже самое с lambda
 df['hehe'] = df['hehe'].apply(split_it)          df['hehe'] = df['hehe'].apply(lambda x: x.re.findall('Y', year))

 print(df) # Вывод ниже

 #   hehe
 # 0  [Y]
 # 1   []


 # Чтобы применить функцию только к одному столбцу:                                    <-----  Важно

 df = pd.DataFrame({'hehe': ['Y Yes', 'N No']})

 def get_first_word(s):
     return s.split(maxsplit=1)[0]

 df['first'] = df['hehe'].apply(get_first_word)

 print(df) # Вывод ниже

 #     hehe first
 # 0  Y Yes     Y
 # 1   N No     N


 # Еще вариант   Также можно воспользоваться готовыми векторизированными Pandas методами:
 df['hehe'].str.split(n=1).str[0]               # Тоже самое
 df['hehe'].str.extract(r'(Y)', expand=False)   # Тоже самое


 # Иногда при работе со строковыми данными list comprehension оказывается быстрее встроенных векторизированных функций.

 df['first'] = [n.split(maxsplit=1)[0] for n in df['hehe']]


 # Интересный пример

 def find_products(products: pd.DataFrame) -> pd.DataFrame:
     df = products
     pattern = r'Y'
     filtered_df = df[(df['low_fats'].str.contains(pattern)) & \
     (df['recyclable'].str.contains(pattern))
     ]
     return filtered_df[['product_id']]


 -- Регулярки в Pandas     Использование регулярных выражений в Pandas --

 Метод Series.str.count()     - подсчитывает вхождения шаблона в строке;
 Метод Series.str.replace()   - заменит каждое вхождение шаблона регулярного выражения;
 Метод Series.str.contains()  - проверяет, содержится ли регулярное выражение в каждой строке;
 Метод Series.str.extract()   - извлекает группы захвата из шаблона регулярного выражения;
 Метод Series.str.findall()   - найдет все вхождения регулярного выражения;
 Метод Series.str.match()     - определяет, начинается ли каждая строка с совпадения с регулярным выражением;
 Метод Series.str.split()     - разбивает строки по заданному разделителю;
 Метод Series.str.rsplit()    - разбивает строки по заданному разделителю начиная справа.





 # Пример JOIN в pandas   pandas.DataFrame.join
 DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False, validate=None)

 df.join(other, lsuffix='_caller', rsuffix='_other')
 df.set_index('key').join(other.set_index('key'))
 df.join(other.set_index('key'), on='key')
 df.join(other.set_index('key'), on='key', validate='m:1')


 # Пример JOINs в pandas   pandas.DataFrame.merge

 DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False,
                 sort=False, suffixes=('_x', '_y'), copy=None, indicator=False, validate=None)

 # Примеры!!!
 df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],
                    'value': [1, 2, 3, 5]})
 df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],
                    'value': [5, 6, 7, 8]})

 # Объединить df1 и df2 в столбцах lkey и rkey. Столбцы значений имеют суффиксы по умолчанию, _x и _y, добавленные.

 df1.merge(df2, left_on='lkey', right_on='rkey')

 # Объединить фреймы данных df1 и df2 с указанными левыми и правыми суффиксами, добавленными ко всем перекрывающимся столбцам.

 df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=('_left', '_right'))

 # Объединить DataFrames df1 и df2, но вызвать исключение, если DataFrames имеют перекрывающиеся столбцы.

 df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))



 df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})
 df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})

 df1.merge(df2, how='inner', on='a')
 df1.merge(df2, how='left', on='a')
 df1.merge(df2, how='cross')



   --- Pandas vs SQL ---

 # Таблицы
 df1 = pd.DataFrame({"key": ["A", "B", "C", "D"], "value": np.random.randn(4)})
 df2 = pd.DataFrame({"key": ["B", "D", "D", "E"], "value": np.random.randn(4)})

 -- INNER JOIN --

 SELECT *                     # Pandas
 FROM df1                     pd.merge(df1, df2, on="key")
 INNER JOIN df2
   ON df1.key = df2.key;


 merge() также предлагает параметры для случаев, когда вы хотите объединить столбец одного DataFrame с индексом другого DataFrame.
 indexed_df2 = df2.set_index("key")
 pd.merge(df1, indexed_df2, left_on="key", right_index=True)


 -- LEFT OUTER JOIN --

 SELECT *                      # Pandas
 FROM df1                      pd.merge(df1, df2, on="key", how="left")
 LEFT OUTER JOIN df2
   ON df1.key = df2.key;


 -- RIGHT JOIN --

 SELECT *                      # Pandas
 FROM df1                      pd.merge(df1, df2, on="key", how="right")
 RIGHT OUTER JOIN df2
   ON df1.key = df2.key;


 -- FULL JOIN --

 SELECT *                       # Pandas
 FROM df1                       pd.merge(df1, df2, on="key", how="outer")
 FULL OUTER JOIN df2
   ON df1.key = df2.key;


 -- UNION --

 df1 = pd.DataFrame({"city": ["Chicago", "San Francisco", "New York City"], "rank": range(1, 4)})
 df2 = pd.DataFrame({"city": ["Chicago", "Boston", "Los Angeles"], "rank": [1, 4, 5]})

 SELECT city, rank              # Pandas
 FROM df1                       pd.concat([df1, df2])
 UNION ALL
 SELECT city, rank
 FROM df2;


 -- LIMIT --

 SELECT * FROM tips             # Pandas
 LIMIT 10;                      tips.head(10)


 Jupyter-ноутбук — это среда разработки, где сразу можно видеть результат выполнения кода и его отдельных фрагментов.
 расширение файлов  .ipynb
 IPython – это интерактивная оболочка с широким набором возможностей и ядро для Jupyter
 Jupyter notebook является графической веб-оболочкой для IPython
 jupyter magics — метаязык, команды которого обычно начинаются с % или %%


 -- Векторные БД ML --

 Вектор - это упорядоченный набор чисел (например, [0.1, -0.5, 0.9]), который:
 - Представляет объект (текст, изображение и т.д.) в числовой форме.
 - Похожие объекты - близкие векторы (по расстоянию, например, косинусному, евклидово).
 - Основа для работы ML-моделей (поиск, классификация, рекомендации).

 Вектор -  используется для представления объектов (текстов, изображений, аудио и т. д.) в числовой форме.
 Вектор - упорядоченный набор чисел, представляющий объект в числовой форме. Основа для ML (поиск, классификация, рекомендации).
 Вектор - числовое представление объекта для ML.

 Векторы в ML часто называют ЭМБЕДДИНГАМИ (англ. embeddings), если они получены в результате преобразования сложных
 данных (например, слов или изображений) в числовые векторы с сохранением семантической близости.


 Векторные БД - это базы данных, оптимизированные для хранения и поиска по векторам (числовым представлениям данных).

 Суть:
 - Данные (текст, изображения и др.) превращаются в векторы (эмбеддинги) через ML-модели (например, OpenAI, BERT).
 - БД быстро ищет ближайшие векторы (похожие объекты) даже среди миллионов записей.

 Векторные базы данных (Vector DB) активно используются в машинном обучении (ML), особенно для работы с векторными
 представлениями данных (эмбеддингами).

 Эмбеддинг (Embedding) - это числовое представление объекта (текста, изображения, аудио и т.д.) в виде вектора (списка чисел).

 Векторный эмбеддинг - это вектор (например, [0.2, -0.5, 0.7, ...]), который:
 - Создаётся нейросетью (например, BERT для текста, ResNet для картинок).
 - Кодирует смысл объекта (похожие объекты → близкие векторы).
 - Используется для поиска, классификации, рекомендаций.

 Примеры:
 Текст    - [0.1, -0.3, 0.8, ...] (например, через OpenAI или BERT).
 Картинка - [0.4, 0.6, -0.2, ...] (например, через CLIP или ResNet).

 Зачем?
 Чтобы ИИ понимал данные не как "слова" или "пиксели", а как числа, с которыми можно работать (сравнивать, искать похожее).

 Эмбеддинг - это вектор чисел, представляющий данные в форме, удобной для ИИ.


 Что такое векторные БД?

 - Это специализированные базы данных, которые эффективно хранят векторы (эмбеддинги) и позволяют быстро искать
  ближайшие соседи (k-NN или приближенный поиск ANN).
 - Они часто используют индексы (HNSW, IVF, PQ и др.) для ускорения поиска.

 - Поиск в векторных БД
 Точный поиск (k-NN, k-Nearest Neighbors) - находит точных ближайших соседей, но медленный на больших данных.
 Приближенный поиск (ANN, Approximate Nearest Neighbors) - жертвует точностью ради скорости (использует индексы вроде HNSW, IVF, LSH).


 Зачем они нужны в ML?
 - Поиск похожих векторов (similarity search) - например, поиск похожих изображений, текстов или рекомендации.
 - Хранение эмбеддингов - векторов, полученных из моделей (BERT, ResNet, GPT и др.).
 - Быстрый поиск в больших объемах – оптимизированы для Nearest Neighbor (ANN) поиска.

 Примеры использования в ML:
 - Семантический поиск (текст, аудио, видео).
 - Рекомендательные системы.
 - Кластеризация и классификация.
 - Векторный поиск в RAG (Retrieval-Augmented Generation) для LLM (ChatGPT и аналоги).

 Векторные БД:

 Pinecone  - лидер, облачный, для продакшена.
 Milvus    - открытый, мощный, для больших данных.
 Weaviate  - с GraphQL и гибридным поиском.
 Qdrant    - быстрый, на Rust, open-source.
 Chroma    - легкий, для разработчиков.


 -- Блок MLE --

 SL (Supervised Learning) - это метод машинного обучения, при котором модель обучается на РАЗМЕЧЕННЫХ данных
 (с правильными ответами). Примеры: классификация, регрессия.

 Библиотеки:
 - scikit-learn (основная библиотека для классического ML).
 - XGBoost, LightGBM, CatBoost (градиентный бустинг для табличных данных), SVM
 - TensorFlow, PyTorch (если используются нейросети).


 LLM (Large Language Models) - LLM (Large Language Models) — большие языковые модели, обученные на огромных текстовых
 данных. Они могут генерировать текст, отвечать на вопросы и т. д. (например, ChatGPT).
 LLM - это подмножество Deep Learning, а не классического ML.

 Библиотеки:
 - transformers (Hugging Face) - основной инструмент для работы с LLM.
 - PyTorch, TensorFlow (для кастомного обучения).
 - langchain - для создания приложений на основе LLM.


 Табличные данные - структурированные данные в виде таблиц (строки и столбцы), часто используемые в анализе и машинном
 обучении (например, Excel, CSV).  Визуализация: matplotlib, seaborn, plotly

 Библиотеки:
 - pandas - основной инструмент для обработки.
 - numpy  - для численных операций.
 - polars - аналог pandas с ускорением.

 Для табличных данных часто используют ансамбли (XGBoost и др.), а для текста - трансформеры (transformers).  <-----


 Для таблиц/временных рядов - LSTM.
 Для текста/аудио           - Transformer.
 RNN почти не используют.

 # ПРИМЕР LSTM vs Transformer
 # LSTM
 tf.keras.layers.LSTM(64)

 # Transformer
 from transformers import BertModel
 model = BertModel.from_pretrained("bert-base-uncased")

 LSTM        - простота, малые данные, edge-устройства.
 Transformer - SOTA  для NLP/временных рядов, но требует GPU и много данных.

 SVM (Support Vector Machine)  - классический алгоритм ML.
 SOTA (State Of The Art)       - лучшая современная модель в задаче.

 Transformer - современный стандарт, но LSTM ещё жив в специфичных задачах.

 Когда что использовать?
 Модель	        Плюсы	            Минусы	                Где применять?
 RNN	        Простота	        Плохая память	        Простые последовательности
 LSTM	        Хорошая память	    Медленная	            Временные ряды, старый NLP
 Transformer	Скорость, точность	Требует МНОГО данных	Современный NLP (GPT, BERT)


 RNN (Рекуррентные нейронные сети)  - архитектура для обработки последовательностей, где информация передаётся от шага
 к шагу. Проблема: сложно запоминать долгосрочные зависимости.

 LSTM (Долгая краткосрочная память) - улучшенная RNN с "воротами" (gate), которые решают, что запомнить или забыть.
 Эффективнее для долгосрочных зависимостей.

 Трансформеры (Transformers) - архитектура на основе внимания (attention), обрабатывает все элементы последовательности
 параллельно (не шаг за шагом, как RNN/LSTM). Основа современных моделей (например, GPT, BERT).

 Ключевые отличия:
 - RNN          - Последовательная обработка, страдает от "забывания".
 - LSTM         - Контролируемое запоминание/забывание.
 - Трансформеры - Параллельная обработка + внимание к важным частям.


 # Примеры трасформеров

 # Для классификации текста с малым датасетом:
 from transformers import pipeline
 classifier = pipeline("text-classification", model="distilbert-base-uncased")

 # Для большого датасета:
 from transformers import Trainer, TrainingArguments
 # Кастомное обучение полной модели

 Трансформеры требуют много данных и вычислительных ресурсов.
 Трансформеры сейчас доминируют в NLP (обработка текста).


 Когда что выбирать?
 Тип данных	                Лучший подход	        Библиотеки
 Табличные данные	        Градиентный бустинг	    XGBoost, LightGBM
 Текст/Естественный язык	LLM (Transformer)	    transformers, PyTorch
 Изображения	            Сверточные сети (CNN)	TensorFlow, PyTorch
 Временные ряды	            RNN/LSTM или ансамбли	statsmodels, sklearn



 -- Чем отличается Machine learning от Deep Learning? --
 Основное различие между глубоким обучением и машинным обучением обусловлено тем, как данные представляются в систему.
  Алгоритмы машинного обучения почти всегда требуют структурированных данных, в то время как сети глубокого обучения
 полагаются на слои ANN (искусственные нейронные сети).


--- ML Data Science ---

 Граф состоит из отдельных точек (вершин) и соединяющих их линий (ребер).
 Граф – это просто набор взаимосвязанных элементов.


 Ациклический граф — граф без циклов (нельзя пройти по рёбрам и вернуться в исходную вершину).

 Виды Ациклических графов:
 - Неориентированный: ациклический граф — лес, а если он связный — дерево.
 - Ориентированный: ациклический граф — DAG (Directed Acyclic Graph), то есть нет направленных циклов.

 Примеры DAG: зависимости задач и пакетов, граф коммитов Git, планирование (критический путь, топологическая сортировка).


 5 типов графов, которые должен знать каждый Data Scientist
 - Разбивка вершин на группы
 - Поиск кратчайшего пути
 - Поиск оптимального пути
 - Рейтинг вершин графа
 - Центральность вершин


 Деревья - разновидность графов, но в деревьях НЕ может быть циклов!

 Дерево
 Как и связный список, дерево (tree) использует элементы, которым для хранения объектов НЕ нужно располагаться в
 физической памяти непрерывно. Ячейки здесь тоже имеют указатели на другие ячейки, однако, в отличие от связных списков,
 они располагаются не линейно, а в виде ветвящейся структуры. Деревья особенно удобны для иерархических данных,
 таких как каталоги с файлами или система субординации.


       Дерево                               - В дереве узлы имеют по крайней мере одного родителя
        10   <-  Узел                       - Существует только один узел без родителя - это Корневой узел
       /  \  <-  Ребро                      - Узлы, НЕ имеющие дочерних узлов, называются листовыми узлами (листьями)
      5    20   <- Родитель узла (25)
     / \    \
    2   7    25 <- Дочерний узел (20)
    ^   ^     ^
   Листовые узлы

 Самое важное, что нужно запомнить - в деревьях НЕ может быть циклов!


  - Короткие деревья работают быстрее

  Оба дерева состоят из 7 Узлов, но сильно различаются по производительности!
  Лучший случай     Худший случай
  Высота: 2         Высота: 6
     0                0
    / \                \
   0   0                0
  /\   /\                \
 0  0 0  0                0
                           \
                            0
                             \
                              0
                               \
                                0
                                 \
                                  0




 Бинарное дерево особая разновидность дерева, узлы которого могут иметь НЕ более двух дочерних узлов!

                      Бинарное Дерево
                           10
                          /  \
 Левый дочерний узел  -> 5    20  <- Правый дочерний узел
                        / \    \
                       2   7    25
                       ^         ^
          Левое поддерево    Правое поддерево


 Двоичное дерево поиска   - Сложность алгоритма - O(log n)
 Двоичное дерево поиска (binary search tree) — это особый тип дерева, поиск в котором выполняется особенно эффективно.
 Узлы в двоичном дереве поиска могут иметь не более двух дочерних узлов. Кроме того, узлы располагаются согласно их
 значению/ключу. Дочерние узлы слева от родителя должны быть меньше него, а справа — больше.

 Сложность операций с бинарным деревом зависит от его структуры:                                        <-----  <-----

 1. **Вставка, удаление и поиск**:
 - В худшем случае (разреженное дерево или не сбалансированное): O(n)
 - В среднем и лучшем случае (сбалансированное дерево): O(log n)

 2. **Обход дерева (инордер, префиксный, постфиксный)**: O(n)

 Таким образом, сложность зависит от того, насколько сбалансировано дерево.


 СБАЛАНСИРОВАННОЕ БИНАРНОЕ ДЕРЕВО ПОИСКА (BST) - Разновидность бинарного дерева    O(log n)
 Значение левого дочернего узла всегда меньше а значение правого дочернего узла всегда больше

            СБАЛАНСИРОВАННОЕ  Бинарное Дерево
 Ниже значения меньше ->   10   <- Ниже значения больше
                          /  \
                  ->     5    20    <-
                        / \    \
                  ->   2   7    25  <-

 Балансировка дерева в Python — это процесс, который помогает поддерживать структуру дерева в равновесии, чтобы операции,
 такие как добавление, удаление и поиск элементов, выполнялись быстро.
 Балансировка дерева в Python (и в других языках программирования) помогает поддерживать структуру дерева в равновесии,
 что позволяет выполнять операции добавления, удаления и поиска элементов эффективно. Это достигается за счет
 поддержания небольшой высоты дерева, что обеспечивает логарифмическое время выполнения этих операций.
 Примеры сбалансированных деревьев включают AVL-деревья и красно-черные деревья. Они автоматически поддерживают балансировку,
 чтобы высота дерева оставалась логарифмической относительно количества элементов, что обеспечивает быструю работу с данными.


 Балансировка дерева. Если вставить в двоичное дерево поиска слишком много узлов, в итоге получится очень высокое
 дерево, где большинство узлов имеют всего один дочерний узел. Например, если последовательно вставлять узлы с
 ключами/значениями, которые всегда больше предыдущих, в итоге получится нечто, похожее на связный список. Однако мы
 можем перестроить узлы в дереве так, что его высота уменьшится. Эта процедура вызывается балансировкой дерева.

 Большинство операций с деревом требует обхода узлов по ссылкам, пока не будет найден конкретный узел. Чем больше высота
 дерева, тем длиннее средний путь между узлами и тем чаще приходится обращаться к памяти. Поэтому важно уменьшать высоту
 деревьев.

 Однако балансировка дерева — дорогостоящая операция, поскольку требует сортировки всех узлов. Если делать балансировку
 после каждой вставки или удаления, операции станут значительно медленнее. Обычно деревья подвергаются этой процедуре
 после нескольких вставок и удалений. Но балансировка от случая к случаю является разумной стратегией только в отношении
 редко изменяемых деревьев.     Идеально сбалансированное дерево имеет минимальную высоту   <-----

 -- Перебалансировка дерева нужна, чтобы: балансировка = скорость.  Суть: меньше высота = быстрее работа.
 - Уменьшить высоту дерева → ускорить поиск, вставку, удаление (сложность O(log n) вместо O(n)).
 - Избегать вырождения в "список" (когда все узлы идут в одну сторону).
 - Методы: AVL, красно-черные деревья, повороты.

 Для эффективной обработки двоичных деревьев, которые изменяются часто, были придуманы сбалансированные двоичные
 деревья(self-balancing binary tree) . Их процедуры вставки или удаления элементов гарантируют,
 что дерево остается сбалансированным.

 Красно-черное дерево (red-black tree) — это хорошо известный пример сбалансированного дерева, которое окрашивает узлы
 красным либо черным цветом в зависимости от стратегии балансировки . Красно-черные деревья часто используются для
 реализации словарей: словарь может подвергаться интенсивной правке, но конкретные ключи в нем по-прежнему будут
 находиться быстро вследствие балансировки.


 AVL-дерево (AVL tree) — это еще один подвид сбалансированных деревьев. Оно требует немного большего времени для вставки
 и удаления элементов, чем красно-черное дерево, но, как правило, обладает лучшим балансом. Это означает, что оно
 позволяет получать элементы быстрее, чем красно-черное дерево. AVL-деревья часто используются для оптимизации
 производительности в сценариях, для которых характерна высокая интенсивность чтения.

 AVL-деревья, как и другие сбалансированные деревья, используют информацию о высоте узлов или коэффициенте балансировки
 для определения необходимости выполнения поворотов и поддержания сбалансированности.

 Как АВЛ-дерево узнает, что требуется поворот?
 АВЛ - деревья балансируют себя с помощью поворотов                                                            <-----
 Чтобы дерево знало, когда требуется сомобалансировка, оно должно хранить дополнительную информацию. В каждом узле
 хранится один или два вида информации: значение высоты или значение, которое иногда называют коэффициентом балансировки
 Этот коэффициент должен быть равен -1, 0, 1

     -1                        |       0         |      1             -1: правое поддерево выше на 1 уровень, чем левое.
      0                        |       0         |      0              0: поддеревья равны по высоте.
     / \                       |      / \        |     / \             1: левое поддерево выше на 1 уровень, чем правое.
    0   0                      |     0   0       |    0   0
   /                           |                 |         \
  0                            |                 |          0
                               |                 |
 Левый дочерний узел на 1 выше |  Сбалансировано |  Правый дочерний узел на 1 выше


 Данные часто хранятся на магнитных дисках, которые считывают их большими блоками. В этих случаях используется
 обобщенное двоичное B-дерево (B-tree). В таких деревьях узлы могут хранить более одного элемента и иметь более двух
 дочерних узлов, что позволяет им эффективно оперировать данными в больших блоках.
 B-деревья обычно используются в системах управления

 Таким образом, чтение каждого узла занимает больше времени. С другой стороны, поиск ускоряется, потому что за один раз
 читается больший объем данных. Именно это обеспечивает высокую скорость работы В-деревьев.


 Двоичная куча
 Двоичная куча (binary heap) — особый тип двоичного дерева поиска, в котором можно мгновенно найти самый маленький
 (или самый большой) элемент.

 Кучи подчиняются тем же правилам размещения узлов, что и двоичные деревья поиска, но есть одно ограничение:
 родительский узел должен быть больше (либо меньше) обоих своих дочерних узлов


 Граф (graph) аналогичен дереву. Разница состоит в том, что у него нет ни дочерних, ни родительских узлов (вершин) и,
 следовательно, нет корневого узла. Данные свободно организованы в виде узлов (вершин) и дуг (ребер) так, что любой
 узел может иметь произвольное число входящих и исходящих ребер. Это самая гибкая из всех структур, она используется
 для представления почти всех типов данных.

 Граф с весами называется взвешенным графом. Граф без весом называется НЕвзвешенным графом.

 Например, графы идеальны для социальной сети, где узлы — это люди, а ребра — дружеские связи.


 Смежность — непосредственная близость, примыкание, тесное соприкосновение.
 Матрица смежности — один из способов представления графа в виде матрицы.
 Список смежности — один из способов представления графа в виде коллекции списков вершин. Каждой вершине графа
 соответствует список, состоящий из «соседей» этой вершины.


 Обход Графа - процесс систематического просмотра всех вершин или рёбер графа, чтобы найти некоторые вершины,
 удовлетворяющие определённым условиям.

 Простыми словами, обход графа — это переход от одной его вершины к другой в поисках свойств связей этих вершин. Связи
 (линии, соединяющие вершины) называются направлениями, путями, гранями или ребрами графа. Вершины графа также именуются узлами.

 Какие бывают алгоритмы обхода графа?
 - Поиск в глубину (Depth-First Search, DFS) - обходит граф, уходя “вглубь” по рёбрам; может найти путь,
   но НЕ гарантирует ни минимальную сумму весов, ни минимальное число рёбер (не кратчайший).
 - Поиск в ширину (Breadth-First Search, BFS) - В результате поиска в ширину находится путь кратчайшей длины в
   невзвешенном графе, т.е. путь, содержащий НАИМЕНЬШЕЕ ЧИСЛО рёбер. Это один из основных алгоритмов на графах.

 Алгоритм Дейкстры (МИНИМАЛЬНУЮ СУММУ) — это метод нахождения кратчайших путей от одной вершины графа ко всем остальным.
 Алгоритм работает только для графов без рёбер отрицательного веса.
 В отличие от BFS, для запоминания просматриваемых вершин алгоритм Дейкстры использует очередь с приоритетом.  <----

 Если вам нужно найти кратчайший путь в графе, с отрицательным весом,  - Алгоритм Беллмана Форда

 Алгоритм Беллмана-Форда — это алгоритм, который используется для нахождения кратчайших путей в графе, который может
 содержать ребра с отрицательным весом. Он работает даже в тех случаях, когда граф содержит отрицательные циклы,
 но не может корректно обработать такие циклы, если они достижимы из начальной вершины.


 -- ГРАФ VS БИНАРНОЕ ДЕРЕВО --

 Граф – произвольная структура данных с узлами и рёбрами (связями) без строгих правил. Может быть ЦИКЛИЧНЫМ,
 иметь любое количество связей.

 Бинарное дерево – частный случай графа с жёсткими условиями:
 - У каждого узла ≤ 2 потомка (левый/правый).
 - Нет циклов.
 - Часто упорядочено (например, бинарное дерево поиска).

 Разница:
 - Бинарное дерево – это упорядоченный граф с жёсткими ограничениями.
 - Обычный граф – свободная структура без таких правил.

 Примеры:
 - Граф: соцсеть (друзья → любые связи).
 - Бинарное дерево: поиск в телефонной книге (слева – меньше, справа – больше).


 -- КАК ВСТАВИТЬ ЭЛЕМЕНТ В БИНАРНОЕ ДЕРЕВО --

 - Начинаем с корня.
 - Сравниваем новый элемент с текущим узлом:
    - Если меньше → идём в левое поддерево.
    - Если больше → идём в правое поддерево.
 - Дошли до пустого места? → Вставляем новый узел.

 - Где применяется: сортировка, поиск, базы данных.


  --- PyTorch ---

 Что такое файлы .pth?
 Файлы с расширением .pth - это файлы, которые показывают Python в каких директориях искать МОДУЛИ Python
 Если нужно импортировать из не стандартных директорий.
 Довольно часто используются с PyTorch.

 PyTorch - это библиотека машинного обучения. 1) Динамические вычисления. 2) Автоматическое дифференцирование.
 Дифференциация (от лат. differentia — «различие») - разделение, разведение процессов или явлений на составляющие части.

 Динамические вычисления. PyTorch динамически пересчитывает граф вычислений «на лету», и это удобно.

  Автоматическое дифференцирование. Дифференцированием называют нахождение производной функции. Это математическая операция,
 важная для машинного обучения — с помощью дифференцирования как раз пересчитывают веса. Так работает алгоритм обратного
 распространения ошибки, ключевой в обучении нейронных сетей.

 Нейронная сеть или модель для машинного обучения представлена как ГРАФ — структура, состоящая из вершин и связей между ними.
 Это «нейроны» и «синапсы», которые хранят и передают информацию. В ВЕРШИНАХ ХРАНЯТСЯ ФОРМУЛЫ, А НА РЕБРАХ — ВЕСА,
 числовые значения, которые пересчитываются на каждом шаге.


 PyTorch поддерживает автоматическое дифференцирование. Совместно с динамическими вычислениями выходит так, что фреймворк
 строит граф и вычисляет веса «на лету», а человеку достаточно написать пару строк кода. Это удобно и позволяет реализовать
 более сложные проекты.

 Некоторые говорят, что PyTorch — один из самых «питоновских» фреймворков для ML.


  -- Hadoop   Spark --
 Выполняют операции MapReduce
 MapReduce Техника
 В модели программирования MapReduce большой набор данных сначала разбивается на меньшие части.
 После того как все задачи для всех поднаборов решены, мы можем объединить результаты.

 MapReduce — это модель обработки больших данных, состоящая из двух этапов:

 1. **Map**: Разделение данных на пары ключ-значение и их предварительная обработка.
 2. **Reduce**: Агрегация промежуточных результатов по ключам для получения финальных результатов.

 Применяется для параллельной обработки и анализа данных

 HDFS (Hadoop Distributed File System) — распределённая файловая система Hadoop для хранения больших файлов на кластере:
 данные разбиваются на блоки и реплицируются на разные узлы для надёжности и скорости.


 -- Модели ML --

 Дисперсия в статистике — это мера, которая показывает разброс между результатами. Если все они близки к среднему,
 дисперсия низкая. А если результаты сильно различаются — высокая.

 СИГМА - Среднеквадратическое отклонение.

 СИГМА В отличии от Дисперсии - показывает РЕАЛЬНОЕ СРЕДНЕЕ ЗНАЧЕНИЕ НАШИХ ОТКЛОНЕНИЙ ОТ СРЕДНЕГО ЗНАЧЕНИЯ В ВЫБОРКЕ

 Может ли показатель стандартного отклонения принимать отрицательные значения?
 Не может, стандартное отклонение всегда НЕотрицательное.

 Квартили — делят УПОРЯДОЧЕННЫЕ ДАННЫХ на 4 равные части
 Квартили — это значения, которые делят распределение на 4 равные части одинакового размера.
 Квартили — это специальные случаи квантилей, которые делят набор данных на четыре равные части

 Квантиль — это общее понятие, которое обозначает значение, делящее набор данных на равные части.

 Перцентили — это статистические меры, которые делят набор данных на 100 равных частей.
 Каждый перцентиль показывает, какой процент данных находится ниже или выше определенного значения.

 Перцентили используются для анализа распределения данных, выявления выбросов и сравнения различных наборов данных.


 Случайная величина — это величина, значение которой не фиксировано и может меняться в зависимости от случайных факторов.
 Проще говоря, это число, которое вы получаете в результате случайного эксперимента.

 -- Основные понятия теории вероятностей включают: --

 1. **Случайная величина**:
 - Это функция, которая сопоставляет каждому элементу исходного пространства (набору возможных исходов эксперимента)
 числовое значение. Случайная величина может быть:
 ДИСКРЕТНОЙ (принимает конечное или счётное множество значений)
 НЕПРЕРЫВНОЙ (принимает значения из непрерывного диапазона).

 2. **Распределение случайной величины**:
 - Это описание вероятностей, с которыми случайная величина принимает различные значения. Для дискретных случайных
 величин используется функция вероятностей, а для непрерывных - функция плотности вероятности.

 3. **Вероятность**:
 Это числовая оценка шанса наступления определённого события, которая выражается в диапазоне
 от 0 (невозможное событие) до 1 (достоверное событие).

 4. **Случайные события**:
 - Это подмножества, состоящие из исходов эксперимента. События могут быть независимыми
 (влияние одного на другое отсутствует) или зависимыми (влияние одного события на другое имеет место).

 5. **Ожидание (математическое ожидание)**:
 - Это среднее значение случайной величины, которое рассчитывается как сумма произведений возможных значений на
 соответствующие вероятности (для дискретных случайных величин) или интеграл (для непрерывных).

 6. **Дисперсия**:
- Это мера разброса значений случайной величины относительно её математического ожидания. Она показывает,
 насколько значения случайной величины варьируются.

 7. **Ковариация**:
 - Это мера, отражающая, как две случайные величины изменяются совместно. Она используется для определения направления
 и силы связи между величинами.

 8. **Стандартное отклонение**:
 - Это квадратный корень из дисперсии, который также измеряет разброс значений случайной величины.


 Ковариация и Корреляция: Методы, позволяющие оценить зависимость между двумя случайными величинами.
 Корреляция — это статистическая мера, которая показывает, насколько сильно и в каком направлении связаны между собой две переменные.

 1. **Положительная корреляция** — при увеличении одной переменной другая также увеличивается.
 2. **Отрицательная корреляция** — при увеличении одной переменной другая уменьшается.
 3. **Отсутствие корреляции**    — изменения одной переменной не влияют на другую.

 Корреляция измеряется с помощью коэффициента корреляции, который варьируется от -1 до +1.
 Значение +1 указывает на идеальную положительную корреляцию, -1 — на идеальную отрицательную, а 0 — на отсутствие корреляции.



 -- МАТЕМАТИЧЕСКИЕ КРИТЕРИИ ПРОВЕРКИ СТАТИСТИЧЕСКИХ ГИПОТЕЗ --

 Проверка статистических гипотез — это важный процесс в статистике, который помогает определить, насколько данные соответствуют
 предположенным утверждениям. Ниже приведены основные математические критерии и методы проверки статистических гипотез:

 1. **Критерий Стьюдента (t-критерий)**:
 - Используется для проверки гипотезы о равенстве средних значений в случае небольших выборок (обычно до 30 наблюдений)
  с неизвестной дисперсией.
 - Различают односторонний и двухсторонний t-критерий в зависимости от формулировки гипотез.

 2. **Критерий Z**:
 - Применяется для проверки гипотез о средних значениях, когда выборка большая (больше 30 наблюдений) или известна
   дисперсия генеральной совокупности.
 - Используется также в случаях проверки пропорций.

 3. **Критерий х² (хи-квадрат)**:
 - Используется для проверки гипотез о распределении категориальных данных и для анализа связи между двумя категориальными переменными.
 - Например, используется в контексте таблиц сопряженности.

 4. **ANOVA (дисперсионный анализ)**:
 - Метод, который позволяет проверить равенство средних значений более чем двух групп.
 - Распределение данных проверяется на нормальность и однородность дисперсий.

 5. **Критерий Манна-Уитни (U-критерий)**:
 - Непараметрический тест, который используется для проверки равенства медиан двух независимых выборок.

 6. **Критерий Уилкоксона**:
 - Непараметрический тест для парных выборок, который сравнивает две связанные выборки, ранжируя данные.

 7. **Критерий Фридмана**:
 - Непараметрический метод для анализа различий между несколькими связанными группами, аналог ANOVA для зависимых выборок.

 8. **Корреляционный анализ**:
 - Проверка гипотезы о наличии зависимости между двумя количественными переменными, обычно с использованием
  коэффициента корреляции (например, Пирсона или Спирмена).

 9. **Бутстрэп-методы**:
 - Непараметрические методы, которые включают повторное выборочное измерение для оценки свойств распределений и проверки гипотез.

 Все эти критерии основаны на статистических распределениях и требуют определения уровня значимости (α), который обычно
 устанавливается на уровне 0.05 или 0.01. Результаты теста используются для принятия решения о начале или отклонении нулевой гипотезы.


 Нормальное распределение, также известное как гауссово распределение - Оно описывает, как значения случайной переменной
 распределены вокруг её среднего.

 Генеральная совокупность — это полное множество всех единиц, объектов или наблюдений, которые соответствуют
 определённым критериям или характеристикам и являются предметом исследования.

 Доверительный интервал — это статистический инструмент, используемый для оценки неопределенности параметра генеральной
 совокупности на основе данных выборки. Он указывает диапазон значений, в котором, с заданной вероятностью
 (например, 95% или 99%), может находиться истинное значение параметра.

 Коэффициент детерминации (R^2) - это статистическая мера, которая используется для оценки качества модели регрессии.
 принимает значения от 0 до 1
 R^2 = 0    - Модель не объясняет вариацию зависимой переменной, т.е. предсказания равны среднему значению.
 R^2 = 1    - Модель идеально объясняет вариацию зависимой переменной, т.е. предсказания совпадают с фактическими значениями.
 0 < R^2 < 1 - Указывает процент общей вариации, который объясняется моделью. Например, \( R^2 = 0.75 \) означает,
               что 75% вариации зависимой переменной объясняется независимыми переменными.
 Высокий  R^2 не всегда означает хорошую модель. - Это может быть связано с ПЕРЕОБУЧЕНИЕМ или некорректным выбором модели.

 Нормализация данных - это процесс приведения различных признаков (например, высота, вес, температура) к общему масштабу,
 чтобы они лучше работали в моделях.
 Нормализация помогает справедливо учитывать все признаки, чтобы они не «перекрывали» друг друга из-за разницы в диапазонах значений.
 В общем, нормализация делает данные более «равноправными», что помогает моделям машинного обучения работать лучше и точнее.

 В библиотеке **scikit-learn** (sklearn) представлены несколько методов нормализации данных. Вот основные из них:

 Min-Max Scaling:
 класс MinMaxScaler   - Приводит данные к заданному диапазону, обычно [0, 1].

 Standardization (Z-score Normalization):
 класс StandardScaler - Приводит данные к нулевому среднему значению и единичному стандартному отклонению.

 Robust Scaling:
 класс RobustScaler   - Использует медиану и интерквартильный размах, что делает его устойчивым к выбросам.

 Normalizer:
 класс Normalizer     - Нормализует каждую строку (образец), приводя к единичной  длине. Полезно для текстовых данных
 и задач, где важно направление вектора.

 Power Transformer:
 классы PowerTransformer или QuantileTransformer - Применяет различные степени и логарифмические трансформации для
 приведения данных к более нормальному распределению.

 Квантиль — это значение, которое делит набор данных на заданное количество равных частей.

 Выбросы в машинном обучении (ML) — это аномальные или экстремальные значения, Очень Высокие или Очень Низкие значения

 Временные ряды(динамический ряд) — это наборы данных, где каждая точка данных связана с определенным моментом времени.
 Временной ряд – упорядоченный набор данных.
 TSFresh - это библиотека Python с открытым исходным кодом и мощными функциями извлечения данных временных рядов. <-----


 # Скользящее окно – это фиксированный отрезок данных, который шаг за шагом перемещается по последовательности,
 # обрабатывая каждый участок (например, считая среднее).

 Данные: [1, 2, 3, 4, 5], окно = 3.

 Шаги:
 [1, 2, 3] → считаем
 [2, 3, 4] → считаем
 [3, 4, 5] → считаем

 Применение: анализ временных рядов, обработка сигналов, NLP (n-граммы), компьютерное зрение.


 Regressors:

 -- LightGBM основан на деревьях решений. --
 LightGBM создает деревья решений, которые растут по листам, что означает, что при заданном условии разделяется только
 один лист, в зависимости от усиления. Деревья с большим числом листьев иногда могут переобучаться, особенно с
 небольшими наборами данных. Ограничение глубины дерева(max_depth)  может помочь избежать переобучения.

 LightGBM - Быстрая, высокая производительность для больших наборов данных, использующая метод ГРАДИЕНТНОГО БУСТИНГА на основе гистограммы.
 Catboost                           - ГРАДИЕНТНЫЙ БУСТИНГ на деревьях решений. Много чего из коробки Акцент на КАТЕГОРИАЛЬНЫЕ переменные
 XGBoost: Scikit-learn              - Деревья с ГРАДИЕНТНЫМ УСИЛЕНИЕМ
 LinearRegression:  SKlearn         - подбирает линейную модель с коэффициентами
 RandomForestRegressor:  SKlearn    - использует усреднение для повышения точности прогнозирования и контроля переобучения
 KNN,  KNeighborsRegressor:sklearn  - Метод K-ближайших соседей, Цель прогнозируется путем локальной интерполяции целей
 AdaBoostRegressor: sklearn         - Она принимает обучающие данные и метки в качестве параметров.
 Sarima                             - определяет, как сезонный, так и несезонный компонент
 ARIMA                              - модель авторегрессии скользящего среднего
 Prophet                            - прогнозирования временных рядов, с Учётом (тренд, сезонность, праздники и т.д.)


 Деревья решений - Это структура данных, которая представляет собой последовательность испытаний (разделений) для
 принятия решений на основе признаков (фич) входных данных.
 Деревья решений могут сильно переобучаться, особенно если глубина дерева велика.

 Узлы НЕ Имеющие дочерних узлов - ЛИСТЬЯ ( по аналогии с листьями настоящего дерева)

 Преимущества дерева решений:
 - Интуитивно понятны
 - Обработка как числовых, так и категориальных данных: Деревья решений могут работать с различными типами данных.
 - Не требуют нормализации данных: Деревья не чувствительны к масштабам признаков.

 Недостатки дерева решений:
 - Переобучение: Глубокие деревья могут хорошо подстраиваться под обучающие данные, приводя к плохой обобщающей
  способности на новых данных.
 - Сложность: При сильной разбросанности данных могут возникать большие и сложные деревья, которые трудно интерпретировать.
 - Неустойчивость: Небольшие изменения в данных могут привести к значительным изменениям в структуре дерева.


 -- Случайный лес (Random Forest) --

 - Принцип работы: Случайный лес создаёт множество деревьев решений (обычно с использованием метода бутстрэпа), каждый
   из которых обучается на случайно выбранном подмножестве данных. Затем предсказания всех деревьев объединяются
   (например, с помощью голосования для классификации или усреднения для регрессии) для получения окончательного результата.

 - Обучение: Деревья в случайном лесу обучаются независимо друг от друга, что делает модель параллельной. Это позволяет
   значительно ускорить процесс обучения, особенно на больших наборах данных.

 - Устойчивость к переобучению: Случайный лес обычно устойчив к переобучению благодаря использованию подвыборок данных
   и случайному выбору признаков при каждом разбиении. Это делает его менее чувствительным к шуму в данных.

 - Применение: хорошо работает на различных типах задач и часто используется как "базовая" линия для сравнения с другими моделями.


 -- Градиентный бустинг (Gradient Boosting) GB --

 - Принцип работы: Градиентный бустинг строит деревья последовательно. Каждое новое дерево обучается на ошибках (остатках)
   предыдущих деревьев с целью их коррекции. Это означает, что каждое следующее дерево пытается улучшить предсказания модели.

 - Обучение: Деревья обучаются последовательно, где каждое следующее дерево зависит от предсказаний предыдущих. Этот подход
   делает градиентный бустинг последовательным, что может увеличить время обучения, но позволяет достигать высокой точности.

 - НЕТ Устойчивости к переобучению: Градиентный бустинг может быть более подвержён переобучению, особенно если не
   контролировать гиперпараметры (например, количество деревьев, глубину деревьев и скорость обучения). Тем не менее,
   существуют методы регуляризации, такие как уменьшение скорости обучения, обрезка деревьев и ранняя остановка.

 - Применение: Градиентный бустинг часто используется в соревнованиях по машинному обучению, таких как Kaggle, поскольку
   при правильной настройке он может обеспечивать выдающиеся результаты.


 Градиентный бустинг - в основном являются деревья решений.
 В отличие от случайного леса, градиентный бустинг строит последовательность деревьев, в которой каждое дерево
 пытается исправить ошибки предыдущего.
 AdaBoost выявляет их на основании высоких значений весов, а GB – на основании градиентов функции потерь.

 Градиентный бустинг — это мощный метод ансамблевого обучения, который используется для решения задач регрессии и
 классификации. Он строит модель заранее определённого типа (обычно деревья решений) и последовательно улучшает её,
 добавляя новые модели, которые корректируют ошибки предыдущих.


 Недостатки градиентного бустинга:
 - Время обучения: Обычно обучение моделей градиентного бустинга занимает больше времени по сравнению с другими методами,
 такими как случайный лес.
 - Переобучение
 - Сложность настройки: Необходимость в умении правильно подбирать гиперпараметры для достижения наилучших результатов.


 -- Градиентный бустинг (Gradient Boosting) GB  vs Случайный лес (Random Forest) --

 ### Случайный лес:
 1. **Принцип работы**:               Создаёт множество независимых деревьев решений и объединяет их предсказания.
 2. **Обучение**:                     Параллельное.
 3. **Устойчивость к переобучению**:  Высокая.
 4. **Параметры**:                    Меньше настроек.
 5. **Производительность**:           Быстрое обучение на больших данных.
 6. **Чувствительность к шуму**:      Менее чувствителен к шуму.

 ### Градиентный бустинг:
 1. **Принцип работы**:               Строит деревья последовательно, каждое следующее исправляет ошибки предыдущих.
 2. **Обучение**:                     Последовательное.
 3. **Устойчивость к переобучению**:  Может быть подвержён переобучению (требует контроля гиперпараметров).
 4. **Параметры**:                    Больше настроек (глубина, скорость обучения и др.).
 5. **Производительность**:           Может медленно обучаться.
 6. **Чувствительность к шуму**:      Более чувствителен к шуму при настройке.

 В контексте машинного обучения **ШУМ** относится к случайным или нерелевантным данным, которые могут искажать или
 усложнять процесс обучения модели. Шум может привести к снижению точности и ухудшению способности модели к обобщению.
 ШУМ - ПЛОХИЕ ДАННЫЕ (например, опечатки в текстовых данных, ошибки измерений и т. д.).

 Главное чтобы не было ПЕРЕОБУЧЕНИЯ!
 Это когда модель настолько хорошо обучена на обучающей выборке данных, что она становится слишком специфичной и в
 результате плохо применимой для НОВЫХ данных.

 «Scikit» и «SkLearn» — это одно и то же!

 Настройка Гиперпараметров:
   - sklearn: GridSearch, RandomizedSearch, BayesSearch.
   - Optuna

   Важные параметры:
   num_leaves - основной параметр, управляющий сложностью модели дерева
   num_iteration - число деревьев (псевдоним n_estimators)
   learning_rate - скорость обучения.
   early_stopping_round - число, определяющее условие останова - если алгоритм не улучшается на протяжении данного количества итераций.
   num_threads/n_jobs - число потоков для обучения.
   max_depth - максимальная разрешенная для деревьев глубина.

 АНСАМБЛЬ МОДЕЛЕЙ/Stacking
 Метод машинного обучения, где несколько моделей обучаются для решения одной и той же проблемы и объединяются
 для получения ЛУЧШИХ результатов

 A/B - тестирование - это метод сравнения двух версий продукта (например, веб-страницы, приложения или рекламного объявления)
 для определения, какая из них лучше работает по заданным метрикам. В процессе тестирования пользователи случайным
 образом делятся на две группы: одной группе показывается версия A, а другой — версия B. Сравниваются результаты
 (например, конверсия, клики, время на странице) для того, чтобы выявить наиболее эффективный вариант. Этот метод
 позволяет принимать обоснованные решения на основе данных и улучшать пользовательский опыт.

 A/B-тестирование - это сравнение двух версий продукта (A и B) на разных группах пользователей, чтобы определить,
 какая работает лучше по выбранным метрикам (конверсия, клики и т. д.). Помогает принимать решения на основе данных

 Многорукие бандиты (multi-armed bandits) — метод, который динамически перераспределяет трафик в пользу более успешных
 вариантов по мере данных, чтобы одновременно учиться и получать максимум результата (меньше “потерь”, чем в классическом A/B).


  -- NP Задачи --
 - Задача относится к классу P, если поиск и проверка ее решения выполняются быстро.
 - Задача относится к классу NP, если ее решение можно быстро проверить, но не обязательно быстро решить.
 - Если можно найти быстрый (с полиномиальным временем) алгоритм для каждой задачи в NP, значит, P = NP.
 - Задача NP является NP-трудной, если ее можно свести к этой задаче.
 - Задача является NP-полной, если она одновременно является и NP задачей, и NP-трудной задачей.

 NP: задачи, решения которых можно проверить за полиномиальное время.
 NP-трудные: задачи, к которым сводятся все задачи из NP.
 NP-полные: задачи, которые являются как NP, так и NP-трудными.

 Полиномиальное время — это время выполнения алгоритма, которое растет как полином относительно размера входных данных.

 Полином — это математическое выражение, представляющее собой сумму одночленов, каждый из которых состоит из переменной,
 возведенной в целую степень, и коэффициента.
 -- END NP Задачи --


 -- NLP --
 Обработка естественного языка (NLP, Natural Language Processing) – это технология машинного обучения, которая дает
 компьютерам возможность интерпретировать, манипулировать и понимать человеческий язык.

 import nltk
 NLTK – это библиотека Python, которую можно использовать в любом приложении для обработки естественного языка - NLP.

 NLTK предлагает удобные инструменты для множества задач NLP: токенизация, стемминг, лемматизация, морфологический
 и синтаксический анализ, а также анализ настроений.

 Перед созданием любого приложения, основанного на обработке естественного языка, нам необходимо обработать данные,
 которые мы используем. Ниже приведены некоторые шаги, которые всегда необходимы при создании приложения NLP:
 Анализ текстовых данных с помощью NLTK и Python:

 - Токенизация: разделение фрагмента текста на токены или слова называется токенизацией.
 Токенизация — это процесс разбиения текста на более мелкие части

 - Удаление стоп-слов: стоп-слова – это самые распространенные слова в любом языке. Нет правильного определения
 игнорируемых слов, вы можете думать об этих словах как о словах, которые используются для создания значимого предложения.
 Например, такие слова, как «the», «is», «a», «as», представляют собой определенный тип стоп-слов, которые необходимо
 удалить из текстовых данных, которые вы используете, иначе это может повлиять на производительность вашей модели.

 - Стемминг — это процесс сведения слов к их основной (корневой) форме, удаляя окончания и суффиксы.
 Это помогает уменьшить сложность текста и улучшить производительность алгоритмов анализа.

 - Лемматизация - В отличие от стемминга, лемматизация сводит слова к их лемме — это более сложный процесс,
 который учитывает морфологический анализ слов. Лемматизация более точно обрабатывает слова, приводя их к словарной форме.

 - Анализ настроений (или сентимент-анализ) в NLTK часто сводится к классификации текста на ПОЗИТИВНЫЙ или НЕГАТИВНЫЙ.



 -- Рекомендательные системы --
 Система рекомендаций — это тип системы машинного обучения, которая предоставляет пользователям персонализированные
 рекомендации на основе их прошлого поведения, предпочтений и шаблонов .


 Для разработки рекомендательных систем используют различные библиотеки и инструменты,
 в зависимости от подхода и архитектуры системы. Вот некоторые из наиболее популярных библиотек и инструментов:

 1. **Scikit-learn** — библиотека для машинного обучения на Python, которая предоставляет множество алгоритмов для
 построения моделей, включая методы, используемые для рекомендаций.

 2. **Surprise** — специализированная библиотека для построения рекомендательных систем. Она поддерживает различные
 алгоритмы, включая матричную факторизацию и коллаборативную фильтрацию.

 3. **LightFM** — библиотека для построения гибридных рекомендательных систем, которая объединяет коллаборативную
 фильтрацию и контентную фильтрацию.

 4. **TensorFlow и Keras** — библиотеки для глубокого обучения, которые можно использовать для создания сложных моделей
 рекомендательных систем (например, нейронные сети).

 5. **PyTorch** — еще одна популярная библиотека для глубокого обучения, используемая для построения рекомендательных
 систем и реализации различных архитектур.

 LightFM — это библиотека для Python, которая поддерживает гибридные рекомендательные системы

 Ранжирование – это сортировка сайтов поисковыми системами в результатах выдачи по соответствующему поисковому запросу.


 -- LLM (Large Language Model) --
 LLM (Large Language Model) — это большие языковые модели, разработанные для обработки и генерации текста на основе
 обширного обучения на текстовых данных. Они могут выполнять задачи, такие как ответ на вопросы, написание текстов,
 перевод, способны понимать и генерировать человеческий язык и многие другие. Они используют архитектуру трансформеров
 для обработки контекста и связи между словами.

 Чтобы использовать LLM:

 1. **Выбор модели**: Определите, какая модель подходит для вашей задачи (например, GPT, BERT и т.д.).
 2. **Доступ к API**: Многие модели доступны через API (например, OpenAI API для GPT).
 3. **Интеграция**: Встраивайте модель в свои приложения для генерации текста или обработки данных.
 4. **Настройка**: При необходимости настраивайте модель под специфические задачи или данные.
 5. **Чат-боты**: Ответы на вопросы и поддержка диaлoгов в реальном времени.
 6. **Перевод**: Автоматический перевод текстов с одного языка на другой.
 7. **Обработка естественного языка (NLP)**: Текстовая классификация, анализ настроений и извлечение информации.

 Использование LLM может значительно улучшить качество обработки языка в ваших проектах.


 LangChain Python - это фреймворк для разработки приложений с языковыми моделями (LLM), который упрощает:

 - Интеграцию с LLM (OpenAI GPT, Anthropic и др.).
 - Построение RAG-систем (достаём и используем внешние данные).
 - Создание цепочек (Chains) из компонентов.
 - Работу с инструментами (API, базы данных, файлы и т.д.).
 Коротко: "Помощник для создания умных приложений с ИИ"


 библиотеки для скачивания видео с Youtube
 pytube     - простая и современная, подходит для большинства задач. Для видео/аудио
 yt-dlp     - мощнее, поддерживает больше сайтов и форматов (лучше для сложных случаев).
 youtube-dl - устаревает, но иногда полезна.  Лучше не использовать, если только нет специфических требований.


 Agile(Еджайл) - гибкий подход к управлению проектами, где работа делится на короткие итерации (спринты).

 DoD (Definition of Done)  - критерии завершения задачи (например: код написан, протестирован, задокументирован).
 DoR (Definition of Ready) - критерии готовности задачи к работе (например: есть описание, оценка, приоритет).

 DOD (Definition of Done)  -  это четкий список критериев, которые должны быть выполнены, чтобы задача считалась завершенной.
 DOT (Definition of Ready) -  это критерии, которым задача должна соответствовать, чтобы её можно было взять в работу
 (например, наличие четкого описания, оценка и т. д.).

 DOD - "что должно быть сделано, чтобы закрыть задачу".
 DOT - "что должно быть, чтобы начать задачу".


 CI/CD — это важный элемент современных подходов к разработке программного обеспечения,
 обеспечивающий эффективное и качественное сопровождение проектов.

 CI — это практика, связанная с регулярной интеграцией изменений кода в общий репозиторий.
 CD — это практика, которая подразумевает, что код, который прошел этапы тестирования в CI, готов к развертыванию на любой среде

 CI/CD объединяет разработку, тестирование и развёртывание приложений.

 Инструменты для CI/CD:
 Существует множество инструментов, которые поддерживают CI/CD процессы, включая:
 - Jenkins
 - GitLab CI/CD
 - Travis CI
 - CircleCI
 - GitHub Actions
 - Azure DevOps и другие.

 Jenkins - это система непрерывной интеграции и доставки (CI/CD), которая автоматизирует процессы сборки,
 тестирования и развертывания программного обеспечения.
 Jenkins — сервер CI/CD: автоматически собирает, тестирует и разворачивает проекты по триггеру (например, при push в Git).

 Legacy проект в контексте разработки программного обеспечения — это проект, который был создан с использованием
 УСТАРЕВШИХ ТЕХНОЛОГИЙ, БИБЛИОТЕК или ПОДХОДОВ.

 Пайплайн (или PIPELINE) — это концепция, используемая в разных областях, таких как разработка программного обеспечения,
 машинное обучение и обработка данных. Основная идея пайплайна заключается в том, что он представляет собой
 последовательность шагов, через которые проходят данные или процесс для достижения конечного результата.

 UI (User Interface) — это пользовательский интерфейс.

 Cookies (куки) - это небольшие фрагменты данных, которые сервер отправляет браузеру и которые браузер сохраняет
 для последующих запросов.

 Файлы cookie — это небольшие текстовые файлы, которые веб-сайты сохраняют на вашем устройстве при посещении. Они нужны для:

 1. **Хранения пользовательских настроек** — запоминают предпочтения пользователя, такие как язык или настройки оформления.
 2. **Аутентификации** — помогают идентифицировать пользователей и сохранять сессии входа.
 3. **Аналитики** — собирают информацию о том, как пользователи взаимодействуют с сайтом, что помогает улучшить его.
 4. **Персонализации контента** — позволяют предлагать пользователям релевантные рекомендации и рекламу.

 В целом, файлы cookie делают веб-серфинг более удобным и персонализированным.


 ДИХОТОМИЯ данных - это разделение данных на две взаимоисключающие группы (например, "да/нет", "истина/ложь", "0/1").
 ДИХОТОМИЯ деление данных на две чёткие категории.
 Примеры: 1) Бинарная классификация в машинном обучении. 2) Разделение пользователей на "активных" и "неактивных".


 1. **Разработка программного обеспечения**: В CI/CD (непрерывная интеграция и непрерывное развертывание) пайплайн
 представляет собой автоматизированный процесс, который проходит через несколько этапов — от написания кода и
 тестирования до развертывания на сервере.

 2. **Машинное обучение**: В этой области пайплайн состоит из последовательности этапов, таких как подготовка данных,
 выбор признаков, обучение модели и оценка ее производительности. Каждая из этих стадий последовательно передает
 свои результаты следующей.

 3. **Обработка данных**: В системах обработки данных, таких как ETL (извлечение, преобразование, загрузка), пайплайн
 описывает процесс перемещения и преобразования данных из одной системы в другую.

 Основная цель пайплайнов — автоматизировать и стандартизировать процессы, чтобы сделать их более надежными и эффективными.


 Кэширование данных — это процесс хранения результатов выполнения функций и запросов во временном хранилище (кэше)
 с целью ускорения повторного обращения к ним. Вместо того чтобы заново выполнять те же операции, можно просто получить
 результат из кэша, что существенно экономит время и ресурсы.

 Кэш является потокобезопасным.

 LRU (Least Recently Used) — алгоритм вытеснения из кэша: удаляется элемент, который дольше всего не использовался
 (“наименее недавно использованный”).

 Типы кэша:
 - По месту: клиентский, серверный, CDN, кэш БД.
 - По виду: локальный (на одном сервере) и распределённый (Redis/Memcached).
 - По “температуре”: cold (пустой), warm (частично), hot (хорошо заполнен).
 - Разогрев/прогрев (warming): заранее заполнить кэш популярными данными.

 CDN (Content Delivery Network) — сеть распределённых серверов, которая кэширует и быстро отдаёт контент
 (картинки, видео, файлы) с ближайшего к пользователю узла, снижая задержку и нагрузку на основной сервер.


 Модели облачных услуг:
 - IaaS (Infrastructure as a Service) — инфраструктура: виртуальные серверы, сети, хранилища. Вы управляете ОС и настройками.
 - PaaS (Platform as a Service) — платформа для запуска приложений. Вы пишете код, а среда/развёртывание на облаке.
 - FaaS (Function as a Service) — функции по событию: загружаете код функции, облако само запускает/масштабирует;
   оплата за вызовы/время (AWS Lambda, Azure Functions).
 - SaaS (Software as a Service) — готовое приложение как сервис (почта, CRM).
 Идея: чем выше уровень (SaaS/FaaS), тем больше абстракции и меньше управления; чем ниже (IaaS),
 тем больше контроля (сети обычно относятся к IaaS).

 Облачные сетевые технологии Azure — это сервисы для построения сети, подключения и защиты ресурсов в облаке.


 Модель C4 — это способ описания архитектуры ПО с помощью диаграмм на 4 уровнях детализации:

 - Context — система и её окружение (пользователи, другие системы)
 - Container — основные “контейнеры” внутри (приложения/сервисы/БД)
 - Component — компоненты внутри контейнера
 - Code — уровень кода (классы/модули), опционально

 Коротко: набор понятных диаграмм от общего к частному.


 2PC (двухфазный коммит) — протокол, чтобы несколько баз/сервисов зафиксировали транзакцию атомарно (“все или никто”) в 2 шага:

 - prepare/голосование — все готовы?
 - commit/abort — если все “да” → commit, иначе → abort.

 Где применяют 2PC:
 распределённые транзакции между несколькими БД/ресурсами, когда нужно строго “всё или ничего” (обычно внутри одного контура/кластера).

 Где НЕ применяют:
 микросервисы и междатацентровые сценарии, высоконагрузка и нестабильная сеть, долгие бизнес-процессы - из-за блокировок,
 задержек и риска зависаний.


 Пиринг (peering) — это прямое соединение двух сетей для обмена трафиком напрямую, без “посредников”,
 чтобы получить меньше задержку, выше скорость и часто дешевле.
 P2P (Peer-to-Peer) — сеть без центрального сервера, где узлы (пиры) напрямую обмениваются данными и могут быть одновременно клиентами и серверами.

 ELK — стек для логов и аналитики:
 - Elasticsearch — хранение/поиск и анализ данных,
 - Logstash — сбор и обработка данных из источников,
 - Kibana — визуализация (дашборды/графики).
 - Elastic Stack — расширенная версия ELK (добавляет другие компоненты).

 QPS (Queries Per Second) — число запросов в секунду, которое получает или обрабатывает система (показатель нагрузки/производительности).

 Нефункциональные требования — требования к качествам системы (как работает), а не к функциям:
 скорость, доступность, безопасность, надёжность, масштабируемость.

 Метрика — измеряемый показатель (число), например QPS, задержка, ошибки, конверсия.
 Дашборд — экран/страница с графиками и метриками для мониторинга.
 Сводное (summary) — краткий итог по данным: ключевые числа и выводы (без деталей).

 Полносвязная сеть (full mesh) — топология, где каждый узел напрямую соединён с каждым другим.
 Плюсы: максимальная надёжность и минимум задержек; минусы: много соединений и высокая стоимость/сложность.

 YARN (Yet Another Resource Negotiator) — компонент Hadoop для управления ресурсами кластера и планирования задач
 (выдаёт CPU/память приложениям).

 OpenCV — это популярная библиотека для обработки и анализа изображений/видео.

 S3 Хранилище - S3 — это объектное хранилище: данные лежат в бакетах как объекты с ключами и метаданными.
 Доступ через HTTP API (PUT/GET/DELETE/LIST), ‘папки’ — это часть ключа. Права задаются политиками/IAM
 (часто через pre-signed URL). Для больших файлов есть multipart upload, можно включить versioning, lifecycle и шифрование.

 S3 доступен по HTTP API. Мы обычно работаем через SDK: в Python — boto3 (AWS) или minio (MinIO/S3-совместимые),
 они внутри делают те же PUT/GET/LIST/DELETE запросы и подпись.

 boto3 — это официальная Python-библиотека (SDK) для AWS, через которую из Python удобно работать с сервисами AWS: S3, DynamoDB, EC2 и т.д.

 ### ГЕОПОИСК
 geo2day geofabrik Overpass osm  osm.pbf

 Ollama — это программа, которая позволяет запускать большие языковые модели (LLM) локально на вашем компьютере
 (без облака), через простые команды и локальный API.
 ### ollama запуск сервера
 ollama serve

 PuTTY — это терминальный эмулятор, позволяющий устанавливать SSH-соединение с удаленными серверами.
 SSH (Secure Shell) С С АШ — это протокол для безопасного удаленного доступа к другим компьютерам, позволяющий шифровать
 данные и обеспечивать защищенное подключение.

 -- Через терминал подключение по SSH к СЕРВЕРУ --
 SSH tech_deleloper14@tvlds-0126      # По ХОСТУ
 SSH tech_deleloper14@10.10.207.111   # По ПРОКСИ

 Активировать venv  (Виртуальное окружение)        .\venv\Scripts\activate
 ДЕ-Активировать venv  (Виртуальное окружение)     .\venv\Scripts\deactivate
 Чтобы деактивировать активное виртуальное окружение, просто введите команду deactivate в командной строке.   <-----


 Notepad++
 Plugins -> JsonFormat                  # Чтобы преобразовать в удобный формат
 Plugins -> JsonFormat -> JsonViewer    # Чтобы добавить слева табличку

 SyntaxWarning: invalid escape sequence - ошибка связанная с многострочной строкой используем r перед строкой r''' '''



 -- ОПИСАНИЕ ДЛЯ SCP --
 Кратко: на сервер “залить” = скопировать файлы с ПК на сервер, обычно через scp (по SSH).
“Залить локально на ПК” = скачать с сервера на ПК (тоже scp, но наоборот).  ВНИМАТЕЛЬНО НА СЛЭШИ  \\\ /// Windows/Linus

 На сервере (Linux) всегда /
 На Windows обычно \, но в scp допускается и / (часто удобнее).

 1) ПК → Сервер (залить на сервер)

 # Один файл:
 scp "C:\path\file.py" root@IP:/путь/на/сервере/

 # Папку целиком:
 scp -r "C:\path\folder" root@IP:/путь/на/сервере/

 2) Сервер → ПК (залить/скачать на ПК)

 # Один файл:
 scp root@IP:/путь/на/сервере/file.py "C:\path\куда\скачать\"

 #Папку целиком:
 scp -r root@IP:/путь/на/сервере/folder "C:\path\куда\скачать\"



 --- ЗАМЕТКИ РАБОТА!!! ---


 # ПРОСМОТРЕТЬ СВОБОДНЫЕ ПОРТЫ Linux
 sudo netstat -tuln | grep LISTEN

 # Прослушать порт
 sudo lsof -i :8000

 # Убить процессы
 kill -9 sudo kill -9 36815 107147

 sudo kill -9 $(sudo lsof -t -i:8000)

 # ТЕРМИНАЛ SSH ПОДКЛЮЧЕНИЕ
 ssh root@84.32.34.126  # terminal

 # ТЕРМИНАЛ ПОСМОТРЕТЬ ИСТОРИЮ КОМАНДА
 history

 # ЧТОБЫ КОМАНДА НЕ СОХРАНЯЛАСЬ В ТЕРМИНАЛЕ СТАВИМ ПЕРЕД КОМАНДОЙ ПРОБЕЛ    <--------

 # ЗАКИНУТЬ НА ПРОЕКТ РЕКУРСИВНО!
 scp -r "C:\Users\a.yora\Desktop\PROJECT_54\*" root@188.214.128.52:/root/alex_project/


 ### КОМАНДЫ ДЛЯ SSH-СЕРВЕР

 ### ЗАЛИТЬ НА SSH-СЕРВЕР

 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe\generate.py" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe\pipe.py" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe\address_constants.py" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\MIID\validator\cheat_detection.py" root@46.175.144.102:/root/alex_project/sn54-miner-main/MIID/validator

 scp -r "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe\bank_last" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe/
 scp -r "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe\bank_first" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe/


 ### НУЖНЫЕ КОМАНДЫ
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe_nominatim\clear_duplicates_3.py" root@176.117.79.67:/root/alex_project/sn54-miner-main/pipe_nominatim
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe_nominatim\clear_duplicates_1.py" root@176.117.79.67:/root/alex_project/sn54-miner-main/pipe_nominatim
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe_nominatim\clear_duplicates_4_for.py" root@176.117.79.67:/root/alex_project/sn54-miner-main/pipe_nominatim
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe_nominatim\clear_duplicates_5_for.py" root@176.117.79.67:/root/alex_project/sn54-miner-main/pipe_nominatim



 ### ВСЕ ОБНОВИТЬ НА СЕРВЕРЕ ИЗ ЛОКАЛЬНОЙ ПАПКИ MIID
 scp -r "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\MIID\*" root@176.117.79.67:/root/alex_project/sn54-miner-main/MIID/

 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\MIID\validator\reward.py"  root@176.117.79.67:/root/alex_project/sn54-miner-main/MIID/validator
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\verify\verify.py"  root@176.117.79.67:/root/alex_project/sn54-miner-main/verify
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\MIID\utils\config.py"  root@176.117.79.67:/root/alex_project/sn54-miner-main/MIID/utils
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\MIID\mock.py"  root@176.117.79.67:/root/alex_project/sn54-miner-main/MIID/
 scp "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\MIID\validator\forward.py"  root@176.117.79.67:/root/alex_project/sn54-miner-main/MIID/validator


 # ДОБАВИТЬ НА СЕРВЕР ПАПКУ ВМЕСТЕ С ФАЙЛАМИ
 scp -r "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe\bank_first" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe/
 scp -r "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA\pipe\bank_last" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe/

 ### END ЗАЛИТЬ НА SSH-СЕРВЕР


 # Скачать из удаленного сервера shh  ОТКУДА И КУДА
 scp -r root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe "C:\Users\a.yora\Desktop\SHH_PROJECT"
 scp -r root@46.175.144.102:/home/sn54-miid/MIID "C:\Users\a.yora\Desktop\STEPA_CODE"
 scp -r root@46.175.144.102:/home/sn54-miid/MIID "C:\Users\a.yora\Desktop\GOOD_SHH_STEPA"

 scp -r root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe "C:\Users\a.yora\Desktop\OLD_CODE_SSH"
 scp -r root@46.175.144.102:/root/alex_project/sn54-miner-main "C:\Users\a.yora\Desktop\PROJECT_54"


 # Перезаписать файл добавить из моего пк на ssh-сервер
 scp -r "C:\Users\a.yora\Desktop\pipe\*" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe/
 scp -r "C:\Users\a.yora\Desktop\SHH_PROJEC_2\pipe\*" root@46.175.144.102:/root/alex_project/sn54-miner-main/pipe/
 scp -r "C:\Users\a.yora\Desktop\STEPA_CODE\pipe" root@46.175.144.102:/root/alex_project/sn54-miner-main/

 # НОВЫЙ СЕРВЕР
 scp -i $env:USERPROFILE\.ssh\id_ed25519 -r "C:\Users\a.yora\Desktop\PROJECT_54_1\*" root@93.115.26.35:/root/alex_project/

 ### ЗАПУСК PYTHON-ФАЙЛА ЗАПУСКАЕМ ИЗ ТОЙ ЖЕ ПАПКИ С ВКЛ VENV
 python3 CHECK_ALL_BUILDING_6_EASY_FAST_3_PLUS_2_SCORE_CLONE_1.py
 python3 clear_duplicates_3.py

 # Посмотреть содержимое файла
 cat generate.py

 # Просмотр первых/последних строк:
 head -n 30 generate.py
 tail -n 30 generate.py

 # Просмотр с прокруткой (less) - рекомендуется:
 less generate.py

 # СКАЧАТЬ КОШЕЛЬКИ ИЗ УДАЛЕННОГО СЕРВЕРА НА ПК!
 scp -r root@46.175.144.102:/root/.bittensor/wallets "C:\Users\a.yora\Desktop\SHH_PROJECT\wallets"

 ###  На новом сервере нужно добиться точно такой же структуры:
 /root/.bittensor/wallets/ck_dg_12412_5

 ### КОМАНДА ДЛЯ ПРОВЕРКИ
 ls -lah /root/.bittensor/wallets


 ### СКОЛЬКО ВЕСИТ ПАПКА
 du -sh /root/.bittensor/miners

 ### СКОЛЬКО СВОБОДНО МЕСТА НА ДИСКЕ
 df -h


 ### НАСТРОЙКА СЕРВЕРА VPS

 ### ЕСЛИ ВОТ ТАКАЯ ОШИБКА

 The virtual environment was not created successfully because ensurepip is not
 available.  On Debian/Ubuntu systems, you need to install the python3-venv
 package using the following command.

     apt install python3.10-venv

 You may need to use sudo with that command.  After installing the python3-venv
 package, recreate your virtual environment.

 Failing command: /root/alex_project/sn54-miner-main/miner_env/bin/python3

 [ERROR] Failed to create virtual environment

 ### ДЕЛАЕМ ТАК!
 apt update
 apt install -y python3.10-venv

 ### END НАСТРОЙКА СЕРВЕРА



 Авторы книг: Марк Лутц, Дасти Филлипс, Дэвид Бизли, Кент Бек, Лучано Рамальо, Мартин Фаулер, Ден Бейдер, Фаулер Мэттью,
 Фило Владстон Феррейра, (Миша Горелик, Йен Освальд), Адитья Бхаргава, Роберт Мартин, Мартин Клеппман,
 Джеффри Д.Ульман/Дженнифер Уидом, Эрик Чоу, Алекс Сюй, Чжиюн Тань

 Можно почитать:  Эндрю Таненбаум


 Дэн Бейдер                       - Чистый код                                              2
 Мэттью Фаулер                    - Asyncio и конкурентное программирование на Python       2
 Владсон Феррейра Фило            - Теоретический минимум по COMPUTER SCIENCE               2
 Миша Горелик (она) , Йен Освальд - Высокопроизводительные Python-приложения                1
 Дэвид Бизли, Брайан К. Джонс     - Python Книга рецептов                                   1
 Адитья Бхаргава                  - Грокаем Алгоритмы Второе Издание                        1
 Лусиану Рамальо                  - Python К вершинам мастерства  Второе Издание            1
 Дэвид Бизли                      - Python Исчерпывающее руководство                        1
 Стивен Ф. Лотт, Дасти Филлипс    - Объектно-оринтированный Python                          1
 Роберт Мартин                    - Чистый код                                              1
 Мартин Клеппман                  - Высонагруженные приложения  КАБАНЧИК                    1
 Бен Стопфорд                     - Проектирование событийно-ориентированных систем Kafka   1
 Джеффри Д.Ульман/Дженнифер Уидом - Введение в системы баз данных                           1
 Эрик Чоу                         - Python для сетевых инженеров  Третье Издание            1
 Алекс Сюй                        - System Design. Подготовка к сложному интервью           1
 Чжиюн Тань                       - System Design: пережить интервью                        1

































































"""