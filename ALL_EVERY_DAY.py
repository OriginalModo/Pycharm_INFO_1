"""

 Самое Важное!!!
 Всегда ДУМАТЬ! перед тем, как что-либо сделать, необходимо всё тщательно обдумать

 Радоваться Жизни  Радоваться разным мелочам

 Осторожность!   Быть осторожным, Думать о последствиях
 Ценить:         Ценить то что есть и стремиться к лучшему, Ценить сегодняшний день и брать МАКСИМУМ
 Быть проще:     Ко всему относиться Проще и Спокойнее без Волнения
 Слушать Других: Прислушиваться к мнению других людей они могут быть правы  И делать выводы
 Время:          Тайм-менеджмент   Грамотное распределение времени, Контроль Времени, Правильно раставлять Приоритеты
 Уверенность:    Быть уверенным в себе НО Оценивать свои силы!
 Развития:       Развиваться, Учиться, учиться и ещё раз - учиться, Саморазвитие
 Не Надеяться:   Надеяться только на себя
 Контроль:       Быть менее Эмоциональным, Совладать с Эмоциями, Контролировать свои эмоции в любой ситуации
 Внимательность: Быть Внимательным
 Спокойствие:    Быть Спокойнее, Перестать Нервничать , Быть Расслабленным, Не Злиться на себя и на других
 Режим:          Правильный Сон, Пить Воду
 Зарядка:        Бег, Тренировки, Стойка на Голове
 Тельце в тепле: НЕ переохлаждаться

 Молчание золото:  Лучше промолчать, чем сказать и потом жалеть о том, что сказал
 Соломон:          Все пройдёт, и это тоже пройдёт
 Вообще это замечательный подход: осознать, что проблема не такая уж и проблема, и вполне решаема.
 Кто ищет-тот всегда найдет!
 Искать Другие способы
 Не спеши, а то успеешь...   Успеешь, но не туда куда хотел...
 Подумай, нужно ли тебе ЭТО и для Чего
 Надо принимать вещи такими, как они есть, и пользоваться ими с наибольшей для себя выгодой.
 Если научиться принимать вещи как они есть, страдание исчезнет.
 У каждого свои недостатки


 Примерные цифры + у каждого свои способности! Миф
 Пирамида Обучения Дейла или Конус опыта Дейла:
 Мы запоминаем в среднем:
 10% того, что читаем,
 20% того, что видим,
 30% того, что слышим,
 50% того, что видим и слышим,
 70% того, что обсуждаем с другими людьми
 90% того, что делаем.

 ЧТЕНИЕ ВСЛУХ для запоминания лучше чем про себя.
 ЧТЕНИЕ ВСЛУХ - увеличению словарного запаса, развитию памяти и повышению грамотности человека.
________________________________________________________________________________________________________________________

 Python является МУЛЬТИПАРАДИГМЕННЫМ языком программирования, поддерживающим:
 Императивное                                                - Инструкции должны выполняться последовательно.
 Процедурное                                                 - Последовательно выполняемые операторы можно собрать в подпрограммы.
 Структурное                                                 - Представление программы в виде иерархической структуры блоков.
 Объектно-ориентированное программирование                   - Набор объектов, взаимодействующих друг с другом
 Метапрограммирование                                        - Метаклассы являются основным инструментом
 Функциональное программирование(заточено под многопоточку)  - в центре внимания которой находятся функции.
 Асинхронное программирование                                - Несколько задач выполняются параллельно и независимо друг от друга

 Python — мультипарадигменный язык программирования со СТРОГОЙ ДИНАМИЧЕСКОЙ ТИПИЗАЦИЕЙ
 СТРОГАЯ ТИПИЗАЦИЯ - означает, что операции между разными типами данных требуют явного их преобразования.

 print(int('10') + 10)                 # -> 20
 print(dict(a=1, b=2) + list([1, 2]))  # -> TypeError: unsupported operand type(s) for +: 'dict' and 'list'

 ДИНАМИЧЕСКАЯ ТИПИЗАЦИЯ - при котором переменная связывается с типом в момент присваивания значения,
 а не в момент объявления переменной.

 a = 10
 print(a)  # -> 10
 a = set((1, 2))
 print(a)  # -> {1, 2}

 Статический прием типизации данных устанавливает тип переменной в процессе компиляции,
 Динамический — во время работы программы.
 Статически типизированный язык программирования проверяет переменную и присваивает ей тип, который в дальнейшем нельзя изменить

 в Python выражения (x) и x Обычно означают одно и тоже
 x = 5
 print((x))  # -> 5
 print(x)    # -> 5


 --- Системы счисления. Как работают ---

 # Пример с числом 4321  в 8 системе счисления    Восьмеричная система (основание 8)
 # 4321 / 8 = 540, остаток 1
 # 540 / 8 = 67, остаток 4
 # 67 / 8 = 8, остаток 3
 # 8 / 8 = 1, остаток 0
 # 1 / 8 = 0, остаток 1


 # Пример с числом 4321  в 16 системе счисления   Шестнадцатеричная система (основание 16)
 # 4321 / 16 = 270, остаток 1
 # 270 / 16 = 16, остаток 14 (E в шестнадцатеричной системе)
 # 16 / 16 = 1, остаток 0
 # 1 / 16 = 0, остаток 1


 # Пример с числом 4321  в 2 системе счисления    Двоичная система (основание 2)
 # 4321 / 2 = 2160, остаток 1
 # 2160 / 2 = 1080, остаток 0
 # 1080 / 2 = 540, остаток 0
 # 540 / 2 = 270, остаток 0
 # 270 / 2 = 135, остаток 0
 # 135 / 2 = 67, остаток 1
 # 67 / 2 = 33, остаток 1
 # 33 / 2 = 16, остаток 1
 # 16 / 2 = 8, остаток 0
 # 8 / 2 = 4, остаток 0
 # 4 / 2 = 2, остаток 0
 # 2 / 2 = 1, остаток 0
 # 1 / 2 = 0, остаток 1


 # Пример с числом 4321  в 8 системе счисления
 print(4321-(4321//8*8))
 print(4321//8 - ((4321//8//8)*8))
 print((4321//8//8) - (4321//8//8//8*8))
 print((4321//8//8//8) - (4321//8//8//8//8*8))
 print((4321//8//8//8//8) - (4321//8//8//8//8//8))

 По умолчанию система счисления десятичная, но можно задать любое основание от 2 до 36 включительно.
 Системы счисления:
 int('0b1010', base=2)  # -> 10   int('0b1010', 2)  # -> 10              bin(10)    # -> 0b1010
 int('0o12', base=8)    # -> 10   int('0o12', 8)    # -> 10              int('10')  # -> 10
 int('10', base=10)     # -> 10   int('10', 10)     # -> 10              oct(10)    # -> 0o12
 int('0xa', base=16)    # -> 10   int('0xa', 16)    # -> 10              hex(10)    # -> 0xa

 --- END Системы счисления. Как работают ---


 -- Блоки Памяти --
 Блоки памяти в компьютерах используются для хранения битов. Это могут быть переменные в вашей программе, а могут быть
 пиксели изображения.
 Таким образом, абстаркция блока памяти - ЭТО РЕГИСТРЫ НА ВАШЕЙ МАТЕРИНСКОЙ ПЛАТЕ, ОПЕРАТИВНАЯ ПАМЯТЬ, ЖЕСТКИЙ ДИСК.
 Одно из основных различий между всеми этими типами блоков памяти заключается в скорости,
 с которой они могут ЧИТАТЬ/ЗАПИСЫВАТЬ данные.

 Например, большинство блоков памяти работает намного лучше, когда читает один большой кусок данных, а не множество
 маленьких (это называется ПОСЛЕДОВАТЕЛЬНЫМ ЧТЕНИЕМ И СЛУЧАЙНЫМИ ДАННЫМИ)

 Скорость чтения/запись                 Размеры
 1) Кэш L1/L2/L3/L4                     1) Обычный жесткий диск (HDD)
 2) ОЗУ                                 2) Твердотельный жесткий диск (SSD)
 3) Твердотельный жесткий диск (SSD)    3) ОЗУ
 4) Обычный жесткий диск (HDD)          4) Кэш L1/L2/L3/L4

  -- END Блоки Памяти --


 Эвристический метод, или просто эвристика, — это метод, который приводит к решению, не гарантируя, что оно — лучшее или оптимальное

 Приближенное решение: ЭВРИСТИЧЕСКИЕ АЛГОРИТМЫ часто находят решения, которые достаточно хороши для практических нужд,
 но не обязательно являются оптимальными.

 ЛИНЕЙНЫЙ АЛГОРИТМ — алгоритм, в котором вычисления выполняются строго последовательно.
 ЖАДНЫЙ АЛГОРИТМ   — алгоритм, заключающийся в принятии локально оптимальных решений на каждом этапе.

 От низкого к высокому вы можете классифицировать языки следующим образом:
 Машинный код -> Язык ассемблера -> Скомпилированный язык -> Интерпретируемый язык

 Парадигма программирования — это классификация, стиль или способ программирования.

 ДВЕ ОСНОВНЫЕ ПАРАДИГМЫ: ИМПЕРАТИВНАЯ и ДЕКЛАРАТИВНАЯ.
 Императивный подход описывает, каким образом ты что-то делаешь.    ->  Java, Python, JavaScript, C, C++, ...
 Декларативный описывает, что именно ты делаешь.                    ->  SQL, HTML, CSS


 Переменные(variables) в Python — это именованные ссылки на объекты, которые хранятся в памяти компьютера.
 Переменные связываются с объектами ТОЛЬКО после создания объектов.    <-----
 class Gizmo:
     def __init__(self):
         print(f'Gizmo id {id(self)}')

 x = Gizmo()       # -> Gizmo id 1818795939984

 # Создается объект Gizmo   НО!  Переменная y так и НЕ будет создана
 y = Gizmo() * 10  # -> Gizmo id 2153632369872
 # TypeError: unsupported operand type(s) for *: 'Gizmo' and 'int'

 В Python присваивание происходит СПРАВА НАЛЕВО. Сначала вычисляется значение правой части
 (объект создается или извлекается), а затем это значение связывается с переменной слева.
 Всегда сначала читайте ПРАВУЮ часть, ту, где объект создается или извлекается                    <-----


 Одинарный символ подчеркивания _:
 Иногда используется в качестве имени временных или незначительных переменных («неважных»). Кроме того, он представляет
 результат последнего выражения в сеансе интерпретатора REPL Python.

 print([1 for _ in 'abc'])  # -> [1, 1, 1]                                  # Помечать можно функции   <-----
                                                                            def _():
 # Можно вторым или первым или любым так использовать _                         pass
 for i, _ in enumerate([1, 2, 3]):
     print(i, end=' ')  # -> 0 1 2


 # Кстати так тоже можно создавать функции  С такими отступами:                                        <-----
 def                func(lst)               :
                    pass


 # Создание ПУСТЫХ Функций/Классов    РАЗНЫЕ СПОСОБЫ
 def function(): pass     class Foo(): pass
 def function(): ...      class Foo(): ...

 def function():          class Foo():
    '''docstring'''           '''docstring'''


 Аннотация(annotation) - подсказка типа(type hint) - подсказки типов данных в Python.   import typing
 def my_func(a: int, b: str) -> None: pass

 PEP 8 - является официальным стандартом написания кода на Python. Этот документ содержит рекомендации и правила.
 Flake8 — это инструмент для проверки кода на соответствие стандартам стиля PEP 8 и поиска ошибок в Python.
 Mypy — статический анализатор типов для Python, который позволяет находить ошибки несоответствия типов в коде.
 Нет, `mypy` не ускоряет скорость работы кода в Python
 Модуль dataclasses - использует type hint

 @dataclasses.dataclass(*, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True,
 kw_only=False, slots=False, weakref_slot=False)

 В Python `dataclasses` — это модуль, который упрощает создание классов, в основном используемых для хранения
 данных. Декоратор `@dataclass` позволяет автоматически генерировать специальные методы
 (такие как `__init__`, `__repr__`, `__eq__` и другие) на основе объявленных полей. Вот краткое описание каждого аргумента:

 1. **init**: Если установлен в `True`, автоматически создается метод `__init__`, который позволяет создавать
  экземпляры класса с заданными полями.

 2. **repr**: Если установлен в `True`, автоматически создается метод `__repr__`, который возвращает строковое
  представление объекта, что особенно полезно для отладки.

 3. **eq**: Если установлен в `True`, автоматически создается метод `__eq__`, который определяет, как сравниваются
  экземпляры класса на равенство.

 4. **order**: Если установлен в `True`, автоматически создаются методы для упорядочивания экземпляров класса
  (`__lt__`, `__le__`, `__gt__`, `__ge__`), позволяя использовать операторы сравнения.

 5. **unsafe_hash**: Если установлен в `True`, создается метод `__hash__`, даже если класс содержит изменяемые поля.
  Это может привести к непредсказуемому поведению, если объект изменяется после добавления в множество или как ключ в словаре.

 6. **frozen**: Если установлен в `True`, класс становится неизменяемым (immutable), что означает, что после его
  создания нельзя изменять его поля.  ЭК НЕ могут изменять поля

 7. **match_args**: Если установлен в `True`, позволяет использовать новую конструкцию `match`
  (введенную в версии Python 3.10) для сопоставления с образцом (pattern matching) с аргументами класса.

 8. **kw_only**: Если установлен в `True`, все поля класса должны быть указаны как именованные аргументы при создании
  экземпляра, что делает вызов конструктора более явным.

 9. **slots**: Если установлен в `True`, используется механизм `__slots__`, который может уменьшить размер экземпляров
  классов и сделать доступ к атрибутам немного быстрее, но не поддерживает динамическое добавление атрибутов.

 10. **weakref_slot**: Этот параметр не является стандартным в Python до версии 3.11. Если он будет добавлен, ожидается,
  что он будет работать с `__slots__`, чтобы поддерживать слабые ссылки на атрибуты.

 Эти аргументы предоставляют множество возможностей для настройки поведения классов, которые создаются с помощью `dataclasses`

 # Сделать по умолчанию пустой список  Сравнение __eq__()  уже встроенно в dataclass
 from dataclasses import dataclass, field

 @dataclass
 class Foo:
     n: int
     s: str = 'a'
     items: list[str] = field(default_factory=list)  # <-- и всё это - чтобы по умолчанию был пустой список

 f = Foo(1)
 f1 = Foo(1)

 print(f.__dict__)    # -> {'n': 1, 's': 'a', 'items': []}
 print(f.__eq__(f1))  # -> True
 print(f == f1)       # -> True

 ff = Foo(11111)
 ff1 = Foo(1)
 print(ff.__eq__(ff1))  # -> False
 print(ff == ff1)       # -> False

 С Pydantic вы можете определять модели данных с помощью подсказок типов (type hints) Python
 FastAPI - основан на Python 3.6+ type hints**

 Pydantic построен на основе `dataclasses` из стандартной библиотеки Python. Он использует их для создания моделей
 данных, добавляя валидацию и аннотации типов.

 1. **Валидация данных**: Проверяет, соответствуют ли данные заданным типам и требованиям.
 2. **Оптимизация кода**: Упрощает работу с данными, минимизируя ошибки и улучшая читаемость кода.
 3. **Поддержка сериализации/десериализации**: Позволяет легко преобразовывать данные из форматов (например, JSON)
    в Python-объекты и наоборот.
 4. **Сложные структуры**: Поддерживает вложенные модели и сложные типы данных.

 Pydantic часто используется в веб-фреймворках, таких как FastAPI, для работы с запросами и ответами.

 Сравните это с пидантиком (Pydantic), в котором, кажется, думают о людях:

  from pydantic import BaseModel

  class Foo(BaseModel):
      items: list[str] = []



 --- FastAPI быстрее работает, чем Django ---
 FastAPI быстрее работает, чем Django, по нескольким причинам:

 1. **Асинхронность**: FastAPI основывается на асинхронном программировании, что позволяет обрабатывать множество
 запросов одновременно. Это особенно полезно для I/O-ориентированных приложений, таких как API, где время ожидания
 ответов от базы данных или внешних сервисов может быть значительным. Django, хотя и поддерживает асинхронность
 начиная с версии 3.1, имеет более сильную синхронную ориентацию и изначально не проектировался с учетом асинхронного подхода.

 2. **Упрощенная архитектура**: FastAPI имеет более легковесную архитектуру и меньшее количество "из коробки"
 фунциональных возможностей, чем Django. Это значит, что вы можете настроить только те компоненты, которые вам
 действительно нужны, а не загружать приложение лишними модулями.

 3. **Использование Starlette**: FastAPI построен на базе Starlette, который является высокопроизводительным асинхронным
 фреймворком для веб-приложений. Starlette предоставляет множество функций, таких как маршрутизация, обработка запросов
 и ответов, которые выполнены оптимально для асинхронного контекста.

 4. **Оптимизация для JSON**: FastAPI использует библиотеку Pydantic для валидации и сериализации данных, что делает
 работу с JSON более быстрой и эффективной по сравнению с Django, где подобные операции реализованы более громоздко.

 5. **Автоматическая документация**: FastAPI автоматически генерирует документацию API на основе аннотаций типов,
 что упрощает разработку и тестирование, но также эффективно поддерживает взаимодействие с клиентами без лишних затрат
 времени на ручное описание.

 Таким образом, для высоконагруженных API приложений FastAPI будет предпочтительным выбором из-за своей
 производительности и простоты использования асинхронных возможностей. Django, с другой стороны, более подходит для
 создания полнофункциональных веб-приложений, где могут быть важны его богатые возможности и встроенные компоненты.


 --- collections.abc — Абстрактные базовые классы для контейнеров ---
 Использовать для наследования или проверки типа объекта.

 АВС                    Наследует от            Абстрактные методы                 Миксинные методы

 Container                                      __contains__

 Hashable                                        __hash__

 Iterable                                       __iter__

 Iterator               Iterable                __next__                           __iter__

 Reversible             Iterable                __reversed__

 Generator              Iterator                send,throw                         close, __iter__, __next__

 Sized                                          __len__

 Callable                                       __call__

 Collection             Sized, Iterable,        __contains__, __iter__,
                        Container               __len__

 Sequence               Reversible,             __getitem__, __len__               __contains__, __iter__,
                        Collection                                                 __reversed__, index, и count

 MutableSequence        Sequence                __getitem__, __setitem__,          Унаследованные Sequence методы и
                                                __delitem__, __len__,              append, clear, reverse, extend,
                                                 insert                            pop, remove, и __iadd__

 ByteString             Sequence                __getitem__, __len__               Унаследованные Sequence методы

 Set                    Collection              __contains__, __iter__,            __le__, __lt__, __eq__, __ne__,
                                                 __len__                           __gt__, __ge__, __and__, __or__,
                                                                                    __sub__, __xor__ и isdisjoint

 MutableSet             Set                     __contains__, __iter__,            Унаследованные Set методы и clear, pop,
                                                __len__, add, discard              remove, __ior__, __iand__, __ixor__, и __isub__

 Mapping                Collection              __getitem__, __iter__,             __contains__, keys, items, values,
                                                 __len__                           get, __eq__, и __ne__

 MutableMapping         Mapping                 __getitem__, __setitem__,          Унаследованные Mapping методы и pop,
                                                __delitem__, __iter__, __len__     popitem, clear, update, и setdefault

 MappingView            Sized                                                       __len__

 ItemsView              MappingView, Set                                            __contains__, __iter__

 KeysView               MappingView, Set                                            __contains__, __iter__

 ValuesView             MappingView, Collection                                     __contains__, __iter__

 Awaitable                                      __await__

 Coroutine              Awaitable               send,throw                          close

 AsyncIterable                                  __aiter__

 AsyncIterator         AsyncIterable            __anext__                           __aiter__

 AsyncGenerator        AsyncIterator            asend,athrow                        aclose, __aiter__,__anext__

 Buffer                                         __buffer__

 print(issubclass(list, collections.abc.Iterable))    # -> True
 print(isinstance(list(), collections.abc.Iterable))  # -> True

 Проверка isinstance(obj, Iterable) обнаруживает классы, зарегистрированные как Iterable или имеющие метод __iter__().
 но он НЕ обнаруживает классы, которые выполняют итерацию с помощью метода __getitem__().
 Единственный надежный способ определить, является ли объект итеративным, — это вызвать iter(obj).


 Инструкция assert - это удобный способ вставить отладочные утверждения в программу.
 assert 2.0 + 2 == 4, "Good"
 assert 2 + 2 == 5, "Houston we've got a problem"  # -> AssertionError: Houston we've got a problem

 Вы запустите assert кортеж (condition, message) в качестве первого параметра.
 assert True, 'no good'    # -> Ничего не происходит
 assert (True, 'no good')  # -> SyntaxWarning: assertion is always true, perhaps remove parentheses?

 # Можно разные проверики делать!
 assert isinstance('1', int | float), 'BAD'  # -> AssertionError: BAD
 assert 1 in [1, 2, 3]

 __debug__ - __debug__ True, если Python не был запущен с опция командной строки -O
 Инструкция assert будет игнорироваться (не будет выполняться) интерпретатором Python, если запустить его в
 оптимизированном режиме (опция командной строки -O при запуске скрипта Python).
 __debug__ = False -> SyntaxError: cannot assign to __debug__

 PYTHONOPTIMIZE – это переменная окружения, которую можно использовать для управления поведением интерпретатора Python.
 Применяется для оптимизации скомпилированного кода. С его помощью можно удалить отладочную информацию, что в итоге
 ускоряет выполнение кода. Обычно -O удаляет из скомпилированного байт-кода docstrings и assert
 PYTHONOPTIMIZE=1 - Игнорирует документационные строки (docstrings), что может снизить использование памяти и немного
 ускорить выполнение
 PYTHONOPTIMIZE=2 - Кроме игнорирования docstrings, дополнительно отключает assert, что может привести к небольшому
 ускорению работы программы

 -- Модуль sys обеспечивает доступ к некоторым переменным и функциям, взаимодействующим с интерпретатором python.

 -- Модуль os предоставляет множество функций для работы с операционной системой

 -- Модуль locale в Python, региональные настройки

 -- Модуль time   -  Реальное время ОС.    Работа с системным временем операционной системы

 -- Модуль datetime  -  Работа с датой и временем. Форматирование, преобразования и обработка даты и времени


  ДВУХРЕЖИМНЫЙ API   re   os
 - Модуль re поддерживает как строковые, так и байтовые операции с регулярными выражениями.   str или bytes
 - Модуль os может работать с аргументами типа str или bytes


 -- Почему так нужно делать!   range(len(lst)-1) --
 `len(lst) - 1` нужно писать, чтобы избежать выхода за границы списка. Когда ты используешь `range(len(lst)-1)`,
 это означает, что цикл будет итерироваться по индексам от `0` до `len(lst) - 2`. Это позволяет безопасно обращаться к
 элементу `lst[i + 1]`, поскольку в этом случае `i + 1` всегда будет находиться в пределах существующих индексов списка.

 Индексы
 range задаётся как [начало, конец):

 range(0, 3) - это [0, 1, 2], и если вам нужны числа от 1 до 10, то выпишете range(1, 11)

 По этой же причине если вам нужно посчитать с 10 до 1 включительно, то вы пишете range(10, 0, -1) Как только нужно
 работать с индексами (что в питоне, к счастью, нечасто), то везде появляются i+1 или i-1, потому что начало включается,
 а конец - нет. Это бесит.


 -- ТИПЫ ДАННЫХ Python --
 Основные ВСТРОЕННЫЕ ТИПЫ ДАННЫХ языка Python:

 ИЗМЕНЯЕМЫЕ(mutable): list, dict, set, Массивы байтов (bytearray), Самописные классы (Classes), (Class Instances - экземпляры классов)
 Изменяемые (англ. Mutable) - содержимое объекта МОЖНО изменить после создания.       НЕ СОЗДАЁМ НОВЫЙ ОБЬЕКТ В ПАМЯТИ

 НЕ ИЗМЕНЯЕМЫЕ(immutable): Числа (int, float, complex), Булевые (bool), Строки (str), Кортежи (tuple),
 НЕизменяемые множества (frozenset), Диапазоны (range), Байтовые строки (bytes)
 НЕизменяемые (англ. Immutable): содержимое объекта НЕЛЬЗЯ изменить после создания.   СОЗДАЁМ НОВЫЙ ОБЬЕКТ В ПАМЯТИ


 АДТ – это МАТЕМАТИЧЕСКОЕ описание некоторых данных и операций над ними (например, стек, очередь, список, множество).
 В Python АДТ может быть реализован с помощью классов и встроенных типов.  АДТ - АБСТРАКТНЫЙ ТИП ДАННЫХ


 -- СТРУКТУРЫ ДАННЫХ Python --

 ВСТРОЕННЫE:
 4 (ЧЕТЫРЕ) Built-in (ВСТРОЕННЫХ) СТРУКТУРЫ ДАННЫХ, которые вы можете использовать для хранения данных:
 list(список), tuple(кортеж), dict(словарь), set(множество), frozenset(неизменяемое множество)


 ВСТРОЕННЫE + ДОПОЛНИТЕЛЬНЫЕ СТРУКТУРЫ ДАННЫХ:


 Список                 (list)
 Массив                 (array)
 Стек                   (LifoQueue)
 Очередь                (Queue)
 Дек                    (deque)
 Очередь с приоритетом  (heapq)
 Кортеж                 (tuple)
 Множество              (set, frozenset)
 Словарь                (dict)
 Модуль collections     (OrderedDict, ChainMap, Counter, defaultdict, deque, namedtuple)


 -- Как Python ХИТРИТ С НЕИЗМЕНЯЕМЫМИ Объектами --
 # Ложь во имя спасения! -  ОНИ ЭКОНОМЯТ ПАМЯТЬ И УСКОРЯЮТ РАБОТУ ИНТЕРПРЕТАТОРА

 # Для КОРТЕЖА [:]  и tuple(t)  - Возвращает ссылку на ИСХОДНЫЙ Кортеж           # Пример с list() и [:]  ДЕЛАЕТ КОПИЮ
 t1 = (1, 2, 3)                                                                  lst1 = [1, 2, 3]
 t2 = tuple(t1)                                                                  lst2 = list(lst1)
 print(t1 is t2)  # -> True                                                      print(lst1 is lst2)  # -> False
 t3 = t1[:]                                                                      lst3 = lst1[:]
 print(t3 is t1)  # -> True                                                      print(lst3 is lst1)  # -> False


 # ТАКОЕ ЖЕ ПОВЕДЕНИЕ У frozenset, str, bytes                                    # Пример str()
 f1 = frozenset({1, 2, 3})                                                       s1 = str({1, 2, 3})
 f2 = frozenset(f1)                                                              s2 = str(s1)
 print(f1 is f2)  # -> True                                                      print(s1 is s2)      # -> True
 f3 = f1.copy()                                                                  s3 = s1[:]
 # f[:] - НЕ работает Потому что frozenset НЕ ПОСЛЕДОВАТЕЛЬНОСТЬ
 # f3 = f1[:]   # -> TypeError: 'frozenset' object is not subscriptable
 print(f3 is f1)  # -> True                                                     print(s3 is s1)       # -> True


 -- Как можно изменить ТИП ОБЪЕКТА  Атрибут  __class__   НЕ рекомендуется!!  --

 # Можно изменить тип объекта, изменив атрибут __class__
 Да, в Python можно изменить тип объекта, присвоив ему другой класс через атрибут __class__. Однако это НЕ рекомендуется,
 так как может привести к непредсказуемым последствиям и нарушению принципов объектно-ориентированного программирования.
 Лучше избегать таких подходов.

 class A:
     def greet(self):
         return "Hello from A"

 class B:
     def greet(self):
         return "Hello from B"

 # Создаем объект класса A
 obj = A()
 print(obj.greet())  # Вывод: Hello from A

 # Изменяем класс объекта на B
 obj.__class__ = B

 print(obj.greet())  # Вывод: Hello from B


 Четыре способа форматирования строк:
 string.Template    from string import Template
                    s = Template('who and what')
                    s.substitute(who='spam', what='eggs')  # -> ('spam', 'eggs')

 сишный стиль      # '%s and %s' % ('spam', 'eggs')      # ->  'spam and eggs'
 f-строки          #  f'spam and eggs'                   # ->  'spam and eggs'
 str.format()      # '{} and {}'.format('spam', 'eggs')  # ->  'spam and eggs'

 Если форматирующие строки поступают от пользователей, то используйте  string.Template, чтобы избежать проблем с безопасностью.
 f-строки     - если Python 3.6+
 str.format() - если меньше чем Python 3.6+




 CPython содержит небольшой целочисленный кеш в диапазоне [-5, 256], поэтому числа в диапазоне [-5, 256] будут ссылаться
 на один и тот же адрес памяти.

 # Работает не только с числами. Работает с НЕ ИЗМЕНЯЕМЫЕ(immutable)                            <-----
 a = 100000000000
 b = 'aaaaaaaaaaaaaa'
 print(a is 100000000000)  # -> True
 print(b is 'aaaaaaaaaaaaaa')  # -> True
 # SyntaxWarning: "is" with a literal. Did you mean "=="?  Но работает Просто предупреждение!


 BSON - Двоичный JSON   - расширение JSON
 JSON - тот же словарь пайтон, обращайтесь к нему по ключам:
  Конвертировать из Python в JSON:
  json.dump() - метод записывает объект Python в файл в формате JSON  - сериализации
  json.dumps() - метод возвращает строку в формате JSON - сериализации

  Конвертировать из JSON в Python:
  json.load() - метод считывает файл в формате JSON и возвращает объекты Python - десериализации
  json.loads() - метод считывает строку в формате JSON и возвращает объекты Python - десериализации

 Не все форматы преобразуются Например datetime НЕ сериализуется Нужно преобразовать к формату которые сериализуються:

 JSON               Python

 object             dict
 array              list
 string             str
 number (int)       int
 number (real)      float
 true               True
 false              False
 null               None


 # Совместимые с json ПСЕВДОНИМЫ   Можно копировать json прямо в консоль
 true, false, null = True, False, None

 fruit = {
     'hehe': true,
     'AAA': false,
     'BBB': null,
 }

 print(fruit)              # -> {'hehe': True, 'AAA': False, 'BBB': None}
 print(true, false, null)  # -> True False None


 Pickle – сериализация, превращение обьектов python в байты и наоборот байтов в обьекты python десериализации
 Также для работы с бинарными файлами Python предоставляет специальный встроенный модуль pickle
 "Pickling" - процесс преобразования объекта Python в поток байтов
 "unpickling" - обратная операция, в результате которой поток байтов преобразуется обратно в Python-объект.

 -- Модуль marshal
 примитивный модуль сериализации. Этот модуль существует главным образом для поддержки файлов Python .pyc


 -- JSON - это текстовый формат сериализации, а pickle - это двоичный формат сериализации.              <-----


 Пакет - набор модулей
 Модули в Python являются полноправными объектами!                                                                <-----
 Модуль — это файл, содержащий определения функций, классов и переменных, а также исполняемый код
 Модуль — это файл с расширением .py

 ТЕРНАРНЫЙ ОПЕРАТОР(Python условие в одну строку) — единственный оператор в Python, который требует три операнда
 Тернарный оператор(Ternary Operator) — способ превратить простую условную инструкцию в выражение:

 Выражение(expression) — это комбинация значений, операторов и литералов, которая что-то дает.
 Инструкция(statement) — это действие или команда, которая что-то делает. Пример: If-Else, циклы... и т. д.

 Основное различие между ними заключается в том, что expression всегда возвращает значение и может быть частью
 statement, в то время как statement выполняет действие и не обязательно возвращает значение. Это как разница между
 задачей (statement) и решением задачи (expression).

  -- командная строка cmd   Terminal commands --
 dir - Отображение файлов и папок в текущем каталоге
 cd - Изменить каталог, если нажать Tab будет автодополнение
 cd .. - переход на 1 папку назад
 cd ..\.. - переход на 2 папки назад
 cd \ - Переход в корневую директорию
 cd . - Переход в текущую директорию (никаких изменений)
 mkdir - создание папки


    -- Отличие Функции генератора от Функции сопрограммы   Generator function vs Coroutine function --
 Функции генератора, с выражением yield from <expr> внутри него, являются почти сопрограммами, но не совсем.

 Единственное отличие состоит в том, что ФУНКЦИЯ ГЕНЕРАТОР НЕ может контролировать где продолжиться выполнение после
 ее завершения, при этом дальнейшее управление всегда передается вызывающей стороне.


  -- Сопрограммы также имеют перечисленные ниже методы, аналогичные методам генераторов (Generator-iterator methods) --
 Однако, в отличие от генераторов, сопрограммы не поддерживают итерацию напрямую.

 Coroutine                                    Generator-iterator                          Asynchronous generator-iterator
                                              generator.__next__()                        coroutine agen.__anext__()
 coroutine.send(value)                        generator.send(value)                       coroutine agen.asend(value)
 coroutine.throw(value)                       generator.throw(value)                      coroutine agen.athrow(value)
 coroutine.throw(type[, value[, traceback]])  generator.throw(type[, value[, traceback]]) coroutine agen.athrow(type[, value[, traceback]])
 coroutine.close()                            generator.close()                           coroutine agen.aclose()


  Интроспекция — это способность программы исследовать тип или свойства объекта во время работы программы. - inspect
 Вы можете поинтересоваться, каков тип объекта, является ли он экземпляром класса
 Интроспекция позволяет вам изучать атрибуты объекта во время выполнения программы, а РЕФЛЕКСИЯ — манипулировать ими.
 В Python самой распространённой формой интроспекции является использование метода dir() , .__dir__()

 dir() , .__dir__()  -  Чтобы посмотреть все методы и атрибуты, связанные с определенным объектом в Python
 import datetime
 [_ for _ in dir(datetime) if 'date' in _.lower()]

 Python поддерживает полную интроспекцию времени исполнения. Это означает, что для любого объекта можно получить
 всю информацию о его внутренней структуре.
 Примеры: __dict__ , __class__ ,    type() , dir() или встроенного модуля inspect

  --- Менеджер пакетов: pip, conda, poetry - новый менеджер пакетов ---

  Менеджер пакетов - это инструмент, который позволяет управлять установкой, обновлением и удалением библиотек
  и зависимостей в проектах на языке Python.

 В общем, Poetry предлагает более мощные и удобные возможности для работы с проектами на Python по сравнению с pip.


 REPL (Read–eval–print loop)  - Цикл «чтение—вычисление—вывод» - Read(Чтение) Evaluate(Оценка) Print(Печать) Loop(Цикл)

 -- Модуль struct - сериализованные С-структуры

 Cython - Python+C/C++(Cтатическая типизация)статически компилируемый, ускоряющий выполнение кода на языке Python
 Из-за этих явных указаний типов в переменной и происходит ускорение кода

 Cython позволяет разработчикам оптимизировать код Python, используя функциональность языка C и библиотек CPython

 CPython является интерпретатором байт-кода, написан на C
 CPython написан на языке C, отсюда и его название

 Байт код — это промежуточный язык программирования, предназначенный для исполнения виртуальной машиной.   import dis

 Python компилирует исходный код в байт-код для внутреннего использования, который затем интерпретируется
 виртуальной машиной Python. Этот байт-код сохраняется в .pyc файлах.

 Исходный код Python компилируется в байт-код — внутреннее представление программы Python в интерпретаторе CPython.
 Байт-код также кэшируется в .pyc  файлах, чтобы второй раз выполнить тот же файл быстрее          <-----

 Вкратце, байт тип — это такая последовательность данных, которая уже готова тут же для сохранения в память.

 Стек является структурой данных, которая используется в качестве внутренней рабочей памяти виртуальной машины.
 Существуют разные классы виртуальных машин, и один из них называется стековой машиной. Виртуальная машина Python
 является реализацией такой стековой машины.

 Для СPython : Реализация самой ВМ управляет стеком и кучей (как и сборкой мусора). Сверху стек, снизу куча.
 В стеке - ссылки на объекты в куче. Фактических значений в стеке нет. Все объекты и структуры данных
 (т.е. и переменные и значения) - в куче. Управляет ей внутренний менеджер памяти.

  PyPy – использует Just-In-Time (JIT) компилятор - обходит GIL для ускорения кода Python
 PyPy для ускорения и оптимизации работы кода Python
 Сборщик мусора PyPy по умолчанию называется incminimark это инкрементальный сборщик мусора с перемещением поколений.
 JIT (Just-In-Time) компилятор – это инструмент, который компилирует код Python в машинный код «на лету», во время выполнения программы.


 --- Что происходит в момент вызова функции? ---
 Если в функцию при вызове передаются ключевые аргументы, то они сначала преобразуются в позиционные аргументы, затем
 создается список незаполненных слотов для формальных параметров/аргументов (которые определялись при написании кода функции).

 ВАЖНО!!!      Значения по умолчанию вычисляются один раз при определении функции!
 Обратите внимание, на то, что изменяемый объект используемый в качестве значения по умолчанию, будет использоваться всеми вызовами.
 Изменяемые объекты как значения по умолчанию: потенциально не безопасно, так как их состояние сохраняется между вызовами функции.



  -- Это происходит из-за того, что определения функций загружаются в память при интерпретации всего модуля, и функции
    становятся доступными для вызова в любом месте, даже до их определения в коде. --

 #  Функция f1 видит f2, потому что Python загружает все определения функций при интерпретации модуля, независимо
 # от их порядка. Поэтому вызов f2() в f1() работает, даже если f2 определена ниже.                               <-----

 # Тоже самое            # Тоже самое  # ПОМЕНЯЛИ ФУНКЦИИ МЕСТАМИ  # Классы должны быть определены ПЕРЕД использованием
 def f1():               def f2():                                 class F1(F):
     return f2()             return 'hehe'                             pass

 def f2():               def f1():                                 class F:
     return 'hehe'           return f2()                               pass

 print(f1())  # -> hehe  print(f1())  # -> hehe                    F1()  # Ошибка: NameError: name 'F' is not defined

 # В Python функции могут быть вызваны до их определения, поскольку интерпретатор загружает все определения функций
 в память при выполнении модуля. Поэтому, когда вы вызываете f1(), интерпретатор уже знает о существовании f2(), даже
 если она определена ниже в коде. Это позволяет функции f1() корректно вызывать f2() и возвращать её результат.   <-----



  --- Что за звери *args и **kwargs ---
 1) args и kwargs в параметрах функции - общепринятые имена, но можно использовать и другие
 2) * позволяет распаковать iterable/sequence, а ** распакуют словарь
 2) Итерируемый объект (iterable) - это объект, который способен возвращать элементы по одному
 2) Кроме того, это объект, из которого можно получить итератор. Примеры итерируемых объектов:
 все последовательности: список, строка, кортеж
 2) Таким образом, получается, что итерируемый объект это любой объект который реализует метод __iter__
 2) Последовательность (sequence) - это итерируемый объект, к элементам которого можно обратиться по целочисленному
 индексу, а также можно узнать общее количество элементов (длину последовательности)
 2) Последовательность (sequence) - можно вызвать метод __getitem__() и __len__() ---> list, tuple, range, str и bytes.
 3) если нет никаких спецсимволов, то аргументы функции можно передавать как позиционно, так и keyword (то есть ключ=значение).
 Важно помнить, что позиционные всегда идут раньше keyword(именованных),
 при этом keyword аргументы между собой не обязаны хранить порядок.
 4) спецсимвол / в параметрах функции говорит, что все, что ДО него должно передаваться как позиционные аргументы
 5) спецсимвол * (без указания переменной), говорит о том что все, что ПОСЛЕ него должно передаваться как keyword аргумент
 6) *args в параметрах функции соберет все позиционные аргументы в кортеж (tuple)
 7) **kwargs в параметрах функции соберет все keyword (именованные) аргументы в словарь (dict)


 # В вызовах функций можно использовать * НЕСКОЛЬКО РАЗ
 def fun(a, b, c, d, *rest):
     return a, b, c, d, rest

 print(fun(*[1, 2], 3, *range(4, 7)))     # -> (1, 2, 3, 4, (5, 6))
 print(fun(*[1, 2], *[3, 4], *range(2)))  # -> (1, 2, 3, 4, (0, 1))


 # В вызовах функций можно использовать ** НЕСКОЛЬКО РАЗ     Оператор **
 def fun(a, b, c, d, **rest):
     return a, b, c, d, rest

 print(fun(*[1, 2, 3, 4], **{'2': '2'}, **{'1': '2'}))  # -> (1, 2, 3, 4, {'2': '2', '1': '2'})


 # Можно использовать много раз ** если ключи НЕ повторяются и КЛЮЧИ = СТРОКИ    Работает так в функциях
 def fun(**kwargs):
     return kwargs

 print(fun(**{'x': 1}, y=2, **{'z': 2}))  # -> {'x': 1, 'y': 2, 'z': 2}
 print(fun(**{'x': 1}, y=2))              # -> {'x': 1, 'y': 2}
 print(fun(**{'x': 1}, y=2, **{1: 2}))    # -> TypeError: keywords must be strings

 # Пример без функции МОЖНО ЛЮБЫЕ КЛЮЧИ ИСПОЛЬЗОВАТЬ
 print({**{'x': 1}, 1: 2, **{1: 2}})  # -> {'x': 1, 1: 2}


 Оператор == используется для сравнения значений двух переменных
 Оператор is используется для проверки, являются ли две переменные одним и тем же объектом

 # Пример l2 = list(l1)    Объекты РАЗНЫЕ                       # Пример l2 = l1    Объекты ОДИНАКОВЫЕ
 l1 = [3, [55, 44], (7, 8, 9)]                                  l1 = [3, [55, 44], (7, 8, 9)]
 l2 = list(l1)                                                  l2 = l1
 print(l2 == l1)   # -> True                                    print(l2 == l1)   # -> True
 print(l2 is l1)   # -> False                                   print(l2 is l1)   # -> True

 # Ссылки внутри ОДИНАКОВЫЕ                                     # Ссылки внутри ОДИНАКОВЫЕ
 print(id(l1[1]), id(l1[2]))  # -> 2018437580864 2017853012992  print(id(l1[1]), id(l1[2]))  # -> 3024612180992 3024027483136
 print(id(l2[1]), id(l2[2]))  # -> 2018437580864 2017853012992  print(id(l2[1]), id(l2[2]))  # -> 3024612180992 3024027483136

 # Объекты РАЗНЫЕ                                               # Объекты ОДИНАКОВЫЕ
 print(id(l1), id(l2))        # -> 2018437200960 2018418215616  print(id(l1), id(l2))        # -> 3024611801216 3024611801216


 # Интересный пример
 a = {'A': 1}
 c = a
 print(c is a)       # -> True

 c['B'] = 2
 print(a)            # {'A': 1, 'B': 2}
 b = {'A': 1, 'B': 2}
 print(b == c)       # -> True                   # Тоже самое
 print(b.__eq__(c))  # -> True                   # Тоже самое
 print(b is c)       # -> False
 print(b is not c)   # -> True

 # is Работает быстрее чем == потому что его НЕвозможно перегрузить  Интерпретатор НЕ ищет и НЕ вызывает   Спец. методы
 #    ==  синтаксический сахар метода __eq__()      a.__eq__(b)

 res = [1, 2]
 res.append(3)

 res_2 = [1, 2, 3]
 print(res, res_2)         # -> [1, 2, 3] [1, 2, 3]
 print(res == res_2)       # -> True
 print(res.__eq__(res_2))  # -> True
 print(res is res_2)       # -> False


 Скорость   ==,    is,    in   Объяснение почему in быстрее ==      Why is 'x' in ('x',) faster than 'x' == 'x'?
 Проверка is может быть гораздо более быстрой, чем проверка на == (которая может потребовать рекурсивной проверки членов).

 Оба (in, ==) отправляются в if (left_pointer == right_pointer); разница лишь в том, сколько работы они прикладывают,
 чтобы достичь этой цели. in просто делает меньше.

 Отличие функции от метода:
 Метод — это функция определенная внутри тела класса.

 Методы отличаются от обычных функций только тем, что экземпляр объекта (self) добавляется к другим аргументам.

 Функции Python — это объекты первого класса
 Функции Python — это Дескрипторы.

 # Пример
 def func():
     '''my func'''

 print(type(func))            # -> <class 'function'>

 # Если вызывать через ЭК  будет метод  Если через Класс функция                                                 <-----
 class Func:
     def class_fun(self):
         '''my func'''

 f = Func()
 print(type(Func.class_fun))  # -> <class 'function'>                                                          # <-----
 print(type(f.class_fun))     # -> <class 'method'>                                                            # <-----
 Благодаря протоколу дескрипторов присоединенная к классу функция становиться методом при обращении через ЭК     <-----


 Любой тип данных (int, list, str, tuple и т.д.) является примером объекта первого класса в python.
 Объект называют «объектом первого класса» (first-class object, first-class entity, first-class citizen) если:
 - С ним можно работать как с переменными
 - Он может быть передан в функцию как аргумент
 - Он может быть возвращен из функции как результат
 - Он может быть включен в другие структуры данных. Например, быть элементом словаря или списка


 Карринг — это преобразование функции от многих аргументов в набор вложенных функций, каждая из которых является
 функцией от одного аргумента:

 Мемоизация — это техника оптимизации, которая позволяет сохранять результаты функций с целью избежать повторных вычислений.
 Мемоизация довольно простая и эффективная практика. Благодаря functools.lru_cache, ей удобно пользоваться в Python.
 Под капотом у нее словарь в виде хэш-таблицы, соответственно ключ должен реализовать хеширование


  Monkey patch (обезьяний патч) — Это просто динамическая замена атрибутов/функций во время выполнения
 class A:
    def func(self):
        return "Hello"

 def new_func(self):
     return "Hello, World"

 A.func = new_func          # monkey-patch        <-----

 Проще говоря, Monkey patch вносит изменения в модуль или класс во время работы программы.
 MonkeyPatch — это часть кода Python, которая расширяет или изменяет другой код во время выполнения (обычно при запуске).

 Простой пример выглядит так:
 from SomeOtherProduct.SomeModule import SomeClass

 def speak(self):
     return "ook ook eee eee eee!"
 SomeClass.speak = speak     # monkey-patch       <-----

 Поскольку классы Python изменяемы, а методы — это всего лишь атрибуты класса, вы можете делать это сколько угодно — и,
 по сути, вы можете даже заменять классы и функции в модуле совершенно таким же образом.
 Будьте осторожны при использовании monkeypatching!                                                            <-----


 --- Closure Замыкание ---
 Замыкание(Closure) - Внутренняя функция которая возвращается из внешней и при этом
 использует переменные из внешнего Scope(Область видимости)
 каждое замыкание хранит своё состояние, они не пересекаются
 хранит состояние(данные), предоставляет интерфейс для работы с ними,
 "скрывает" данные и помогает избегать global

 По сути замыкание - это внутренняя функция, которая возвращается из внешней и использует переменные из внешнего скоупа
 (которые ей не принадлежат). Функция как бы "замыкает", захватывает переменные из внешней функции. Вы могли встречать
 такое например в декораторах.

 Каждый объект замыкания независим, они не пересекаются, у каждого свои данные.
 Замыкания это еще один шаг в сторону ООП, так как тут мы имеем некоторое состояние (данные)
 сокрытое от посторонних глаз и с которым можно взаимодействовать только с помощью заранее написанного интерфейса (функция).
 Замыкания могут быть полезны для того чтобы избегать использования global, а также и в других случаях,
 когда нам важно, чтобы наши данные не изменили невалидным способом, чтобы с данными работали только через нашу логику.
 НО(!) до этих данных тоже можно добраться при определенном желании, нужно понимать что нет полного сокрытия данных.
 boys.__closure__[0].cell_contents

 def names():
    all_names = []

    def inner(name: str) -> list:
        all_names.append(name)
        return all_names

    return inner

 boys = names()
 boys('Vasya')
 boys('Sasya')
 print(boys.__closure__[0].cell_contents)  # -> ['Vasya', 'Sasya']

 Замыкание lambda
 def pow_(base):
    return lambda value: value**base


 # Интересный пример     При изменении   переменной Видит её как локальную!
 # Всякая переменная, которой присваивается значение в теле Функции считается ЛОКАЛЬНОЙ                       <-----

 b = 6                      b = 6                                   b = 6
 def func(a):               def func(a):                            def func(a):
     print(a, end=' ')          print(a, end=' ')                       global b
     print(b)                   print(b)                                print(a, end=' ')
                                b = 9                                   print(b)
                                                                        b = 9
 func(3)  # -> 3 6          func(3)  # -> 3  UnboundLocalError       func(3)  # -> 3 6



 # Замыкание class vs def    Реализация своей функции avg
 # Функция НЕЭФФЕКТИВНА              # ЭФФЕКТИВНА                               # Через КЛАСС
 def make_averager():                def make_averager():                       class Averager:
     series = []                         total = 0                                  def __init__(self):
                                         count = 0                                      self.series = []
     def averager(new_value):
         series.append(new_value)        def averager(new_value):                   def __call__(self, new_value):
         total = sum(series)                 nonlocal total, count                      self.series.append(new_value)
         return total / len(series)          count +=1                                  total = sum(self.series)
     return averager                         total += new_value                         return total / len(self.series)
                                             return total/count
 avg = make_averager()                   return averager                        avg = Averager()
 print(avg(10))  # -> 10.0                                                      print(avg(10))  # -> 10.0
 print(avg(11))  # -> 10.5                                                      print(avg(11))  # -> 10.5
 print(avg(12))  # -> 11.0                                                      print(avg(12))  # -> 11.0

 # __code__ Откомпилированное ТЕЛО ФУНКЦИИ
 print(avg.__code__.co_varnames)          # -> ('new_value', 'total')  # Локальные переменные функции
 print(avg.__code__.co_freevars)          # -> ('series',)             # Свободные переменные замыкания
 print(avg.__closure__[0].cell_contents)  # -> [10, 11, 12]            # Текущее содержимое


  --- ДЕКОРАТОР (ФУНКЦИЯ ВЫСШЕГО ПОРЯДКА) СТРУКТУРНЫЙ ПАТТЕРН ПРОЕКТИРОВАНИЯ ---
 ДЕКОРАТОР — это ФУНКЦИЯ ВЫСШЕГО ПОРЯДКА, которая принимает другую функцию в качестве аргумента,
 добавляет к ней некоторую дополнительную функциональность и возвращают функцию с измененным поведением.
 Функция - полноправный обьект
 внутренняя функция может захватывать переменные из внешней
 Суть декоратора в том, что мы можем менять поведение декорируемого объекта,
 при этом не меняя его собственную реализацию, его код.

 Проще говоря: декораторы обертывают функцию, изменяя ее поведение.

 Декораторы в Python выполняются сразу после загрузки модуля, когда интерпретатор встречает их.    <-----
 Декораторы выполняются при импорте модуля, тогда как декорируемые функции вызываются только в момент их использования.
 Это и демонстрирует различие между ЭТАПОМ ИМПОРТА (где работают декораторы) и ЭТАПОМ ВЫПОЛНЕНИЯ
 (где вызываются декорируемые функции).

 ФАБРИКА ДЕКОРАТОРОВ - функция которая возвращает декоратор
 Декораторы классов позволяют изменять поведение класса без изменения его исходного кода.    Модуль dataclasses

 @dataclasses.dataclass(*, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True,
 kw_only=False, slots=False, weakref_slot=False)

  # Модуль dataclasses
 from dataclasses import dataclass

 @dataclass
 class InventoryItem:
     name: str
     unit_price: float
     quantity: int = 0

 item = InventoryItem(name="HEHE", unit_price=12, quantity=100)
 print(item.__dict__)  # -> {'name': 'HEHE', 'unit_price': 12, 'quantity': 100}


  -- Интересный пример Наследование в Dataclasses  Декорировать нужно все классы при Наследовании --

 # Вот ещё пример логичности и простоты. Датаклассы Base и Child:

 from dataclasses import dataclass

 @dataclass
 class Base:
   a: str

 class Child(Base):
   b: str


 # Внезапно оказывается, что Base ведёт себя как приличный датакласс, а Child — как неприличный:

 print(Base())                     # -> TypeError: Base.__init__() missing 1 required positional argument: 'a'
 print(Base(a='test'))             # -> Base(a='test')
 print(Child(a='test'))            # -> Child(a='test')
 print(Child(a='test', b='test'))  # -> TypeError: Base.__init__() got an unexpected keyword argument 'b'

 # Почему так? Потому что «датаклассовость» не наследуется. Если хочется, чтобы Child тоже был датаклассом,
 # его нужно тоже декорировать.             Так что будьте осторожны при наследовании.                      <-----



 -- Атрибут экземпляра  и   Атрибут класса        dataclass  с аннотацией и БЕЗ --

 from dataclasses import dataclass
 from typing import ClassVar       #  ClassVar - атрибут является атрибутом класса

 # Обьяление С аннотацией типа  БУДЕТ  Атрибут экземпляра
 @dataclass
 class Spam:
     repeat: int       # Атрибут экземпляра

                                                # Объяление БЕЗ аннотации типа  БУДЕТ  Атрибут Класса    ClassVar
 @dataclass                                     @dataclass                           @dataclass
 class Spam:                                    class Spam:                          class Spam:
     repeat: int = 99  # Атрибут экземпляра         repeat = 99  # Атрибут Класса        repeat: ClassVar[int] = 99

 s = Spam()                                     s = Spam()
 s1 = Spam()                                    s1 = Spam()

 # Значение изменяется для всех ЭК              # Значение НЕ изменяется для всех ЭК
 Spam.repeat = 10                               Spam.repeat = 10
 print(s.repeat)   # -> 99                      print(s.repeat)    # -> 10
 print(s1.repeat)  # -> 99                      print(s1.repeat)   # -> 10


 ДЕКОРАТОР С ПАРАМЕТРАМИ - ЭТО ДОБАВЛЕНИЕ ЕЩЕ ОДНОГО УРОВНЯ ВЛОЖЕННОСТИ - чтобы передать параметр


 Принципы написания кода
 DRY - don't repeat yourself  - не повторяйся
 YAGNI - You aren't gonna need it  - это не понадобится
 KISS - Keep it simple, stupid  - будь проще
 POLA - Principle Of Least Astonishment  - не удивляй пользователя
 EAFP - Easier to Ask for Forgiveness than Permission  - проще извиниться, чем просить разрешения (сначала действуй)
 EAFP - Оптимистичное программирование   Optimistic Programming  'Python Way'
 LBYL - Look Before You Leap - смотри, прежде чем прыгнуть (сначала спроси)
 LBYL - Defensive programming  Защитное программирование
 Помните, что нет правил без исключений, все принципы и даже дзен - рекомендации, а не неоспоримый закон!
 Дзен Python - философии программирования от Тима Петерса (PEP20), состоит из 19 «руководящих принципов» написания
 компьютерных программ, влияющих на структуру языка программирования Python.
 команда - import this

 GRASP — принципы для распределения обязанностей в ОБЪЕКТНО-ОРИЕНТИРОВАННОМ программировании.


  --- Паттерны ---
 Паттерны или ШАБЛОНЫ разработки - это общие способы решения частых задач и проблем

 Паттерны в Python – это шаблоны для решения задач, которые часто встречаются в практике программиста.
 Паттерны — это типовые решение для типовых задач.
 Существует три основные разновидности паттернов:
 1) Архитектурные паттерны, 2) паттерны Проектирования, 3) Идиомы
 ПАТТЕРНЫ ПРОЕКТИРОВАНИЯ:
 1) Порождающие паттерны — отвечают за процесс создания объектов.
 2) Структурные паттерны — определяют отношения между объектами, облегчая их взаимодействие
 3) Поведенческие паттерны — определяют способы коммуникации между объектами

 «Шаблон проектирования» — это конструкция относительно «высокого уровня»
 «Идиома» — это относительно «низкоуровневая» конструкция

 Идиомы — это низкоуровневые шаблоны, специфичные для языка программирования. Идиома описывает, как реализовать
 определенные аспекты компонентов или отношения между ними с учетом особенностей данного языка.
 примеры идиом: lambda, enumerate, yield, set, dict, with, тернарный оператор, zip, Listcomps, genexp  и другие
 from collections import Counter, OrderedDict, ChainMap, deque, defaultdict, namedtuple    и другие
 from itertools import chain, combinations, permutations, groupby, zip_longest             и другие
 CamelCase: OrderedDict, ChainMap   если в формате(стиле) CamelCase то значит написано на чистом  Python

  АРХИТЕКТУРНЫЕ ПАТТЕРНЫ - Определяет архитектуру приложения, задают его логику
 отвечает на вопрос «как будет устроен продукт»
 ПАТТЕРНЫ ПРОЕКТИРОВАНИЯ - предоставляют решения на уровне кода
 отвечает на вопрос «как лучше организовать составные части продукта»:
 как эффективнее создавать объекты, настраивать обмен данными между ними и их взаимодействие.


 --- Анти-паттерны — полная противоположность паттернам ---
  Если паттерны проектирования — это примеры практик хорошего программирования, так называемые шаблоны решения определённых
 задач, то антипаттерны — их полная противоположность. Это шаблоны ошибок, которые совершаются при решении различных задач.

 Анти-паттерны -     Божественный обьект(God object) , Singleton Одиночка

 Божественный объект — анти-паттерн, который довольно часто встречается у ООП разработчиков. Такой объект берет на себя
 слишком много функций и/или хранит в себе практически все данные. В итоге мы имеем НЕпереносимый код, в котором,
 к тому же, сложно разобраться. Также подобный код довольно сложно поддерживать, учитывая, что вся система зависит
 практически только от него.

 Бороться с анти-паттерном «Божественный объект» следует путем разбивания одного большого класса на несколько небольших.


  -- Конструкция __MAIN__ для чего и кому нужна. --
 1) любой код на питоне лежит в модуле (файл с расширением py)
 2) любой модуль при запуске программы получает атрибут __name__
 3) один(!) модуль, с которого программа началась (точка входа) получает имя __main__, все остальные
 (которые импортированы) получают имя, равное имени в файловой системе, без расширения. Например first
 4) Крайне важное для понимания! Любой модуль при импорте выполняется, как если бы мы его запустили отдельно.
 То есть все принты будут напечатаны(если они не в функциях), любой вызов функции выполнен.

 if __name__=='__main__' означает  "если этот модуль НЕ был импортирован, а запущен напрямую,
 то..." и все действия в данном блоке НЕ будут выполнены при импорте модуля.

      __main__
 если ты запущен напрямую а не импортирован
 у каждого модуля есть __name__
 __main__ - это имя среды, в которой выполняется код верхнего уровня. «Код верхнего уровня» - это первый
 пользовательский модуль Python, который начинает выполняться. Он «верхнеуровневый», потому что импортирует все
 остальные модули, необходимые программе. Иногда «код верхнего уровня» называют точкой входа в приложение.

   --- lambda ---
 в lambda можно написать всё ,то что можно написать после return в обычной функции
 аналог def!!
 можно писать всё что допустимо после return в def
 не выполняется до вызова ()!!!
 значения по умолчанию вычисляются в момент создания функции (аргумент функции)
 lambda не сериализуется pickle
 AttributeError: Can't pickle local object 'run.<locals>.<listcomp>.<lambda>'

 Аргумент по умолчанию будет создан 1 раз при интерпретации

 lambda-функции - это анонимные функции, которые могут включать только одно выражение.
 lambda - Анонимная встроенная функция, состоящая из одного выражения , которое вычисляется при вызове функции.

 Лямбды в Питоне могут состоять только из одного выражения. Используя синтаксис скобок, можно оформить тело лямбды в несколько строк.
 lambda НЕ поддерживает аннотацию и нет return   <----

  --- Hash ---
 Hashable - обьект является Hashable если у него есть Hash и за время жизни обьекта этот Hash никогда не меняется
 Hashable Строки, Числа, Кортежи(если они состоят из неизменяемых обьектов, строк, чисел)
 Hashable != Immutable
 Для встроенных типов Hashable == Immutable
 Hashable - (int, float, bool, str, frozenset, tuple(если он состоит из НЕизменяемых обьектов), bytes, complex, range, ЭК)
 unhashable type - (list, set, dict, tuple(если есть хоть 1 изменяемый обьект), bytearray)

 Большинство НЕИЗМЕНЯЕМЫХ встроенных объектов Python являются хешируемыми. ИЗМЕНЯЕМЫЕ контейнеры
 (такие как списки или словари) — нет. НЕИЗМЕНЯЕМЫЕ контейнеры (такие как кортежи и frozenset) хешируются только в том
 случае, если их элементы хешируются. Объекты, являющиеся экземплярами пользовательских классов, по умолчанию хешируются.
 Все они сравниваются неодинаково (кроме самих себя), а их хеш-значение получается из их id().

 # hash() чтобы узнать  hashable  или НЕТ
 first_tuple = (1, 2, 3)
 second_tuple = ([1], 2, 3)

 print(hash(first_tuple))   # -> 529344067295497451
 print(hash(second_tuple))  # -> TypeError: unhashable type: 'list'


 --- Рекурсия Recursion   Рекурсивная функция ---
 Рекурсия - функция которая вызывает сама себя - Рекурсия не очень оптимизирована
 Хвостовая Рекурсия - функция вызывает сама себя в самом конце программы - Не отимизирована
 вместо Хвостовой Рекурсии - используем for или while
 Множественная рекурсия - это когда функция вызывает саму себя несколько раз в своем теле.
 Множественная рекурсия возникает, когда в теле функции происходит более одного рекурсивного вызова.
 Изменение лимита рекурсия по умолчанию глубина рекурсии = 1000

 import sys
 sys.setrecursionlimit(4000) - Эта функция устанавливает максимальную глубину рекурсии на указанное значение.

 # Показать глубину рекурсии
 print(sys.getrecursionlimit())  # -> 1000


 # НЕ ЯВЛЯЕТСЯ ХВОСТОВОЙ РЕКУРСИЕЙ   ХВОСТОВОГО ВЫЗОВА НЕТ       # ХВОСТОВАЯ РЕКУРСИЯ   ХВОСТОВОЙ ВЫЗОВ ЕСТЬ
 def factorial(n):                                               def factorial_tc(n, product=1):
     if n < 2:                                                       if n < 1:
         return 1                                                        return product
     return n * factorial(n - 1)                                     return factorial_tc(n - 1, product * n)

 print(factorial(5))  # -> 120                                   print(factorial_tc(5))  # -> 120

 # factorial - результат рекурсивного вызова нужно умножить на n после возвращения.
 # factorial_tc - рекурсивный вызов является последним выражением, и результат может быть возвращен напрямую
   БЕЗ дополнительных операций.


  -- Как работает РЕКУРСИЯ??? --                                                                            <----   <----
 Стек сначала накапливает вызовы рекурсивной функции, потом как только мы дошли до первого условия выхода(BASE CASE)
 Стек обратно разматывает вызовы рекурсивной функции и получает результат.
 (СНАЧАЛА ВСЕ НАКАПЛИВАЕТСЯ ПОТОМ ВСЕ РАСКРУЧИВАЕТСЯ НА СТЕКЕ ВЫЗОВОВ И МЫ ПОЛУЧАЕМ ГОТОВЫЙ РЕЗУЛЬТАТ)


  --- модуль warning ---
 В модуле warning собраны функции для работы с предупреждениями.


 --- Exceptions   Исключения ---
 Исключение — это событие, возникающее во время исполнения программы, нарушающее нормальный ход выполнения

 BaseException - Базовый класс для всех встроенных исключений. Есть 4 его подкласса исключений:

 SystemExit - исключение, порождаемое функцией sys.exit при выходе из программы. Вызывается функцией sys.exit()
 KeyboardInterrupt - порождается при прерывании программы пользователем (обычно сочетанием клавиш Ctrl+C).
 GeneratorExit - порождается при вызове метода close объекта generator.
 Exception - является родителем всех основных исключений, с которыми мы сталкивается при написании программ.


 `try` и `except` занимают немного ресурсов, НО ИХ ИСПОЛЬЗОВАНИЕ ДОБАВЛЯЕТ НАКЛАДНЫЕ РАСХОДЫ ПРИ ВОЗНИКНОВЕНИИ ИСКЛЮЧЕНИЙ.
  В нормальных условиях их влияние незначительно.


 -- Что такое коллизия? --
 Когда хеш-функция возвращает один и тот же ответ для разных данных.

 Коллизия - если у двух НЕ равных элементов hash одинаковый(больше относиться к самописным классам)
 Коллизия - Событие, когда два хеша совпали, а не должны, называется коллизией

 Зондирование - это технология, используемая в хэш-таблицах для нахождения свободных ячеек в случае коллизий. Когда два
 ключа имеют одинаковый хэш и пытаются занять одно место, применяется зондирование для поиска следующего доступного места.

 Зондирование — это метод разрешения коллизий в хэш-таблицах. Когда два ключа имеют одинаковый хэш и пытаются занять
 одно и то же место, зондирование помогает найти следующую свободную ячейку, проверяя соседние индексы.
 Существует несколько стратегий зондирования, включая линейное и квадратичное.


 -- Как решить проблему коллизии? --
 Проблемы коллизии в Python, связанные с хешированием объектов (например, при использовании СЛОВАРЕЙ или МНОЖЕСТВ),
 разрешаются с помощью следующих методов:

 1. **Хеш-функция**: Python использует хеш-функцию для преобразования объектов (как правило, неизменяемых) в хеш-значения.
 Если два объекта имеют одинаковое хеш-значение, это называется коллизием.

 2. **Методы разрешения коллизий**:
 - **Метод цепочек**: Каждое ведро (или ячейка) хеш-таблицы может содержать список или другой контейнер для хранения
 всех элементов, имеющих одно и то же хеш-значение.

 - **Линейное или квадратичное пробирование**: Используется для поиска следующей пустой ячейки в случае коллизии.
 При этом происходит последовательная проверка ячеек в хеш-таблице.

 - **Удвоение размера хеш-таблицы**: Когда хеш-таблица достигает определенного уровня заполненности, её размер может
 быть увеличен, чтобы уменьшить количество коллизий.

 3. **Кастомизация хешей и сравнений**: Можно переопределить методы `__hash__()` и `__eq__()` в пользовательских классах,
 чтобы изменить способ, которым определяются хеши и равенства объектов, также что может помочь избежать коллизий.

 Эти подходы помогают эффективно управлять хеш-значениями и минимизировать влияние коллизий на производительность операций.


 -- Проблему коллизии в Python можно решить с помощью следующих подходов --:

 1. **Метод цепочек**: Каждый элемент, имеющий одинаковый хеш, хранится в виде списка (или других контейнеров)
  внутри одной ячейки хеш-таблицы.

 2. **Линейное пробирование**: При коллизии проверяется следующая ячейка последовательно, пока не найдется пустая.

 3. **Квадратичное пробирование**: Аналогично линейному, но шаг увеличивается квадратично (1, 4, 9 и т. д.) для поиска
  свободной ячейки.

 4. **Пересчитывание хеш-таблицы**: Если таблица становится слишком заполненной, её размер можно увеличить
  (реализация ресайза) и перераспределить элементы по новым хеш-значениям.

 5. **Переопределение методов**: В пользовательских классах можно переопределить методы `__hash__()` и `__eq__()`,
  чтобы управлять хешированием и сравнением объектов, уменьшая вероятность коллизий.

 Эти стратегии помогают эффективно справляться с коллизиями и обеспечивают быструю работу хеш-таблиц.

 На практике вам не нужно беспокоиться о коллизиях в Python, так как встроенные структуры данных, такие как словари,
 множества уже эффективно обрабатывают их.


 set и dict используют один и тот же алгоритм хэширования

 O(1) - значит за постоянное время
 скорость(время) доступа к элементам одинаковая, скорость не зависит от количества элементов

 Сложность получения элемента в Dict и Set в наилучшем случае составляет O(1),
 поскольку элемент может быть получен просто с помощью хэш-функции в качестве индекса массива.
 Однако в худшем случае, когда возникают хэш-коллизии, сложность может вырасти до O(n), где n - количество элементов в таблице.

 Ну и сложность операций добавления, удаления и поиска элементов в Set и Dict также составляет O(1) в наилучшем случае
 и O(n) в худшем случае.


 # Словари и множества очень зависят от своих хэш-функций. Если хэш-функция некоторого типа данных не дает скорость О(1),
 # любой словарь или набор содержащий этот тип, больше не будет иметь гарантии скорость О(1)             <-----  <-----

 Зависимость от хэш-функций: Таким образом, эффективность словарей и множеств действительно сильно зависит от качества
 и производительности их хэш-функций. Если хэш-функция для определенных типов данных не обеспечивает быстрое построение
 хэш-значений или если она ведет к множеству коллизий, производительность структуры данных может упасть,
 и доступ к элементам перестанет быть O(1).
 В заключение, правильные хэш-функции критически важны для достижения оптимальной производительности словарей и множеств.



 для самописных классов если мы НЕ переопределяем __hash__
 hash будет равен адрес в памяти // поделенный на 16                      # Так тоже может Опеределить пустой Класс
 class Cat:                                                               class Cat:          class Cat:
     pass                                                                     ...                 '''docstring'''

 tom = Cat()
 print(hash(tom))      # ->  151995328801
 print(id(tom))        # ->  2431925260816
 print(id(tom)//16)    # ->  151995328801


 УПОРЯДОЧЕННЫЙ - означает, что элементы структуры хранятся в том порядке, в котором они были добавлены
 НЕ УПОРЯДОЧЕННЫЙ - означает, что элементы структуры хранятся в случайном порядке.

 --- set (Множество) ---
 set - множество, хешсет, ИЗМЕНЯЕМЫЙ, НЕУПОРЯДОЧЕННЫЙ набор hashable объектов, доступ и проверка наличия O(1)
 Элементами множества может быть любой НЕизменяемый тип данных
 Множество, Набор(set) - это НЕупорядоченная коллекция без повторяющихся элементов.
 Примечание: чтобы создать пустое Множество(set) - нужно использовать set(), а не {}
 {} - создаст пустой словарь(dict)


 # Устранить дубликаты и оставить Порядок элементов                 <-----
 my_lst = [10, 10, 10, 2, 3]
 # Порядок НЕ УПОРЯДОЧЕННЫЙ!!!
 print(set(my_lst))                          # -> {3, 10, 2}

 # Сохраняем порядок
 print(dict.fromkeys(my_lst).keys())        # -> dict_keys([10, 2, 3])
 print(list(dict.fromkeys(my_lst).keys()))  # -> [10, 2, 3]


 --- dict (Словарь) ---
 dict (Словари)  — это ИЗМЕНЯЕМЫЕ УПОРЯДОЧЕННЫЕ коллекции без повторяющихся элементов,
 где каждый элемент представляет собой пару «ключ-значение»
 1) dict - словарь, отображение, хеш-мап, ассоциативный массив, коллекция пар ключ-значение,
 где ключом может быть только hashable тип, доступ по ключу и проверка наличия ключа O(1),
 с питона 3.7 хранит порядок вставки
 2) пустой словарь создавать лучше через {},а не dict(), под капотом сразу будет создано 8 элементов
 3) Hashable != Immutable, эти понятия часто путают, помните что это не одно и то же.
 4) Значения по ключу могут быть любого типа данных. Чтобы взять значение по ключу,
 необходимо указать ключ в квадратных скобках после имени словаря
 a_dict = {'good': 'a', 2: 'b'}, a_dict['good']
 5) алгоритм работы словаря и сета: Получаем хеш -} высчитываем позицию в массиве -} если элемента нет
 то действуем соответственно задаче -} если элемент есть то сравниваем ключ == тому что ищем -}
 если ключ не равен искомому то ищем дополнительный бакет
 Бакет — это просто номер в массиве элементов
 6) По умолчанию самописные классы возвращают хеш основанный на id, если переопределяете хеш, то всегда проверяйте,
 что у равных объектов одинаковый хеш
 7) За скорость словаря и сета мы платим большей памятью и тем, что положить туда можно не любые элементы
 используй {} для создания пустого dict

 хэш-таблицы (hash-table) - позволяет находить элементы за O(1).   НО с  хэш-коллизиями за O(n)
 Поиск занимаем постоянное время - ищете вы в 10 млн элементов  или среди 10

 Хеш-таблица (hash table) — это структура данных, которая позволяет находить элементы за O(1). Поиск занимает постоянное
 время вне зависимости от того, ищете вы среди 10 млн элементов или всего среди 10.
 Так же, как массив, хеш для хранения данных требует предварительного выделения большого блока последовательной памяти.
 Но, в отличие от массива, его элементы хранятся не в упорядоченной последовательности. Позиция, занимаемая элементом,
 «волшебным образом» задается хеш-функцией. Это специальная функция, которая на входе получает данные, предназначенные
 для хранения, и возвращает число, кажущееся случайным. Оно интерпретируется как позиция в памяти, куда будет помещен
 элемент. Это позволяет нам получать элементы немедленно. Заданное значение сначала пропускается через хеш-функцию.
 Она выдает точную позицию, где элемент должен находиться в памяти. Если элемент был сохранен, то вы найдете его в этой
 позиции.


 Нужно гарантировать, чтобы в хэш-таблице оставалось незанятым по крайней мере 50% пространства.    <-----
 В противном случае коллизии начнут происходить слишком часто и производительность хэш-таблица значительно упадет!

 Для корректной работы работы хэш-таблицы требуют выделения очень большого блока непрерывной памяти <-----

 # .keys() и .items() объекта  dict()  Удивительно похожи на frozenset()
 # .keys() и items() -  Поддерживают наиболее полезные операторы и методы класса frozenset()   <-----
 d1 = dict(a=1, b=2, c=3, d=4)
 d2 = dict(b=20, d=40, e=50)

 # Легко получить пересекающиеся ключи
 print(d1.keys() & d2.keys())     # ->  {'d', 'b'}

 print(d1.keys() - d2.keys())     # ->  {'a', 'c'}
 print(d1.keys() | d2.keys())     # ->  {'c', 'e', 'd', 'a', 'b'}

 print(d1.items() & d2.items())   # ->  set()
 print(d1.items() - d2.items())   # ->  {('b', 2), ('d', 4), ('a', 1), ('c', 3)}
 print(d1.items() | d2.items())   # ->  {('b', 2), ('b', 20), ('a', 1), ('e', 50), ('c', 3), ('d', 40), ('d', 4)}


 -- Что еще может быть в словаре ключем и значением
 def func(): return 200

 x = lambda x: x

 a_dict = {lambda x: x * 2: x(10), func: func(),}         # в словаре может быть функция, lambda-функция
 print(a_dict)  # -> {<function <lambda> at 0x0000026691F499E0>: 10, <function func at 0x0000026691C50D60>: 200}
 b_dict = {(i for i in range(3)): [i for i in range(2)]}  # в словаре может быть genexp, listcomps
 print(b_dict)  # -> {<generator object <genexpr> at 0x0000020C6C2A0040>: [0, 1]}

 Mapping Types — dict,  from collections import defaultdict, OrderedDict, Counter


 --- list (Список) ---
 list (Список) - список, ИЗМЕНЯЕМЫЙ УПОРЯДОЧЕННЫЙ, обычно хранит значения одного типа, О(1) доступ к элементу
 используй [] для создания пустого списка
 если заранее известен размер, то НЕ используй append (для 8000 добавлений выделяется 8600 ячеек памяти)
 используй листкомпс
 не пытайся заменять список кортежом, там где идет изменение размера

 У list и tuple скорость(время) доступа к элементам одинаковая, скорость не зависит от количества элементов

 Python когда стартует он сразу при старте в ОС (операционной системе)
 резервирует место под tuple. Python не ходит в ОС и не просит дай мне еще памяти как это происходит со списками

 каждый раз когда мы создаем список Python обращается к ОС и просит место                               <-----


 pop([i]), index(x[, start[, end]])
 Квадратные скобки вокруг [i], [, start[, end]. [] - Означает, что параметр является необязательным.    <-----

  -- Очень интересный момент С id Посмотри Внимательно --

 # Конструкция lst_1 *= 3  Изменение обьекта       # Конструкция lst_1 = lst_1 * 3  Создание НОВОГО обьекта

 # id НЕ Изменился                                  # id Изменился
 lst_1 = [1, 2, 3]                                  lst_1 = [1, 2, 3]
 print(id(lst_1))  # -> 2002285652864               print(id(lst_1))  # -> 2671279192832
 lst_1 *= 3                                         lst_1 = lst_1 * 3
 print(id(lst_1))  # -> 2002285652864               print(id(lst_1))  # -> 2671281626048
 print(lst_1)      # -> [1, 2, 3, 1, 2, 3, 1, 2, 3] print(lst_1)      # -> [1, 2, 3, 1, 2, 3, 1, 2, 3]


 # Построение списка списков
 good_lst = [['_'] * 3 for i in range(3)]
 print(good_lst)  # -> [['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]

 good_lst[1][2] = 'X'
 print(good_lst)  # -> [['_', '_', '_'], ['_', '_', 'X'], ['_', '_', '_']]


 bad_lst = [['_'] * 3] * 3
 print(bad_lst)  # -> [['_', '_', '_'], ['_', '_', '_'], ['_', '_', '_']]

 bad_lst[1][2] = 'X'
 print(bad_lst)  # -> [['_', '_', 'X'], ['_', '_', 'X'], ['_', '_', 'X']]



 # Получится список содержащий три ссылки на один о тот же внутренний список
 my_lst = [[]] * 3
 my_lst[0].append(1)

 # Таким образом, все три элемента ссылаются на один и тот же объект.
 print(my_lst)       # Выведет: [[1], [1], [1]]                                                <-----

 # Интересный момент  +=  vs  extend()   В Python Console  Странное поведение оператора  +=    <-----
 # Будет TypeError   и Список ИЗМЕНИЛСЯ                            # Будет Работать
 t = (1, 2, [30, 40])                                              t = (1, 2, [30, 40])
 t[2] += [50, 60]                                                  t[2].extend([50, 60])
 # -> TypeError: 'tuple' object does not support item assignment
 t   # -> (1, 2, [30, 40, 50, 60])                                 t                # -> (1, 2, [30, 40, 50, 60])

 # Тоже самое с *=                                                 # Будет Работать
 t = (1, 2, [30, 40])                                              t = (1, 2, [30, 40])
 t[2] *= 2                                                         t[2].extend(t[2])
 # -> TypeError: 'tuple' object does not support item assignment
 t   # -> (1, 2, [30, 40, 30, 40])                                 t                # -> (1, 2, [30, 40, 30, 40])

 Составное присваивание (например, a += 1) в Python НЕ является АТОМАРНОЙ ОПЕРАЦИЕЙ. Это означает, что оно выполняет
 несколько шагов: чтение значения переменной, выполнение операции и запись результата обратно в переменную.
 Поэтому между этими шагами может произойти изменение значения переменной, особенно в многопоточных приложениях.

 # Если что-то НЕпонятно смотим байт-код    += *=    НЕ АТОМАРНЫЕ ОПЕРАЦИИ     Составное присваивание
 print(dis.dis('s[a] += b'))
 print(dis.dis('s[a] *= b'))


 -- Модуль copy --
 copy.copy(x)             - возвращает мелкую НЕглубокую(shallow) копию x
 copy.deepcopy(x[, memo]) - возвращает глубокую(deep) копию x

 # ВСЕ примеры создают поверхностную копию списка                                                      <-----
 res = [1, 2, 3]
 list_1 = res[:]
 list_2 = res[::]
 list_3 = res.copy()
 list_4 = list(res)
 list_5 = res + []
 print(id(res), id(list_1), id(list_2), id(list_3), id(list_4), id(list_5))
 # 2225921212864 2225893028160 2225897553216 2225903276864 2225902572736 2225902572544    # id  ВСЕ РАЗНЫЕ

 Разница между поверхностным и глубоким копированием актуальна только для составных объектов
 (объектов, которые содержат другие объекты, например списки или экземпляры классов):
 - Неглубокая копия создает новый составной объект, а затем (насколько это возможно) вставляет в него ссылки на объекты,
 найденные в оригинале.
 - Глубокая копия создает новый составной объект, а затем рекурсивно вставляет в него копии объектов, найденных в оригинале.

 При операциях глубокого копирования часто возникают две проблемы, которых нет при операциях поверхностного копирования:
 - Рекурсивные объекты (составные объекты, которые прямо или косвенно содержат ссылку на себя) могут вызывать рекурсивный цикл.
 - Потому что глубокое копирование копирует все, что может копировать слишком много, например данные, которые
  предназначены для совместного использования между копиями.

 Метод sort() - сортирует элементы списка по алфавиту в порядке возрастания НЕ СОЗДАВАЯ ЕГО КОПИЙ. # in-place  O(n log(n))
 sorted() возвращает новый отсортированный список, не затрагивая исходный.          O(n log n)
 Метод sort() сортирует элементы списка на месте (in-place), то есть изменяет исходный список и ничего не возвращает.

 Функции sorted() и метод list.sort() реализуют алгоритм TimSort                                            <-----

 Алгоритмы сортировки Python:

 1. **Пузырьковая сортировка (Bubble Sort)**:
 - **Описание**: Сравнивает пары соседних элементов и меняет их местами, если они находятся в неправильном порядке.
   Процесс повторяется, пока массив не будет отсортирован.
 - **Сложность**:
 - Время: O(n²) в худшем и среднем случаях, O(n) в лучшем (при уже отсортированном массиве).
 - Пространство: O(1). (сортировка выполняется in-place, без использования дополнительной памяти).

 2. **Сортировка выбором (Selection Sort)**:
 - **Описание**: Находит минимальный элемент в неотсортированной части массива и меняет его местами с первым элементом.
 Этот процесс повторяется для каждого элемента до конца массива.
 - **Сложность**:
 - Время: O(n²) во всех случаях.
 - Пространство: O(1).

 3. **Сортировка вставками (Insertion Sort)**:
 - **Описание**: Постепенно строит отсортированную последовательность, вставляя элементы из неотсортированной части в
 нужное место отсортированной части.
 - **Сложность**:
 - Время: O(n²) в худшем случае, O(n) в лучшем (если массив почти отсортирован).
 - Пространство: O(1).

 4. **Быстрая сортировка (Quick Sort)**:
 - **Описание**: Использует принцип "разделяй и властвуй". Выбирает опорный элемент и делит массив на две части —
 меньшие и большие по сравнению с опорным — и рекурсивно сортирует эти части.
 - **Сложность**:
 - Время: O(n log n) в среднем случае, O(n²) в худшем (при плохом выборе опорного элемента).
 - Пространство: O(log n) для рекурсии.

 5. **Сортировка слиянием (Merge Sort)**:
 - **Описание**: Разделяет массив на две половины, сортирует каждую половину и затем сливает их обратно в
 отсортированный массив. Также использует "разделяй и властвуй".
 - **Сложность**:
 - Время: O(n log n) во всех случаях.
 - Пространство: O(n).

 6. **Пирамидальная сортировка (Heap Sort)**:
 - **Описание**: Преобразует массив в структуру кучи (heap) и извлекает элементы по одному, чтобы получить
 отсортированный массив. Работает in-place.
 - **Сложность**:
 - Время: O(n log n) во всех случаях.
 - Пространство: O(1).

 7. **Тим-сорт (Tim Sort)**:
 - **Описание**: Адаптивный алгоритм, основанный на сортировке вставками и сортировке слиянием. Используется в Python в
 функции `sorted()` и методе `list.sort()`.
 - **Сложность**:
 - Время: O(n log n) в среднем, O(n) в лучшем случае (при почти отсортированных данных).
 - Пространство: O(n).

 8. **Сортировка Шелла (Shell Sort)**:
 - **Описание**: Улучшенная версия сортировки вставками, которая сравнивает и сортирует элементы, находящиеся на
 определенном расстоянии друг от друга, уменьшая расстояние до 1.
 - **Сложность**:
 - Время: O(n²) в худшем, O(n log n) в среднем (зависит от выбора последовательности).
 - Пространство: O(1).

 9. **Сортировка битом (Radix Sort)**:
 - **Описание**: Сортирует элементы по каждому разряду значений, начиная с наименьших и переходя к более значительным.
 Чаще всего используется для сортировки целых чисел.
 - **Сложность**:
 - Время: O(nk), где k — количество разрядов.
 - Пространство: O(n + k).

 10. **Сортировка подсчётом (Counting Sort)**:
 - **Описание**: Использует дополнительный массив для подсчета числа вхождений каждого уникального элемента.
   Сравнительно быстрая, но работает только для ограниченных диапазонов значений.
 - **Сложность**:
 - Время: O(n + k), где k — максимальное значение в массиве.
 - Пространство: O(k).

 11. **Сортировка по ведрам (Bucket Sort)**:
 - **Описание**: Делит элементы на несколько "ведер" и сортирует каждое ведро индивидуально, после чего объединяет все
   ведра в окончательный отсортированный массив.
 - **Сложность**:
 - Время: O(n + k) для равномерно распределенных данных, где k — количество ведер.
 - Пространство: O(n + k).


 Плоский Список(Flat list) - одномерный массив
 Вложенный список(Nested list) - многомерный массив
 Связный список(Linked List) - эффективность добавления элементов в начало, середину и конец.
 Связный список (связанный, список узлов и ссылок или указателей) (Linked List) - deque
 deque - doubly-linked list (двусвязный список)

 Односвязный список(Singly linked list) - однонаправленный связный список, можно передвигаться только в сторону конца списка.
 В практике применим редко, В Python встроенной реализации не имеет. Можно написать свою реализацию или deque

 Односвязный - Потому что каждый элемент хранит ровно 1 связь.   1 указатель/ссылка на следующий элемент
 Двусвязный - Хранит  указатель/ссылку на предыдущий и на следующий элемент


 Связный список
 Cвязный список (linked list) позволяет хранить элементы в цепи ячеек, которые не обязательно должны находиться
 в последовательных адресах памяти. Память для ячеек выделяется по мере необходимости. Каждая ячейка имеет указатель,
 сообщающей об адресе следующей в цепи. Ячейка с пустым указателем (NULL) отмечает конец цепи. Связные списки
 используются для реализации стеков, списков и очередей. При наращивании связного списка не возникает никаких проблем:
 любая ячейка может храниться в любой части памяти. Таким образом, размер списка ограничен только объемом имеющейся
 свободной памяти. Также не составит труда вставить элементы в середину списка или удалить их — достаточно просто
 изменить указатели ячеек.

 Связный список тоже имеет свои недостатки: мы не можем сразу получить n-й элемент. Сначала придется прочитать первую
 ячейку, извлечь из нее адрес второй ячейки, затем прочитать вторую ячейку, извлечь из нее указатель на следующую
 ячейку и т. д., пока мы не доберемся до n-й ячейки. Кроме того, когда известен адрес всего одной ячейки, не так просто
 ее удалить или переместиться по списку назад. Не имея другой информации, нельзя узнать адрес предыдущей ячейки в цепи.


 Двусвязный список
 Двусвязный список (double linked list) — это связный список, где ячейки имеют два указателя: один на предыдущую ячейку,
 другой — на следующую.

 Он обладает тем же преимуществом, что и связный список: не требует предварительного выделения большого блока памяти,
 потому что пространство для новых ячеек может выделяться по мере необходимости. При этом дополнительные указатели
 позволяют двигаться по цепи ячеек вперед и назад. В таком случае, если известен адрес всего одной ячейки, мы сможем быстро ее удалить.

 И тем не менее мы по-прежнему не имеем прямого доступа к n-му элементу. Кроме того, для поддержки двух указателей в
 каждой ячейке требуется более сложный код и больше памяти.


 Массив (array) — это самый простой способ хранения набора элементов в памяти компьютера. Он заключается в выделении
 единого пространства в памяти и последовательной записи в него ваших элементов. Конец последовательности отмечается
 специальным маркером NULL.

 Каждый объект в массиве занимает такой же объем памяти, что и любой другой. Представим массив, начинающийся с
 адреса ячейки памяти s, где каждый элемент занимает b байт. Чтобы получить n-й элемент, нужно извлечь b байт,
 начиная с позиции в памяти    s + (b × n).
 Это позволяет напрямую обращаться к любому элементу массива.

 Когда нам понадобится пятый элемент мы перейдем к элементу по адресу M + 5. То есть, если мы хотим получить i-й
 элемент из нашего массива, мы переходим к M + i
 Итак, если данные хранятся в памяти по порядку, то зная порядок расположенных данных, мы можем найти их за один шаг O(1)


 Массив
  - Предоставляет Константный доступ по индексу.
  - Занимает НЕПРЕРЫВНЫЙ кусок в памяти.
  - Размер куска памяти фиксируется при создании.

  Массив — это структура данных, которая хранит набор элементов одного типа и фиксированного размера.
  В массиве все элементы занимают одинаковое количество памяти, поскольку массив состоит из элементов одного типа.
  Это значит, что каждый элемент массива будет иметь одинаковый размер в байтах.

 Список и Массив  list vs array
 Список - можно хранить объекты разного размера и типа/размер списка НЕ ограничен
 Массив - Данные одного типа/размер массива ограничен.  Размер массива задается в момент создания.
 для создания массива нужно использовать  array.array, NumPy np.array()


  -- Как хранятся элементы списка и массива в памяти --                                       <-----     <-----

 В Python списки (объект типа `list`) представляют собой динамические структуры данных,
 и их элементы могут храниться в разных местах памяти.                                                  <-----

 Все элементы массива располагаются рядом друг с другом в оперативной памяти. Это свойство массивов позволяет эффективно
 использовать память и обеспечивает быстрый доступ к элементам массива.                                 <-----

 Random Access Memory(RAM, ОЗУ) - Оперативная память   ОЗУ - Оперативное запоминающее устройство

 Все элементы массива располагаются один за другим в непрерывном блоке памяти. Это позволяет быстро получать
 доступ к элементам массива с использованием индексов, поскольку адрес каждого элемента можно вычислить на основе
 адреса первого элемента и размера каждого элемента

 Элементы списков хранятся не обязательно в смежных участках памяти. Когда вы добавляете элементы в список, Python
 может выделять память динамически, что может означать размещение элементов в разных областях памяти.

 Массивы имеют фиксированный тип и размещение элементов в памяти, тогда как Списки представляют собой более гибкие
 структуры данных, которые могут содержать разные типы элементов, и размещение их в памяти НЕ обязательно будет смежным.



 Внутреннее строение списка — массив (точнее, vector) указателей, т. е. список является динамическим массивом.
 Вектор - одномерный массив проиндексированных элементов. В NumPy np.array()
 Векторы: это массивы с динамическим размером. размер памяти не ограничен

 Чтобы получить производительность с амортизируемым временем O(1) для вставок и удалений, новые элементы должны
 добавляться в конец списка методом append() и снова удалятся из конца методом pop().

 Добавление и удаление элементов вначале списка намного медленнее и занимает O(n) времени, поскольку существующие
 элементы должны сдвигаться, чтобы создать место для нового элемента. Такого антишаблона производительности следует избегать.

 К линейным структурам можно отнести массивы, очереди, стеки, деки, линейные списки. Структуры стека.   <-----


 -- Модуль bisect - реализация алгоритма бинарного поиска
 Обеспечивает поддержку вставки значений в отсортированный список, без необходимости сортировать этот список после каждой вставки.
 Бинарный поиск существенно быстрее, чем обычный, но требует предварительной сортировки коллекции, по которой осуществляется поиск.

 bin_lst = [-1, -3, 2, 4, 5, 7, 8, 9]  # Будем искать 9
 # БИНАРНЫЙ поиск  O(log n) - кратко как работает берем серединный элемент например  4
 # сравниваем 9 > 4  и будем искать  ТОЛЬКО УЖЕ ТУТ   [4, 5, 7, 8, 9]
 # берем серединный элемент например  7
 # сравниваем 9 > 7  и будем искать  ТОЛЬКО УЖЕ ТУТ   [8, 9]

 Левый и Правый двоичный поиск
 from bisect import bisect_left, bisect_right


 --- tuple (Кортеж) ---
 tuple кортеж, НЕИЗМЕНЯЕМЫЙ УПОРЯДОЧЕННЫЙ, обычно хранит значения РАЗНЫХ типов, О(1) доступ к элементу
 используй кортежи везде, где это возможно и обоснованно
 tuple хранит ссылки на элементы,
 список внутри кортежа мы можем изменить потому что кортеж хранит ссылки,
 tuple в Python оптимизированы Python знает сколько места сразу нужно занять в памяти
 используй () для создания пустого tuple

 # Хороший пример  Список внутри кортежа мы МОЖЕМ изменить потому что кортеж хранит ссылки
 a_tuple = (1, 2, ['a', 'b'])
 a_tuple[2].append(9999999)
 print(a_tuple)  # -> (1, 2, ['a', 'b', 9999999])


  # Кортеж внутри МЕНЯЕТ ссылку   l2 = list(l1)         Можно попробовать вариант  l2=l1  Будет другой результат)
 l1 = [3, [66, 55, 44], (7, 8, 9)]
 l2 = list(l1)
 #l2 = l1

 l1.append(100)
 l1[1].remove(55)
 # Ссылки внутри ОДИНАКОВЫЕ
 print(id(l1[1]), id(l1[1]), id(l1[2]))  # -> 2413815154112 2413815154112 2415377546240
 print(id(l2[1]), id(l2[1]), id(l2[2]))  # -> 2413815154112 2413815154112 2415377546240

 l2[1] += [33, 22]
 l2[2] += (10, 11)

 print(id(l1[1]), id(l1[1]), id(l1[2]))  # -> 2413815154112 2413815154112 2415377546240

 # ИЗМЕНИЛАСЬ Только ссылка на Кортеж    Был создан НОВЫЙ КОРТЕЖ                                    <-----
 print(id(l2[1]), id(l2[1]), id(l2[2]))  # -> 2413815154112 2413815154112 2413814827904

 # Объекты РАЗНЫЕ
 print(id(l1), id(l2))                   # -> 2415381110464 2413796420288


  -- Интересный факт про append --
 Иногда append может быть за O(n)
 Таким образом, хотя `append` обычно выполняется за O(1), в особых ситуациях, связанных с перераспределением памяти,
 она может быть O(n). В среднем же, для обычного использования, её можно считать O(1).

 в Python операция `append` для списков в основном выполняется за O(1), но в редких случаях, когда требуется
 перераспределение памяти (например, при достижении максимальной емкости списка), она может занять O(n).
 Поэтому в худшем случае, когда происходит перераспределение, `append` может быть O(n), но это случается редко и в
 среднем время выполнения составляет O(1).


  --- Символьный ад ---
 Уберите детей от экрана.

 {'a': 1}   # это словарь
 dict(a=1)  # это то же самое
 {'a'}      # это множество (set)
 {}         # это не пустое множество, это пустой словарь
 set()      # а вот это пустое множество

 1        # это просто число (int)
 (1)      # это тоже просто число
 (1, 2)   # это кортеж (tuple)
 1,       # это тоже кортеж
 ()       # это пустой кортеж
 (1+2)    # это число
 tuple()  # а это тоже пустой кортеж

 []                     # это пустой список
 [1]                    # это список с одним элементом
 [1, 2, 3]              # это список с 3 элементами
 [i for i in range(3)]  # это тоже список с 3 элементами
 ()                     # это пустой кортеж
 (1)                    # это просто число
 (1, 2, 3)              # это (ха-ха) кортеж
 (i for i in range(3))  # это не кортеж, это генератор :D


 --- Символьный ад  2 ---

 {'a': 1}               # это словарь
 {'a'}                  # это множество (set)
 {}                     # это не пустое множество, это пустой словарь
 set()                  # а вот это пустое множество

 1                      # это просто число (int)
 (1+2)                  # это число
 (1, 2)                 # это кортеж (tuple)
 (1)                    # это тоже просто число
 1,                     # это тоже кортеж
 ()                     # это пустой кортеж
 (,)                    # SyntaxError

 []                     # это пустой список
 [1]                    # это список с одним элементом
 [1, 2, 3]              # это список с 3 элементами
 [i for i in range(3)]  # это тоже список с 3 элементами
 ()                     # это пустой кортеж
 (1)                    # это просто число
 (1, 2, 3)              # это кортеж
 (i for i in range(3))  # это не кортеж, это generator expression


 Модель Random Access Machine (RAM) - это абстрактная модель вычислений, которая используется для анализа алгоритмов.
 Основная цель RAM — упростить анализ сложности алгоритмов, предполагая мгновенный доступ к данным.


 ПРОСТРАНСТВЕННАЯ (емкостаная) СЛОЖНОСТЬ - относиться к объему используемой ПАМЯТИ, необходимой для выполнения алгоритма


 --- Big O или Big Oh    Временная сложность ---
 Простыми словами Big O показывает как будет меняться производительность алгоритма в зависимости от роста входящих данных.
 Описания алгоритмической сложности. Она показывает, как сильно увеличится количество операций при увеличении размера данных.
 Примеры нотаций Big O:

 O(1):  Константная сложность. - Время выполнения алгоритма остается постоянным и не зависит от объема данных.
 O(log n): Логарифмическая сложность.  - Время выполнения алгоритма растет медленно с увеличением размера входных данных.
 Например Бинарный поиск в отсортированном массиве.   Везде где есть ДЕЛЕНИЕ будет   O(log n)   Бинарное дерево   <-----
 O(n):  Линейная сложность. - Время выполнения алгоритма растет линейно с увеличением размера входных данных.
 O(n log n): Линейно-логарифмическая сложность. - Время выполнения алгоритма растет быстрее, чем линейно, но медленнее,
 чем квадратично. Например, сортировка слиянием (merge sort).
 O(n^2): Квадратичная сложность. - Время выполнения алгоритма зависит от квадрата размера входных данных.
 Например, сортировка пузырьком (bubble sort).
 O(n^3): Кубическая сложность. Время выполнения алгоритма зависит от размера входных данных в кубе. Например, алгоритмы,
 которые имеют три вложенных цикла, такие как некоторые методы многомерной обработки данных.
 O(n!): Факториальная сложность. Это самая высокая степень роста времени выполнения алгоритма. Время выполнения
 алгоритма растет факториально от размера входных данных. Этот тип сложности встречается, например, при переборе всех
 возможных комбинаций элементов, что делает его чрезвычайно неэффективным для больших значений n.

 Скорость Алгоритмов:
 0(1) -> O(log n) -> O(n) -> O(n log n) -> O(n^2) -> O(n^3) -> O(n!)

 Худший случай (worst case)    - Когда входные данные требуют максимальных затрат времени и памяти.
 Лучший случай (best case)     - Полная противоположность worst case, самые удачные входные данные.
 Средний случай (average case) - Самый хитрый из тройки. Между best case и worst case, расчёт average case - Дело сложное

 Amortized Амортизированный анализ — это метод анализа сложности данного алгоритма или того , сколько ресурсов, особенно
 времени или памяти, требуется для его выполнения . Усредняет время выполнения операций в последовательности


 Big O или Big Oh - Верхняя оценка сложности алгоритма      Асимптотика    Асимптотический анализ
 Нотация большого O обозначает верхнюю границу времени выполнения алгоритма. Таким образом,
 она указывает на сложность алгоритма в худшем случае.

 --- Как определить Вычислительную сложность АЛГОРИТМА? ---

 O(1):  Константная сложность.
 Пример как работает   НЕ важно сколько команд входит в эту операцию:

 # Не важно сколько операций ГЛАВНОЕ ЧТОБЫ ВСЕ ОНИ БЫЛИ ЗА O(1)
 var_a = 10
 inf = float('inf')
 print(inf)

 # Все отбрасывается  Будет только 0(1)
 0(10) = 0(1)
 0(C) = 0(1)
 0(1 + 1 + 1) = 0(3) = 0(1)



 O(n):  Линейная сложность.  n == переменная

 # Число итераций зависит от Размерности lst    ЧЕМ  БОЛЬШЕ N тем больше итераций нужно сделать в цикле
 lst = [1, 4, 10, -5, 0, 2, 3, 18, 32]

 # Все команды будут 0(n)
 for i in lst:
     print(i, end=' ')
     i += 1

 # Все отбрасывается  Будет только 0(n)
 0(2n) = 0(n)
 0(1 + n) = 0(n)
 0(A * n) = 0(n)
 0(n + C) = 0(n)
 0(A * n + C) = 0(n)

 # Более сложный пример КАК РАСЧИТАТЬ СЛОЖНОСТЬ АЛГОРИТМА  Вычислительная сложность АЛГОРИТМА       <-----     <-----

 # 0(1)  Выполнение команды
 lst = [1, 4, 10, -5, 0, 2, 3, 18, 32]   # 0(1)  Выполнение команды

 # 0(n)  Выполнение Цикла
 for i, v in enumerate(lst):             # 0(n)  Выполнение Цикла
     lst[i] += 1

 # 0(n)  Выполнение Цикла
 for i in lst:                           # 0(n)  Выполнение Цикла
     print(i, end=' ')

 # Важно!!!  <-----                                                                                            <-----
 # Выходит следующее выражение   0(1) + 0(n) + 0(n) = 0(1 + n + n) = 0(1 + 2n)   Все отбрасывается  Будет только 0(n)


 # Правила сложения и умножения   ОТБРАСЫВАТЬ МОЖНО ТОЛЬКО КОНСТАНТЫ НО НЕ ПЕРЕМЕННЫЕ                          <-----

 # Для последовательных циклов сложение(+)   0(n + m)
 # 0(n)  Выполнение Цикла
 for i in range(n):             # 0(n)  Выполнение Цикла    переменная  n
     print(i)

 # 0(n)  Выполнение Цикла
 for i in range(m):             # 0(n)  Выполнение Цикла    переменная  m
     print(i)

 # Важно!!!  <-----         ОТБРАСЫВАТЬ МОЖНО ТОЛЬКО КОНСТАНТЫ НО НЕ ПЕРЕМЕННЫЕ                                <-----
 # Выходит следующее выражение  0(n) + 0(m) = 0(n + m)   СОКРАТИТЬ НИЧЕГО НЕЛЬЗЯ ПОТОМУ ЧТО ОБЕ ВЕЛИЧИНЫ == ПЕРЕМЕННЫЕ!!


 # Для вложенных циклов умножение(*)   0(n * m)
 for i in range(n):                             # 0(n * m) Потому что делаем перебор
     for j in range(m):
         print(i, y)

 # Если n = m:   0(n + n) = 0(2n) = 0(n)      При ВЛОЖЕННЫХ ЦИКЛАХ будет 0(n^2)   0(n * n) = 0(n^2)   Важно!!! <-----



 НЕ ВАЖНАЯ СЛОЖНОСТЬ
 # Для вложенных циклов умножение(*)   0(n * n) = 0(n^2)
 for i in range(n):                             # 0(n * n) = 0(n^2)
     for j in range(n):
         print(i, y)

 # Добавили еше 1 цикл
 # 0(n^2 + n) = 0(n^2)
 for t in range(n):                             # 0(n^2 + n) = 0(n^2)
     print(t)

 # Объяснение n^2  ЗНАЧИТЕЛЬНО БОЛЬШЕ ЧЕМ   n - Можно отбросить  будет 0(n^2 + n) = 0(n^2)
 # Если одно слагаемое принимает значение БОЛЕЕ ЧЕМ В 2 (ДВА) РАЗА БОЛЬШЕ чем другое значит МОЖНО ОТБРОСИТЬ
 # Примеры: 0(n + log n) = 0(n)         0(3n + 10n^2 + 2^n) = 0(n +10n^2 + 2^n) = 0(2^n)    Важно!!!   <-----
 # Если ничего не знаем об A      0(2^n + A) = 0(2^n + A)   Ничего не отбрасываем



 O(log n): Логарифмическая сложность.   Как пример бинарный поиск

 bin_lst = [-1, -3, 2, 4, 5, 7, 8, 9]  # Будем искать 9
 # БИНАРНЫЙ поиск  O(log n) - кратко как работает берем серединный элемент например  4
 # сравниваем 9 > 4  и будем искать  ТОЛЬКО УЖЕ ТУТ   [4, 5, 7, 8, 9]
 # берем серединный элемент например  7
 # сравниваем 9 > 7  и будем искать  ТОЛЬКО УЖЕ ТУТ   [8, 9]



 --- Big O различных операций в CPython ---
 n — это количество элементов, находящихся в контейнере в данный момент.
 k — это либо значение параметра, либо количество элементов в параметре.


 Список (list):
 Внутри список представлен как массив; самые большие затраты возникают из-за превышения текущего размера выделения
 (поскольку все должно перемещаться) или из-за вставки или удаления где-то в начале (поскольку все, что после этого,
 должно перемещаться). Если вам нужно добавить/удалить на обоих концах, рассмотрите возможность использования вместо
 этого Collections.deque.

 Operation  Average Case  Amortized Worst Case
 Copy       O(n)          O(n)
 Append     O(1)          O(1)
 Pop last   O(1)          O(1)
 Pop(i)     O(n)          O(n)
 Insert     O(n)          O(n)
 Get Item   O(1)          O(1)
 Set Item   O(1)          O(1)
 Del Item   O(n)          O(n)
 Iteration  O(n)          O(n)
 Get Slice  O(k)          O(k)
 Del Slice  O(n)          O(n)
 Set Slice  O(k+n)        O(k+n)
 Extend     O(k)          O(k)
 Sort       O(n log n)    O(n log n)
 Multiply * O(nk)         O(nk)
 x in s     O(n)
 min(s)     O(n)
 max(s)     O(n)
 Get Length O(1)          O(1)
------------------------------------------------------------------------------------------------------------------------

 collections.deque
 collections.deque (double-ended queue) (двусторонняя очередь) внутренне представляется как двусвязный список.
 (Ну, для большей эффективности это список массивов, а не объектов.) Оба конца доступны, но даже просмотр середины
 происходит медленно, а добавление или удаление из середины происходит еще медленнее.

 Operation  Average Case  Amortized Worst Case
 Copy       O(n)          O(n)
 append     O(1)          O(1)
 appendleft O(1)          O(1)
 pop        O(1)          O(1)
 popleft    O(1)          O(1)
 extend     O(k)          O(k)
 extendleft O(k)          O(k)
 rotate     O(k)          O(k)
 remove     O(n)          O(n)
 Get Length O(1)          O(1)
------------------------------------------------------------------------------------------------------------------------

 Множество (set)
 реализация намеренно очень похожа на Словарь (dict)
 ПОИСК/ВСТАВКА/УДАЛЕНИЕ в среднем за O(1) в худшем O(n)  set реализован как хеш-таблица

 Operation                         Average Case            Amortized Worst Case
 x in s                            O(1)                    O(n)
 Union s|t                         O(len(s)+len(t))
 Intersection s&t                  O(min(len(s), len(t)))  O(len(s) * len(t))     замените «min» на «max», если t не является set
 Multiple intersection s1&s2&..&sn                         (n-1)*O(l) where l is max(len(s1),..,len(sn))
 Difference s-t                    O(len(s))
 s.difference_update(t)            O(len(t))
 Symmetric Difference s^t          O(len(s))               O(len(s) * len(t))
 s.symmetric_difference_update(t)  O(len(t))               O(len(t) * len(s))
------------------------------------------------------------------------------------------------------------------------

 Словарь (dict)
 ПОИСК/ВСТАВКА/УДАЛЕНИЕ в среднем за O(1) в худшем O(n)  dict реализован как хеш-таблица

 Operation  Average Case  Amortized Worst Case
 k in d     O(1)          O(n)
 Copy       O(n)          O(n)
 Get Item   O(1)          O(n)
 Set Item   O(1)          O(n)
 Del Item   O(1)          O(n)
 Iteration  O(n)          O(n)


 --- Интересный факт ---                                                                        <------
 Обратите внимание, что существует быстрый путь для dict, которые (на практике) работают только с ключами str; это НЕ
 влияет на алгоритмическую сложность, но может существенно влиять на постоянные факторы: как быстро завершается типичная программа.

 -- Строковые (str) ключи в словаре (dict) - ПРЕДПОЧТИТЕЛЬНЕЕ    работают быстрее               <------
 # Таким образом, использование строковых ключей происходит примерно на 30% +/- быстрее даже по сравнению с int ключами
 print(timeit.timeit('a["500"]', 'a ={}\nfor i in range(1000): a[str(i)] = i')) # Поиск str key быстрее
 # 0.04318580008111894
 print(timeit.timeit('a[500]', 'a ={}\nfor i in range(1000): a[i] = i'))        # Поиск str key быстрее чем другие
 # 0.068284000037238

 -- Но специально преобразовывать их будет Дороже                                               <------
 Преобразование вашего ключа в строку будет стоить (намного) дороже, чем небольшой прирост, который вы могли бы получить,
 если бы он был строкой для dict.
 # Как вы можете видеть, несмотря на то, что строковый dict работает быстрее, преобразование ключа по сравнению с ним
 очень затратно, что полностью снижает выигрыш (и даже больше).

 print(timeit.timeit("a[500]", "a={key: 1 for key in range(1000)}" ))
 # 0.06836240005213767
 print(timeit.timeit("a[\"500\"]", "a={str(key): 1 for key in range(1000)}" ))
 # 0.04284069989807904
 print(timeit.timeit("a[str(500)]", "a={str(key): 1 for key in range(1000)}" ))
 # 0.16474479995667934

 -- Вывод --                                                                                    <------
 Так что да, если данные, которые вы используете, используются только в качестве ключей к словарю, и не имеет значения,
 в каком формате вы их храните, тогда предпочтительнее использовать строки в небольшом словаре. <------



 --- Затраты памяти и времени на выполнение append и списковое включение (list comprehensions) memory_profiler ---

 #  В общем случае, списковое включение (list comprehension) обычно потребляет меньше памяти,
 #  чем использование метода append, потому что оно создает список в одном непрерывном блоке памяти, тогда как метод
 #  append может потребовать больше перераспределений памяти, если список растет.

 from memory_profiler import memory_usage, profile

 # Функция с использованием append
 def create_list_append():
     l = []
     for i in range(100_000):
         l.append(i * 2)
     return l

 # Функция с использованием спискового включения
 def create_list_comprehension():
     return [i * 2 for i in range(100_000)]

 # Замер памяти
 def memory_usage_test():
     # Замер памяти для append
     append_mem_usage = memory_usage((create_list_append, ))
     print(f"Peak memory usage for append: {max(append_mem_usage) - min(append_mem_usage)} MiB")

     # Замер памяти для спискового включения
     comprehension_mem_usage = memory_usage((create_list_comprehension, ))
     print(f"Peak memory usage for comprehension: {max(comprehension_mem_usage) - min(comprehension_mem_usage)} MiB")

 @profile
 def profile_memory():
     create_list_append()
     create_list_comprehension()

 # Запуск теста
 if __name__ == '__main__':
     memory_usage_test()
     profile_memory()

 # Выводы Всегда разные проверь сам!!!                                                       <-----   <-----
 # Peak memory usage for append:        1.59375 MiB
 # Peak memory usage for comprehension: 1.37109375 MiB


 # Line #    Mem usage    Increment  Occurrences   Line Contents
 # =============================================================
 #   1076     22.1 MiB     22.1 MiB           1   @profile
 #   1077                                         def profile_memory():
 #   1078     22.4 MiB      0.3 MiB           1       create_list_append()
 #   1079     22.4 MiB      0.0 MiB           1       create_list_comprehension()



 # Легкий пример как использовать profile                                                      <-----   <-----
 from memory_profiler import profile

 @profile
 def my_function():
     a = [i for i in range(100000)]
     b = [i * 2 for i in a]
     return b

 if __name__ == "__main__":
     my_function()

 # Line #    Mem usage    Increment  Occurrences   Line Contents
 # =============================================================
 #   1052     20.9 MiB     20.9 MiB           1   @profile
 #   1053                                         def my_function():
 #   1054     22.9 MiB      2.0 MiB      100001       a = [i for i in range(100000)]
 #   1055     25.1 MiB  -1819.1 MiB      100001       b = [i * 2 for i in a]
 #   1056     25.1 MiB      0.0 MiB           1       return b



 # Более того даже если мы создадим список без добавления (append), он всеравно будет занимать больше памяти чем кортеж
 # Потому что списки хранят дополнитулью информацию о своем текущем состоянии, чтобы эффективно изменять размер <------
 # Дополнительная информация занимает мало места (порядка одного дополнительного элемента) однако при использовании
 # миллиона списков мы начинаем чувствовать разницу!                                                            <------

 import timeit
 import sys

 # Функции для создания списка и кортежа
 def create_list():
     return [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

 def create_tuple():
     return (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)

 # Замер времени
 list_time = timeit.timeit(create_list, number=1000000)
 tuple_time = timeit.timeit(create_tuple, number=1000000)

 # Размеры
 list_size = sys.getsizeof(create_list())
 tuple_size = sys.getsizeof(create_tuple())

 # Вывод результатов
 print(f'Время создания списка: {list_time:.3f} секунд')    # ->  Время создания списка:  0.147 секунд
 print(f'Время создания кортежа: {tuple_time:.3f} секунд')  # ->  Время создания кортежа: 0.078 секунд
 print(f'Размер списка: {list_size} байт')                  # ->  Размер списка:          68 байт
 print(f'Размер кортежа: {tuple_size} байт')                # ->  Размер кортежа:         60 байт



 -- Вычисляем, сколько оперативной памяти используем! --

 memit берет данные об использовании ОЗУ от оперативной системы, а asizeof запрашивает у объектов их размер(который может
 быть сообщен неверно). Обычно asizeof работает медленнее, чем memit, но asizeof полезна при анализе небольших объектов
 Функция memit, вероятно, более подходит для реальных приложений, поскольку измеряет потребление памяти точнее (не на
 основе предположений)


 # memory_usage позволяет более точно оценить потребление памяти в реальном времени, а asizeof может быть полезен для
 # анализа небольших объектов, когда вам нужно измерить их размер более детально.

 from pympler.asizeof import asizeof                                                 <-----   <----- Интересный импорт

 # Создание списка
 my_list = [i for i in range(1000000)]

 # Измерение размера объекта
 size = asizeof(my_list)
 print(f"Size of my_list: {size} bytes")                # -> Size of my_list: 20224368 bytes



 from memory_profiler import memory_usage

 def my_function():
     return [i for i in range(1000000)]

 if __name__ == '__main__':
     # Измерение использования памяти
     mem_usage = memory_usage(my_function)
     print(f"Peak Memory Usage: {max(mem_usage)} MiB")  # -> Peak Memory Usage: 79.70703125 MiB




 --- Замеры размеров Python ---

                            -- Примеры Списков deque vs list --                                 <-----
 my_list = [1, 2, 3, 4, 5]
 print(f'getsizeof list:  {sys.getsizeof(my_list)} байт')     # -> getsizeof list:  104 байт
 print(f'asizeof   list:  {asizeof.asizeof(my_list)} байт')   # -> asizeof   list:  264 байт

 from collections import deque
 my_deque = deque([1, 2, 3, 4, 5])
 print(f'getsizeof deque: {sys.getsizeof(my_list)} байт')     # -> getsizeof deque: 104 байт
 print(f'asizeof   deque: {asizeof.asizeof(my_deque)} байт')  # -> asizeof   deque: 760 байт



                            -- Примеры Кортежей namedtuple vs tuple --                          <-----
 my_tuple = (1, 2, 3, 4, 5)
 print(f'getsizeof tuple:       {sys.getsizeof(my_tuple)} байт')    # -> getsizeof tuple:       80 байт
 print(f'asizeof   tuple:       {asizeof.asizeof(my_tuple)} байт')  # -> asizeof   tuple:       240 байт

 from collections import namedtuple
 nt_tuple = namedtuple('nt_tuple', ['a', 'b', 'c', 'd', 'e'])
 p = nt_tuple(1, 2, c=3, d=4, e=5)
 print(f'getsizeof namedtuple:  {sys.getsizeof(p)} байт')           # -> getsizeof namedtuple:  80 байт
 print(f'asizeof   namedtuple:  {asizeof.asizeof(p)} байт')         # -> asizeof   namedtuple:  240 байт

 Создание объекта namedtuple накладывает некоторые накладные расходы (например, ХРАНЕНИЕ ИМЕН ПОЛЕЙ), но для
 НЕБОЛЬШИХ ОБЪЕКТОВ эти накладные расходы могут быть минимальными.
 Однако в большинстве случаев namedtuple будет занимать больше места в памяти по сравнению с обычным кортежем


                            -- Примеры Словарей OrderedDict vs dict --                          <-----
 my_dict = {1: 'a', 2: 'b', 3: 'c'}
 print(f'getsizeof dict:         {sys.getsizeof(my_dict)} байт')     # -> getsizeof dict:       224 байт
 print(f'asizeof   dict:         {asizeof.asizeof(my_dict)} байт')   # -> asizeof   dict:       488 байт


 from collections import OrderedDict
 or_dict = OrderedDict({1: 'a', 2: 'b', 3: 'c'})

 print(f'getsizeof OrderedDict:  {sys.getsizeof(or_dict)} байт')    # -> getsizeof OrderedDict: 448 байт
 print(f'asizeof   OrderedDict:  {asizeof.asizeof(or_dict)} байт')  # -> asizeof   OrderedDict: 712 байт



                            -- Примеры Множества frozenset vs set --                            <-----
 my_set = {1, 2, 3, 4, 5}
 print(f'getsizeof set:        {sys.getsizeof(my_set)} байт')     # -> getsizeof set:       472 байт
 print(f'asizeof   set:        {asizeof.asizeof(my_set)} байт')   # -> asizeof   set:       632 байт


 fz_set = frozenset({1, 2, 3, 4, 5})

 print(f'getsizeof frozenset:  {sys.getsizeof(fz_set)} байт')    # -> getsizeof frozenset:  472 байт
 print(f'asizeof   frozenset:  {asizeof.asizeof(fz_set)} байт')  # -> asizeof   frozenset:  632 байт

 # Если list внутри
 fz_set = frozenset([1, 2, 3, 4, 5])

 print(f'getsizeof frozenset:  {sys.getsizeof(fz_set)} байт')    # -> getsizeof frozenset:  728 байт
 print(f'asizeof   frozenset:  {asizeof.asizeof(fz_set)} байт')  # -> asizeof   frozenset:  888 байт

 # Если tuple внутри
 fz_set = frozenset((1, 2, 3, 4, 5))

 print(f'getsizeof frozenset:  {sys.getsizeof(fz_set)} байт')    # -> getsizeof frozenset:  728 байт
 print(f'asizeof   frozenset:  {asizeof.asizeof(fz_set)} байт')  # -> asizeof   frozenset:  888 байт



                            -- Примеры Строки/Числа str vs int --                              <-----
 my_string = "Hello, World!"
 print(f'getsizeof str:  {sys.getsizeof(my_string)} байт')    # -> getsizeof str:  62 байт
 print(f'asizeof   str:  {asizeof.asizeof(my_string)} байт')  # -> asizeof   str:  64 байт


 my_int = 10000

 print(f'getsizeof int:  {sys.getsizeof(my_int)} байт')       # -> getsizeof int:  28 байт
 print(f'asizeof   int:  {asizeof.asizeof(my_int)} байт')     # -> asizeof   int:  32 байт


 # Чем больше строка тем больше размер так же и с другими обьектами

 my_string = "a"

 print(f'getsizeof str:  {sys.getsizeof(my_string)} байт')    # -> getsizeof str:  50 байт
 print(f'asizeof   str:  {asizeof.asizeof(my_string)} байт')  # -> asizeof   str:  56 байт


 my_int = 1

 print(f'getsizeof int:  {sys.getsizeof(my_int)} байт')       # -> getsizeof int:  28 байт
 print(f'asizeof   int:  {asizeof.asizeof(my_int)} байт')     # -> asizeof   int:  32 байт



                            -- Сравнение slots vs no_slots --                                  <-----
                            -- @dataclass(slots=True)  vs  @dataclass() --
 from dataclasses import dataclass

 @dataclass(slots=True)
 class WithSlots:
     value: int

 with_slots = WithSlots(10)
 print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  40 байт
 print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  72 байт


 @dataclass
 class NoSlots:
     value: int

 no_slots = NoSlots(10)
 print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    56 байт
 print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    440 байт



  --- Замеры ПУСТЫХ обьектом встроенных Python ---

                             -- Примеры list vs [] vs deque() vs heapq --                       <-----

 my_list = list()
 print(f'getsizeof list():      {sys.getsizeof(my_list)} байт')    # -> getsizeof list():       56 байт
 print(f'asizeof   list():      {asizeof.asizeof(my_list)} байт')  # -> asizeof   list():       56 байт

 my_list = []
 print(f'getsizeof []:          {sys.getsizeof(my_list)} байт')    # -> getsizeof []:           56 байт
 print(f'asizeof   []:          {asizeof.asizeof(my_list)} байт')  # -> asizeof   []:           56 байт


 from collections import deque

 my_deque = deque()
 print(f'getsizeof deque():     {sys.getsizeof(my_deque)} байт')    # -> getsizeof deque():     760 байт
 print(f'asizeof   deque():     {asizeof.asizeof(my_deque)} байт')  # -> asizeof   deque():     760 байт


 import heapq

 my_heapq = []
 heapq.heapify(my_heapq)
 print(f'getsizeof heapq:       {sys.getsizeof(my_heapq)} байт')    # -> getsizeof heapq:       56 байт
 print(f'asizeof   heapq:       {asizeof.asizeof(my_heapq)} байт')  # -> asizeof   heapq:       56 байт





                             -- Примеры set() vs frozenset() --                                 <-----

 my_set = set()
 print(f'getsizeof set():       {sys.getsizeof(my_set)} байт')    # -> getsizeof set():         216 байт
 print(f'asizeof   set():       {asizeof.asizeof(my_set)} байт')  # -> asizeof   set():         216 байт

 my_set = frozenset()
 print(f'getsizeof frozenset(): {sys.getsizeof(my_set)} байт')    # -> getsizeof frozenset():   216 байт
 print(f'asizeof   frozenset(): {asizeof.asizeof(my_set)} байт')  # -> asizeof   frozenset():   216 байт





                            -- Примеры tuple() vs namedtuple() vs () --                         <-----
 my_tuple = tuple()
 print(f'getsizeof tuple():    {sys.getsizeof(my_tuple)} байт')    # -> getsizeof tuple():      40 байт
 print(f'asizeof   tuple():    {asizeof.asizeof(my_tuple)} байт')  # -> asizeof   tuple():      40 байт
 print(f'asizeof   ():         {asizeof.asizeof(())} байт')        # -> asizeof   ():           40 байт


 from collections import namedtuple

 my_tuple = namedtuple('C', '')
 nt_tuple = my_tuple()
 print(f'getsizeof namedtuple: {sys.getsizeof(nt_tuple)} байт')    # -> getsizeof namedtuple:    40 байт
 print(f'asizeof   namedtuple: {asizeof.asizeof(nt_tuple)} байт')  # -> asizeof   namedtuple:    40 байт





                             -- Примеры dict() vs {} vs OrderedDict() vs defaultdict() vs ChainMap() --        <-----
 my_dict = dict()
 print(f'getsizeof dict():       {sys.getsizeof(my_dict)} байт')    # -> getsizeof dict():       64 байт
 print(f'asizeof   dict():       {asizeof.asizeof(my_dict)} байт')  # -> asizeof   dict():       64 байт


 my_dict = {}
 print(f'getsizeof {{}}:         {sys.getsizeof(my_dict)} байт')    # -> getsizeof {}:           64 байт
 print(f'asizeof   {{}}:         {asizeof.asizeof(my_dict)} байт')  # -> asizeof   {}:           64 байт


 from collections import OrderedDict

 my_OrDt = OrderedDict()
 print(f'getsizeof OrderedDict:  {sys.getsizeof(my_OrDt)} байт')    # -> getsizeof OrderedDict:  128 байт
 print(f'asizeof   OrderedDict:  {asizeof.asizeof(my_OrDt)} байт')  # -> asizeof   OrderedDict:  128 байт


 from collections import defaultdict

 my_defa = defaultdict(int)     # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict(str)     # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict(list)    # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict(set)     # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict(dict)    # Все будут весить ОДИНАКОВО!!!
 my_defa = defaultdict()        # Все будут весить ОДИНАКОВО!!!
 print(f'getsizeof defaultdict():  {sys.getsizeof(my_defa)} байт')    # -> getsizeof defaultdict():  72 байт
 print(f'asizeof   defaultdict():  {asizeof.asizeof(my_defa)} байт')  # -> asizeof   defaultdict():  72 байт


 from collections import ChainMap

 my_chain = ChainMap()
 print(f'getsizeof ChainMap():  {sys.getsizeof(my_chain)} байт')    # -> ggetsizeof ChainMap():  56 байт
 print(f'asizeof   ChainMap():  {asizeof.asizeof(my_chain)} байт')  # -> aasizeof   ChainMap():  536 байт





                             -- Сравнение slots vs no_slots --                                  <-----
                             -- @dataclass(slots=True)  vs  @dataclass() --

 from dataclasses import dataclass

 @dataclass(slots=True)
 class WithSlots:pass

 with_slots = WithSlots()
 print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  32 байт
 print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  32 байт


 @dataclass
 class NoSlots:pass

 no_slots = NoSlots()
 print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    56 байт
 print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    352 байт





                             -- Обычные классы По размеру тоже самое что @dataclass(slots=True)  vs  @dataclass() --
                             -- Сравнение slots vs no_slots --

 class WithSlots:__slots__ = ()

 with_slots = WithSlots()
 print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  32 байт
 print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  32 байт


 class NoSlots:pass

 no_slots = NoSlots()
 print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    56 байт
 print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    352 байт





                            -- Примеры int() float() complex() True False str() range(0) bytes() bytearray() vs None --
                            -- И пустые объекты Тоже самое 0  ''  0.0  0j  b""  bytearray(b"")  object()  --     <-----

 my_int = int()
 print(f'getsizeof int():  {sys.getsizeof(my_int)} байт')            # -> getsizeof int():      28 байт
 print(f'asizeof   int():  {asizeof.asizeof(my_int)} байт')          # -> asizeof   int():      32 байт
 print(f'asizeof   0:      {asizeof.asizeof(0)} байт')               # -> asizeof   0:          32 байт


 my_float = float()
 print(f'getsizeof float():  {sys.getsizeof(my_float)} байт')        # -> getsizeof float():    24 байт
 print(f'asizeof   float():  {asizeof.asizeof(my_float)} байт')      # -> asizeof   float():    24 байт
 print(f'asizeof   0.0:      {asizeof.asizeof(my_float)} байт')      # -> asizeof   0.0:        24 байт


 my_comp = complex()
 print(f'getsizeof complex():  {sys.getsizeof(my_comp)} байт')       # -> getsizeof complex():  32 байт
 print(f'asizeof   complex():  {asizeof.asizeof(my_comp)} байт')     # -> asizeof   complex():  32 байт
 print(f'asizeof   0j:         {asizeof.asizeof(my_comp)} байт')     # -> asizeof   0j:         32 байт


 # True
 print(f'getsizeof True:  {sys.getsizeof(True)} байт')               # -> getsizeof True:       28 байт
 print(f'asizeof   True:  {asizeof.asizeof(True)} байт')             # -> asizeof   True:       32 байт


 # False
 print(f'getsizeof False:  {sys.getsizeof(False)} байт')             # -> getsizeof False:      28 байт
 print(f'asizeof   False:  {asizeof.asizeof(False)} байт')           # -> asizeof   False:      32 байт


 # None  занимает фиксированное количество памяти!!!        Один из самых маленьких объектов по памяти!!!      <-----
 print(f'getsizeof None:  {sys.getsizeof(None)} байт')               # -> getsizeof None:       16 байт
 print(f'asizeof   None:  {asizeof.asizeof(None)} байт')             # -> asizeof   None:       16 байт


 my_str = str()
 print(f'getsizeof str():  {sys.getsizeof(my_str)} байт')            # -> getsizeof str():      49 байт
 print(f'asizeof   str():  {asizeof.asizeof(my_str)} байт')          # -> asizeof   str():      56 байт
 print(f'asizeof   "":     {asizeof.asizeof("")} байт')              # -> asizeof   "":         56 байт


 my_range = range(0)
 print(f'getsizeof range(0):  {sys.getsizeof(my_range)} байт')       # -> getsizeof range(0):   48 байт
 print(f'asizeof   range(0):  {asizeof.asizeof(my_range)} байт')     # -> asizeof   range(0):   48 байт


 my_bytes = bytes()
 print(f'getsizeof bytes():  {sys.getsizeof(my_bytes)} байт')        # -> getsizeof bytes():    33 байт
 print(f'asizeof   bytes():  {asizeof.asizeof(my_bytes)} байт')      # -> asizeof   bytes():    40 байт
 print(f'asizeof   b"":      {asizeof.asizeof(b"")} байт')           # -> asizeof   b"":        40 байт


 my_b_arr = bytearray()
 print(f'getsizeof bytearray():  {sys.getsizeof(my_b_arr)} байт')    # -> getsizeof bytearray():              56 байт
 print(f'asizeof   bytearray():  {asizeof.asizeof(my_b_arr)} байт')  # -> asizeof   bytearray():              56 байт
 print(f'asizeof   bytearray(b""):  {asizeof.asizeof(bytearray(b""))} байт')  # -> asizeof   bytearray(b""):  56 байт


 my_object = object()
 print(f'getsizeof object():  {sys.getsizeof(my_object)} байт')      # -> getsizeof object():   16 байт
 print(f'asizeof   object():  {asizeof.asizeof(my_object)} байт')    # -> asizeof   object():   0 байт

 object() возвращает 0 байт, потому что функция `asizeof` не находит вложенных объектов или атрибутов для учета,
 так как `object` не содержит информации.  Сам по себе object() - весит 16 байт
 object() - является базовым пустым объектом без дополнительных атрибутов или содержимого.



 --- Сравнение объектов ---
 # Если объекты не могут быть сравниваемы (например, если они разных типов), то произойдет ошибка типа `TypeError`.

 # В Python функции `max()`, `min()` и `sorted()` и другие подобные функции используют стандартные методы сравнения
 # объектов, такиме как `__lt__`, `__le__`, `__gt__`, `__ge__`, и `__eq__`.

 print(sorted([1, 2, 'a']))  # -> TypeError: '<' not supported between instances of 'str' and 'int'   Тут метод __lt__
 print(min([1, 2, 'a']))     # -> TypeError: '<' not supported between instances of 'str' and 'int'   Тут метод __gt__
 print(max([1, 2, 'a']))     # -> TypeError: '>' not supported between instances of 'str' and 'int'   Тут метод __gt__


 # Чтобы можно было сравнивать ЭК нужно прописать методы         по умолчанию будет ошибка
 class Person:
     def __init__(self, name, age):
         self.name = name
         self.age = age

     def __lt__(self, other):
         return self.age < other.age

 p1 = Person("Alice", 30)
 p2 = Person("Bob", 25)

 print(max(p1, p2).name)  # Вывод: Alice (поскольку Alice старше)
 print(min(p1, p2).name)  # Вывод: Bob (поскольку Bob младше)

 # Без методов будет ошибка
 class Person:
     def __init__(self, name, age):
         self.name = name
         self.age = age

 p1 = Person("Alice", 30)
 p2 = Person("Bob", 25)

 print(p1 > p2)           # -> TypeError: '>' not supported between instances of 'Person' and 'Person'
 print(max(p1, p2).name)  # -> TypeError: '>' not supported between instances of 'Person' and 'Person'
 print(min(p1, p2).name)  # -> TypeError: '<' not supported between instances of 'Person' and 'Person'


 # __eq__ по умолчанию сравнивает адрес в памяти

 # В dataclass eq встроен сразу           # В обычном классе Сравниваем адрес в памяти
 from dataclasses import dataclass

 @dataclass                               class Person:
 class Person:                                def __init__(self, name, age):
     name: str                                    self.name = name
     age: int                                     self.age = age

 p1 = Person("Bob", 25)                   p1 = Person("Bob", 25)
 p2 = Person("Bob", 25)                   p2 = Person("Bob", 25)
 print(p1 == p2)  # -> True               print(p1 == p2)  # -> False


 Протокол итератора (iterator protocol) - объекты итератора должны поддерживать  iterator.__iter__()  и iterator.__next__()
 Для создания объекта типа Iterator можно воспользоваться встроенной функцией iter(). По итератору можно двигаться с помощью функции next().

 it = iter([i*i for i in range(10)])
 - Нельзя получить длину итератора функцией len():
 len(it) -> TypeError: object of type 'list_iterator' has no len()

 НО МОЖНО ИСПОЛЬЗОВАТЬ more_itertools.ilen(iterable) - Возвращает количество элементов в iterable .
 from more_itertools import ilen

 ilen(it) # -> 10

 - Итератор не поддерживает получение элемента по индексу:
 it[2] ->  TypeError: 'list_iterator' object is not subscriptable

 - К итератору нельзя применить обычные операции среза или функцию slice(). Для этих целей, можно использовать функцию
 itertools.islice() модуля itertools. - так же подходит и к генератору
 it = iter([i*i for i in range(10)])
 it[2:5]  -> TypeError: 'list_iterator' object is not subscriptable

 import itertools
 list(itertools.islice(it, 2, 5)) -> # [49, 64, 81]

 class more_itertools.islice_extended(iterable, stop)  для получения отрицательных индексов
 from more_itertools import islice_extended

 iterable = iter('abcdefgh')
 list(islice_extended(iterable, -4, -1))  # -> ['e', 'f', 'g']

 # Срезы напрямую Любые Отрицательные или Положительные
 iterable = iter('abcdefgh')
 list(islice_extended(iterable))[-4:-1]  # -> ['e', 'f', 'g']

 - После прохождения по итератору, он остается пустым:
 it = iter([i*i for i in range(10)])
 list(it)
 # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
 list(it)
 # []


 Недостаток типа Iterator состоит в том, что при первом его вызове вычисляются сразу все значения последовательности как
 физической, так и виртуальной, к тому же все они хранятся в памяти до их исчерпания. Этот недостаток решает тип generator (генератор).


 --- Generator Types Протокол генератора в Python и выражение yield ---
 Важно! Так как генератор - это "улучшенный" итератор, следовательно на тип generator распространяются такие же
 ограничения как и на тип iterator.

 - Нельзя получить длину генератора функцией len():
 - не поддерживает получение элемента по индексу
 - После прохождения по генератору, он остается пустым

 для срезов используем - itertools.islice()   но НЕ поддерживает отрицательные индексы -1...
 для Отрицательных или Положительных срезов используем - from more_itertools import islice_extended

 Например, такой генератор, как:
 def squares(start, stop):
    for i in range(start, stop):
        yield i * i

 generator = squares(a, b)

 или эквивалентное выражение генератора (genexp)
 generator = (i*i for i in range(a, b))

 # Как перевернуть генератор/итератор?     reversed(list)

 # Что будет на выходе ПОСМОТРИ
 my_list = [1, 2, 3, 4, 5]
 my_generator = (x**2 for x in my_list)

 print(reversed(list(my_generator)))  # -> <list_reverseiterator object at 0x000001C4286F3A00>
 print(my_generator)                  # -> <generator object <genexpr> at 0x000001CA17B23850>
 print(list(my_generator))            # -> []


  -- YIELD FROM И СУБГЕНЕРАТОРЫ --
 # Тоже самое                          # Тоже самое                         # return + yield   # Разбери пример
 def sub_gen():                        def sub_gen():                       def sub_gen():
     yield 1.1                             yield 1.1                            yield 1.1
     yield 1.2                             yield 1.2                            yield 1.2
                                                                            return 'Done'
 def gen():                            def gen():
     yield 1                               yield 1                          def gen():
     for i in sub_gen():                   yield from sub_gen()                 yield 1
         yield i                           yield 2                              result = yield from sub_gen()
     yield 2                                                                    print('<--', result, end=' ')
                                                                                yield 2

 for i in gen():                       for i in gen():                      for i in gen():
     print(i, end=' ') # 1 1.1 1.2 2      print(i, end=' ') # 1 1.1 1.2 2       print(i, end=' ') # 1 1.1 1.2 <-- Done 2


 -- КОГДА НУЖНО ОБОРАЧИВАТЬ ГЕНЕРАТОР В СКОБКИ ()

 # Если АГРУМЕНТА НЕТ   СКОБКИ ()  НЕ НУЖНЫ
 print(i for i in range(10))            # -> <generator object <genexpr> at 0x000002D45E5B3B90>

 # Если ЕСТЬ АГРУМЕНТ то нужно ОБОРАЧИВАТЬ ГЕНЕРАТОР   В СКОБКИ ()
 print((i for i in range(10)), sep='')  # -> <generator object <genexpr> at 0x000002D45E5B3B90>

 # БУДЕТ SyntaxError
 # print(i for i in range(10), sep='')  # -> SyntaxError: Generator expression must be parenthesized


 А, чуть не забыл - генераторы ещё экономят память, потому что не вываливают все результаты целиком,
 а генерируют элементы на лету. Поэтому в памяти хранится не вся коллекция, а только текущее состояние генератора.

 Генераторы откладывают выполнение кода
 Поэтому заранее знать, где код будет выполнен - та ещё задача.



 --- Context Manager Types     Тип контекстный менеджер ---

 В питоне есть оператор with вход и выход, при работе с файлами, файл автоматически закрывается.
 Для создания своего контекстного менеджера нужно определить класс с двумя специальными методами:
  __enter__() и __exit__().

 Мы также можем создать контекстный менеджер, используя функцию и декоратор contextlib.contextmanager.
 В этом случае функция должна быть генератором, который yield‘ит объект для использования в блоке with.
 Все, что находится до yield, будет выполняться перед входом в блок, а после yield — после выхода из блока.

 from contextlib import contextmanager

 @contextmanager
 def open_file(name):
     f = open(name, 'w')
     try:
         yield f
     finally:
         f.close()


 Контекстные менеджеры в Python используются для управления ресурсами, автоматически выполняя определённые действия
 при входе и выходе из блока кода. Наиболее распространённые применения:

  ВСЕГДА УКАЗЫВАЕМ КОДИРОВКУ    encoding                                                        <----- Важно

 1. **Файлы**: Автоматическое закрытие файлов после их использования.
 python
    with open('file.txt') as f:
        data = f.read()


 2. **Сессии базы данных**: Управление подключениями к БД, обеспечивая корректное закрытие соединений
      (например, с помощью `with` с библиотекой SQLAlchemy).

 3. **Блокировки**: Управление мьютексами и семафорами для обеспечения потокобезопасности.

 4. **Транзакции**: Упрощение работы с транзакциями, автоматически откатывая их в случае ошибок.

 5. **Кастомные ресурсы**: Создание собственных контекстных менеджеров для управления любыми ресурсами
    (например, сетевыми соединениями), используя `__enter__` и `__exit__`.

 Таким образом, контекстные менеджеры помогают избежать утечек ресурсов и делают код более читаемым и безопасным.

 Транзакции в Python — это последовательность операций, которые выполняются как единое целое. Если одна операция
 НЕ удается, все изменения отменяются, что гарантирует целостность данных. Обычно используются при работе с базами данных.

  Примеры Транзакций в Python:
 - Использование SQLite
 - Использование SQLAlchemy


 --- LEGB-rule. Как Python ищет имена переменных ---
 Буквы в аббревиатуре LEGB обозначают локальную, вложенную, глобальную и встроенную
 (Local, Enclosing, Global и Built-in Scope) области. Поиск идёт снизу-вверх сначала L-E-G-B
 даже для встроенных функций если не нашел переменную, то ошибка  NameError:

 Особенности LEGB:
 1) сначала поиск идет в локальном пространстве имен, максимально близко к использованию имени и
 далее идет снизу-вверх, изнутри-наружу к глобальному пространству имен
 2) после локального пространства имен интерпретатор посмотрит в enclosing, то есть в функцию,
 которая содержит текущую (если она есть) и далее проверит глобальное пространство имен
 3) последним шагом будут проверены имена в модуле builtins (встроенные функции)
 4) если на любом этапе имя найдено, то далее поиск не идет. Если все этапы неудачны то выбрасывается NameError
 5) важно понимать что даже если мы используем встроенную функцию, типа max/min/sum/print,
 то интерпретатор сначала проведет поиск по всем скоупам. Вот почему крайне важно НИКОГДА не давать своим переменным,
 функциям, модулям имена встроенных функций или библиотек (самые частые фейлы это имена типа len, list, sum, json, dict)

 Локальные переменные видны только в локальной области видимости, которой может выступать отдельно взятая функция
 Глобальные переменные видны во всей программе.
 Локальные переменные создаются каждый раз при входе в функцию и уничтожаются при выходе из нее.   <-----

 Локальные переменные НЕ требуют поиска в словаре, так как хранятся в очень ТОНКОМ массиве с малым временем поиска <-----

 END --- LEGB ---

 -- Почему локальные переменные работают быстрее, чем глобальные - Это деталь реализации CPython.
 Вы можете спросить, почему хранить локальные переменные быстрее, чем глобальные. Это деталь реализации CPython.

 Помните, что CPython компилируется в байт-код, который запускает интерпретатор. Когда функция компилируется, локальные
 переменные сохраняются в массиве фиксированного размера ( не в dict), а имена переменных присваиваются индексам.
 Это возможно, поскольку вы не можете динамически добавлять локальные переменные в функцию. Тогда получение локальной
 переменной — это буквально поиск указателя в списке и увеличение счетчика ссылок, что PyObject тривиально.

 Сравните это с глобальным поиском ( LOAD_GLOBAL), который представляет собой настоящий dict поиск, включающий хэш и т. д.
 Кстати, именно поэтому вам нужно указать, global i хотите ли вы, чтобы она была глобальной: если вы когда-либо
 присваиваете значение переменной внутри области видимости, компилятор выдаст STORE_FASTs для доступа к ней,
 если вы не скажете ему этого не делать.

 Кстати, глобальный поиск все еще довольно оптимизирован. Поиск атрибутов foo.bar очень медленный !


 Почему код Python в функции выполняется быстрее?
 Важно!!!
 Причина, по которой в локальных переменных это происходит быстрее, заключается в том, что локальные области фактически
 реализуются как массивы, а НЕ словари (поскольку их размер известен во время компиляции).


 --- global и nonlocal ---
 global и nonlocal нужны только для изменения значений
 global может создать переменную, nonlocal не может!
 nonlocal ищет только во внешних скоупах, но не в глобальном и не builtins
 НЕ используйте global, nonlocal


 -- Модуль heapq обеспечивает реализацию алгоритма очереди кучи, также известного как алгоритм очереди приоритетов.
 Можно рассматривать кучу как обычный список Python без сюрпризов.
 Python min-куча (наименьшее значение всегда лежит в корне) реализована на базе списка при помощи встроенного модуля heapq


 Вот основные виды тестирования в Python:

 1. **Unit Testing (Модульное тестирование)**: Тестирует отдельные функции или классы для проверки их корректности.
  Используются библиотеки `unittest` и `pytest`.

 2. **Integration Testing (Интеграционное тестирование)**: Проверяет взаимодействие между несколькими модулями или
  системами, чтобы убедиться в их совместной работе.

 3. **Functional Testing (Функциональное тестирование)**: Проверяет функциональность приложения в соответствии с
 требованиями, не углубляясь в внутренние детали реализации.

 4. **End-to-End Testing (Тестирование "от конца до конца")**: Тестирует полный пользовательский поток в приложении,
  включая все интеграции и зависимости.

 5. **Regression Testing (Регрессионное тестирование)**: Убеждается, что изменения в коде не нарушили существующую
  функциональность.

 6. **Тестирование пользовательского интерфейса (UI Testing)**: Проверяет интерфейс приложения, чтобы убедиться в
  правильном отображении и работе элементов.

 7. **Тестирование производительности (Performance Testing)**: Оценивает скорость и стабильность приложения под нагрузкой.

 8. **Приемочное тестирование (Acceptance Testing)**: Проверяет готовность системы к использованию конечными пользователями.

 Эти виды тестирования обеспечивают базовую проверку качества и надежности приложений.


 --- Тестирование ---
 Selenium — это один из самых популярных и широко используемых инструментов для автоматизации тестирования веб-приложений.

 -- Методы Тестирование --

 1. **Тестирование по классам эквивалентности (Equivalence Class Testing)**: Делит входные данные на эквивалентные классы,
  чтобы минимизировать количество тестов. Проверяется только одно значение из каждого класса.

 2. **Тестирование граничных значений (Boundary Value Testing)**: Тестирует значения на границах эквивалентных классов
  (например, минимальные и максимальные значения), так как ошибки часто возникают именно на этих границах.

 3. **Тестирование случайных значений (Random Testing)**: Случайно выбирает входные данные для тестирования, чтобы
 выявить ошибки, которые могут быть пропущены более структурированными методами.

 4. **Тестирование по состояниям (State Transition Testing)**: Основано на моделировании состояний системы. Проверяет,
  как система реагирует на переходы между различными состояниями.

 5. **Тестирование на основе требований (Requirement-Based Testing)**: Разрабатывает тесты на основе спецификаций и
 требований к системе, чтобы убедиться, что функциональность реализована правильно.

 6. **Тестирование изомерной (Combinatorial Testing)**: Исследует разные комбинации входных значений и их влияние на
  систему, что особенно полезно для систем с множеством параметров.

 7. **Тестирование сценариев (Scenario Testing)**: Оценивает систему на основе реальных сценариев использования.

 8. **Проверка условий (Condition Testing)**: Проверяет, как программа обрабатывает разные логические условия.


 Эти методы помогают обеспечить качественное тестирование программного обеспечения с разных сторон и выявить
  потенциальные ошибки в коде.


 Классы эквивалентности - это разделение функционала или данных на определенные наборы,
 с которыми тестируемое приложение должно работать одинаково.

 Класс эквивалентности (Equivalence class) – это набор входных (или выходных) данных ПО, которые обрабатываются
 программой по одному алгоритму или приводят к одному результату.

 # Хороший ответ
 Значения называется эквивалентными друг другу если они приводят к одному и тому же результату.        <-----
 Пример калькулятор: Ужимать тесты
 Выделить классы эквивалентности: это значит написать 10 тестов вместо 100. С тем же результатом!


 Граничные значения (Boundary Values) — это значения, в которых один класс эквивалентности переходит в другой диапазон.
 Эта техника тестирования фокусируется на анализе поведения программы на ее границах или близких к ним цифрах.
 Это метод тестирования, в котором основное внимание уделяется значениям на границах допустимого диапазона.



 Фикстуры (fixtures) в pytest — это специальные функции, которые подготавливают данные или состояние,
 необходимые для выполнения тестов. Они позволяют создавать общее окружение для нескольких тестов, упрощают код,
 обеспечивают настройку и очистку ресурсов. Фикстуры определяются с помощью декоратора `@pytest.fixture` и могут быть
 использованы, передавая их как аргументы в тестовые функции. Это делает тесты более читаемыми и поддерживаемыми.

 ### Основные цели фикстур:
 1. **Подготовка данных**: Создание необходимых объектов, например, подключение к базе данных или подготовка тестовых данных.
 2. **Очистка ресурсов**: Освобождение ресурсов после завершения теста (например, закрытие соединения с базой данных).
 3. **Упрощение кода**: Устранение дублирования кода за счет централизации логики подготовки.


 --- Unittest ---
 Unittest — это модуль стандартной библиотеки Python. Внутри есть фреймворк для создания и запуска тестов.
 С его помощью можно создавать мок-объекты, которые имитируют поведение зависимых компонентов
 и помогают изолировать тестируемый код. Нельзя лишь имитировать внешние сервисы.

 from unittest import TestCase, main
 from Again.unit_tests.calculator_new import calculator

 class CalculatorTest(TestCase):

     def test_plus(self):
         self.assertEqual(calculator('2+2'), 4)

     def test_many_sings(self):
         with self.assertRaises(ValueError) as e:
             calculator('2+2*10')
         self.assertEqual('Выражение должно содержать 2 целых числа и 1 знак', e.exception.args[0])

 if __name__ == '__main__':
     main()

  Типы проверок в классе TestCase
 assertEqual(a, b)          a == b
 assertNotEqual(a, b)       a != b
 assertTrue(x)              bool(x) is True
 assertFalse(x)             bool(x) is False
 assertIs(a, b)             a is b
 assertIsNot(a, b)          a is not b
 assertIsNone(x)            x is None
 assertIsNotNone(x)         x is not None
 assertIn(a, b)             a in b
 assertNotIn(a, b)          a not in b
 assertIsInstance(a, b)     isinstance(a, b)
 assertNotIsInstance(a, b)  not isinstance(a, b)


 Модуль Mock: макеты-пустышки в тестировании

 unittest.mock предоставляет базовый Mock класс, устраняющий необходимость создания множества заглушек в вашем наборе тестов.


 Фикстуры (fixtures) в юнит-тестировании — это функции или объекты, которые предварительно настраивают необходимое
 окружение или состояние для тестов. Они помогают избежать дублирования кода, обеспечивают подготовку и очистку ресурсов,
 такие как создание тестовых данных или подключение к базе данных, и могут использоваться в нескольких тестах для
 повышения читаемости и поддерживаемости кода.

 ### Основные цели фикстур:
 1. **Инициализация ресурсов**: Подготовка необходимых объектов или данных, таких как создание тестовой базы данных,
  настройка файлов и т.д.
 2. **Очистка ресурсов**: Освобождение ресурсов после выполнения тестов для предотвращения утечек памяти или конфликтов
  в будущем тестировании.

 ### Использование:
 Фикстуры определяются с помощью методов `setUp()` и `tearDown()` в классе тестов. Метод `setUp()` выполняется перед
 каждым тестом, а `tearDown()` — после него. Эти методы позволяют централизовать логику подготовки и очистки,
 что упрощает поддержку тестов.


 -- Разница между стандартным `unittest` и `django.test` (Django unittest) --

 Разница между стандартным `unittest` и `django.test` (Django unittest) заключается в следующем:

 1. **Контекст**: Стандартный `unittest` используется для тестирования общих Python-приложений, тогда как `django.test`
  предоставляет специфичные инструменты и поддержку для тестирования Django-приложений.

 2. **Функциональность**: Django unittest включает дополнительные утилиты, такие как тестовые базы данных, утилиты для
  тестирования представлений (views), моделей и форм, а также инструменты для проверки работы с URL и шаблонами.

 3. **Тестовая база данных**: При использовании Django unittest автоматически создается тестовая база данных,
  которая изолирована и очищается после каждого теста, что упрощает тестирование при работе с базой данных.

 В целом, `django.test` предоставляет больше удобства и возможностей для тестирования в контексте Django-приложений.

 Основные отличия:

 1. **Контекст**: `unittest` — общий фреймворк для тестирования Python, в то время как `django.test` —
 специализированный для Django-приложений.

 2. **Инструменты**: `django.test` предлагает дополнительные функции, такие как тестовая база данных,
  средства для тестирования представлений, моделей и форм.

 3. **Автоматизация**: В Django unittest автоматически создается и очищается тестовая база данных, обеспечивая изолированное тестирование.

 Таким образом, Django unittest предоставляет более удобные инструменты для тестирования в контексте веб-приложений.
 Тестирование unittest в Django


 --- Конкурентность (concurrency)  Параллельность (parallel) ---
 Конкурентность (concurrency) - запуск на выполнение сразу нескольких задач
 (не обязательно в 1 момент времени выполняется несколько). Зависит от ПО Первые ОС с процессором без ядер -использовали только ее.
 Проще говоря быстрое переключение между задачами

 Параллельность (parallel) - конкурентность, когда 2+ задачи выполняются одновременно. Зависит от железа
  Вы не можете одновременно (!) выполнять больше задач, чем есть ядер в системе.

 thread-safe - потокобезопасность, означает что при работе с обьектом не возникают известные проблемы
 при работе с конкурентностью

 Часть кода является потокобезопасной, если она корректно работает при одновременном выполнении несколькими потоками.

 Если код или объект считается thread-safe, это означает, что его использование не приведет к непредсказуемому поведению
 при одновременном выполнении нескольких потоков.


 --- Global Interpreter Lock (GIL) ---
 GIL (Global Interpreter Lock) - глобальная блокировка интерпретатора -
 механизм гарантирующий, что в любой момент времени выполняется только 1 инструкция в питоне.

 GIL - Гарантирует нам что в один момент времени в Python работает РОВНО 1 ПОТОК(инструкция),
 даже если потоков больше

 GIL может быть удержан потоком не дольше 5 мс  - 0.0000005

 Чтобы помешать потоку Python удерживать GIL БЕСКОНЕЧНО, Интерпретатор байт-кода Python периодически
 (По умолчанию раз в 5 миллисекунд) приостанавливает текущий поток и тем самым освобождает GIL.

 В CPython глобальная блокировка интерпретатора, или GIL, представляет собой мьютекс, который защищает доступ к
 объектам Python, НЕ позволяя нескольким потокам одновременно выполнять байт-коды Python. GIL предотвращает состояния
 гонки и обеспечивает безопасность потоков.
 Короче говоря, этот мьютекс необходим главным образом потому, что управление памятью CPython не является потокобезопасным.

 GIL - Механизм, используемый интерпретатором CPython для обеспечения того, чтобы только один поток выполнял байт-код Python одновременно.

 Краткие сведения о GIL:
 - Одновременно может выполняться только один поток.
 - Интерпретатор Python переключается между потоками для достижения конкурентности.
 - GIL применим к CPython (стандартной реализации). Но такие как, например, Jython и IronPython не имеют GIL.
 - GIL делает однопоточные программы быстрыми.
 - Операциям ввода/вывода, обычно GIL не мешает.
 - GIL позволяет легко интегрировать непотокобезопасные библиотеки на языке C
 - Благодаря GIL есть много высокопроизводительных расширений/модулей, написанных на языке C.

 Для CPU зависимых задач интерпретатор делает проверку каждые N тиков и переключает потоки.
 Таким образом один поток не блокирует другие.


 Да, GIL (Global Interpreter Lock) в Python помогает Garbage Collector (GC) работать корректно.
 Он предотвращает одновременный доступ нескольких потоков к объектам Python, что минимизирует риски гонок данных и
 обеспечивает безопасную работу с памятью. Это позволяет GC надежно отслеживать и освобождать память,
 избегая проблем с памятью, которые могут возникнуть при работе в многопоточной среде.


 Таким образом, GIL способствует безопасности и эффективности работы сборщика мусора в Python.    <-----   Важно


  --- Сборщик мусора (Garbage Collector, GC) ---
  Задача сборщика мусора — именно поиск циклических ссылок на объекты

  Сборщик мусора не умный и не глупый. Он просто ищет циклы из ссылок и удаляет их.
  Большая часть объектов удаляется ещё до сборщика, так как в Python есть подсчёт ссылок.

 Сборщик мусора автоматически освобождает память, которая больше не используется.
 Он определяет, что память больше не используется, если на объект нет ссылок
 модуль gc - для дополнительных возможностей сборщика мусора
 Сборщик мусора (Garbage Collector, GC)
 import gc, gc.collect() - Принудительный запуск Сборщика мусора
 gc.collect(generation=2)  generation - какое поколение собирать (от 0 до 2)
 gc.set_threshold(...) - Управление частотой вызова Сборщик мусора
 gc.enable(), gc.disable()  - вкл, выкл Сборщик мусора
 gc.freeze() - Заморозить все объекты, отслеживаемые сборщиком мусора; переместите их в постоянное поколение
 и игнорируйте во всех будущих коллекциях.
 gc.unfreeze() - Разморозьте объекты в постоянном поколении и поместите их обратно в самое старое поколение.


 В Основном НЕИЗМЕНЯЕМЫЕ НЕ отслеживаються а ИЗМЕНЯЕМЫЕ отслеживаються
 Можно самому посмотреть кто отслеживается а кто нет:
 import gc
 gc.is_tracked(0)           # -> False    # int не могут находиться в цикле ссылок
 gc.is_tracked("a")         # -> False    # str не могут находиться в цикле ссылок
 gc.is_tracked([])          # -> True     # list может быть в цикле ссылок
 gc.is_tracked(tuple())     # -> False    # пустой tuple не отслеживается
 gc.is_tracked({})          # -> False
 gc.is_tracked({"a": 1})    # -> False
 gc.is_tracked({"a": []})   # -> True
 gc.is_tracked((i for i in range(10)))   # -> True     Generator
 gc.is_tracked(iter([1, 2, 3]))          # -> True     Iterator

 ПОДСЧЕТ ССЫЛОК также является формой сборки мусора(GC).
 В Python, алгоритм подсчета ссылок является фундаментальным и НЕ может быть отключен, тогда как GC опционален и может быть отключен.

 Поскольку сборщик дополняет подсчет ссылок, уже используемый в Python, вы можете отключить сборщик,        <-----
 если уверены, что ваша программа не создает циклы ссылок.

 --- Подсчет ссылок ---
 Подсчет ссылок Этот метод отслеживает количество ссылок, указывающих на каждый объект.
 Когда счетчик ссылок для объекта достигает нуля, что означает их отсутствие,
 объект считается “мусором” (ненужным), он уничтожается сборщиком мусора и память освобождается

 Для всех объектов в программе Python ведется подсчет ссылок. Счетчик ссылок на объект увеличивается всякий раз,
 когда ссылка на объект записывается в новую переменную или когда объект помещается в контейнер, такой как список,
 кортеж или словарь

 Счетчик ссылок (отслеживает создание/удаление обьектов) - ПОТОКОНЕБЕЗОПАСЕН не умеет работать чтобы сразу
 2 потока создавали или удаляли обьекты

 sys.getrefcount()  - функция показывает сколько ссылок ссылается на обьект. Возвращаемое значение обычно (+1) на
 единицу больше, чем вы могли ожидать, поскольку оно включает (временную) ссылку в качестве аргумента getrefcount().
 x = []
 print(sys.getrefcount(x)) # -> 2    +1 за (временную) ссылку в качестве аргумента getrefcount().
 в REPL создаете автоматическую ссылку в переменной  '_'  которая  добавляет +1

 Основной алгоритм сборки мусора, используемый CPython, — подсчет ссылок:
 x = object()
 sys.getrefcount(x) # -> 2 ссылки на обьект +1 из-за getrefcount
 y = x
 sys.getrefcount(x) # -> 3 ссылки на обьект +1 из-за getrefcount
 del y
 # del (y)  # Тоже самое
 sys.getrefcount(x) # -> 2 ссылки на обьект +1 из-за getrefcount
 import gc
 print(gc.collect()) # -> 0  ПОДСЧЕТ ССЫЛОК сработал

 Основная проблема схемы подсчета ссылок заключается в том, что она не обрабатывает циклы ссылок:
 container = []
 container.append(container)
 sys.getrefcount(container)   # -> 3

 Циклические ссылки происходят когда один или более объектов ссылаются на друг друга.
 ЦИКЛ ССЫЛОК - когда ПОДСЧЕТ ССЫЛОК не справляется с удалением обьектов
 Ссылочный цикл просто означает, что один или несколько объектов ссылаются друг на друга


 Основная проблема схемы подсчета ссылок заключается в том, что она не обрабатывает циклы ссылок:
 container = []
 container.append(container)
 sys.getrefcount(container)   # -> 3
 print(container)  # -> [[...]]  # показывает, что структура данных ссылается на себя, избегая бесконечного рекурсивного вывода.

 del container
 import gc
 print(gc.collect()) # -> 1  СБОРЩИК МУСОРА сработал

 Garbage Collector (GC) в Python справится с этой ситуацией. Он может обрабатывать циклические ссылки, такие как
 `container`, содержащий сам себя. Хотя сборка мусора может потребовать немного больше ресурсов из-за необходимости
 отслеживания циклических ссылок, утечек памяти не произойдет, и GC не "ЗАХЛЕБНЕТСЯ".

 # В Python `[...]` используется, чтобы показать циклическую ссылку, поскольку повторяющееся содержание может привести
 # к бесконечному выводу

 --- Слабые и Обычные Ссылки на обьекты ---
 Обычная ссылка увеличивает счетчик ссылок на объект и предотвращает сбор мусора.
 Слабые ссылки - не увеличивают счетчик ссылок. Соответственно, объект на который есть такая ссылка, может быть уничтожен
 Модуль weakref позволяет создавать "слабые" ссылки на объекты.


 Задачи могут быть:
 CPU-bound - зависит от мощности процессора # расчеты внутри python власть GIL
 IO-bound - зависит от системы ввода/вывода # обращение к файлу, обращение к сайту # GIL не трогает

 threading - IO-bound задачи # GIL не помешает
 asyncio - IO-bound задачи , 1 поток использует Конкурентность (concurrency) по полной
 multiprocessing - любые задачи

 # Потоки ИСПОЛНЯЮТСЯ на уровне ОС а asyncio на уровне Интерпретатора                                 <-----


 -- КОНКУРЕНТНОСТЬ vs ПАРАЛЛЕЛИЗМ --

 В случае КОНКУРЕНТНОСТИ несколько задач работают в течении одного промежутка времени, но только одна активна в каждый
 момент.
 В случае ПАРАЛЛЕЛИЗМА несколько задач активно одновременно.

 В случае КОНКУРЕНТНОСТИ мы переключаемся между двумя приложениями.
 В случае ПАРАЛЛЕЛИЗМА мы активно выполняем два приложения одновременно.


 Примеры I/O-bound задач в Python:
 **Загрузка данных из веб-API**:            - ожидание ответа от сервера.
 **Чтение и запись файлов**:                - работа с файлами на диске
 **Взаимодействие с базами данных**:        - выполнение запросов и получение данных.
 **Отправка и получение данных по сети**:   - работа с сокетами и сетевыми протоколами.

  Для I/O-bound задач особенно эффективны многопоточность и асинхронное программирование, так как они позволяют
 выполнять другие задачи, пока происходит ожидание.


 --- Многопоточность (multithreading) ---
 multithreading - многопоточность, подходит для IO-bound задач, использует ОС, страдает от GIL(важно помнить)
 Полезно для ускорения выполнения задач для того, чтобы текущий поток занялся другой задачей
 Любая программа это минимум один процесс и один поток
 Полезно использовать daemon=True (чтобы остальные потоки не висели), Queue(очереди), pool executor,
 НО в любом случае все зависит от программиста!

 Плюсы:
 + просто(сравнительно)
 + быстро
 + не умирает из-за одного(!)

 Минусы:
 - потребление ресурсов(ОС)
 - неуправляемость(старт, приостановка, переключение)
 - проблема потоков(гонка, блокировки)


  Race condition (состояние гонки) - это ситуация, при которой несколько ПОТОКОВ (ИЛИ ПРОЦЕССОВ) одновременно пытаются
  выполнить операции чтения или записи к общим ресурсам без должной синхронизации.

  Состояние гонки могут возникать, когда два потока одновременно обращаются к одному обьекту Python

  Если два потока одновременно увеличивают счетчик ссылок, то может случиться, что счетчик обнулится, хотя обьект еще
  используется.

  Состояние гонки, возникающее, когда два потока одновременно пытаются увеличить счетчик ссылок.


 -- Модуль concurrent.futures --
 concurrent.futures — Запуск параллельных задач, Параллельное выполнения задач в разных процессах или потоках

 - при использовании класса ThreadPoolExecutor, задачи выполняются в потоках;
 - при использовании класса ProcessPoolExecutor, задачи выполняются на ядрах процессора;
 - оба класса реализуют одинаковый API-интерфейс, который определяется абстрактным классом concurrent.futures.Executor,
 поэтому приложения могут переключаться между потоками и процессами с минимальными изменениями;
 - легко интегрируется в модуль asyncio, для запуска блокирующих операции в отдельных потоках/процессах.

 Класс ThreadPoolExecutor() модуля concurrent.futures -  Создает пул потоков для асинхронного выполнения вызовов
 ThreadPoolExecutor(max_workers=None) - использует пул не более max_workers потоков для асинхронного выполнения вызовов.

 Класс ProcessPoolExecutor() модуля concurrent.futures - Создает пул из ядер процессора для асинхронного выполнения вызовов
 ProcessPoolExecutor(max_workers=None) - использует пул не более чем max_workers ядер процессора для асинхронного выполнения вызовов.

 ProcessPoolExecutor в Python использует СЕМАФОР для управления количеством одновременно работающих процессов.
 Семафор ограничивает количество потоков или процессов, которые могут одновременно выполнять определенные операции,
 что помогает предотвратить чрезмерное использование ресурсов системы.

 Семафор — это синхронизирующий объект, который используется для управления доступом к общему ресурсу в многопоточной
 или многопроцессорной среде. Он может быть представлен как счетчик, который указывает, сколько потоков или процессов
 могут одновременно выполнять определенную задачу. Если счетчик достигает нуля, остальные потоки или процессы должны
 ожидать, пока другие освободят ресурсы.


 Многопоточность достигается модулем Threading.
 Это нативные Posix-треды. Такие треды исполняются операционной системой, а не виртуальной машиной.
 from threading import Thread


  -- Модуль joblib --
 Модель joblib использует библиотеку Loky, которая является улучшенной версией concurrent.futures,
 и применяет модуль cloudpickle для сериализации функций, определенных в интерактивной оболочке,
 что позволяет их использовать в многопроцессорных вычислениях.

 from joblib import Parallel, delayed

 def my_function(x):
     return x * x

 # Параллельное выполнение функции с использованием 4 процессов
 results = Parallel(n_jobs=4)(delayed(my_function)(i) for i in range(10))

 print(results)  # -> [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]



 --- Greenlet == Green thread == Зеленые треды ---
 Что такое гринлеты. Общее понятие. Примеры реализаций
 Greenlet == Green thread == Зеленые треды == легковесные треды внутри виртуальной машины. Могут называться корутинами,
 сопроцессами, акторами и т.д. в зависимости от платформы. Операционная система не видит их.
 С точки зрения ОС запущен один процесс виртуальной машины, а что внутри нее – неизвестно.
 Такими тредами управляет сама вируальная машина: порождает, исполняет, согласует доступ к ресурсам.

 Треды в Питоне — это нативные треды или нет
 Да, это (native threads) нативные Posix-совместимые треды, которые исполняются на уровне операционной системы.
 POSIX (англ. Portable Operating System Interface — переносимый интерфейс операционных систем)
 Стандарт POSIX определяет интерфейс операционной системы
 Стандарты, такие как POSIX, позволяют легко перемещаться между различными операционными системами.
 это стандарт, описывающий интерфейс между операционной системой и прикладной программой. Цель создания этого стандарта
 – обеспечение совместимости unix-like операционных систем, а также переносимости программ на уровне исходного кода.

 В чем отличие тредов от мультипроцессинга?
 Главное отличие в разделении памяти. Процессы независимы друг от друга, имеют раздельные адресные пространства,
 идентификаторы, ресурсы. Треды исполняются в совместном адресном порстранстве,
 имеют общий доступ к памяти, переменным, загруженным модулям.


 Многопоточность - имеет смысл использовать только для блокирующего ввода-вывода. GIL не будет мешать.
 Если ЗНАЧИТЕЛЬНАЯ часть библиотеки написаны на C например hashlib, Numpy. - GIL не будет мешать.

 Numexpr — это библиотека Python, которая позволяет эффективно выполнять выражения с использованием NumPy.
 Установка библиотеки NumExpr УСКОРЯЕТ выполнение функции eval в Pandas, но библиотека Pandas не сообщает пользователю
 о необходимости установки NumExpr.                              ВАЖНО ПРО функцию eval          <-----   <-----

 Помните, что Pandas без NumExpr выполняет функцию eval крайне медленно!
 NumExpr оптимизирует вычисления и использует многоядерные процессоры, что значительно ускоряет операции с массивами данных.

 # В этом примере, если NumExpr установлен, выполнение df.eval будет быстрее.                    <-----   <-----
 import pandas as pd
 import numpy as np

 # Создание DataFrame
 df = pd.DataFrame({
     'a': np.random.rand(1000000),
     'b': np.random.rand(1000000)
 })

 # Использование eval без NumExpr
 result_without_numexpr = df.eval('c = a + b')

 # Использование eval с NumExpr
 result_with_numexpr = df.eval('c = a + b')  # NumExpr будет использован автоматически, если установлен


  --- Стек вызовов  call stack ---
  — это структура данных, которая управляет вызовами функций во время выполнения программы.
 Если программа пытается разместить на стеке больше данных, чем он может вместить, происходит переполнение стека.
 Когда стек переполняется, он не может больше хранить элементы, и любая попытка добавления нового элемента приведет к ошибке
 на дне стека есть функция module(ИСПОЛНЯЕТ НАШИ ЗАПРОСЫ) 1) когда мы вызываем функцию она попадает в СТЕК.
 2) когда функция завершается она снимается со СТЕКА
 Все функции внутри стека вызова исполняются но только одна функция выполняется реально(которая на верхушке стека)
 а остальные функции ждут соседа сверху пока он исполнится
 СТЕК ВЫЗОВОВ ОТОБРАЖАЕТ ВСЕ ФУНКЦИИ КОТОРЫЕ ИСПОЛНЯЮТСЯ В ДАННЫЙ МОМЕНТ И КТО КОГО ЖДЕТ

 Стек (stack) — это область памяти, в которой программа хранит информацию о вызываемых функциях, их аргументах и
 каждой локальной переменной в функциях. Размер области может меняться по мере работы программы. При вызове функций
 стек увеличивается, а при завершении — уменьшается.

 Стек — это область оперативной памяти, которая создаётся для каждого потока. Он работает в порядке
 LIFO (Last In, First Out), то есть последний добавленный в стек кусок памяти будет первым в очереди на вывод из стека.

 Очереди похожи на стеки, и разница между ними в том, как удаляются элементы.
 stack - принцип «последним пришел — первым ушел», или LIFO
 queue - принцип «первым пришел — первым ушел», или FIFO есть и другие варианты помимо FIFO

 По умолчанию размер стека в Питоне ограничен 1000 вызовов.


 -- Модуль queue, очереди в Python --
 import queue, from queue import Queue

 Модуль queue реализует очереди с несколькими производителями и несколькими потребителями. Это особенно полезно в
 многопоточном программировании, когда необходимо безопасно обмениваться информацией между несколькими потоками.
 Класс Queue в этом модуле реализует всю необходимую семантику блокировки.

 Модуль реализует три типа очереди, которые отличаются только порядком, в котором извлекаются записи:

 1) В очереди FIFO первые добавленные задачи являются первыми извлеченными.  queue.Queue()
 2) В очереди LIFO самая последняя добавленная запись является первой извлеченной (работающей как СТЕК). queue.LifoQueue()
 3) В очереди с приоритетами записи сохраняются отсортированными с использованием модуля heapq(очереди с кучей) и сначала
    извлекается запись с наименьшим значением.  queue.PriorityQueue()

 1) Очередь FIFO - Класс queue.Queue():
 Класс queue.Queue() реализует базовый контейнер типа FIFO - "первым пришел - первым вышел". Элементы добавляются
 к одному концу очереди с помощью метода put(), а удаляются с другого конца с помощью метода get().

 2) Очередь LIFO - Класс queue.LifoQueue():
 В отличие от стандартной реализации очереди FIFO, в queue.LifoQueue() используется порядок
 "последним пришел - первым вышел", который обычно связан со структурой данных стека.

 3) Очередь с приоритетом - Класс queue.PriorityQueue():
 Иногда порядок обработки элементов в очереди должен основываться на характеристиках этих элементов, а не только на
 порядке их создания или добавления в очередь. Например, задания на печать из финансового отдела могут иметь приоритет
 над списком заданий из отдела технической поддержки. Класс модуля queue.PriorityQueue() использует порядок сортировки
 содержимого очереди, чтобы решить, какой элемент получить.


 Внутренне эти три типа очередей используют блокировки для временного блокирования конкурирующих потоков,
 однако они НЕ предназначены для обработки повторного входа в поток.

 Кроме того, модуль реализует простой тип очереди FIFO - queue.SimpleQueue(), специфическая реализация которого
 обеспечивает дополнительные гарантии в обмен на меньшую функциональность.
 Очередь FIFO без отслеживания задач queue.SimpleQueue():
 Класс SimpleQueue() модуля queue представляет собой конструктор для простой неограниченной очереди FIFO.

 -- Класс Lock() модуля threading в Python --
 Синхронизация потоков при помощи блокировок
 Для защиты от одновременного доступа к объекту нескольких потоков используйте объект threading.Lock().

 Методы объекта threading.Lock:
 Lock.acquire() -  устанавливает блокировку
 Lock.release() -  снимает блокировку
 Lock.locked()  -  проверяет состояние блокировки


 --- multiprocessing (многопроцессорность) ---
 multiprocessing - использование нескольких ядер
 Породить процессы с помощью multiprocessing.Pool. Число, которое будет передано в Pool(),
 будет равно числу порожденных процессов
 import multiprocessing, from multiprocessing import Process, Pool

 --- Multiprocessing выдуман чтобы побороть GIL! Как его побороть? - Сделать много GIL каждый из которых независимый ---
 --- Multiprocessing работает только с обьектами которые поддерживают сериализацию через pickle
 В других ЯП: параллельность вычислений достигается с помощью 2-х средств - ПОТОКИ или asyncio. Потому что у них нет GIL
 Multiprocessing - позволяет решать любые задачи (IO-bound или CPU-bound)
 Ускорение любые задачи, распараллеливая их на ядра процессора (лишь до определенного предела, закон Амдала)
 закон Амдала - с какого-то момента бессмысленно добавлять дополнительных рабочих(ЦПУ, доп.ядра) быстрее не станет
 Ускорение не идеально и возможно только до определенного предела, смотрим закон Амдала.
 Создает несколько процессов, у каждого из которых своя память и свой GIL, каждый выполняет свою задачу, взаимодействие
 между ними требует pickle(сериализация, превращение обьектов python в байты и наоборот байтов в обьекты python)
 API принципиально похоже на многопоточность, выгодно использовать Pool, а для взаимодействия между процессами Queue и Pipe
 Pipe - когда нужно 2 процесса между собой подружить чтобы они передавали друг другу данные.


 Плюсы:
 + реальная параллельность любых задач
 + не умирает из-за одного(!)
 + процессы не зависят друг от друга(у каждого процесса своя память и GIL)
 Минусы:
 - потребление ресурсов (памяти, процессора, времени)
 - необходимость сериализации в pickle
 - проблемы синхронизации (взаимодействие между процессами)


 -- ЦЕЛЬ СЕРИАЛИЗАЦИИ с помощью pickle в multiprocessing --

 ЦЕЛЬ СЕРИАЛИЗАЦИИ с помощью pickle в multiprocessing заключается в том, чтобы преобразовать объекты Python в формат,
 который можно сохранить в файл или передать между процессами. Это необходимо, поскольку каждый процесс в
 multiprocessing имеет своё собственное пространство памяти. Сериализация позволяет копировать данные и передавать их,
 обеспечивая тем самым возможность взаимодействия между процессами.


 --- Основы асинхронного программирования в Python ---
 Асинхронное программирование позволяет запускать операции параллельно, НЕ дожидаясь выполнения последовательности.

 Кроме того, в асинхронном коде используются конструкции await и async with,
 которые позволяют приостановить выполнение текущей задачи и переключиться на другую,
 пока выполняется асинхронная операция. Это позволяет эффективно использовать ресурсы процессора и снизить время ожидания.

 --- asyncio ---
 asyncio - Асинхронное выполнение, подходит для IO-bound задач, работает ровно 1 поток - использует Конкурентность по полной
 Плюсы:
 + скорость и экономия времени, вместо x + y + z -> max(x, y, z)
 + управляемость
 + меньше потребление ресурсов (в сравнении с потоками)
 Минусы:
 - "умирает" из-за одного блокирующего вызова (!)
 - event loop НЕ безразмерный, нужно понимать, что корутины не бесплатные

 1) корутина работает как генератор
 2) async - явный флаг, что данная функция является асинхронной (корутиной)
 3) await - явный флаг, что в этом месте функция встает на паузу и дает работать другим, пока ждёт свои данные
 4) event loop - цикл событий, механизм, который отвечает за планирование и запуск корутин. Можно представить как
 список/очередь, из которого в вечном цикле достаются и запускаются корутины
 Частые ошибки:
 - не использование await внутри корутины
 - создание корутины, но использование ее, как функции
 - использование внутри корутин синхронного(блокирующего) кода, в том числе IO


 asyncio.sleep(0) приостанавливает выполнение текущей корутины, позволяя циклу событий выполнить другие задачи.
 Это полезно для перераспределения управления между корутинами без фактической задержки.


 -- Блокирующие API --

 Примеры блокирующих API:
 библиотека requests или функция time.sleep

 Вообще, любая функция, которая выполняет ввод-вывод, не-являясь сопрограммой, или занимает процессор длительными
 операциями, может считатся блокирующей.

 библиотека requests - Блокирующая т.е. блокирует поток, в котором выполняется. Поскольку asyncio ОДНОПОТОЧНАЯ,
 requests блокирует цикл событий и не дает ничему выполняться конкурентно.

 Чтобы использовать async c requests  Нужно Указать asyncio задействовать многопоточность с помощью исполнителя ПУЛА  ПОТОКОВ

 Большинство API с которыми мы работаем, в настоящий момент являються БЛОКИРУЮЩИМИ и без ДОРАБОТОК РАБОТАТЬ с asyncio
 НЕ БУДУТ!
 Нужно использовать библиотеки, которые поддерживают СОПРОГРАММЫ и НЕБЛОКИРУЮЩИЕ сокеты.
 Можно использовать aiohttp - в которой используются НЕБЛОКИРУЮЩИЕ сокеты и которая возвращает СОПРОГРАММЫ!


 **Блокирующий сокет** — это сокет, который при выполнении операций останавливает выполнение программы до их завершения.
 Если нет данных для чтения или операция не может быть выполнена, программа "зависает".

 **Неблокирующий сокет** — это сокет, который не останавливает выполнение программы. Если операция не может быть
 выполнена мгновенно (например, нет данных), она завершится неудачей, и программа может продолжать свою работу или
 пытаться повторить операцию позже.

 Таким образом, блокирующий сокет прост в использовании, но может замедлить программу, тогда как неблокирующий позволяет
 более гибко управлять потоком выполнения, но требует более сложной обработки ошибок.


 -- namedtuple, dataclasses и typing.NamedTuple    Построители классов данных  --

 - namedtuple: легковесные объекты с именованными полями.
 - dataclass: классы с автоматическим созданием метода __init__ и другими.
 - typing.NamedTuple: именованные кортежи с поддержкой аннотаций типов.

 Примеры:

 # dataclass                              # typing.NamedTuple                    # namedtuple
 from dataclasses import dataclass        from typing import NamedTuple          from collections import namedtuple

 @dataclass                               class Point(NamedTuple):               Point = namedtuple('Point', ['x', 'y'])
 class Point:                                 x: int
     x: int                                   y: int
     y: int

 p = Point(10, 20)                        p = Point(10, 20)                      p = Point(10, 20)
 Point.__doc__ # Point(x: int, y: int)    print(Point.__doc__)  # Point(x, y)    print(Point.__doc__)  # Point(x, y)
 print(p)  # Point(x=10, y=20)            print(p.x, p.y)  # 10 20               print(p.x, p.y)  # 10 20

 print(Point.__annotations__)               print(Point.__annotations__)                print(Point.__annotations__)
 # {'x': <class 'int'>, 'y': <class 'int'>} # {'x': <class 'int'>, 'y': <class 'int'>}  # {}



 --- OOP ---
 ООП основано на «трех китах» - трех важнейших принципах придающих объектам новые свойства.
 Этими принципами являются ИНКАПСУЛЯЦИЯ, НАСЛЕДОВАНИЕ и ПОЛИМОРФИЗМ.


 Encapsulation (Инкапсуляция)
 public(публичный) = без _ , __

 _ - protected(защищенный) знак того, что этот атрибут не предназначен для прямого использования.
 Работа обьекта не гарантируется, при использовании таких атрибутов

 from mymod import *     Имена с префиксом _ не будут импортироваться из mymod                      <-----

  __ - private(приватный) под капотом преобразуется в object._Class__attribute (только для случаев когда начинается с __)
 Name MangLing (Преобразование в не явное)
 Явное лучше неявного!!!

 __ private сделано для того чтобы при наследовании НЕ переопределить(изменить) атрибут предка

 IS-A является (наследование)
 HAS-A содержит (композиция)

 Правильно исполненная композиция дадут сто очков вперед наследованию.                  <-----


 Итак, в статических языках ИНКАПСУЛЯЦИЯ реализуется более ЖЕСТКО с помощью модификаторов доступа,
 в то время как в Python она основывается на соглашениях и предоставляет более гибкий подход.


 --- ЛЮБОЙ обьект в Python наследуется от object ---
 object() не поддерживает атрибуты экземпляра, поскольку он является основой для всех пользовательских классов Python,
 которые должны поддерживать отсутствие атрибута __dict__ при определении слотов .


 Numpy не имеет __dict__ и поэтому НЕ поддерживает произвольные атрибуты .
 Большинство (возможно, все) классов, определенных в C, НЕ имеют словаря для оптимизации.   <-----


 --- Mixin (Миксины, Примеси) ---
 Миксины — это особый вид множественного наследования в Python,
 которые используются для предоставления дополнительной функциональности классам.
 Основная цель миксинов - предоставить какие-то дополнительные методы.


 --- Множественное наследование   Multiple inheritance ---
 Python позволяет использовать концепцию множественного наследования. Это значит,
 что ваш класс может наследовать атрибуты и методы сразу от нескольких родительских классов.
 Это позволяет создавать более сложные иерархии классов, объединяя функциональность из разных источников


 --- MRO (method resolution order) ---
 Аббревиатура MRO – method resolution order (переводится как «порядок разрешения методов»).
 Этот порядок относится не только к поискам методов, но и к прочим атрибутам класса,

 Class.mro()   Class.__mro__
 Поиск идёт по алгоритму С3-линеаризации
 Используется первое обнаруженное вхождение. Такой порядок называется DFLR (Обход вглубину и слева направо).


 -- Diamond problem --
 В Python, наследование ромбовидной формы (или diamond inheritance) возникает, когда класс наследует от двух классов,
 которые, в свою очередь, наследуют от одного общего базового класса. Это может привести к неоднозначности,
 когда необходимо определить, какой метод или атрибут должен быть использован.

 # Пример ромбовидного наследования:
 class A:
     def say_hello(self):
         print("Hello from A")

 class B(A):
     def say_hello(self):
         print("Hello from B")

 class C(A):
     def say_hello(self):
         print("Hello from C")

 class D(B, C):
     pass

 d = D()
 d.say_hello()  # Вывод: Hello from B


 -- Композиция vs Наследование   Composition vs Inheritance  --
 **Композиция** и **наследование** — это два основных подхода к организации кода и связыванию объектов
 в объектно-ориентированном программировании (в том числе в Python).

 ### Наследование:
 - **Определение**: Наследование позволяет создать новый класс на основе существующего. Новый класс (подкласс) наследует
  атрибуты и методы родительского класса (суперкласса).
 - **Преимущества**: Позволяет повторно использовать код и расширять функциональность. Обеспечивает иерархическую структуру классов.
 - **Недостатки**: Может приводить к "жесткой" привязке между классами, что делает код менее гибким и трудным для поддержки.

 **Пример наследования**:
 python
 class Animal:
     def speak(self):
         return "Animal sound"

 class Dog(Animal):
     def speak(self):
         return "Bark"


 ### Композиция:
 - **Определение**: Композиция подразумевает создание классов, которые содержат экземпляры других классов как атрибуты.
  Вместо наследования класс использует функциональность других классов.
 - **Преимущества**: Обеспечивает большую гибкость, позволяет изменять поведение на лету, снижает зависимость между классами.
 - **Недостатки**: Может потребовать больше кода для определения взаимодействий между компонентами.

 **Пример композиции**:
 python
 class Engine:
     def start(self):
         return "Engine starts"

 class Car:
     def __init__(self):
         self.engine = Engine()  # композиция

     def start(self):
         return self.engine.start() + " and Car moves"


 ### Основные различия:
 1. **Структура**: Наследование создает иерархические связи, тогда как композиция использует "составляет" отношения.
 2. **Связь классов**: Наследование подразумевает сильную зависимость, а композиция — более слабую.
 3. **Использование**: Наследование подходит, когда есть "является" (is-a) отношения, а композиция — когда есть "имеет"
    (has-a) отношения.

 Таким образом, выбор между наследованием и композицией зависит от требований вашего проекта и архитектуры вашего кода.


  --- Polymorphism (Полиморфизм) ---
 Если не вдаваться в теории, то полиморфизм о котором вам нужно знать и о котором спросят на собеседовании
 - это механизм, позволяющий выполнять один и тот же код по-разному.
 Polymorphism (Полиморфизм) в объектно-ориентированном программировании – это возможность обработки разных типов данных,
 т. е.(то есть) принадлежащих к разным классам, с помощью "одной и той же" функции, или метода.
 Ducktyping (утиная типизация) - наличие поведения для использования в полиморфизме

 Пример Полиморфизма!!!
 class Dog:
     def speak(self):
         return "Woof!"

 class Cat:
     def speak(self):
         return "Meow!"

 def animal_sound(animal):
     print(animal.speak())

 dog = Dog()
 cat = Cat()

 animal_sound(dog)  # Вывод: Woof!
 animal_sound(cat)  # Вывод: Meow!


 Специальный атрибут __slots__ позволяет вам явно указать, какие атрибуты экземпляра вы ожидаете иметь у
 экземпляров вашего объекта, с ожидаемыми результатами:
 1) более быстрый доступ к атрибутам.
 2) экономия места в памяти.

 Экономия места происходит от:
 1) Хранение ссылок на значения в слотах вместо __dict__.
 2) Запрещение создания __dict__ и __weakref__, если родительские классы запрещают их, а вы объявляете __slots__.

 __slots__ — уменьшить объем памяти, занимаемый каждым экземпляром объекта.


 --- Абстрактный класс ---
 Зачем нужны АБСТРАКТНЫЕ классы в Python?
 ОНИ ПОЗВОЛЯЮТ ЯВНО УКАЗАТЬ, КАКИЕ МЕТОДЫ ДОЛЖНЫ БЫТЬ РЕАЛИЗОВАНЫ В КЛАССАХ-ПОТОМКАХ.  или NotImplementedError

 Зачем нужен ABC в Python?
 ПОЗВОЛЯЮТ ОПРЕДЕЛИТЬ КЛАСС, УКАЗАВ ПРИ ЭТОМ, КАКИЕ МЕТОДЫ ИЛИ СВОЙСТВА ОБЯЗАТЕЛЬНО ПЕРЕОПРЕДЕЛИТЬ В КЛАССАХ-НАСЛЕДНИКАХ.
 Абстрактный класс - это класс, который НЕ предназначен для создания объектов напрямую.
 NotImplementedError - Исключение, возникающее в случаях, когда наследник класса не переопределил метод, который должен был.


 abc (аббревиатура от Abstract Base Classes) from abc import ABC, abstractmethod
 и вешаем декоратор @abstractmethod на pass функции

 from abc import ABC, abstractmethod
 class Shape(ABC):
     @abstractmethod
     def area(self):
         pass

 # Нельзя создать экземпляр класса Shape:
 s = Shape()  # -> TypeError: Can't instantiate abstract class Shape with abstract method area

 # Нужно реализовать метод area
 class MyClass(Shape):
     pass

 c = MyClass()  # -> TypeError: Can't instantiate abstract class MyClass with abstract method area

 # Всё работает
 class MyClass(Shape):
     def area(self):
         return 1000

 c = MyClass()
 print(c.area())  # -> 1000

 Класс, который наследует абстрактный класс, должен реализовать все его абстрактные методы, иначе он НЕ будет создан.


 --- Дескрипторы ---
 Дескрипторы - это объекты Python, которые определяют, как другие объекты должны вести себя при доступе к атрибуту.
 Descriptor(Дескриптор) -  Любой объект, определяющий методы __get__(), __set__(), или __delete__(). ТОЧЕЧНЫЙ ПОИСК


 # __get__, __set__, __delete__
 # Дескриптор — это то, как реализован тип Python property
 # Функции/методы, связанные методы, property, classmethod и staticmethod все они используют эти специальные методы
 # для управления доступом к ним с помощью ТОЧЕЧНОГО ПОИСКА.

 # Встроенные объектов дескрипторы: classmethod, staticmethod, property, функции в целом
 # свойства с property: Создайте функции для управления получением, установкой и удалением атрибута
 class C:
     def __init__(self):
         self._x = None

     @property
     def x(self):
         '''I'm the 'x' property.'''
         return self._x

     @x.setter
     def x(self, value):
         if not isinstance(value, int):
             raise ValueError
         self._x = value

     @x.deleter
     def x(self):
         del self._x

 c = C()
 c.x = 10
 print(c.x)  # 10


 # Примеры встроенных объектов дескрипторов: classmethod, staticmethod, property, функции в целом      <-----
 def has_descriptor_attrs(obj):
     return set(['__get__', '__set__', '__delete__']).intersection(dir(obj))

 def is_descriptor(obj):
     '''obj can be instance of descriptor or the descriptor class'''
     return bool(has_descriptor_attrs(obj))

 def has_data_descriptor_attrs(obj):
     return set(['__set__', '__delete__']) & set(dir(obj))

 def is_data_descriptor(obj):
     return bool(has_data_descriptor_attrs(obj))



 # Мы можем видеть, что это classmethod и staticmethod и функции в целом есть Non-Data-Descriptors:
 print(is_descriptor(classmethod), is_data_descriptor(classmethod))    # -> True False
 print(is_descriptor(staticmethod), is_data_descriptor(staticmethod))  # -> True False

 # Обычные функция  Тоже Non-Data-Descriptors
 def foo(): pass
 my_func = lambda: 5

 print(is_descriptor(foo), is_data_descriptor(foo))                    # -> True False
 print(is_descriptor(my_func), is_data_descriptor(my_func))            # -> True False

 # Только метод __get__
 print(has_descriptor_attrs(classmethod))   # -> {'__get__'}
 print(has_descriptor_attrs(staticmethod))  # -> {'__get__'}
 # Обычные функции __get__
 print(has_descriptor_attrs(foo))           # -> {'__get__'}
 print(has_descriptor_attrs(my_func))       # -> {'__get__'}



 # Дескриптор данных, @property    Data-Descriptor
 # @property
 print(is_data_descriptor(property))    # -> True
 print(has_descriptor_attrs(property))  # -> {'__get__', '__delete__', '__set__'}

 --- @property ---
 property - это удобный механизм создания геттеров и сеттеров
 У класса (и объекта) есть два основных инструмента взаимодействия — свойства и методы.
 Свойства — это данные, которые лежат внутри объекта.
 Методы — это то, что объект умеет делать или как реагирует на внешние запросы.
 Геттеры и сеттеры являются методами класса, которые используются для получения
 и установки значений атрибутов экземпляра класса, соответственно.

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, value):
        if not value:
            raise AttributeError('Name cant be empty')
        self._name = value

 Возможность установки/получения атрибутов с логикой
 Зарпетить менять атрибут или добавлять новые атрибуты
 __dict__ - это атрибут обьектов в питоне, который хранит состояние
 __setattr__ вызывается при попытке установить атрибут
 @property - это удобный механизм создания геттеров и сеттеров
 в __init__ нужно вызывать setter
 как можно сделать чтобы нельзя было поменять атрибут?
 можно сделать при помощи @property или переопределение __setattr__ просто прописывает условия

 getter еще называют - аксессор,  setter - мутатор


  --- Встроенные функции vs Магические методы ---

 1. Оптимизация: Встроенные функции в Python часто оптимизированы для производительности.
 2. Накладные расходы: Вызов метода требует дополнительных накладных расходов на доступ к атрибуту объекта,
    что может замедлить выполнение.

 На практике, разница во времени может зависеть от конкретной версии Python и системы, на которой вы выполняете код,
 но в общем случае встроенные функции будут более быстрыми и предпочтительными.

 Для встроенных объектов в CPython встроенные функции (например, len(), str()) работают напрямую с объектами и НЕ
 вызывают соответствующие магические методы. Это позволяет избежать накладных расходов, связанных с вызовом методов,
 что делает их более быстрыми.


 # Сравнение built-in function vs dunder method
 from timeit import timeit

 #  str() vs __str__()
 print(timeit('str(100)'))                                # -> 0.10770369996316731
 print(timeit('100 .__str__()'))                          # -> 0.15946240001358092

 #  iter() vs __iter__()
 print(timeit('iter([100, 1000])', number=1000000))       # -> 0.11073940002825111
 print(timeit('[100, 1000].__iter__()', number=1000000))  # -> 0.1557056000456214


 Для встроенных объектов интерпретатор CPython вообще НЕ вызывает никаких методов: длина просто читается из поля
 C-структуры.  Эффективно работает для встроенных типов как str, list, memoryview и т.п.     <-----  Важно!!!

 # Например функция len()
 # Сравнение tuple, set, dict, list
 #  len() vs __len__()    len()  Намного быстрее работает                                    <-----  Важно!!!

 # РАБОТАЕТ В 2 РАЗА БЫСТРЕЕ  для []
 print(f"{timeit('len([100, 1000])', number=1000000):.6f}")        # -> 0.072438
 print(f"{timeit('[100, 1000].__len__()', number=1000000):.6f}")   # -> 0.145085

 # tuple  ()
 print(f"{timeit('len((100, 1000))', number=1000000):.6f}")        # -> 0.036578
 print(f"{timeit('(100, 1000).__len__()', number=1000000):.6f}")   # -> 0.103023

 # set
 print(f"{timeit('len({100, 1000})', number=1000000):.6f}")        # -> 0.127049
 print(f"{timeit('{100, 1000}.__len__()', number=1000000):.6f}")   # -> 0.183266

 # dict
 print(f"{timeit('len({100: 1000})', number=1000000):.6f}")        # -> 0.117994
 print(f"{timeit('{100: 1000}.__len__()', number=1000000):.6f}")   # -> 0.171075

 # Расширенный пример

 # Измеряем время для встроенной функции str()
 builtin_time = timeit('str(100)', number=1000000)  # Выполняем 1,000,000 раз
 print(f'Встроенная функция str(): {builtin_time:.6f} секунд')  # -> Встроенная функция str(): 0.107998 секунд

 # Измеряем время для дандер-метода
 dunder_time = timeit('100 .__str__()', number=1000000)  # Выполняем 1,000,000 раз
 print(f'Метод __str__(): {dunder_time:.6f} секунд')            # -> Метод __str__():          0.161750 секунд


 -- ОГРАНИЧЕНИЯ! На перегрузку Операторов  --
 ОГРАНИЧЕНИЯ! запрещается перегружать операторы встроенных типов, запрещается создавать новые операторы и перегружать
 операторы is, and, or, not

 Методы __and__, __or__, __is__ и __not__ действительно могут быть определены в вашем классе, но это НЕ означает,
 что перегруженные версии этих операторов будут работать так же, как и у встроенных операторов.                   <-----


 --- Dunder methods (Double Underscores) Магические методы ---

 Магические методы - dunder методы, методы которые начинаются и заканчиваются __
 для самописных классов нужно переопеделить магические методы для нужного поведения и действий

 Метод __new__ вызывается при создании нового экземпляра класса. Он отвечает за выделение памяти под новый объект
  и возвращает этот объект. __new__ является статическим методом и должен возвращать экземпляр класса.
 __init__ отвечает за инициализацию экземпляров класса после их создания.
 __init__ используется для инициализации атрибутов объекта при его создании
 __init__ по умолчанию не ждет аргументов
 __new__() для его создания, а __init__() для его настройки

 Создание атрибутов экземпляра вне метода __init__ может привести к созданию ненужных атрибутов и увеличению
 потребления памяти, особенно если они не используются.
 Для экономим памяти избегайте создания атрибутов ЭКЗЕМПЛЯРА ВНЕ метода __init__           <-----  Важно!!!

 Добавление атрибута экземпляра после возврата из __init__ заставляет Python создать хэш-таблицу для хранения __dict__
 только для ОДНОГО этого экземпляра это оптимизация сокращает потребление памяти на 10% - 20%.    PEP 412

 В Python, если атрибут экземпляра добавляется после вызова __init__, интерпретатор может оптимизировать использование
 памяти, создавая хэш-таблицу (__dict__) только для этого экземпляра. Это может сократить потребление памяти на 10% - 20%,
 особенно если экземпляр имеет немного атрибутов.

 What the f*ck Python!   # Проект  СПОРНЫЙ МОМЕНТ ПРОВЕРЯЙ САМ!!!                          <-----  Важно!!!
 Небольшой совет, если вы хотите уменьшить объем памяти, занимаемый вашей программой: не удаляйте атрибуты экземпляра
 и обязательно инициализируйте все атрибуты в вашем __init__!

 __repr__ - для програмистов, возвращает строку, по которой видно (и можно воссоздать) состояние обьекта  !r
 __str__ - для людей, возвращает строку    !s
 если не реализован репр и стр, то будет возвращать адрес в памяти (который в Python являет-ся адресом объекта в оперативной памяти).
 __eq__ по умолчанию сравнивает адрес в памяти, в реализации лучше сразу проверить тип
 если определяете метод __eq__ то теряется возможность нахождение hash()
 если методы сравнения не реализованы то падает ошибка
 contains для реализации проверки IN
 bool для самодельных обьектов всегда вернет True, для изменения поведения нужно написать __bool__
 len вернет ошибку если не переопределить метод __len__
 чтобы обьект стал вызываемым (callable) нужно реализовать __call__, иначе ошибка
 __iter__ возвращает обьект итератор, тот кто реализует итер = Итерабл
 __iter__ = должен вернуть обьект который умеет делать __next__
 __next__ должен вернуть следующий обьект из контейнера, кто его реализует = Итератор, for работает до StorIteration
 Итератор = обьект в нём есть данные и он может по вызову обьекта __next__ данные выдавать
 __getitem__ нужен для функционала [] (аналог списка и словаря)
 __setitem__ для присвоения через [], если не реализовать = ошибка
 если в обьекте не реализован __iter__ то для цикла фор будет использован __getitem__ там ожидается падение IndexError


 -- reprlib --                                                                              <-----
 reprlib в Python используется для создания сокращенных представлений объектов, которые могут быть слишком большими
 для стандартного repr()

 import reprlib

 large_list = list(range(1000))

 print(reprlib.repr(large_list))  # -> [0, 1, 2, 3, 4, 5, ...]
 print(repr(large_list))          # -> Будет ОГРОМНЫЙ ВЫВОД!!!

 # По умолчанию 30 символов лимит
 large_str = 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'

 print(reprlib.repr(large_str))  # -> 'aaaaaaaaaaaa...aaaaaaaaaaaaa'
 print(repr(large_str))          # -> 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'

 # ИЗМЕНЯЕМ ЛИМИТЫ
 # Создаем объект Repr    Там ЕЩЕ МНОГО АТРИБУТОВ
 r = reprlib.Repr()
 r.maxlist = 10    # Максимум элементов в списке
 r.maxdict = 5     # Максимум элементов в словаре
 r.maxstring = 10  # Максимум элементов в строке

 # Вывод после ИЗМЕНЕНИЯ  ЛИМИТОВ
 print(r.repr(large_list))       # -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...]
 print(r.repr(large_str))        # -> 'aa...aaa'



 Ах, да... Магические методы не предусматривают работы с async
 Большинство магических методов не предназначены для работы с async def/ await— как правило, вам следует использовать
 только await внутри выделенных асинхронных магических методов — __aiter__, __anext__, __aenter__и __aexit__.


 async и await всего лишь синтаксический сахар вокруг генераторов!                                              <-----

 # До появления ключевых слов async и await в версии Python 3.5 связь между сопрограммами и генераторами были очевидной.
 # Старый синтаксис с декораторами и генераторами.

 # В таком стиле писать код НЕ СТОИТ! Старый стиль с использованием декораторов и генераторов в asyncio
 # использование `@asyncio.coroutine` более НЕ поддерживается.
 # 3.11 и более поздних версиях действительно была удалена поддержка декоратора `@asyncio.coroutine`

 import asyncio

 Старый стиль/синтаксис                             # Тоже самое Новый стиль/синтаксис  async и await  с версии Python 3.5
 @asyncio.coroutine                                 async def coroutine():
 def coroutine():                                       print('Засыпаю!')
     print('Засыпаю!')                                  await asyncio.sleep(1)
     yield from asyncio.sleep(1)                        print('Проснулась!')
     print('Проснулась!')
                                                    asyncio.run(coroutine())

 asyncio.run(coroutine())  # AttributeError: module 'asyncio' has no attribute 'coroutine'. Did you mean: 'coroutines'?


  ----- ИТОГ S.O.L.I.D -----
 1) ПРИНЦИП ЕДИНСТВЕННОЙ ОТВЕТСТВЕННОСТИ (Single Responsibility Principle):
 1) У класса должна быть всего одна причина для изменения.
 1) Один класс выполнял только одну работу.


 2) ПРИНЦИП ОТКРЫТОСТИ/ЗАКРЫТОСТИ (Open-Closed Principle):
 2) Программные сущности (классы, модули, функции и т.п.) должны быть открыты для расширения, но закрыты для изменений.


 3) ПРИНЦИП ПОДСТАНОВКИ БАРБАРЫ ЛИСКОВ (Liskov Substitution Principle):
 3) подклассы не должны противоречить надклассам
 3) функции, которые используют базовый тип, должны иметь возможность использовать подтипы базового типа, не зная об этом.

 3) Объекты в программе должны быть заменяемы экземплярами их подтипов без ущерба корректности работы программы.
 3) Проще говоря, это значит, что подкласс, дочерний класс должны соответствовать их родительскому классу или супер классу
 3) использовать любой подкласс базового класса, не замечая разницы между ними


 4) ПРИНЦИП РАЗДЕЛЕНИЯ ИНТЕРФЕЙСОВ (Interface Segregation Principle):
 4) Ни один клиент не должен зависеть от методов, которые он не использует.
 4) слишком «толстые» интерфейсы необходимо разделять на более маленькие и специфические
 4) «Программные сущности не должны зависеть от методов, которые они не используют»

 4) «Много интерфейсов, специально предназначенных для клиентов, лучше чем один интерфейс общего назначения»


 5) ПРИНЦИП ИНВЕРСИИ ЗАВИСИМОСТЕЙ (Dependency Inversion Principle):
 5) Модуль высокого уровня не должен зависеть от модулей низкого уровня. И то, и другое должно зависеть от абстракций.
 5) Абстракции не должны зависеть от деталей реализации. Детали реализации должны зависеть от абстракций.

 5) «Зависимость на Абстракциях. Нет зависимостри на что-то конкретное»


 Абстракция — основной способ борьбы со сложностью в программировании. Она позволяет уйти от деталей реализации
 и сосредоточиться на главном. Хороший пример абстракции — функция сортировки списка.
 Не важно, как она устроена, важно, что она делает то, что нам нужно.

 Абстракция гласит что мы должны выделять важные характеристики объекта.



 --- Метаклассы metaclass ---
 Если коротко, то метаклассы - это классы, которые конструируют другие классы
 Основная цель метаклассов — автоматически изменять класс в момент создания.

 type это встроенный метакласс, который использует Питон, но вы, конечно, можете создать свой.

 Зачем вообще использовать метаклассы?
 Основное применение метаклассов это создание API. Типичный пример — Django ORM.   <----

 Django использует метаклассы для создания моделей базы данных

 Пример Создание своего Метакласса!

 # Таким образом, вы создали метакласс `MyMeta`, который добавляет атрибут `custom_attribute` в класс `MyClass`.

 class MyMeta(type):
     def __new__(cls, name, bases, attrs):
         # Модификация атрибутов класса
         attrs['custom_attribute'] = 'This is a custom attribute'
         attrs['hehe'] = '123'
         return super().__new__(cls, name, bases, attrs)

 class MyClass(metaclass=MyMeta):
     pass

 instance = MyClass()
 print(instance.custom_attribute)  # Вывод: This is a custom attribute
 print(instance.hehe)              # Вывод: 123
 print(instance.__dict__)          # Вывод: {}
 print(MyClass.hehe)               # Вывод: 123
 print(MyClass.custom_attribute)   # Вывод: This is a custom attribute

 # Важно Интересный пример!!!
 print(type(MyClass))              # -> <class '__main__.MyMeta'>
 print(type(instance))             # -> <class '__main__.MyClass'>


 - Как создать класс без слова class?
 Kласс можно создать без использования ключевого слова class, используя типы type :

 MyClass = type('MyClass', (), {'x': 42, 'foo': lambda self: self.x})

 # Тоже самое что и выше но с ключевым словом class!
 class MyClass:
    x = 42
    def foo(self):
        return self.x


 # ТОЖЕ САМОЕ НО УКАЗЫВАЕМ  НАСЛЕДОВАНИЕ
 MyClass = type('MyClass', (MySuperClass, MyMixin), {'x': 42, 'foo': lambda self: self.x})

 # Тоже самое что и выше но с ключевым словом class!
 class MyClass(MySuperClass, MyMixin):
     x = 42

     def foo(self):
         return self.x

 # type - это тип всех типов, для которых не указан явно иной метакласс
 print(type(type))    # -> <class 'type'>
 print(type(object))  # -> <class 'type'>
 print(type(list))    # -> <class 'type'>
 print(type(set))     # -> <class 'type'>
 print(type(dict))    # -> <class 'type'>
 print(type(bool))    # -> <class 'type'>
 print(type(int))     # -> <class 'type'>
 print(type(str))     # -> <class 'type'>
 print(type(collections.deque))     # -> <class 'type'>

 class Bar(object): pass

 b = Bar()
 print(type(Bar))     # -> <class 'type'>
 print(type(b))       # -> <class '__main__.Bar'>


 НЕЛЬЗЯ изменять сам type, вы НЕ можете добавлять атрибуты напрямую к встроенному типу type   <-----
 type является частью внутреннего устройства Python и НЕ предназначен для изменения.
 Безопасно изменять только свои собственные метаклассы, созданные на основе type.


 --- Важные модули ---

 from timeit import timeit     # библиотека замера скорости
 from pympler import asizeof   # библиотека замера размера структур
 import dis                    # библиотека работы с байт кодом


 Jupyter-ноутбук — это среда разработки, где сразу можно видеть результат выполнения кода и его отдельных фрагментов.
 в Jupyter-ноутбук можно замерить время ->   %time a = list(range(100))  есть много магических команд  %time, %timeit, %%time


 --- Логирование ---
 Logging — это важный инструмент для отладки и отслеживания работы программы

 Модуль logging
 Этот модуль определяет функции и классы, которые реализуют гибкую систему регистрации событий для приложений и библиотек.


 -- Профилирование(profile)   Профилировщики(profilers) --
 Профилирование кода на Python — это важный этап оптимизации, который помогает улучшить производительность программы.
 Bottleneck - Узкое место программы.

 Стандартный профилировщик Python:
 cProfile - расширение C
 profile  - модуль на чистом Python, интерфейс которого имитируется cProfile


 Оптимизация потребления памяти в Python:
 - Модуль memory_profiler - измеряет использование памяти конкретной функцией построчно.
 - Модуль Pympler - много полезных функций включая отслеживание экземпляров классов или выявление утечек памяти.
 - NumPy , Scipy , Pandas , __slots__ , Trie-префиксное дерево
 - Использование генераторов Python и/или модуль mmap - ускорение операций ввода-вывода

 ОПТИМИЗАЦИЯ в Python:
 - Используйте операции с множествами set - Методы set  O(1) без hash-коллизий  O(n) c hash-коллизиями
 - Set - увеличивает скорость выполнения но будет потреблять еще больше оперативной памяти.
 - ОПТИМИЗАЦИЯ ИСПОЛЬЗОВАНИЯ СТРОК: Не используем Конкатенация строк - лучше join или f-строки
 - Интернирование строк, Кэширование чисел, Кэширование результатов functools декораторов
 - more-itertools, itertools, functools, collections, operator, built-in functions, используем and or замыкания,
 - list comprehensions, generator expression, iterators, set comprehensions, dict comprehensions, with
 - timeit(замера скорости) , asizeof(размера структур) , dis(байт код), Логирование logging
 - Профилирование кода - timeit и модуль cProfile (собирает статистику, анализ производительности)
 - multiprocessing (ядра пк), multithreading (потоки) , asyncio (асинхронные операции) , Numba - ускоряет код Python
 - Тестирование Pytest, unittest


 --- Python Snippets ---
 Python Snippets (Сниппет) - это какой-то отрывок кода, который может быть использован повторно.

 Set Operations, Functools, Metaclasses, asyncio, Dataclasses, Decorators with Arguments, Asynchronous Iterators,
 getattr, list/dict/set Comprehensions, genexp, with open, lambda, import, built-in functions, f-string ...


 Наводим на номер строки жмем другой кнопкой мыши и выбираем   Annotate with Git Blame    -  Показать кто делал код

 --- Какие есть ветки в Git ---

 Вот основные ветки, которые часто используются в Git:

 1. **`master` (или `main`)** - Основная ветка, содержащая стабильную и готовую к производству версию проекта.

 2. **`develop`** - Ветка для разработки, в которую интегрируются новые функции и изменения перед слиянием с
 `master` или `main`.

 3. **`feature/*`** - Ветки для разработки конкретных функций, которые создаются от `develop` и сливаются обратно
 после завершения.

 4. **`bugfix/*`** или **`hotfix/*`** - Ветки для исправления ошибок, которые могут создаваться как от `develop`,
 так и от `master`, в зависимости от ситуации.

 5. **`release/*`** - Ветки, предназначенные для подготовки к выпуску новой версии, где проводятся финальные тесты и
 фиксации изменений перед слиянием с `master`.

 Эти ветки помогают организовать процесс разработки и управления версиями в проекте.



 --- В чем разница между git rebase и git merge? ---

 merge  - Добавляет коммит мержа и сохраняет историю коммитов
 rebase - Применяет все коммиты на целевую ветку и удаляет историю коммитов

 Git Merge - Объединение веток
 - Команда `git merge` объединяет две ветки, создавая новый коммит, который включает изменения из обеих веток.
 В результате появляется так называемый "мердж-коммит".

 Git Rebase - Перемещение коммитов     "перезаписывает" историю.

 **Merge:** Используйте `merge`, когда хотите сохранить всю историю изменений, включая ветвление и слияния. Это особенно
 полезно в больших проектах с несколькими разработчиками.
 **Rebase:** Используйте `rebase`, чтобы иметь более чистую и понятную историю, особенно при работе с локальными
 ветками перед отправкой (push) на удалённый репозиторий.


 -- git stash --
 Команда git stash позволяет на время «сдать в архив» (или отложить) изменения, сделанные в рабочей копии,
 чтобы вы могли применить их позже.
 Откладывание изменений полезно, если вам необходимо переключить контекст и вы пока не готовы к созданию коммита.

 `git stash` — это команда в системе контроля версий Git, которая позволяет временно сохранить текущие изменения в
 рабочем каталоге и индексе (stage), чтобы вы могли вернуться к чистому состоянию репозитория. Это может быть полезно,
 если вы хотите переключиться на другую ветку или выполнить какую-либо другую задачу, не теряя незавершенные изменения.

 Когда вы выполняете `git stash`, Git делает следующее:

 1. Сохраняет ваши изменения (как отслеживаемые, так и неотслеживаемые файлы) в специальном хранилище (stash).
 2. Возвращает ваш рабочий каталог и индекс к состоянию последнего коммита.

 После этого вы можете переключаться на другие ветки, выполнять коммиты или любые другие действия, не беспокоясь о том,
 что потеряете ваши изменения.

 Для восстановления ваших изменений из стека вы можете использовать команду `git stash pop` или `git stash apply`.
 Первая команда удаляет сохранённые изменения из стека после их применения, а вторая — применяет изменения, оставляя их в стеке.

 git stash apply "stash@{3}"     - Выбор нужного stash  "stash@{1}", "stash@{2}", "stash@{3}"
 git stash apply                 - Выбор ПОСЛЕДНЕГО stash

 Некоторые часто используемые подкоманды:

 - `git stash list` — показывает список всех сохранённых изменений.
 - `git stash show` — отображает изменения, сохранённые в последнем хранилище (по умолчанию).
 - `git stash drop` — удаляет указанный элемент из стека.
 - `git stash clear` — очищает весь стек сохранённых изменений.

 Таким образом, `git stash` предоставляет удобный способ временно "отложить" изменения, чтобы вы могли продолжить
 работу с чистым состоянием репозитория.


 Посмотреть изменения файла - Нажимаем на файлик и выбираем Show diff

 -- cherry-picking --
 В GitLab (и Git) **cherry-picking** — это процесс выбора отдельных коммитов из одной ветки и применения их в другую ветку.
 Это позволяет перенести конкретные изменения или исправления без слияния всей ветки. Обычно используется для быстрого
 исправления ошибок или выборочного внедрения функций.

 Cherry-picking — это выборочное применение изменений из одной ветки репозитория в другую.
 1. Найдите нужный коммит:

    git log

 2. Перейдите на целевую ветку:

    git checkout target-branch

 3. Примените коммит:

    git cherry-pick <commit-hash>

 Это добавит изменения из указанного коммита в текущую ветку.


 -- Как создать ветку в Git --
 git branch имя_ветки

 Например:
 git branch feature/new-feature

 **Переключитесь на эту ветку**
 git checkout имя_ветки             git switch имя_ветки     # Лучше использовать switch  для переключения между ветками

 **Создание новой ветки и переключение на нее**
 git checkout -b имя_ветки

 **Просмотреть все ветки** в репозитории:
 git branch

 **Удалить ветку** (если требуется):
 git branch -d имя_ветки


 -- Pull request --

 Pull request (PR) — это функция в системах управления версиями, таких как Git, которая позволяет разработчикам
 сообщать о том, что они внесли изменения в код и предлагают интегрировать эти изменения в основную ветку проекта.
 Pull request обычно создаётся, когда разработчик завершает работу над какой-либо задачей в отдельной ветке и хочет,
 чтобы другие участники проекта просмотрели, обсудили и, при необходимости, внесли изменения, прежде чем эти изменения
 будут объединены (или «вмержены») в основную ветку (например, `main` или `master`).

 Процесс работы с pull request обычно включает следующие шаги:

 1. **Создание ветки**: Разработчик создаёт новую ветку для выполнения своей работы.
 2. **Внесение изменений**: Проделываются необходимые изменения в коде.
 3. **Коммит и пуш**: Изменения коммитятся в локальной ветке и отправляются (push) на удалённый репозиторий.
 4. **Создание Pull request**: Через интерфейс удалённого репозитория (например, GitHub, GitLab, Bitbucket и др.)
 создаётся pull request на слияние созданной ветки с основной.
 5. **Код-ревью**: Другие разработчики или участники проекта могут просмотреть изменения, оставить комментарии,
 предложить улучшения или задать вопросы.
 6. **Слияние**: После обсуждения и одобрения изменений pull request может быть объединён с основной веткой.

 Pull request не только упрощает процесс интеграции кода, но и способствует улучшению качества кода благодаря код-ревью
 и обсуждениям среди команды.

 Pull request (PR) — это запрос на слияние изменений из одной ветки кода в другую в системе контроля версий Git.
 Он позволяет разработчикам предложить свои изменения, получить отзывы от команды и обсудить их, прежде чем объединить
 с основной веткой проекта. PR включает описание изменений, обсуждения и возможность проведения код-ревью.

 **Создание Pull Request**:
 - Перейдите в ваш репозиторий на GitHub (или другой платформе).
 - В разделе "Pull requests" нажмите на кнопку "New pull request".
 - Выберите вашу ветку (`feature/my-feature`) и основную ветку (`main`).
 - Заполните заголовок и описание PR, объясняя изменения и их цель.
 - Нажмите на кнопку "Create pull request".


 -- Команды GIT --
 Основные команды для работы с состояниями файлов

 1. git status: Показывает текущее состояние репозитория, включая изменения, которые были сделаны, но еще не
    зафиксированы, и файлы, готовые к коммиту.

 2. git add: Добавляет изменения в указанных файлах в индекс (staging area) для последующего коммита.

 3. git commit: Фиксирует изменения в локальном репозитории с соответствующим сообщением о коммите.

 4. git diff: Показывает различия между файлами: изменения в рабочем каталоге, разницу между индексом и
    последним коммитом или между двумя коммитами.

 5. git reset: Отменяет изменения в индексе или в рабочем каталоге; может быть использована с разными уровнями
    "жесткости" (например, --soft, --mixed, --hard) в зависимости от того, что нужно сбросить.


 -- ПОДТЯНУТЬ ИЗМЕНЕНИЯ --
 1) git stash
 2) git checkout main
 3) git pull origin main
 4) git checkout имя_ветки
 5) git rebase main
 6) git status
 7) git stash apply


 -- Сделать Pull Request --
 Переходим в папку проекта через cd
 1) git checkout -b имя_ветки
 2) git push   или   Команда git push --set-upstream origin имя_ветки
 3) Переходим по ссылке на Pull Request

 Команда git push --set-upstream origin имя_ветки используется для отправки локальной ветки имя_ветки на
 удалённый репозиторий origin и установки её в качестве "отслеживаемой" ветки.
 Это означает, что в дальнейшем вы сможете использовать просто git push и git pull без указания имени ветки,
 и Git будет знать, что работать нужно с этой веткой на удалённом репозитории.


 Команда git rebase -i HEAD~4 используется в Git для интерактивного перебазирования (interactive rebasing)
 последних 4 коммитов, находящихся в истории вашей текущей ветки. Вот более подробное объяснение:


 -- Отменить commit --
 Сверху Pycharm выбираем Git -> VCS Operations -> Show History -> Другой кнопкой мышки -> Revert commit   или Alt + 9
 (Файлы должны быть в git stash)

 Git Stash: Перед отменой коммита вам не обязательно помещать файлы в git stash, если вы просто хотите отменить
 последний коммит и оставить изменения в рабочем каталоге. Однако, если вы хотите сохранить текущие изменения,
 которые не были добавлены в коммит, тогда использование git stash может быть хорошей идеей.

 Шаги для отмены коммита:

 В верхнем меню PyCharm выберите Git -> VCS Operations Popup (можно также использовать комбинацию клавиш Alt + 9).
 Далее выберите Show History, чтобы открыть историю коммитов.
 Найдите нужный коммит в списке, щелкните по нему правой кнопкой мыши.
 Выберите Revert Commit, чтобы отменить изменения, внесенные в этом коммите.
 Результат: После выполнения этих шагов изменения, сделанные в отмененном коммите,
 будут добавлены обратно в ваш рабочий каталог, и вы сможете их отредактировать или создать новый коммит.

 Если у вас есть какие-то изменения в рабочем каталоге, которые вы не хотите потерять, используйте git stash перед
 отменой коммита. Если вы хотите просто отменить коммит, можно обойтись и без этого шага.


 -- Отменить commit через Терминал --

 Проверьте статус репозитория (необязательно, но полезно):
 git status

 Отмените последний коммит: Чтобы отменить последний коммит, оставив изменения в рабочем каталоге, используйте следующую команду:
 git reset --soft HEAD~1

 Если вы хотите удалить коммит и убрать изменения из рабочего каталога, используйте:
 git reset --hard HEAD~1

 (Опционально) Использование git stash: Если вы хотите сохранить текущие изменения перед отменой коммита,
 вы можете использовать команду git stash:
 git stash

 Затем выполните команду для отмены коммита:
 git reset --soft HEAD~1

 После этого вы можете вернуть изменения из stash:
 git stash pop


  -- Редактор Vim (ТЕКСТОВЫЙ РЕДАКТОР)--

 Основные команды:
 dd - удалить текущую строку.
 i - войти в режим вставки (insert mode).
 d30 - удалить 30 строк, начиная с текущей (вводится как d30 + Enter). Вместо 30 можно указать любое другое число.
 Сохранение и выход из редактора
 :w - сохранить файл.
 :q - выйти из Vim (если не было изменений).
 :q! - выйти из Vim без сохранения изменений.
 :wq - сохранить файл и выйти из Vim.


 Редактирование текста    Выход из режима РЕДАКТИРОВАНИЯ: Esc или Ctrl + C.
 i - вставить текст перед курсором.
 a - вставить текст после курсора.
 o - вставить новую строку после текущей строки и перейти в режим вставки.
 yy - скопировать текущую строку.
 p - вставить скопированный или вырезанный текст после курсора.
 u - отменить последнее действие.
 Ctrl + r - повторить последнее отмененное действие.


 Дополнительные команды
 h - показать справку с командами редактора (в режиме команд).
 x - удалить текущий символ (не строку).
 j - перейти к следующей строке.
 k - перейти к предыдущей строке.
 gg - перейти к первой строке.
 G - перейти к последней строке.
 :set number - включить отображение номеров строк.
 :set nonumber - отключить отображение номеров строк.
 /текст - найти строку (поиск по тексту, после / вводите текст для поиска).
 c - изменить текущую строку (входит в режим вставки).
 P - вставить строку перед текущей.

 Все эти команды относятся к редактору Vim, который является мощным инструментом для редактирования текста в Unix-подобных системах.

 git bash
 squash - в режиме редактирования (i) pick меняем на squash
 squash - сжатие или объединение данных


 --- Модуль functools предназначен для функций высшего порядка: функций, которые действуют или возвращают другие функции.
 В общем, любой вызываемый объект может рассматриваться как функция для целей этого модуля.

 Кэширование результатов представлено тремя функциями (можно использовать в качестве декоратора)
 - functools.lru_cache(),- functools.cache(),- functools.cached_property().

 @functools.total_ordering - Автоматическая реализация операторов сравнения.
 functools.cmp_to_key(func) - преобразует функцию сравнения старого стиля в ключевую функцию сравнения.
 functools.partial(func, /, *args, **keywords) - Заморозить часть аргументов вызываемой функции.
 functools.reduce(function, iterable[, initializer]) - берет итерируемый объект и уменьшает (или складывает) все его значения в одно.
 @functools.singledispatch - Декоратор @singledispatch модуля functools создает из обычной функции - универсальную
 функцию одиночной диспетчеризации.
 class functools.singledispatchmethod(func) - Декоратор singledispatchmethod() модуля functools создает из обычного
 метода класса - универсальный метод одиночной диспетчеризации.

 @functools.wraps(wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES) - Сохранение имени и строк документации
 декорированной функции.   Заменить атрибуты декоратора на атрибуты исходной функции.

 Модуль multipledispatch и его декоратор @dispatch ведут себя очень похоже на @singledispatchmethod.
 Единственная разница заключается в том что он может принимать в качестве аргументов несколько типов


 --- Модуль itertools в Python, эффективные итераторы для циклов   - сборник полезных итераторов ---

 Но если itertools вам уже не хватает, то вэлкам: more-itertools.
 Тут есть chunked, spy, first, one, only, unique_everseen и прочие прелести. Осторожно, с этой дряни невозможно слезть.


 --- Бесконечные итераторы   Infinite iterators ---

 Iterator       Arguments         Results                            Example

 count()        [start[, step]]   start, start+step, start+2*step, … count(10) → 10 11 12 13 14 ...

 cycle()        p                 p0, p1, … plast, p0, p1, …         cycle('ABCD') → A B C D A B C D ...

 repeat()       elem [,n]         elem, elem, elem, … endlessly      repeat(10, 3) → 10 10 10
                                  or up to n times


 --- Конечные итераторы    Iterators terminating on the shortest input sequence ---
 --- Итераторы, оканчивающиеся на самой короткой входной последовательности ---


 Iterator       Arguments          Results                             Example

 accumulate()   p [,func]          p0, p0+p1, p0+p1+p2, …              accumulate([1,2,3,4,5]) → 1 3 6 10 15

 batched()      p, n               (p0, p1, …, p_n-1), …               batched('ABCDEFG', n=3) → ABC DEF G

 chain()        p, q, …            p0, p1, … plast, q0, q1, …          chain('ABC', 'DEF') → A B C D E F

 chain.from_iterable()  iterable   p0, p1, … plast, q0, q1, …          chain.from_iterable(['ABC', 'DEF']) → A B C D E F

 compress()     data, selectors    (d[0] if s[0]), (d[1] if s[1]), …   compress('ABCDEF', [1,0,1,0,1,1]) → A C E F

 dropwhile()    predicate, seq     seq[n], seq[n+1], starting when     dropwhile(lambda x: x<5, [1,4,6,3,8]) → 6 3 8
                                   predicate fails

 filterfalse()  predicate, seq     elements of seq where               filterfalse(lambda x: x<5, [1,4,6,3,8]) → 6 8
                                   predicate(elem) fails

 groupby()      iterable[, key]    sub-iterators grouped by value     [k for k, g in groupby('AAAABBBCCDAABBB')] → A B C D A B
                                   of key(v)                          [list(g) for k, g in groupby('AAAABBBCCD')] → AAAA BBB CC D

 islice()       seq, [start,]      elements from seq[start:stop:step] islice('ABCDEFG', 2, None) → C D E F G
                stop [, step]

 pairwise()     iterable           (p[0], p[1]), (p[1], p[2])         pairwise('ABCDEFG') → AB BC CD DE EF FG

 starmap()     func, seq           func(*seq[0]), func(*seq[1]), …    starmap(pow, [(2,5), (3,2), (10,3)]) → 32 9 1000

 takewhile()   predicate, seq      seq[0], seq[1],                    takewhile(lambda x: x<5, [1,4,6,3,8]) → 1 4
                                   until predicate fails

 tee()         it, n               it1, it2, … itn splits       [list(i) for i in tee([1, 2, 3], 2)] →[[1, 2, 3], [1, 2, 3]]
                                   one iterator into n

 zip_longest() p, q, …             (p[0], q[0]), (p[1], q[1]), …  zip_longest('ABCD', 'xy', fillvalue='-') → Ax By C- D-



 --- Комбинаторные итераторы   Combinatoric iterators ---

 Iterator       Arguments          Results                             Example

 product()      p, q, … [repeat=1] cartesian product, equivalent      product('ABCD', repeat=2)
                                   to a nested for-loop               AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DD

 permutations() p[, r]             r-length tuples, all possible      permutations('ABCD', 2)
                                   orderings, no repeated elements    AB AC AD BA BC BD CA CB CD DA DB DC

 combinations() p, r               r-length tuples, in sorted order,  combinations('ABCD', 2)
                                   no repeated elements               AB AC AD BC BD CD

 combinations_with_replacement() p, r     r-length tuples, in sorted order,     combinations_with_replacement('ABCD', 2)
                                          with repeated elements                AA AB AC AD BB BC BD CC CD DD



 --- Модуль operator в Python  Функциональный интерфейс для встроенных операторов ---

 - Общий интерфейс поиска атрибутов и элементов модуля operator в Python -

 - Поиск атрибутов или элементов последовательностей/итераций -

 Одной из самых необычных особенностей модуля operator является концепция геттеров. Это вызываемые объекты, созданные
 во время выполнения для извлечения атрибутов объектов или содержимого из последовательностей.

 Геттеры особенно полезны при работе с итераторами или последовательностями генераторов, где они предназначены для
 быстрого извлечения полей аргументов из функций для map(), sorted(), itertools.groupby() или других функций, которые
 ожидают аргумент переданной функции.

 --- Полезный пример ---
 from operator import attrgetter, itemgetter

 class Cat:
     def __init__(self, name, age):
         self.name = name
         self.age = age

     def __repr__(self):
         return f'Cat {self.name}, age is {self.age}'

 if __name__ == '__main__':
     ints = list(range(20))
     print(list(map(lambda x: x ** 2, filter(lambda x: x % 2 == 0, ints))))
     print([i**2 for i in range(20) if i % 2 == 0])
     a_dict = {'a': 3, 'b': 2, 'd': 1, 'c': 4}
     print(sorted(a_dict.items(), key=lambda x: x[0]))
     print(sorted(a_dict.items(), key=itemgetter(1)))
     cats = [Cat('Tom', 3), Cat('Angela', 4)]
     print(sorted(cats, key=lambda x: x.age))
     print(sorted(cats, key=attrgetter('age')))
     print(sorted(cats, key=attrgetter('name')))


 --- Модуль collections ---
 # OrderedDict нужен для действий со словарем где необходим порядок элементов, например
 # сравнение с учетом порядка, перестановки элементов с сохранением порядка. Платим памятью!!!

 # ChainMap нужен для логического обьединения словарей для поиска информации, но при изменениях меняется первый словарь

 # Counter нужен для подсчета элементов в последовательности, работает только с hashable
 from collections import Counter

 print(Counter([1, 2]))          # -> Counter({1: 1, 2: 1})
 print(Counter([1, 2, [1, 2]]))  # -> TypeError: unhashable type: 'list'

 # defaultdict нужен для создания словаря по умолчанию. Значение подставляется при обращении к несуществующему ключу

 # deque(двунаправленная очередь) потокобезопасна, быстро оперирует с обеими сторонами

 # namedtuple нужен для создания структуры данных, нечто среднее между стандартными типами и самописными классом.
 # Неизменяемый, позволяет обращаться по имени атрибута, позволяет использовать индексы.



  --- Паттерны ---
 Паттерны или ШАБЛОНЫ разработки - это общие способы решения частых задач и проблем

 Паттерны в Python – это шаблоны для решения задач, которые часто встречаются в практике программиста.
 Паттерны — это типовые решение для типовых задач.
 Существует три основные разновидности паттернов:
 1) Архитектурные паттерны, 2) Паттерны проектирования, 3) Идиомы

 ПАТТЕРНЫ ПРОЕКТИРОВАНИЯ:
 1) Порождающие паттерны — отвечают за процесс создания объектов.
 2) Структурные паттерны — определяют отношения между объектами, облегчая их взаимодействие
 3) Поведенческие паттерны — определяют способы коммуникации между объектами

  Книга о разработке и алгоритмах. Банда Четырех Паттерны Проектирования.
  Авторы: Гамма Эрих, Хелм Ричард, Джонсон Роберт, Влиссидес Джон


 Порождающие паттерны:

 1)  Абстрактная фабрика (Abstract factory):
 создает семейства связанных объектов, не привязываясь к конкретным классам создаваемых объектов.


 2)  Строитель (Builder):
 От абстрактной фабрики отличается тем, что делает акцент на пошаговом конструировании объекта.
 Строитель возвращает объект на последнем шаге, тогда как абстрактная фабрика возвращает объект немедленно.

 Используется для пошагового создания сложных обьектов


 3)  Фабричный метод (Factory method):
 Создает объекты без указания точного класса, но оставляет подклассам решение о том, какой класс инстанцировать.

 Инстанцирование — создание экземпляра класса

 4) Прототип (Prototype):
 Создает новые экземпляры объекта путем клонирования прототипа: Пример Модуль copy


 5) Одиночка (Singleton):
 У класса есть только один экземпляр : Пример Модуль — это файл с расширением .py

 С помощью паттерна одиночка могут быть реализованы многие паттерны (АБСТРАКТНАЯ ФАБРИКА, СТРОИТЕЛЬ, ПРОТОТИП).


Структурные паттерны:

 1) Адаптер (Adapter):
  позволяет объектам с несовместимыми интерфейсами работать вместе


 2) Мост (Bridge):
  Отделяет абстракцию от ее реализации так, чтобы и то и другое можно было изменять независимо.


 3) Компоновщик (Composite):
  упрощает создание иерархий объектов:  Python это не особенно нужно, так как для этой цели хватает словарей

  Описывает группу объектов, которая рассматривается как один экземпляр.


 4) Декоратор (Decorator):
 Динамически Добавляет дополнительную функциональность, не изменяя класс.


 5) Фасад (Facade):
 Фасад надстраивает простой интерфейс поверх сложного

  Фасад надстраивает простой интерфейс поверх сложного,
  Адаптер надстраивает унифицированный интерфейс над каким-то другим (необязательно сложным).

 Унифицировать - приведение к единообразию, к единому виду
 Унифицированный интерфейс упрощает поддержку версионности(способность иметь версии), что облегчает последующую эволюцию продукта.

 6) Приспособленец (Flyweight):
 для уменьшения затрат памяти при работе с большим количеством мелких объектов:  Пример - Менеджер памяти Python


 7) Заместитель (Proxy, Surrogate)
 позволяет создать объект-замену для другого объекта: Пример - Mock(макет-пустышка)


 Поведенческие Паттерны :

 1) Цепочка ответственности (Chain of responsobility):
  Цель Разрешить передачу запроса по цепочке получателей до тех пор, пока он(запрос) не будет обработан.


 2) Команда (Command):
  превращает запросы в объекты, позволяя передавать их как аргументы при вызове методов, Пример: Django HttpRequest (без метода выполнения)
  ставить запросы в очередь, логировать их, а также поддерживать отмену операций


 3) Интерпретатор (Interpreter):
 Для заданного языка определяет представление его грамматики,
 а также интерпретатор предложений этого языка.


 4) Итератор (Iterator):
 позволяет последовательно обойти элементы коллекции, не раскрывая внутренних деталей реализации.


 5) Посредник (Mediator):
 Инкапсулирует взаимодействие набора объектов которые ничего не знают друг о друге


 6) Хранитель (Memento):
 Предоставляет возможность восстановить объект в предыдущее состояние: Пример: модуль json, модуль pickle
 Позволяет вернуть предыдущее состояние объекта


 7) Наблюдатель (Observer):
 Определяет зависимость типа "один ко многим" между объектами таким образом,
 что при изменении состояния одного объекта все зависящие от него оповещаются об этом
 и автоматически обновляются

 # Пример: Django Signals(Сигналы), Flask Signals(Сигналы)

 8) Состояние (State):
 создания объектов, поведение которых изменяется при изменении состояния


 9) Стратегия (Strategy):
 Позволяет выбрать алгоритм во время выполнения.


 10) Шаблонный метод (Template method):
 --- CBV  -  Class-Based Views --- базовый класс - оставив реализацию некоторых шагов подклассам.  Django CBV


 11) Посетитель (Visitor):
 когда нужно применить функцию к каждому элементу коллекции: Пример - функция map()



 --- REGEXP ---

 x(?=y) находит x, только если за x следует y             # Positive Lookahead
 x(?!y) находит x, только если за x НЕ следует y          # Negative Lookahead
 (?<=y)x находит x, только если перед x следует y         # Positive Lookbehind
 (?<!y)x находит x, только если перед x НЕ следует y      # Negative Lookbehind

 Модуль re - это просто модуль расширения на языке C, включенный в Python   Шаблоны обрабатываются как строки   <-----


 Специальные символы:
 '.'              - любой символ кроме новой строки;  кроме переноса строки \n
 '^'              - началу строки;   Добавляем в НАЧАЛЕ строки
 '$'              - конец строки;    Добавляем в КОНЦЕ строки
 '*'              - Предыдущий символ 0 или более повторений;   сокращенная форма: {0,}    Жадный
 '+'              - Предыдущий символ 1 или более повторений;   сокращенная форма: {1,}    Жадный
 '?'              - Предыдущий символ 0 или 1 повторений;       сокращенная форма: {0,1}
 '*?', '+?','??'  - ограничение жадности;
 '*+', '++', '?+' - притяжательные квантификаторы, (новое в Python 3.11);
 '{m}'            - m повторений;                                             Нет смысла ставить '?'  Жадный/НЕ Жадный
 '{m,n}'          - как можно больше повторений в промежутке от m до n ;    Жадный
 '{m,n}?'         - как можно меньше повторений в промежутке от m до n;  НЕ Жадный/Ленивый
 '{m,n}+'         - притяжательная версия квантификатора выше, (новое в Python 3.11);
 '\'              - экранирование специальных символов;
 '[]'             - символьный класс;           Символьный класс [] - описывает только 1 символ
 '|'              - или  or;        если A совпало, то B не будет проверяться,  оператор ' | ' никогда не бывает жадным.
 '(...)'          - группа с захватом;


 Квантификаторы '*', '+' и '?' являются жадными. Они пытаются захватить как можно больше текста для анализа.
 Жадный      - означает что квантификатор находит самые 'ДЛИННЫЕ' последовательности     'ДЛИННЫЕ'       <-----
 НЕ Жадный ? - означает что квантификатор находит самые 'КОРОТКИЕ' последовательности    'КОРОТКИЕ'      <-----


 Расширения регулярных выражений:
 '(?aiLmsux)'                         - установка флагов регулярного выражения;
 '(?aiLmsux-imsx:...)'                - установка и удаление флагов;
 '(?>...)'                            - атомарная группа, (новое в Python 3.11);
 '(?:...)'                            - группа без захвата;
 '(?P<name>...)'                      - именованная группа;
 '(?P=name)'                          - обратная ссылка на именованную группу;
 '(?#...)'                            - комментарий;
 '(?=...)'                            - опережающая позитивная проверка;
 '(?!...)'                            - опережающая негативная проверка;
 '(?<=...)'                           - позитивная ретроспективная проверка;
 '(?<!...)'                           - негативная ретроспективная проверка;
 '(?(id/name)yes-pattern|no-pattern)' - стараться соответствовать yes-pattern;

  Вот полный список метасимволов: '.', '^', '$', '*', '+', '?', '{', '}', '[', ']', '\', '|', '(', ')'

 Для часто используемых символьных классов существуют краткие обозначения.                                     <-----
 Специальные последовательности:                              Эквивалентно символьному классу []:
 '\number' - соответствие группы с тем же номером;
 '\A'      - только с начало строки;
 '\b'      - пустая строка (начало или конец слова);                                   инверсия - это противоположность
 '\B'      - пустая строка (НЕ начало или НЕ конец слова);                                 '\B' инверсия '\b'
 '\d'      - любая десятичная цифра;                          [0-9]
 '\D'      - НЕ десятичная цифра;                             [^0-9]               [^\d]   '\D' инверсия '\d'
 '\s'      - пробельный символ;                               [ \t\n\r\f\v]
 '\S'      - НЕ пробельный символ;                            [^ \t\n\r\f\v]       [^\s]   '\S' инверсия '\s'
 '\w'      - буквенно-цифровые(str.isalnum()) и _             [A-Za-zА-Яа-я0-9_]
 '\W'      - НЕ буквенно-цифровые(str.isalnum()) и _          [^A-Za-zА-Яа-я0-9_]  [^\w]   '\W' инверсия '\w'
 '\Z'      - только с конец строки;


 fr'' rf'' - Нет разницы
 2 префикса сразу. fr-строка:  fr'test'       fr'' rf''  для escape в регулярках не работает квантификатор  {}
 2 префикса сразу. rf-строка:  rf'test'       fr'' rf''  для escape в регулярках не работает квантификатор  {}
 Решение!!!     - Используем двойные {{ }} и всё будет работать
 Можно использовать двойные фигурные скобки при f-string & r-raw, т.е. это будет как \w{,2}     re.sub(fr'{a}\w{{,2}}'

 Пример fr'' rf''  Используем двойные {{ }} и всё будет работать

 re - Могут принимать в качестве аргумента  str  или bytes                                  <----   ВАЖНО!!!

 # rb  Для поиска в b''  БАЙТОВЫХ СТРОКАХ                                                   <----   ВАЖНО!!!
 text = '111'
 text_rb = b'111'
 print(re.findall(r'1', text))      # -> ['1', '1', '1']
 print(re.findall(rb'1', text_rb))  # -> [b'1', b'1', b'1']         # rb''

 --- Монолитное приложение и Микросервисная архитектура — это два подхода к разработке программного обеспечения. ---

 Монолитное приложение - Это единое, целостное приложение, в котором все компоненты и функциональности интегрированы
 и развиваются вместе. Это единый код, который управляет всеми бизнес-функциями и интерфейсами.

 - **Преимущества**:
 - Простота разработки и тестирования.
 - Меньше накладных расходов на взаимодействие между компонентами.
 - **Недостатки**:
 - Сложность масштабирования: при увеличении нагрузки нужно копировать всю систему.
 - Трудности с обновлениями и поддержкой, изменения в одном месте могут влиять на все приложение.


 Микросервисы - Это архитектурный стиль, в котором приложение разбивается на мелкие, независимые сервисы, каждое из
 которых выполняет  свою задачу и может быть разработано и развернуто отдельно/самостоятельно .

 **Преимущества**:
 - Гибкость в использовании разных технологий для отдельных сервисов.
 - Упрощенное масштабирование: можно отдельно масштабировать только нужные сервисы.
 - Лучшая устойчивость: сбой в одном сервисе не приводит к сбоям в целом приложении.
 - **Недостатки**:
 - Увеличенная сложность в управлении и взаимодействии между сервисами.
 - Необходимость в более сложной инфраструктуре для развертывания и мониторинга.



 --- Django ---

 -- raw запросы --
 people = Person.objects.raw("SELECT id, name FROM hello_person")

 Пример raw запроса:

 class Person(models.Model):
     first_name = models.CharField(...)
     last_name = models.CharField(...)
     birth_date = models.DateField(...)
 You could then execute custom SQL like so:

 for p in Person.objects.raw("SELECT * FROM myapp_person"):
     print(p)

 # John Smith
 # Jane Jones

 # Сопоставление полей запроса с полями модели
 name_map = {"first": "first_name", "last": "last_name", "bd": "birth_date", "pk": "id"}
 Person.objects.raw("SELECT * FROM some_other_table", translations=name_map)



 Django использует метаклассы для создания моделей базы данных

 Django во многом работает через метаклассы.
 Поэтому когда Django конструирует ваш класс, она делает это с помощью своего метакласса.
 Чтобы при конструировании ей знать какие-то параметры вашего класса, ну, например модель или поля в вашем случае,
 она ищет в вашем классе класс с названием Meta.

 select_related(key) - 'ЖАДНАЯ' загрузка связанных данных по внешнему ключу key, который имеет тип ForeignKey, OneToOneField
 уменьшение количество запросов к базе данных

 'ЖАДНАЯ' загрузка - загрузка сразу всех данных
 'ЛЕНИВАЯ' загрузка - происходит в момент обращения к тому или иному атрибуту

 prefetch_related(key) - 'ЖАДНАЯ' загрузка связанных данных по внешнему ключу key, который имеет тип ManyToManyField
 уменьшение количество запросов к базе данных



 -- Транзакции/transaction в django --

 Поведение транзакции по умолчанию в Django

 По умолчанию Django работает в режиме автоматической фиксации. Каждый запрос немедленно фиксируется в базе данных,
  если транзакция не активна.

 Django автоматически использует транзакции или точки сохранения, чтобы гарантировать целостность операций ORM,
  которые требуют нескольких запросов, особенно запросов delete () и update () .

 TestCase Класс Django также включает каждый тест в транзакцию из соображений производительности.


 -- Привязка транзакций к HTTP-запросам --

 Распространенный способ обработки транзакций в Интернете - заключить каждый запрос в транзакцию. Установите
  ATOMIC_REQUESTS значение True в конфигурации каждой базы данных, для которой вы хотите включить это поведение.

 Это работает вот так. Перед вызовом функции просмотра Django запускает транзакцию. Если ответ получен без проблем,
 Django фиксирует транзакцию. Если представление вызывает исключение, Django откатывает транзакцию.

 Вы можете выполнять частичные транзакции, используя точки сохранения в коде представления, обычно с помощью atomic()
 диспетчера контекста. Однако в конце просмотра все изменения или ни одно из них не будут зафиксированы.

 Когда ATOMIC_REQUESTS он включен, по-прежнему можно запретить выполнение представлений в транзакции.

 non_atomic_requests(using=None)
 Этот декоратор нейтрализует эффект ATOMIC_REQUESTS для данного вида:

 from django.db import transaction

 @transaction.non_atomic_requests
 def my_view(request):
     do_stuff()

 @transaction.non_atomic_requests(using='other')
 def my_other_view(request):
     do_stuff_on_the_other_database()

 Он работает только в том случае, если он применяется к самому представлению.


 -- Явное управление транзакциями --

 atomic(using=None, savepoint=True, durable=False)

 Атомарность - определяющее свойство транзакций базы данных. atomic позволяет нам создать блок кода, в котором
 гарантируется атомарность базы данных. Если блок кода успешно завершен, изменения фиксируются в базе данных.
 Если есть исключение, изменения отменяются.

 atomic блоки могут быть вложенными. В этом случае, когда внутренний блок завершается успешно, его эффекты все еще могут
 быть отменены, если во внешнем блоке позже возникнет исключение.

 Иногда полезно убедиться, что atomic блок всегда является самым внешним atomic блоком, гарантируя, что любые изменения
 базы данных будут зафиксированы при выходе из блока без ошибок. Это называется долговечностью и достигается за счет
 схватывания durable=True. Если atomicблок вложен в другой, возникает ошибка RuntimeError.


 atomic можно использовать как декоратор :

 from django.db import transaction

 @transaction.atomic
 def viewfunc(request):
     # This code executes inside a transaction.
     do_stuff()

 и как менеджер контекста :

 from django.db import transaction

 def viewfunc(request):
     # This code executes in autocommit mode (Django's default).
     do_stuff()

     with transaction.atomic():
         # This code executes inside a transaction.
         do_more_stuff()

 Заключение atomic в блок try / except позволяет естественным образом обрабатывать ошибки целостности:

 from django.db import IntegrityError, transaction

 @transaction.atomic
 def viewfunc(request):
     create_parent()

     try:
         with transaction.atomic():
             generate_relationships()
     except IntegrityError:
         handle_exception()

     add_children()


 -- Автокоммит --

 Почему Django использует автокоммит?

 В стандартах SQL каждый запрос SQL запускает транзакцию, если она еще не активна. Затем такие транзакции должны быть
 явно зафиксированы или отменены.

 Это не всегда удобно для разработчиков приложений. Чтобы решить эту проблему, в большинстве баз данных предусмотрен
 режим автоматической фиксации. Когда автоматическая фиксация включена и транзакция не активна, каждый запрос SQL
 помещается в свою собственную транзакцию. Другими словами, каждый такой запрос не только запускает транзакцию,
 но транзакция также автоматически фиксируется или откатывается, в зависимости от того, был ли запрос успешным.

 PEP 249 , спецификация API базы данных Python v2.0, требует, чтобы функция автоматической фиксации была изначально
 отключена. Django отменяет это значение по умолчанию и включает автоматическую фиксацию.

 Чтобы этого избежать, вы можете отключить управление транзакциями , но это НЕ рекомендуется.    <-----

 Django предоставляет API в django.db.transaction модуле для управления состоянием автоматической фиксации каждого
 соединения с базой данных.

 get_autocommit(using=None)
 set_autocommit(autocommit, using=None)
 Эти функции принимают using аргумент, который должен быть именем базы данных. Если он не указан, Django использует
 "default" базу данных.

 Автокоммит изначально включен. Если вы выключите его, вы обязаны восстановить его.

 Вы должны убедиться, что ни одна транзакция не активна, обычно с помощью символа a commit() или a rollback(),
 прежде чем снова включить автоматическую фиксацию.

 Django откажется выключать автоматическую фиксацию, когда atomic() блок активен, потому что это нарушит атомарность.


 -- Отключение управления транзакциями --
 Вы можете управление транзакциями полностью отключить Джанго для данной базы данных, установив AUTOCOMMIT для False
 в его конфигурации. Если вы это сделаете, Django не включит автокоммит и не будет выполнять никаких коммитов.
 Вы получите обычное поведение базовой библиотеки базы данных.


 -- Выполнение действий после коммита --

 Иногда вам нужно выполнить действие, связанное с текущей транзакцией базы данных, но только в том случае, если транзакция
 успешно зафиксирована. Примеры могут включать задачу Celery , уведомление по электронной почте или аннулирование кеша.

 Django предоставляет on_commit() функцию для регистрации функций обратного вызова, которые должны выполняться после
 успешной фиксации транзакции:

 on_commit(func, using=None, robust=False)
 Передайте любую функцию (не принимающую аргументов) в on_commit():

 from django.db import transaction

 def do_something():
     pass  # send a mail, invalidate a cache, fire off a Celery task, etc.

 transaction.on_commit(do_something)
 Вы также можете заключить свою функцию в лямбду:

 transaction.on_commit(lambda: some_celery_task.delay('arg1'))

 Обратным вызовам не будут переданы никакие аргументы, но вы можете связать их с помощью functools.partial():

 from functools import partial

 for user in users:
     transaction.on_commit(partial(send_invite_email, user=user))


 -- Транзакции --
 Транзакция - это атомарный набор запросов к базе данных. Даже если ваша программа выйдет из строя, база данных
 гарантирует, что будут применены либо все изменения, либо ни одно из них.

 Django не предоставляет API для запуска транзакции. Ожидаемый способ начать транзакцию - отключить автоматическую
 фиксацию с помощью set_autocommit().

 После того, как вы участвуете в транзакции, вы можете либо применить изменения, которые вы выполняли до этого момента
 commit(), либо отменить их с помощью rollback(). Эти функции определены в django.db.transaction.

 commit(using=None)
 rollback(using=None)

 Эти функции принимают using аргумент, который должен быть именем базы данных. Если он не указан, Django использует
 "default" базу данных.

 Django откажется выполнять фиксацию или откат, когда atomic() блок активен, потому что это нарушит атомарность.


 -- Точки сохранения --

 Точка сохранения - это маркер внутри транзакции, который позволяет вам откатить часть транзакции, а не полную транзакцию.

 Точки сохранения управляются тремя функциями в django.db.transaction:

 savepoint(using=None)
 Создает новую точку сохранения. Это отмечает точку транзакции, которая, как известно, находится в «хорошем» состоянии.
 Возвращает идентификатор точки сохранения ( sid).

 savepoint_commit(sid, using=None)
 Освобождает точку сохранения sid. Изменения, внесенные с момента создания точки сохранения, становятся частью транзакции.

 savepoint_rollback(sid, using=None)
 Откатывает транзакцию до точки сохранения sid.

 Эти функции ничего не делают, если точки сохранения не поддерживаются или если база данных находится в режиме автоматической фиксации.


 Кроме того, есть служебная функция:

 clean_savepoints(using=None)
 Сбрасывает счетчик, используемый для генерации уникальных идентификаторов точек сохранения.

 -- END Транзакции/transaction в django --


    -- командная строка cmd   Terminal commands --
 dir - Отображение файлов и папок в текущем каталоге
 cd - Изменить каталог, если нажать Tab будет автодополнение
 cd .. - переход на 1 папку назад
 cd ..\.. - переход на 2 папки назад
 cd \ - Переход в корневую директорию
 cd . - Переход в текущую директорию (никаких изменений)
 mkdir - создание папки

 python --version           - Посмотреть версию Python
 python -V                  - Тоже самое сокращенная команда
 python3.12 -m venv venv    - Установка python и запуск локального окружения
 Ctrl + Z выйти из терминала или   exit()

 Активировать venv  (Виртуальное окружение)        .\venv\Scripts\activate       # venv\Scripts\activate   # Тоже самое
 ДЕ-Активировать venv  (Виртуальное окружение)     .\venv\Scripts\deactivate
 Чтобы деактивировать активное виртуальное окружение, просто введите команду deactivate в командной строке.   <-----

 source venv/bin/activate - Активировать venv  (Виртуальное окружение)  в Linux
 .\venv\Scripts\activate  - Активировать venv  (Виртуальное окружение)  в Windows

 $env:Path -split ';'     - разбивает строку переменной окружения Path на массив строк, используя символ ; в качестве разделителя.

 # Команда используется в PowerShell для добавления пути к установленному Python
 [Environment]::SetEnvironmentVariable("Path", $Env:Path + ";C:\Program Files\Python312", [EnvironmentVariableTarget]::User)

 cd папка\папка\папка   -  Быстрый переход за одну операцию
 cd "имя имя"  - если между именами пробел
 cd "Абсолютный путь" - Сразу перейдем туда куда нужно
 Название_Диска: - чтобы сменить диск  - Примеры C:  D:  Только название диска и двоеточие в терминале(командной строке)


 Команды для Unix/Linux/macOS:

 1. **`ls`**     - Показать список файлов и папок в текущем каталоге.
 2. **`cd`**     - Перейти в указанный каталог. Пример: `cd /path/to/directory`.
 3. **`pwd`**    - Показать текущий рабочий каталог.
 4. **`mkdir`**  - Создать новый каталог. Пример: `mkdir new_directory`.
 5. **`rm`**     - Удалить файл или каталог. Пример: `rm file.txt` (для удаления каталога используйте `rm -r directory_name`).
 6. **`cp`**     - Копировать файлы или каталоги. Пример: `cp source.txt destination.txt`.
 7. **`mv`**     - Переместить или переименовать файл или каталог. Пример: `mv old_name.txt new_name.txt`.
 8. **`touch`**  - Создать новый пустой файл или обновить дату и время существующего файла.
 9. **`cat`**    - Показать содержимое файла. Пример: `cat file.txt`.
 10. **`grep`**  - Поиск строк в файлах, содержащих заданный шаблон. Пример: `grep 'search_term' file.txt`.  GREP <-----

 grep(global regular expression print) - При этом с ее помощью можно не просто находить куски текста, но и с высокой
 эффективностью фильтровать вывод другой команды.

 11. **`chmod`** - Изменить права доступа к файлу или каталогу. Пример: `chmod +x script.sh`.
 12. **`man`**   - Показать руководство по использованию команды. Пример: `man ls`.


 ls -a           - Отображения списка файлов и директорий в текущем каталоге включая скрытые файлы
 Команда `ls -a` в Unix-подобных системах используется для отображения списка файлов и директорий в текущем каталоге,
 включая скрытые файлы, имя которых начинается с точки (`.`). Флаг `-a` означает "all" (все).


 **`uname`**       - Показывает информацию о системе. Этот вариант выводит полную информацию о системе,
 включая имя хоста, версию ядра и тип операционной системы.

 **`lsb_release`** - Показывает информацию о дистрибутиве (если доступно).

 **Bash** (Bourne Again SHell) — это командный интерпретатор и оболочка для UNIX и UNIX-подобных операционных систем,
 таких как Linux и macOS. Он является улучшенной версией оригинальной оболочки Bourne shell (sh) и включает множество
 дополнительных функций  Аналогом Bash в Windows является **PowerShell**

 bash
     uname -a        - Показывает информацию о системе.

 bash
     lsb_release -a  - Показывает информацию о дистрибутиве (если доступно).


 При вводе пароля в терминале Linux сочетание `Shift + Insert` вставляет содержимое буфера обмена. ИЛИ  ПКМ
 Однако в большинстве случаев символы не отображаются на экране.         Shift + Insert ИЛИ  ПКМ


 ### Команды для Windows:

 1. **`dir`**    - Показать список файлов и папок в текущем каталоге.
 2. **`cd`**     - Перейти в указанный каталог. Пример: `cd C:\path\to\directory`.
 3. **`mkdir`**  - Создать новый каталог. Пример: `mkdir new_directory`.
 4. **`del`**    - Удалить файл. Пример: `del file.txt`.
 5. **`copy`**   - Копировать файлы. Пример: `copy source.txt destination.txt`.
 6. **`move`**   - Переместить или переименовать файл. Пример: `move old_name.txt new_name.txt`.
 7. **`type`**   - Показать содержимое текстового файла. Пример: `type file.txt`.
 8. **`find`**   - Поиск строк в файлах. Пример: `find "search_term" file.txt`.
 9. **`attrib`** - Изменить атрибуты файла. Пример: `attrib +r file.txt` (для установки файла как только для чтения).
 10. **`help`**  - Показать доступные команды и их короткие описания.

 **`systeminfo`** - Показывает подробную информацию о системе.

 Аналогом Bash в Windows является **PowerShell**

 Чтобы открыть PowerShell в Windows, выполните следующие шаги:

 ### Способ 1: Поиск в меню

 1. Нажмите **Windows** на клавиатуре или щелкните на кнопку **Пуск**.
 2. Введите **PowerShell** в строке поиска.
 3. Выберите **Windows PowerShell** из результатов.

 ### Способ 2: Сочетание клавиш

 1. Нажмите **Win + X** на клавиатуре.
 2. Выберите **Windows PowerShell** или **Windows PowerShell (Администратор)** в открывшемся меню.

 ### Способ 3: Выполнение команды

 1. Нажмите **Win + R** для открытия окна "Выполнить".
 2. Введите `powershell` и нажмите **Enter**.

 Чтобы выйти обратно в командную строку вводим команду     exit


 Эти методы позволяют быстро открыть PowerShell для выполнения команд и сценариев.

 Эти команды являются базовыми, и существуют более продвинутые команды и параметры. Выбор команд зависит от конкретной
 задачи, которую вы хотите выполнить


 curl - `curl` — это командная строка и библиотека для передачи данных с использованием различных протоколов,
 таких как HTTP, HTTPS, FTP и многих других. Он позволяет отправлять запросы к серверам и получать ответы,
 что делает его полезным инструментом для разработчиков и системных администраторов.

 `curl` — мощный инструмент для работы с сетевыми запросами. Он поддерживает множество опций и позволяет
 взаимодействовать с API, загружать и отправлять данные, а также осуществлять диагностику сетевых проблем.

 ### Полезные опции

 - `-o` или `--output`: сохранять вывод в файл.
 - `-O` или `--remote-name`: сохранять файл с оригинальным именем.
 - `-I` или `--head`: запрашивать только заголовки.
 - `-H`: добавлять дополнительные заголовки.
 - `-d`: отправлять данные в теле запроса (обычно используется с POST).
 - `-X`: указывать метод запроса (GET, POST, PUT, DELETE и т.д.).
 - `-v` или `--verbose`: выводить подробную информацию о запросе и ответе.

 # Простой пример POST-запроса
 # По умолчанию curl использует метод POST, когда вы указываете данные с помощью флага -d.
 curl -H "Accept: application/json" -d '{"key":"value"}' https://api.example.com/data           # Тоже самое

  # Объяснение запроса
 -H "Content-Type: application/json" указывает, что вы отправляете данные в формате JSON.

 -d '{"key":"value"}' содержит данные, которые вы хотите отправить на сервер.

 # явно указываем метод POST с помощью флага -X POST.
 curl -X POST -H "Accept: application/json" -d '{"key":"value"}' https://api.example.com/data   # Тоже самое

 # GET-запрос   убрать флаг -X POST и флаг -d, так как GET-запросы не содержат тела запроса.
 curl -H "Accept: application/json" "https://api.example.com/data?key=value"


 - **cURL** лучше подходит для разработчиков, которые предпочитают работать в терминале и нуждаются в автоматизации.
 - **Postman** удобнее для визуального тестирования API и разработчиков, которые хотят более простой и наглядный интерфейс.

 Swagger - Использует CURL  <-----                                                                           <-----


 GRUB (Grand Unified Bootloader) — это загрузчик операционных систем, используемый в Linux и Unix-подобных системах.
 Он позволяет запускать разные операционные системы на одном компьютере, предоставляет меню для выбора ОС при загрузке
 и обеспечивает конфигурацию параметров загрузки.

 GRUB (также известный как GNU GRUB или GNU Grand Unified Bootloader) — загрузчик и менеджер загрузки для
 Linux и других ОС на базе Unix .


 папка migrations: предназначена для хранения миграций - скриптов,
 которые позволяют синхронизировать структуру базы данных с определением моделей


 # **Прямое выполнение миграций через код**
 Если вам нужно выполнить миграции программно, вы можете использовать следующий код в Python

 from django.core.management import call_command

 call_command('migrate', 'app_name')  #  Замените `'app_name'` на имя вашего приложения

 Имейте в виду, что `migrate` остается основным способом применения миграций и рекомендуется использовать его для
 адекватного управления базой данных.

 - Создайте каталог `management/commands` внутри приложения, если его еще нет.
 - Создайте файл, например, `run_migrations.py`:

 python
      # myapp/management/commands/run_migrations.py

      from django.core.management import BaseCommand, call_command

      class Command(BaseCommand):
          help = 'Выполнить миграции для приложения'

          def handle(self, *args, **kwargs):
              call_command('migrate', 'app_name')  # Замените 'app_name' на имя вашего приложения

  Запускайте команду в терминале:
  python manage.py run_migrations

 Не забывайте, что выполнять миграции программно — это менее распространенная практика, и делайте это осознанно,
 чтобы избежать непредвиденных проблем с базой данных.


 -- Создание миграции вручную --

 Чтобы создать миграции вручную через файл миграции `initial` в Django, выполните следующие шаги:

 1. **Создайте файл миграции вручную**:
 - В каталоге вашей модели приложения создайте директорию `migrations`, если она еще не существует.
 - Создайте файл миграции, назовите его, например, `0001_initial.py`.

 2. **Напишите код миграции**:
 - Откройте созданный файл и добавьте следующий код:


    from django.db import migrations, models

    class Migration(migrations.Migration):

        initial = True

        operations = [
            migrations.CreateModel(
                name='YourModelName',
                fields=[
                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                    ('field_name', models.CharField(max_length=100)),
                    # добавьте другие поля вашего модели
                ],
            ),
        ]

 - Замените `YourModelName` на название вашей модели и добавьте нужные поля.

 3. **Примените миграцию**:
 - Запустите команду, чтобы применить миграцию:

 bash
    python manage.py migrate

 Это создаст соответствующую таблицу в базе данных на основе вашей модели.

 ### Примечания:
 - Убедитесь, что вы правильно указали все поля и их типы в файле миграции.
 - Файл миграций должен иметь уникальный номер (например, `0001`, `0002` и т.д.) для правильного управления миграциями.


 MIDDLEWARE - это промежуточный слой между запросом и ответом в Django

 Django Debug Toolbar - Помогает оптимизировать проект

 ОСНОВНОЕ ОТЛИЧИЕ между Swagger и Postman заключается в том, что Swagger сконцентрирован на проектировании
 и документировании API, в то время как Postman — на тестировании и создании запросов.

 CSRF-токен помогает удостовериться, что запрос исходит от законного пользователя.
 CSRF-токен (Cross-Site Request Forgery) используется для защиты веб-приложений от атак

 CSRF токен — это случайное значение, генерируемое веб-приложением и связываемое с текущей сессией пользователя
 CSRF токен - нужен для защиты межсайтовых атак
 Django требует CSRF токен при всех POST - запросах

  -- HTTP --
 HTTP сообщения - это обмен данными между сервером и клиентом
 Http запрос, Http ответ между Клиентом и Сервером

 Полный цикл HTTP-запроса включает следующие шаги:

 1. Клиент (обычно браузер) формирует HTTP-запрос.
 2. Запрос отправляется на сервер через Интернет.
 3. Сервер принимает запрос и обрабатывает его, выполняя необходимые операции (например, извлечение данных из базы).
 4. Сервер формирует ответ в формате HTTP, включая статус (успешно, ошибка и т.д.) и, при необходимости, данные
   (например, HTML-страницу).
 5. Ответ отправляется обратно клиенту по тому же соединению.
 6. Клиент получает ответ и обрабатывает его (например, отображает страницу).

 Этот цикл обеспечивает взаимодействие между клиентом и сервером для передачи данных в веб-приложениях.

 СТАТУСЫ HTTP запросов:

 100-199: ИНФОРМАЦИОННЫЕ ответы.
 200-299: УСПЕШНАЯ обработка запроса.
 300-399: ПЕРЕНАПРАВЛЕНИЕ запроса.
 400-499: ОШИБКИ КЛИЕНТА.
 500-599: ОШИБКИ СЕРВЕРА.


  Пинг может влиять на скорость запроса   <-----


 -- Методы HTTP-запросов --
 GET – получить объект или список объектов
 HEAD – получить метаданные объекта.  Запрашивает ресурс так же, как и метод GET, но без тела ответа.
 POST – создать объект
 PUT – обновить существующий объект
 PATCH – частично обновить существующий объект
 DELETE – удалить объект
 TRACE - выполняет вызов возвращаемого тестового сообщения с ресурса
 OPTIONS - используется для описания параметров соединения с ресурсом
 CONNECT - устанавливает "туннель" к серверу, определённому по ресурсу


 ИДЕМПОТЕНТНЫЕ (GET, HEAD, PUT, DELETE, OPTIONS, TRACE) — при повторном выполнении результаты будут ожидаемо ОДИНАКОВЫМИ
 НЕИДЕМПОТЕНТНЫЕ (POST, PATCH)                          — при повторном выполнении результаты будут РАЗНЫМИ


 Таким образом, ИДЕМПОТЕНТНЫЕ запросы приводят к одному и тому же состоянию системы НЕЗАВИСИМО от количества вызовов,
 а НЕИДЕМПОТЕНТНЫЕ могут изменять состояние при каждом вызове.


 Фреймворк Django реализует АРХИТЕКТУРНЫЙ паттерн Model-View-Template или сокращенно MVT,
 который по факту является модификацией распростаненного в веб-программировании паттерна MVC (Model-View-Controller).

 Model View Template(MVT) (MVT – модель, представление и шаблон) - АРХИТЕКТУРНЫЙ ПАТТЕРН

 Основные элементы паттерна:
 URL dispatcher: при получение запроса на основании запрошенного адреса URL определяет, какой ресурс должен обрабатывать данный запрос

 View: получает запрос, обрабатывает его и отправляет в ответ пользователю некоторый ответ.
 Если для обработки запроса необходимо обращение к модели и базе данных, то View взаимодействует с ними.
 Для создания ответа может применять Template или шаблоны. В архитектуре MVC этому компоненту соответствуют контроллеры (но не представления).

 Model: описывает данные, используемые в приложении. Отдельные классы, как правило, соответствуют таблицам в базе данных.

 Template: представляет логику представления в виде сгенерированной разметки html.
 В MVC этому компоненту соответствует View, то есть представления.

 QuerySet, по сути, — список объектов заданной модели. QuerySet позволяет читать данные из базы данных,
 фильтровать и изменять их порядок


  Существует два основных типа сокетов:
 -- TCP-сокеты (Transmission Control Protocol) — обеспечивают надежное соединение между двумя устройствами. Они обычно
 используются в веб-серверах и клиентах, почтовых серверах и других приложениях, где требуется надежность и порядок доставки данных.

 -- UDP-сокеты (User Datagram Protocol) — обеспечивают передачу данных без установления соединения. Они обычно
 используются в видео и аудио потоковых приложениях, играх и других приложениях, где скорость важнее, чем надежность.


 Сокет - низкоуровневая обстракция отправки и получения данных по сети. Именно с помощью ее производится обмен данными
 между клиектами и серверами. Сокеты поддерживают две основные операции: отправку и получение байтов.


 Вот краткое описание ключевых интернет-архитектур, технологий, протоколов и форматов:

 1. **TCP/IP**: Протоколы, обеспечивающие передачу данных по сети. TCP (Transmission Control Protocol)
  отвечает за надежную доставку, а IP (Internet Protocol) — за маршрутизацию данных.

 2. **HTTP**: Протокол передачи данных, используемый для обмена информацией между веб-серверами и клиентами (браузерами).
  Он лежит в основе работы Всемирной паутины.

 3. **AJAX**: Технология для асинхронной загрузки данных на веб-страницах, позволяющая обновлять интерфейс без перезагрузки страницы.

 4. **HTML**: Язык разметки для создания и структурирования веб-страниц, который описывает содержимое и структуру документов.

 5. **CSS**: Язык стилей, используемый для оформления HTML-документов. Он управляет внешним видом элементов веб-страниц,
  включая цвета, шрифты и макеты.

 6. **XML**: Язык разметки, используемый для структурированного хранения и передачи данных с возможностью определения
  собственных тегов.

 7. **JSON**: Формат обмена данными, основанный на JavaScript, который является более легковесной альтернативой
  XML для передачи структурированных данных.

 8. **WSDL**: Язык описания веб-служб, который определяет интерфейс и методы, доступные в веб-службах, позволяя
  программам взаимодействовать друг с другом.

 9. **REST**: Архитектурный стиль для разработки веб-сервисов, использующий стандартные HTTP-методы
  (GET, POST, PUT, DELETE) для взаимодействия с ресурсами, которые представляются в виде URL.

 Эти технологии и протоколы создают основу для работы интернета и веб-приложений.


 В чем разница между РЕЛЯЦИОННЫМИ и НЕРЕЛЯЦИОННЫМИ базами данных?                        <------

 В Реляционныx БД - Данные Хранятся в виде СВЯЗНЫХ ТАБЛИЦ   Внешние ключи FOREIGN KEY    <------

 Реляционные и НЕреляционные базы данных – два способа хранения данных для приложений.
 Реляционная база данных (или база данных SQL) хранит данные в табличном формате со строками и столбцами.
 Столбцы содержат атрибуты данных, а строки – значения данных.

 НЕреляционные базы данных (или базы данных NoSQL) используют различные модели данных для доступа к данным и управления ими.


 Нормализация баз данных — это процесс организации данных в базах данных с целью уменьшения избыточности и зависимости.
 Основная цель нормализации — разделение данных на таблицы так, чтобы каждая таблица содержала информацию о конкретном
 объекте и обеспечивала целостность данных.

 Нормализация баз данных — это процесс организации данных в базе таким образом, чтобы избежать избыточности и снизить
 вероятность ошибок. Простыми словами, это упорядочивание информации так, чтобы каждая деталь хранилась только в одном месте.

 Когда база данных организована таким образом, что НЕ содержит повторяющейся информации, говорят, что она НОРМАЛИЗОВАНА.
 Процесс преобразования базы данных с дубликатами в базу данных без таковых называется НОРМАЛИЗАЦИЕЙ.   <-----

 Основные цели нормализации:
 - Устранение дублирующихся данных.
 - Обеспечение целостности данных.
 - Облегчение обновления и управления данными.

 В результате нормализации достигается более эффективное использование ресурсов и упрощается работа с данными.

 СУБД - Система Управления Базами Данных
 Реляционные базы данных - SQL       - Все данные хранятся в виде Таблиц
 Строго структурированные (Есть привязки к типу донных)

 НЕ Реляционные базы данных - NOSQL  - Данные могут хранится различным способом
 НЕТ Строгой Структуры (НЕТ привязки к типу донных)
  - ключ-значение
  - графовые
  - документно-ориентированные
  - колоночные

 Реляционные (SQL):      Oracle, PostgreSQL, MariaDB, MySQL, SQLite (Из коробки)
 НЕ Реляционные (NOSQL): MongoDB(документо-ориентированная), Elasticsearch(документо-ориентированная), Redis(ключ-значение)
                         ClickHouse(колоночные) -   поддерживает язык запросов на основе SQL немного модифицированный


  -- Базовые атаки --

 DoS/DDoS атаки - цель состоит в том, чтобы перегрузить сервер. Часто путем отправки большого количества запросов.

 SQL-инъекции — это один из самых распространенных методов атак на базы данных.
 SQL-инъекция (SQLi) - это уязвимость веб-безопасности, которая позволяет злоумышленнику вмешиваться в запросы,
 которое приложение делает к своей базе данных. Как правило, это позволяет просматривать данные,
 которые он обычно не может получить

 Способы избежать SQL-инъекция (SQLi):

 - Проверка пользовательского ввода
  Всегда проверяйте и фильтруйте пользовательский ввод, включая символы, которые могут быть потенциально опасными
 (например, кавычки, точки с запятой и т. д.).

 - Ограничение прав доступа
   НЕ давайте учетной записи, используемой веб-приложением, права на изменение структуры базы данных.

 - Логи и Мониторинг
   Ведите логи всех запросов к вашей базе данных и следите за необычной активностью, что позволит быстро обнаружить потенциальные атаки.

 - Использование ORM (Object-Relational Mapping)
   SQLAlchemy (Python), автоматически генерируют SQL-запросы и уменьшают риск инъекций.
   Предотвращение SQLi в Django

  Django ORM повсеместно использует параметризованные операторы, поэтому он очень устойчив к SQLi. Таким образом,
  если вы используете ORM для выполнения запросов к базе данных, вы можете быть уверены, что ваше приложение в безопасности.

 - Экранирование данных — это процесс обработки пользовательского ввода в таком виде, чтобы вредоносные символы НЕ
  интерпретировались как части SQL-команды. Это обеспечивает безопасность, предотвращая возможность SQL-инъекций.

  Пример:
  import sqlite3
  connection = sqlite3.connect('example.db')
  cursor = connection.cursor()
  input_data = "O'Reilly"  # Пользовательский ввод
  escaped_input = input_data.replace("'", "''")  # Экранирование
  query = f"SELECT * FROM authors WHERE name = '{escaped_input}'"

 - Подготовленные запросы SQL (prepared statements) — это механизм работы с базами данных, который позволяет заранее
  компилировать SQL-запросы с параметрами.
  В Python: Используйте библиотеки, такие как `sqlite3` или `SQLAlchemy`, которые поддерживают подготавливаемые запросы.

 Django был разработан таким образом, чтобы быть устойчивым к SQL-инъекциям (и другим распространенным веб-уязвимостям).
 Большинство распространенных вариантов использования Django будут автоматически защищены, поэтому уязвимости SQLi
 в реальных приложениях Django, к счастью, встречаются редко.

 Bento автоматически проверяет код Django на наличие шаблонов SQL-инъекций.   <-----


 Что такое ОРМ?
 Прежде всего, давайте заложим основу. Объектно-реляционные отображения (ORM) — это уровень абстракции,
 который позволяет разработчикам взаимодействовать с базами данных с помощью объектов Python вместо написания сырых SQL-запросов

  -- SQLAlchemy vs Django ORM --
 Django ORM обладает теми же свойствами: более-менее типичные запросы на ней можно сделать очень легко.
 А вот со сложными запросами Django ORM не справится.

 SQLAlchemy наоборот: позволяет создавать очень сложные запросы, но в среднем формирование запроса будет сложнее.


 необходимо Django ORM, если:
 Вы создаете новое веб-приложение с помощью Django.
 Вы отдаете предпочтение быстрой разработке и читаемости кода.
 У вас нет требований к базе данных.

 понадобится SQLAlchemy, если:
 Вам нужна максимальная гибкость для сложного взаимодействия с базами данных.
 Вы работаете со схемой базы данных.
 Вам требуется детальная оптимизация производительности.
 Вы используете веб-фреймворк, отличный от Django, или создаете отдельный проект.

 -- END SQLAlchemy vs Django ORM --

 ORM Django позволяет разработчикам взаимодействовать с базами данных с помощью кода Python

 ORM
 Таблица - Это Класс
 Записи в таблице - Это Экземпляры класса

 Фильтры запросов  Lookups
 # https://docs.djangoproject.com/en/5.0/ref/models/querysets/

 Movies.objects.filter(budget=1000)                    ==   фильтр на равенство поля
 Movies.objects.filter(budget__gt=1000)                 >   фильтр на поле больше значения (great then)
 Movies.objects.filter(budget__lt=1000)                 <   фильтр на поле меньше значения
 Movies.objects.filter(budget__gte=1000)               >=   фильтр на поле больше либо равно значения
 Movies.objects.filter(budget__lte=1000)               <=   фильтр на поле меньше либо равно значения
 Movies.objects.exclude(budget=1000)                   !=   фильтр на поле не равно значению
 Movies.objects.filter(year__isnull=True)                   фильтр на поле пустое (False - не пустое)
 Movies.objects.filter(year__isnull=True, name=’Avatar’)    фильтр на два поля
 Movies.objects.exclude(budget=1000).filter(name=’Avatar’)  фильтр на два поля
 Movies.objects.filter(name__contains=’Avatar’)             поле содержит значение, чувствителен к регистру
 Movies.objects.filter(name__icontains=’Avatar’)            поле содержит значение, НЕ чувствителен к регистру
 Movies.objects.filter(name__startswith=’a’)                поле начинается с “a”
 Movies.objects.filter(name__endswith=’a’)                  поле заканчивается на “a”
 Movies.objects.filter(id__in=[3,5,6])                     выбираются все значения из списка



 ORM (Object-Relation Mapping) – общее название для фреймворков или библиотек,
 позволяющих автоматически связать базу данных с кодом

 Запросы в Django ORM ЛЕНИВЫЕ и они не отправляются до тех пор,
 пока их не запустят (в англоязычных текстах это называют evaluate)

  В Django ORM запросы по умолчанию ЛЕНИВЫЕ. Это означает, что они не выполняются до тех пор, пока не потребуется
 получение данных. Чтобы применить запрос и получить данные, вы можете использовать следующие методы:

 **Конвертация в другие структуры данных**:
 **`list()`**: Преобразует `QuerySet` в список и выполняет запрос.   ** list() или tuple() или dict() или другие **:
 results = list(MyModel.objects.all())


 **Использование `next()` или `list()`**: Если вам нужно только одно значение, вы можете использовать `next()`:
 queryset = MyModel.objects.all()
 first_item = next(iter(queryset))  # Возвращает первый элемент, выполняя запрос


 **`len()`**: Получает количество объектов в `QuerySet` и выполняет запрос.
 count = len(MyModel.objects.all())


 **`for` циклы**: Итерирование по `QuerySet` также выполняет запрос.
 for obj in MyModel.objects.all():
        print(obj)


 **`get()`**: Получает единственный объект и выполняет запрос.
 obj = MyModel.objects.get(id=1)


 **Методы `get()` и `filter()`**:
- Вызов `get()` возвращает конкретный объект, тогда как `filter()` возвращает QuerySet, который будет выполнен позже.

 single_object = MyModel.objects.get(id=1)                  # Выполняет SQL-запрос
 filtered_objects = MyModel.objects.filter(name='example')  # Запрос выполняется при дальнейшей обработке


 **`first()` и `last()`**: Получает первый или последний объект и выполняет запрос.
 first_obj = MyModel.objects.first()
 last_obj = MyModel.objects.last()


 **`exists()`**: Проверяет наличие объектов и выполняет запрос.
 exists = MyModel.objects.filter(condition).exists()


 **`count()`**: Возвращает количество объектов в `QuerySet` и выполняет запрос.
 count = MyModel.objects.all().count()


 **`aggregate()` и `annotate()`**: Эти методы возвращают агрегированные данные и также выполняют запрос.
 from django.db.models import Count

 result = MyModel.objects.aggregate(Count('field_name'))

 # Подсчитываем общее количество объектов модели MyModel
 result = MyModel.objects.aggregate(total_count=Count('field_name'))

 print(result)  # Вывод: {'total_count': 42} (пример результата)


 from django.db.models import Count
 from myapp.models import MyModel, RelatedModel

 Получаем все объекты MyModel с подсчитанным количеством связанных объектов из RelatedModel
 result = MyModel.objects.annotate(related_count=Count('relatedmodel'))

 for obj in result:
     print(f'{obj.pk}: {obj.related_count}')  # Выводит идентификатор объекта и количество связанных объектов

 # Получаем общее количество объектов модели и аннотируем каждую запись с суммой значений определенного поля
 result_aggregate = MyModel.objects.aggregate(total_count=Count('id'), total_sum=Sum('field_name'))
 result_annotate = MyModel.objects.annotate(field_name_sum=Sum('field_name'))

 print(result_aggregate)  # Выводит агрегированные результаты, например: {'total_count': 42, 'total_sum': 1000}
 for obj in result_annotate:
     print(f'{obj.pk}: {obj.field_name_sum}')

 - `aggregate()` возвращает итоговые значения по всему набору данных,
 - `annotate()` добавляет агрегированные значения к каждому объекту в queryset.


 **`values()` и `values_list()`**: Эти методы возвращают список словарей или кортежей соответственно, выполняя запрос.
 queryset = MyModel.objects.values('id', 'name')           # Выполняет запрос и возвращает словари с указанными полями
 queryset = MyModel.objects.values_list('id', 'name')      # Возвращает список кортежей
 queryset = MyModel.objects.values_list('name', flat=True) # результаты в виде списков   flat=True


 **Срезы [ ]**: Использование срезов для получения определенного количества объектов.
 first_five = queryset[:5]  # Выполняет запрос и возвращает первые пять объектов

 Использование одного из этих методов заставит Django выполнить запрос к базе данных и вернуть результаты.

 CRUD - Create(создание), Read(чтение), Update(обновление), Delete(удаление)


 __Class__ = имя класса в модели
 __Class__.objects.all()
 __Class__.objects.create(title='')
 __Class__.objects.filter(title='Best')
 __Class__.objects.filter(pk__gte=2) # __gt, __lt, __lte, __contains   <--- Lookup
 __Class__.objects.get(id=1) # получить строго одну запись
 __Class__.objects.order_by("blog__name") # сортировка по выбранному полю
 __Class__.objects.order_by("-blog__name") # обратная сортировка
 __Class__.objects.order_by("blog__name").desc()) # обратная сортировка
 __Class__.objects.filter(pk__lte=4).update('') # update - применяется к QuerySet, не работает с одной записью
 __Class__.objects.filter(cat_id=1).count() # количество записей
 __Class__.objects.get(pk=3).posts.exists() # существуют записи или нет # posts = 'название модели'_set


  Можно использовать регулярные выражения

 # Очень полезные методы   __regex  - чувствительное к регистру       __iregex - НЕчувствительное к регистру
 # https://docs.djangoproject.com/en/5.0/ref/models/querysets/#regex
 Синтаксис регулярных выражений соответствует синтаксису используемой базы данных.

 # Должны быть выведены первые 3 строки в зависимости от установленного вами порядка.
 Model.objects.filter(adv_images__regex=r'^\d\.')[:3]
 Model.objects.filter(adv_images__regex=r'^\d\.')[:3]


 Entry.objects.get(title__regex=r"^(An?|The) +")

 # Регулярные выражения в SQL
 SQL equivalents:
 SELECT ... WHERE title REGEXP BINARY '^(An?|The) +'; -- MySQL

 SELECT ... WHERE REGEXP_LIKE(title, '^(An?|The) +', 'c'); -- Oracle

 SELECT ... WHERE title ~ '^(An?|The) +'; -- PostgreSQL

 SELECT ... WHERE title REGEXP '^(An?|The) +'; -- SQLite
 Рекомендуется использовать необработанные строки (например, r'foo'вместо 'foo') для передачи синтаксиса регулярных выражений.

 Примечание Еще зависит от БД

 Единственное условие: LIKE быстрее
 Несколько условий: REGEXP быстрее

 # Единственное условие
 SELECT * FROM comments WHERE text LIKE '%\\\\n%'; 🚀 Faster
 SELECT * FROM comments WHERE text REGEXP '\\\\n'; 🐢 Slower

 # Множественные условия
 SELECT * FROM comments
 WHERE text LIKE '%\\\\r\\\n%'
 OR text LIKE '%\\\\n%'
 OR text LIKE '%\\\\r%'; 🐢 Slower

 SELECT * FROM comments
 WHERE text REGEXP '((\\\\r\\\\n)|(\\\\(n|r)))'; 🚀 Faster

 Заключение:
 Используется LIKE для запросов с одним условием и REGEXP для запросов с несколькими условиями.   <----  ВАЖНО

 Фреймворк Djnago имеет три специальных класса для организации связей:

 ForeignKey – для связей Many to One (Один ко многим) (поля отношений)
 ManyToManyField – для связей Many to Many (Многие ко многим)
 OneToOneField – для связей One to One (Один к одному)


 --- Field types Типы полей ---
 classes:

 BooleanField  - Поле истина/ложь. Значение по умолчанию BooleanField равно None
 CharField     - Строковое поле, для строк малого и большого размера.
 TextField     - Большое текстовое поле.
 DateField     - Дата, представленная в Python экземпляром datetime.date
 DateTimeField - Дата и время, представленные в Python экземпляром datetime.datetime.
 TimeField     - Время, представленное в Python экземпляром datetime.time.
 EmailField    - Проверяет является ли значение действительным адресом электронной почты, используя EmailValidator
 FileField     - Поле для загрузки файла.
 ImageField    - Наследуется от FileField, но также проверяет, что загруженный объект является допустимым изображением.
 JSONField     - Поле для хранения данных в кодировке JSON.
 SlugField     - Slug - газетный термин. Слаг - это короткая метка для чего-либо, содержащая только буквы, цифры,
                 подчеркивания или дефисы. Они обычно используются в URL.
 URLField      - CharField для URL, проверяется валидатором URLValidator.
 IntegerField  - Целое число. Значения от -2147483648 до 2147483647  использует MinValueValidator и MaxValueValidator
 PositiveBigIntegerField, PositiveIntegerField, PositiveSmallIntegerField, SmallIntegerField - Классы чисел


 -- Поля моделей в Django являются ДЕСКРИПТОРАМИ --

 Поля моделей в Django являются дескрипторами. Они определяют, как данные хранятся и обрабатываются в базе данных. <----

 from django.db import models

 class Book(models.Model):
     title = models.CharField(max_length=100)    # Дескриптор для поля строкового типа      <----
     published_date = models.DateField()         # Дескриптор для поля даты                 <----


  --- ПРОБЛЕМА N+1 запроса ---

 ПРОБЛЕМА N+1 запроса — это НЕ эффективный способ обращения к базе данных, когда приложение генерирует запрос на каждый вызов объекта.
 Эта проблема обычно возникает, когда мы получаем список данных из базы данных без использования ленивой или
 жадной загрузки (lazy load, eager load)

 ПРОБЛЕМА N+1 запросов, когда каждая возвращенная строка из сотен строк вызывает дополнительный запрос.
 Проблема N + 1 возникает, когда фреймворк доступа к данным выполняет N дополнительных SQL‑запросов для получения
 тех же данных, которые можно получить при выполнении одного SQL‑запроса.

 Чтобы РЕШИТЬ ПРОБЛЕМА N+1 запросов:  select_related, prefetch_related, annotate и подзапросы   <-----

 select_related(key) - 'ЖАДНАЯ' загрузка связанных данных по внешнему ключу key, который имеет тип ForeignKey, OneToOneField
 уменьшение количество запросов к базе данных

 prefetch_related(key) - 'ЖАДНАЯ' загрузка связанных данных по внешнему ключу key, который имеет тип ManyToManyField
 уменьшение количество запросов к базе данных

 Запросы N+1 происходят из-за способа, которым ORM Django обрабатывает связанные объекты по умолчанию.



 -- Агрегация и агрегирующие функции --

 Для того чтобы получить уже агрегированные данные, нужно воспользоваться методом .aggregate(), вызвав его у имеющегося
 менеджера или QuerySet. Этот метод принимает в качестве параметров так называемые агрегирующие функции. Функций этих
 достаточно много, но все они используются примерно одинаково, поэтому рассмотрим для примера функцию Avg:

 from django.db.models import Avg

 # Получение средней цены среди всех книг магазина
 Book.objects.aggregate(Avg('price'))
 # {'price__avg': 34.35}

 # Можно задать имя ключа результирующего словаря явно
 Book.objects.aggregate(average_price=Avg('price'))
 # {'average_price': 34.35}

 Если аргументы указываются как позиционные, то имена для ключей генерирует Django ORM на основе имени поля и имени
 агрегирующей функции. Аргументов можно указать сразу несколько и генерируемые имена не дадут запутаться:

 from django.db.models import Avg, Max, Min

 Book.objects.aggregate(Avg('price'), Max('price'), Min('price'))
 # {'price__avg': 34.35, 'price__max': Decimal('81.20'), 'price__min': Decimal('12.99')}

 Как можно заметить, каждый запрос на агрегацию возвращает не сами книги, а только итоговый результат. Таким образом со
 стороны Python никаких промежуточных объектов создавать не приходится!

 Вернёмся к учебному проекту, который моделирует платформу для ведения блогов. Никаких "цен" в этом проекте нет,
 но задачи для анализа найдутся. Предположим, что нужно для каждой записи в блоге некоторого автора узнать количество
 комментариев. Агрегация на первый взгляд не подходит: сами посты тоже нужны. Можно решить задачу "в лоб", написав:

 author = User.objects.get(id=1)
 posts = [(p, p.postcomment_set.count()) for p in author.post_set.all()]

 Такое решение имеет своё собственное название – "N+1 запросов" – поскольку будет выполнен один запрос N постов,
 а затем N запросов комментариев к каждому. Легко представить, насколько это неэффективно.

 Для того чтобы для каждой возвращаемой сущности вычислить некоторое значение в рамках одного запроса, Django ORM
 предоставляет механизм аннотирования.


 Процесс, при котором к каждому объекту из выборки применяется агрегирующая функция, назвается аннотированием.


  -- Аннотирование и дубликаты в выдаче --

 Например, .aggregate(Count('postcomment')) подсчитает количество всех комментариев, а .annotate(Count('postcomment'))
 даст количество комментариев к каждому посту. Так выглядит подсчёт количества тегов, которыми помечен каждый пост:

 posts = Post.objects.annotate(Count('tags'))
 posts[0].tags__count  # только тут финализируется запрос!
 # SELECT "blog_post"."id",
 #        ...
 #        COUNT("blog_post_tags"."tag_id") AS "tags__count"
 #   FROM "blog_post"
 #   LEFT OUTER JOIN "blog_post_tags"
 #     ON ("blog_post"."id" = "blog_post_tags"."post_id")
 #  GROUP BY ...
 #  LIMIT 1

 # Execution time: 0.000563s [Database: default]

 # => 2

 Здесь новый атрибут получил имя "tags__count", но имя можно было указать вручную, как и в случае обычной агрегации.


 Если вы уже имеете некоторый опыт в SQL, вы можете задаться вопросом: а не добавляет ли OUTER JOIN, который можно
 заметить в примере выше, в выборку дублирующиеся элементы, если присовокупляемые сущности соотносятся с текущей как
 "многие к одному"? Добавляет! Более того, агрегация в таких случаях даёт неверные результаты, так как учитывает и
 повторяющиеся строки. И тем больше дублей вы увидите, чем больше разных связей "многие к одному" задействуете
 (и даже одну и ту же, но несколько раз).

 Увы, в общем виде эту проблему не решить. Но конкретно агрегирующая функция Count имеет опцию distinct=True,
 которая убирает дублирование, пока вы используете только этот вид аннотаций и каждый Count используете с distinct=True.


 -- Агрегация аннотированных значений --
 Аннотирование позволяет добавить вычислимые данные к каждому элементу запроса, а это значит, что можно выполнить
 итоговую агрегацию с использованием этих значений! Получение среднего количества тегов среди всех постов будет выглядеть так:

 Post.objects.annotate(Count('tags')).aggregate(Avg('tags__count'))
 # SELECT AVG("tags__count")
 #   FROM (
 #         SELECT COUNT("blog_post_tags"."tag_id") AS "tags__count"
 #           FROM "blog_post"
 #           LEFT OUTER JOIN "blog_post_tags"
 #             ON ("blog_post"."id" = "blog_post_tags"."post_id")
 #          GROUP BY "blog_post"."id"
 #        ) subquery

 # Execution time: 0.000361s [Database: default]

 # => {'tags__count__avg': 1.5}


 Функция annotate() в Django — это мощный инструмент, который позволяет разработчикам добавлять дополнительные данные
 в свои наборы запросов, не прибегая к использованию чистого SQL

 annotate() - метод позволяет создавать новые вычисляемые поля для нашей выборки
 lst = Husband.objects.all().annotate(is_married=Value(True))

 annotate() - Позволяет Создавать дополнительное поле в Запросе к примеру в SQL. Новые поля на основе Имеющихся.
 Нужно использовать класс F            Пример выше.    annotate() - Позволяет Создавать Новые дополнительные вычисляемые поля

 Для того чтобы получить уже агрегированные данные, нужно воспользоваться методом .aggregate()
 Агрегирующие функции: Count, Sum, Avg, Max, Min. Метод values()
 from django.db.models import Count, Sum, Avg, Max, Min

# Задача на ORM
 class City(models.Model):
    name = models.CharField()


 class Person(models.Model):
     name = models.CharField()
     city = models.ForeignKey(City)

 Вывести список людей и городов где они живут?
 Вывести всех людей, живущих в городе N
 Вывести 5 городов с наибольшим населением, упорядочив по убыванию.

 # Ответ:
 Вот пример определения моделей с учетом этих деталей:
 from django.db import models

 class City(models.Model):
     name = models.CharField(max_length=255)

     def __str__(self):
         return self.name


 class Person(models.Model):
     name = models.CharField(max_length=255)
     city = models.ForeignKey(City, on_delete=models.CASCADE, related_name='people')

     def __str__(self):
         return self.name

 # Теперь перейдем к выполнению задач:
 1. Вывести список людей и городов, где они живут:

 people_with_cities = Person.objects.select_related('city').values('name', 'city__name')
 for person in people_with_cities:
     print(f'Человек: {person["name"]}, Город: {person["city__name"]}')


 2. Вывести всех людей, живущих в городе N:
 city_name = 'N'  # укажите название города
 people_in_city_n = Person.objects.filter(city__name=city_name)

 for person in people_in_city_n:
     print(f'Человек: {person.name}')

 3. Вывести 5 городов с наибольшим населением, упорядочив по убыванию.
 Для этого нам нужно будет добавить поле для хранения количества людей в каждом городе. Однако, чтобы подсчитать
 это количество динамически, мы можем использовать аннотирование с `Count`.

 from django.db.models import Count

 top_cities = City.objects.annotate(population=Count('people')).order_by('-population')[:5]

 for city in top_cities:
     print(f'Город: {city.name}, Население: {city.population}')



 --- CBV  -  Class-Based Views ---
 CBV могут наследоваться от множества классов и миксинов,
 но все CBV берут начало от класса View.


 from django.views.generic.edit import UpdateView # можно импортировать так
 from django.views.generic import UpdateView      # можно импортировать так тоже

 View         - Базовый класс представлений
 TemplateView - он служит для обработки шаблонов и отправки результата пользователю
 RedirectView - Осуществляет перенаправление на любой GET запрос.


 ListView   - Страница, представляющая список объектов
 DetailView - Базовое представление для отображения одного объекта.

 Редактирование просмотров:
 FormView   - Базовое представление для отображения формы
 CreateView - Служит для добавления новых записей в БД через форму
 UpdateView - Служит для изменения существующих записей в БД через форму
 DeleteView - Служит для удаления существующих записей в БД через форму



 LoginView                 - класс представления для авторизации пользователей
 LogoutView                - класс представления для выхода пользователя из системы
 AuthenticationForm        - класс формы обработки аутентификации пользователя
 LoginRequiredMixin        - Миксин для проверки аутентификации пользователя, требует, чтобы пользователь был авторизован.
 UserCreationForm          - Класс для регистрации пользователей
 PasswordChangeView        - для обработки формы изменения пароля
 PasswordChangeDoneView    - для отображения результата успешного изменения
 PasswordResetView         - Позволяет пользователю сбросить свой пароль,
 PasswordResetDoneView     - после того, как пользователю была отправлена по электронной почте ссылка для сброса пароля
 PasswordResetConfirmView  - Представлена форма для ввода нового пароля.
 PasswordResetCompleteView - Позволяет пользователю сбросить свой пароль,
 PasswordResetConfirmView  - Представляет представление, которое информирует пользователя об успешном изменении пароля
 PermissionRequiredMixin   - как и permission_required декоратор, проверяет, имеет ли пользователь, обращающийся к
                             представлению, все заданные разрешения.


 Paginator                 - Он выполняет всю тяжелую работу по фактическому разбиению объекта QuerySet на Page объекты



 CRM-система – это сервис для автоматизации бизнес-процессов

  Что такое сигналы? Зачем нужны? Назовите основные
 Сигналы – это события в экосистеме Джанго. С помощью сигналов подсистемы оповещают приложение о том, что случилось.
 Чтобы читать сигналы, программист регистрирет обработчики сигналов. Сигналы распространяются синхронно. Это значит,
 подписав на один сигнал сотню обработчиков, мы увеличим время, необходимое на отдачу ответа.

 Основные сигналы это начало запроса и его окончание, перед сохранением модели и после, обращение к базе данных. <-----

 Django Signals - Чтобы принять сигнал, зарегистрируйте функцию приемник с помощью метода Signal.connect().
 Функция-приемник вызывается при отправке сигнала. Все функции-приемники сигнала вызываются по очереди,
 в том порядке, в котором они были зарегистрированы
 Сигналы распространяются синхронно.

 В Django built-in signals позволяет пользовательскому коду получать уведомления об определенных действиях.

 В Django сигналы позволяют вам связывать определенные действия с событиями, происходящими в модели, например,
 сохранением или удалением объектов. Это полезно для выполнения дополнительных задач, таких как отправка уведомлений
 или обновление связанных данных.

 ### Как использовать сигналы:
 1. **Импортируйте необходимые модули:** Вам нужно импортировать `signals` и `receiver` из `django.db.models.signals`.

 2. **Создайте обработчик сигнала:** Напишите функцию, которая будет выполнена при срабатывании сигнала.

 3. **Подпишитесь на сигнал:** Используйте декоратор `@receiver` для связывания сигнала с обработчиком.

 ### Пример

 Допустим, у вас есть модель `MyModel`, и вы хотите выполнить действие при ее сохранении.

 from django.db import models
 from django.db.models.signals import post_save
 from django.dispatch import receiver

 class MyModel(models.Model):
     name = models.CharField(max_length=100)

 @receiver(post_save, sender=MyModel)
 def mymodel_saved(sender, instance, created, **kwargs):
     if created:
         print(f'Объект {instance.name} был создан!')
     else:
         print(f'Объект {instance.name} был обновлён!')

 # Пример использования:
 # Создание нового объекта:
 new_object = MyModel.objects.create(name='Пример')

 # Обновление существующего объекта:
 new_object.name = 'Обновлённый пример'
 new_object.save()

 - **Сигналы** позволяют автоматически выполнять действия при возникновении событий в моделях.
 - **Пример** показывает, как обрабатывать сигналы `post_save` для создания и обновления объектов.



 Django-allauth — Библиотека авторизации:
 помогает реализовать функции регистрации, авторизации и управления учётными записями

 ipython  - для удобства в терминале
 django-extensions - набор инструментов, которые помогут вам в вашей повседневной работе. manage.py shell_plus --print-sql

 django-ckeditor - Для редактирования в Админке

 pillow - Для работы с изображениями

 Crispi-forms - для работы с формами. Позволят вам управлять поведением рендеринга ваших форм

 WebSocket — это протокол связи, который обеспечивает ПОСТОЯННОЕ соединение между клиентом и сервером для обмена данными
 в реальном времени. Он позволяет отправлять и получать сообщения в обоих направлениях с низкой задержкой, что удобно
 для приложений, требующих мгновенной передачи данных, таких как чаты, онлайн-игры и финансовые приложения.

 WebSocket работает по следующему принципу:

 1. Установление соединения: Клиент (например, веб-браузер) инициирует подключение к серверу WebSocket путем отправки
  HTTP-запроса с заголовком Upgrade. Это запрос сообщает серверу, что клиент хочет переключиться на протокол WebSocket.

 2. Подтверждение соединения: Сервер, приняв запрос, отвечает с кодом 101 (Switching Protocols), что означает успешное
  переключение на протокол WebSocket. После этого устанавливается постоянное соединение.

 3. Двусторонняя связь: В отличие от традиционных HTTP-запросов, где связь осуществляется только от клиента к серверу,
  WebSocket позволяет обмениваться данными в обоих направлениях одновременно. Это означает, что сервер может отправлять
  сообщения клиенту без предварительного запроса.

 4. Закрытие соединения: Когда соединение больше не требуется, оно может быть закрыто одной из сторон
  (клиентом или сервером), что освобождает ресурсы.

 Этот процесс обеспечивает эффективный обмен данными в реальном времени с минимальной задержкой.


  **Django** не поддерживает WebSocket "из коробки", но есть способы реализации этого протокола с помощью дополнительной
 библиотеки **Django Channels**.

 Создать файл  consumers.py

 Пример:
 from channels.generic.websocket import AsyncWebsocketConsumer
 import json

 class MyConsumer(AsyncWebsocketConsumer):
     async def connect(self):
         await self.accept()

     async def disconnect(self, close_code):
         pass

     async def receive(self, text_data):
         data = json.loads(text_data)
         await self.send(text_data=json.dumps({
             'message': data['message']
         }))



 --- Celery ---
 Celery - это асинхронная очередь задач, используемая для распределенной обработки сообщений.
 Он позволяет выполнять задачи в фоновом режиме, не загружая основной поток выполнения

 Celery — это программа, которая отслеживает задачи (tasks), которые необходимо выполнить,
 и в которой есть набор обработчиков (workers), которые будут выполнять эти задачи.
 Основной смысл в том, что она (программа) может выполнять несколько задач параллельно
 и что она не блокирует поставщиков (producers) этих самых задач.

 Celery на самом деле не хранит все эти задачи в памяти, брокер сообщений хранит задачи.

 Чтобы получать задачи от вашей программы и отправлять результаты в серверную часть, Celery требуется брокер сообщений
 для связи. Redis и RabbitMQ — это два брокера сообщений, которые разработчики часто используют вместе с Celery.

 Брокер сообщений - это архитектурный паттерн в распределённых системах, где элементы системы общаются через посредника
 Брокер упрощает работу веб-сервисов, отвечая за пересылку сообщений и все связанные задачи.


 Есть два основных алгоритма передачи сообщений в очередь:
 1) Есть Producer, который направляет сообщения в брокер, и есть Consumer, который их получает.
 2) Есть Publisher, который закидывает сообщения в очередь, и есть n-ое количество Subscriber'ов, которые их обрабатывают в дальнейшем.


 Изначально Celery НЕ предназначен для работы с Kafka, поэтому вам может понадобиться настроить много дополнительных
 вещей, чтобы все работало гладко.

 Некоторые функции Celery, такие как механизм приоритетов или задержки, могут НЕ работать или требовать дополнительных настроек в Kafka.

 -- Брокеры Сообщений --

 -- Особенности Rabbit MQ --  Удаляет сообщение после доставки его получателю.

 Принцип «Умный брокер, тупой потребитель» по отношению к RabbitMQ означает, что брокер берёт на себя много
 дополнительных действий. Например, следит за прочитанными сообщениями и удаляет их из очереди. Или сам организует
 процесс распределения сообщений между подписчиками.

 Для Kafka принцип «Тупой брокер, умный потребитель» означает, что, в отличие от RabbitMQ, он не занимается контролем и
 распределением сообщений. Потребители сами опрашивают брокер и решают, какие сообщения им читать, брокер только хранит данные.

  Apache Kafka:
 - когда нужно обработать большой объем данных, которые очень быстро генерируются   нет большого потока данных
 - при реализации транзакционных или конвейерных систем
 - при построении событийно-ориентированной архитектуры
 - идеально подходит в тех случаях, где требуется персистентность.

 RabbitMQ:
 - когда нужно обработать большой объем данных, которые очень быстро генерируются	нет большого потока данных
 - важна гибкость маршрутизации сообщений внутри системы
 - важен факт доставки сообщений

 Redis:
 - необходима обработка больших объемов данных
 - не требуется персистентность
 - необходима высокая скорость доставки сообщений


 Персистентность в программировании означает способность состояния существовать дольше, чем процесс, создавший его.
 персистентность - возможность долговременного хранения состояния
 Персистентные структуры данных (англ. persistent data structures) — это структуры данных, которые при внесении в них
 изменений сохраняют доступ ко всем своим предыдущим состояниям.

 Уровни персистентности:
 - Частичная — к каждой версии можно делать запросы, но изменять можно только последнюю.
 - Полная — можно делать запросы к любой версии и менять любую версию.
 - Конфлюэнтная — помимо этого можно объединять две структуры данных в одну (например, сливать вместе кучи или деревья поиска).
 - Функциональная — структуру можно реализовать на чистом функциональном языке: для любой переменной значение может
   быть присвоено только один раз и изменять значения переменных нельзя.


 --- Тестирование unittest в Django ---

 В Django тесты определяются в специальном модуле tests.py внутри приложения.
 В файлике test пишем тесты

 # Пример
 from django.test import TestCase

 class TestHoroscope(TestCase):

    # Тест на статут кода
    def test_index(self):
        response = self.client.get('/horoscope/')  # client - Замена Браузера
        self.assertEqual(response.status_code, 200)

    # Тест на статут кода и перенаправление
    def test_libra_redirect(self):
        response = self.client.get('/7')
        self.assertEqual(response.status_code, 302)
        self.assertEqual(response.url, '/horoscope/libra/')

 # Затем в терминале пишем команду
 python manage.py test название_приложения   Или через   Edit Configurations Django tests   target == Приложение или все

 Типы проверок в классе TestCase
 assertEqual(a, b)          a == b
 assertNotEqual(a, b)       a != b
 assertTrue(x)              bool(x) is True
 assertFalse(x)             bool(x) is False
 assertIs(a, b)             a is b
 assertIsNot(a, b)          a is not b
 assertIsNone(x)            x is None
 assertIsNotNone(x)         x is not None
 assertIn(a, b)             a in b
 assertNotIn(a, b)          a not in b
 assertIsInstance(a, b)     isinstance(a, b)
 assertNotIsInstance(a, b)  not isinstance(a, b)


 Django предназначен для создания полнофункциональных веб-приложений,
 в то время как DRF специализируется на создании RESTful API.


 -- Что такое REST --
 в веб-программировании — это архитектурный стиль, который используется для создания веб-сервисов.

 **REST (Representational State Transfer)**:
 - Это архитектурный стиль для проектирования сетевых приложений. REST использует стандартные HTTP-методы
  (GET, POST, PUT, DELETE и т.д.) для взаимодействия между клиентом и сервером.
 - RESTful API (интерфейсы программирования приложений) соответствуют принципам REST и обычно обмениваются данными в
  формате JSON или XML.
 - Основные принципы REST:
 - **Статус представление**: Клиент и сервер взаимодействуют путем обмена состояниями ресурса (например, данные о пользователе).
 - **Безсостояние**: Каждый запрос от клиента к серверу должен содержать всю информацию, необходимую для его обработки,
   и сервер не хранит состояние клиента.
 - **Кешируемость**: Ответы должны указывать, могут ли они быть закешированы для повышения производительности.
 - **Уровень абстракции**: Каждый ресурс в REST может быть доступен по уникальному URI (Uniform Resource Identifier).


 -- Что в rest значить Path и Query? --
 В REST (Representational State Transfer) API, `path` и `query` являются частями URL, которые используются для
 определения ресурсов и параметров запроса. Давайте подробнее рассмотрим каждую из них.

 1) Path (путь)
 `path` — это то, что идет после доменного имени и определяет конкретный ресурс или коллекцию ресурсов. Обычно он
 используется для идентификации ресурса, с которым вы взаимодействуете.

 Пример URL:
 https://api.example.com/users/123
 В этом URL `users/123` — это Path (путь). Он обозначает, что вы хотите получить информацию о пользователе с ID 123.

 2) Query (параметры запроса)
 query` — это часть URL, которая начинается со знака вопроса `?` и используется для передачи дополнительных параметров,
 которые могут уточнить или изменить запрос. Параметры в запросе могут быть ключами и значениями, разделенными знаком `&`.

 Пример URL:
 https://api.example.com/users?age=25&active=true

 В этом URL `age=25&active=true` — это параметры запроса. Они могут использоваться для фильтрации или настройки данных,
 которые вы хотите получить. В данном случае, это запрос списка пользователей, которые активны и имеют возраст 25 лет.

 ### Примеры использования

 **Path без query**:
 Это запрос на получение информации о продукте с ID 456.
 GET https://api.example.com/products/456


 **Path с query**:
 Это запрос на получение списка продуктов в категории "электроника", отсортированных по цене.
 GET https://api.example.com/products?category=electronics&sort=price

 Таким образом, `path` используется для навигации к определенному ресурсу, а `query` для передачи параметров, которые
 могут изменить или уточнить запрос.



--- DRF  Django REST Framework ---
 Фреймворк, работающий со стандартными моделями Django для создания гибкого и мощного API для проекта.

 Установка  pip install djangorestframework

 DRF облегчает взаимодействие между сервером и клиентами, позволяя передавать данные в формате JSON
 и выполнять различные операции, такие как чтение, запись и удаление данных. Он также обеспечивает безопасность,
 авторизацию и управление правами доступа.

 Архитектура DRF:

 Сериализатор (Serializer): преобразует информацию, хранящуюся в базе данных и определенную с помощью моделей Django,
 в формат JSON, который легко и эффективно передаётся через API.

 Представление (View, ViewSet): определяет функции (чтение, создание, обновление, удаление), которые будут доступны через API.

 Маршрутизатор (Router): определяет URL-адреса, которые будут предоставлять доступ к каждому представлению.


 Как работает Serializer в Django REST Framework?
 Serializer преобразует информацию, хранящуюся в базе данных и определенную с помощью моделей Django, в формат,
  который легко и эффективно передается через API - JSON.

 Наиболее распространенной формой, которую принимает сериализатор DRF, является тот, который привязан непосредственно к модели Django:

 class ThingSerializer(serializers.ModelSerializer):
     class Meta:
         model = Thing
         fields = (‘name’, )

 Если взять за пример Serializer, то можно посмотреть на код джанги:

@six.add_metaclass(SerializerMetaclass)
class Serializer(BaseSerializer):
  ...
SerializerMetaclass - это тот самый метакласс, который конструирует класс ModelForm.


 За что отвечает Meta в сериализаторе?
 В классе Meta сериализатора можно задать модель по которой будет создан сериализатор, поля, которые будут включены
 (или exclude для исключения), list_serializer_class, например для того чтобы задать специфическую валидацию списков и тд.


 Throttling в Django REST Framework (DRF) — это механизм, который ограничивает количество запросов, которые клиент
 может сделать к API в заданный период времени. Это помогает предотвратить злоупотребления, защитить сервер от
 перегрузок и обеспечить справедливое распределение ресурсов между пользователями.

 В DRF есть несколько встроенных классов для реализации throttling, таких как:

 1. **AnonRateThrottle** — ограничивает анонимных пользователей.
 2. **UserRateThrottle** — ограничивает зарегистрированных пользователей.
 3. **ScopedRateThrottle** — позволяет устанавливать разные лимиты для различных частей API.

 Настройки throttling можно задать в файле конфигурации Django или в представлениях (views). Это делает его гибким и
 настраиваемым под нужды вашего приложения.


 Какая разница между аутентификацией и авторизацией?
 - Сначала определяют имя (логин или номер) – идентификация
 - Затем проверяют пароль (ключ или отпечаток пальца) – аутентификация
 - И в конце предоставляют доступ – авторизации

 -- nginx --
 Nginx – это веб сервер. Он хранит файлы сайта и направляет их по запросу на компьютер или мобильное устройство.
 То есть он нужен для быстрого отображения интернет-страничек.
 Его основная задача заключается в обработке статичного контента.
 Сервер Nginx выполняет две функции:
 - Принимает, обрабатывает и отправляет запросы клиентам
 - Играет роль прокси-сервера

 Сайты в интернете работают на веб-серверах, которые обрабатывают запросы пользователей и отвечают на них.
 Сегодня один из самых популярных веб-серверов — Nginx
 NGINX (Engine X, или «Энджин-икс») — это программное обеспечение с открытым исходным кодом для создания веб-серверов.
 Оно принимает запрос клиента, например браузера, обрабатывает его и возвращает ответ.

 --- Типа Советы ---

 -- Рефакторить и замерять скорость при помощи  @funcy.log_durations. - там на сайте еще много чего

 Ну, например, select_related на самом деле делает JOIN. А prefetch_related не делает. Не всегда синтаксис django ORM
 выдерживает реальности SQL, и появляются всякие странные вещи типа OuterRef, F, Q, и иже с ними. GROUP BY вообще замаскирован.

 Если вы хотите выполнить агрегацию, вы можете использовать функции агрегации ORM :

 from django.db.models import Count
 result = (Members.objects
     .values('designation')
     .annotate(dcount=Count('designation'))
     .order_by()
)


 -- Docker  Kubernetes (K8s)--

  Docker     - Изолирует приложение и упаковывает все в контейнер
  Kubernetes - Инструмент для оркестровки и управления деплоем контейнеров на нескольких машинах паралельно

 Docker — это платформа, которая позволяет упаковать в контейнер приложение со всем окружением и зависимостями,
 а затем доставить и запустить его в целевой системе.

 Приложение, упакованное в контейнер, изолируется от операционной системы и других приложений.

 Контейнеры позволяют разработчикам упаковать приложение со всеми его зависимостями и развернуть как единое целое.

 Контейнеризация — это способ упаковки приложения и всех его зависимостей в один образ, который запускается
 в изолированной среде, не влияющей на основную операционную систему.

 Виртуализация в Docker реализуется на уровне ОС. Виртуальная среда запускается прямо из ядра основной
 операционной системы и использует её ресурсы.


 Чем виртуализация отличается от контейнеризации?
 Контейнеры и виртуальные машины — это разные способы виртуализации. Только виртуалка реализует её на уровне железа,
 а Docker — на уровне операционной системы.

 Виртуальная машина функционирует как отдельный компьютер с собственным оборудованием и операционной системой.

 Когда использовать Kubernetes, а когда достаточно Docker-контейнеров?

 Если у вас мало контейнеров и они все работают на одном узле, вам будет достаточно простых Docker-контейнеров.
 Kubernetes нужен, когда у вас много контейнеров и узлов, которыми нужно управлять. Также Kubernetes подходит,
 если вам нужна распределенная отказоустойчивая система.


 Docker host — это операционная система, на которую устанавливают Docker и на которой он работает.
 Docker daemon — служба, которая управляет Docker-объектами: сетями, хранилищами, образами и контейнерами.
 Docker client — консольный клиент, при помощи которого пользователи взаимодействуют с Docker daemon и отправляют
 ему команды, создают контейнеры и управляют ими.
 Docker image — это неизменяемый образ, из которого разворачивается контейнер.
 Docker container — развёрнутое и запущенное приложение.
 Docker Registry — репозиторий, в котором хранятся образы.
 Dockerfile — файл-инструкция для сборки образа.
 Docker Compose — инструмент для управления несколькими контейнерами. Он позволяет создавать контейнеры и задавать их конфигурацию.
 Docker Desktop — GUI-клиент, который распространяется по GPL. Бесплатная версия работает на Windows, macOS,
 а с недавних пор и на Linux. Это очень удобный клиент, который отображает все сущности Docker и позволяет запустить
 однонодовый Kubernetes для компьютера.


 Podman — это инструмент для управления контейнерами, который позволяет создавать, запускать и управлять контейнерами и
 подами без необходимости в демоне, как в Docker. Он обеспечивает совместимость с Docker CLI и позволяет работать
 с контейнерами от имени обычного пользователя, что повышает безопасность.


 Podman и Docker — это инструменты для управления контейнерами, но имеют несколько ключевых отличий:

 Архитектура:

 Docker использует клиент-серверную архитектуру, где демон (Docker daemon) управляет контейнерами.
 Podman работает без демона и использует архитектуру "без демона", что делает его более безопасным и простым в
 использовании в некоторых сценариях.

 Пользовательские права:

 Docker обычно требует привилегированных прав для работы с демоном.
 Podman может работать от имени обычного пользователя без необходимости в повышенных привилегиях.
 Совместимость:

 Podman поддерживает команды Docker CLI, что облегчает переход с Docker на Podman.
 Поддержка подов:

 Podman имеет встроенную поддержку для управления подами (группами контейнеров), что делает его более подходящим для
 работы с многоконтейнерными приложениями.

 Эти отличия делают каждый инструмент более подходящим для различных сценариев использования.



 Docker - Изолирует приложение и упаковывает все в контейнер
 Kubernetes - Инструмент для оркестровки и управления деплоем контейнеров на нескольких машинах паралельно
 K8s - Сокращенное название Kubernetes      В сокращении “K8S” цифра 8 - это восемь букв между K и S.


 --- BI  (Business Intelligence, Бизнес-Аналитика) ---
 ETL (Extract, Transform, Load) — это процесс, который используется для интеграции данных из разных источников в единую
 систему, обычно в системы бизнес-аналитики (BI)

 1. **Extract (Извлечение)**: Собираются данные из различных источников, таких как базы данных, файлы, API и другие
 системы. Это может включать как структурированные, так и неструктурированные данные.

 2. **Transform (Преобразование)**: Данные обрабатываются и преобразуются в нужный формат. Этот этап может включать
 фильтрацию, агрегацию, очистку и интеграцию данных, чтобы обеспечить их качество и целостность.

 3. **Load (Загрузка)**: Преобразованные данные загружаются в целевую систему хранения, такую как дата-warehouse
 (хранилище данных) или BI-платформу, где они могут быть использованы для анализа и отчетности.

 MicroStrategy — BI Система - это мощная платформа для бизнес-аналитики (BI), которая предоставляет инструменты для
 анализа данных, визуализации и создания отчетов.

 Datalens (или Яндекс.Даталеннс) — это инструмент для бизнес-аналитики и визуализации данных, разработанный Яндексом.
 Он позволяет пользователям анализировать и представлять свои данные в удобном и наглядном формате.




 -- SQL --

 Напишите запрос, который выводит информацию о версии MySQL сервера.
 SELECT VERSION();

 Напишите запрос, который выводит список баз данных на сервере
 SHOW DATABASES;

 Какие таблицы есть в  БД.
 SHOW TABLES;

 Выведите информацию о структуре таблицы statistics.
 USE название_схемы;
 SHOW COLUMNS FROM statistics;

 - **DATABASE** — это более широкий контейнер с данными.
 - **SCHEMA** — это способ организации данных внутри базы данных.


 Таблицы — это основа хранения данных в SQL, а представления — инструмент для извлечения конкретной информации.

 В SQL представление(VIEW) — это виртуальная таблица, основанная на SELECT операторе.   CREATE VIEW НАЗВАНИЕ AS SELECT ...
 Примеры:
 CREATE VIEW top_rated_movies           CREATE VIEW NamesView
 AS                                     AS
 SELECT id,                             SELECT pl.vcName, count(*)
       title,                           FROM tbPeoples pl
       release_year,                    GROUP BY vcName
       genre,
       rating
 FROM movies
 WHERE rating = 9;


 Виртуальная таблица - Таблица созданная на основе источника посредством выполнения запроса.
 Итак, вьюшка – это просто запрос на языке SQL, который выбирает данные, а в базе данных она выглядит как таблица
 и работа с ней происходит также.

 Чем View отличается от таблицы?
 View — виртуальная (логическая) таблица, представляющая собой поименованный запрос (синоним к запросу), который будет
 подставлен как подзапрос при использовании представления. В отличие от обычных таблиц реляционных баз данных,
 представление не является самостоятельной частью набора данных, хранящегося в базе.

 Таблица содержит данные, Представление(View) — это просто оператор SELECT, который был сохранен в базе данных

 Таблица — это сущность базы данных, которая хранит данные в виде строк и столбцов.
 Представление — это виртуальная таблица, используемая для просмотра или манипулирования некоторыми частями таблицы .

 Представление(View) является результатом SQL-запроса и представляет собой виртуальную таблицу, тогда как таблица состоит из
 строк и столбцов, которые хранят информацию о любом объекте и используются для извлечения этих данных по мере необходимости.


 -- Что такое ИНДЕКСЫ в SQL и как их использовать --

 По сути, индекс — это сбалансированное двоичное дерево поиска. Каждая строка в таблице соответствует узлу в дереве.

 Производительность: Проблема в том, что, когда новая запись вставляется в таблицу или удаляется из нее, приходится
 обновлять все индексы, чтобы отразить это изменение. Если индексов много, то обновление, вставка или удаление строк
 могут стать в вычислительном плане дорогостоящими операциями (ВСПОМНИМ ПРО БАЛАНСИРОВКУ ДЕРЕВА).

 Более того, индексы занимают ограниченное дисковое пространство.

 - Ответ 1:

 Если в кратце, то индекс, это поле по которому оптимизирован(ускорен) поиск.                            <-----

 Поскольку индекс занимает место, то индексировать нужно только те поля, по которым происходит выборка.  <-----

 Допустим есть таблица.

 CREATE TABLE MyGuests (
     id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,
     firstname VARCHAR(30) NOT NULL,
     lastname VARCHAR(30) NOT NULL,
     email VARCHAR(50),
     reg_date TIMESTAMP
 )

 id - уже индекс

 Допустим вам нужен поиск по имени (firstname).

 SELECT * FROM MyGuests WHERE firstname = "Вася"

 тогда есть смысл добавить индекс по данному полю.

 CREATE INDEX firstname_index ON MyGuests (firstname) USING BTREE;    # Создание Индекса

 Будет созданна "карта" которая позволет легко находить записи в оригинальном списке.


 - Ответ 2:

 Вкратце, индексы создаются для повышения производительности поиска данных. Таблицы могут иметь огромное количество строк,
 которые хранятся в произвольном порядке. Без индекса поиск нужных строк идёт по порядку (последовательно), что на
 больших объемах данных отнимает много времени.

 Индекс - обычно один или несколько столбцов таблицы и указателей на соответствующие строки таблицы, позволяет искать
 строки, удовлетворяющие критерию поиска. Ускорение работы с использованием индексов достигается в первую очередь за
 счёт того, что индекс имеет структуру, оптимизированную под поиск — например, в MySQL b-дерева. Индекс лучше
 использовать на тех столбцах таблицы, на которые вы чаще всего будете накладывать условия через where column_name = ...

 Индекс создаётся по правилу:

 create index название_индекса
 on название_таблицы (название_столбца)

 Например, у вас таблица называется test, где хранятся данные по городам России с улицами вида Город, Улица, Дом.Понятно,
 что строк в таблице при таком раскладе будет много. Если вы часто делаете выборку по определенному городу, например:

 select *
 from test
 where city = 'Омск'

 то, чтобы этот запрос отработал быстрее обычного, следует добавить индекс по вышеуказанному правилу:

 create index city_index
 on test (city)

 Тогда тот же самый запрос

 select *
 from test
 where city = 'Омск'

 отработает гораздо быстрее, если столбец city будет проиндексирован.


 Внешний ключ - это столбец для соединения с другими таблицами.
 Индекс - столбец, по которому можно задать условие и тогда запрос отработает гораздо быстрее.


 -- Ответ 3:
 На пальцах можно объяснить так:

 Когда Вы создаёте таблицу, добавляете в неё данные, то таблица разрастается и она выглядит как просто последовательный
 список, упорядоченный по тому как в неё данные добавлялись.

 Когда данных мало, список маленький и все запросы к ней выполняются, почти, незаметно. Но когда количество записей в
 таблице начинает переваливать за миллион (в разных случаях по разному, но как пример миллион), то у Вас поиск уже идёт
 не так быстро и с добавлением всё новых и новых записей - ещё медленнее.

 Это связано с тем, что когда Вы ищите какую-то запись, то просматриваются все записи, пока не дойдут до нужной.

 Когда Вам это окончательно надоедает и Вы хотите что-нибудь сделать, то к Вам на помощь приходят индексы.

 Индекс создаётся по какому-то определённому полю (можно по нескольким) по которому, обычно, выполняется поиск.
 Когда Вы создаёте индекс, то MySql (и любая другая БД) обходит все записи в таблице и строит дерево
 (скорее всего B-дерево или разновидность), в котором ключами выступает выбранное поле, а содержимым ссылки на записи в таблице.

 И когда Вы делаете очередной свой select запрос по таблице, по полю для которого создали индекс MySql
 (и любая другая БД) знает что у неё есть индекс, по которому пройтись будет быстрее, нежели перебирать все записи и
 Ваш запрос будет направлен этому индексу и записи, удовлетворяющие условию, будут найдены гораздо быстрее, так как
 поиск по построенному дереву будет гораздо быстрее, нежели простой перебор всех записей.


 B-Tree индекс дает скорость выборки порядка O(log n): Логарифмическая сложность
 hash в среднем обеспечивают скорость выборки O(1):  Константная сложность.   если происходит множество коллизий  линейную O(n).

 Таким образом:
- B-Tree обеспечивает логарифмическую сложность O(log n) при поиске.
- Хэш-индексы в среднем обеспечивают постоянную сложность O(1) для поиска, но могут достигать до O(n) в худшем случае при наличии коллизий.

 Важно также отметить, что хэш-индексы НЕ поддерживают диапазонный поиск, в отличие от B-Tree.

 В реальной жизни hash и B-Tree применяются совместно, то есть для вычисления значений B-Tree индекса все равно применяются хэши.

 ### Сравнение   B-Tree vs Hash:

 - **Производительность**:
  B-Tree лучше для операций, требующих диапазонного поиска, тогда как хэш-индексы оптимальны для точных совпадений.

 - **Структурные особенности**:
  B-Tree — это сбалансированная структура, которая остается упорядоченной, в то время как хэш-индексы НЕ упорядочены.

 - **Гибкость**:
  B-Tree более универсален и подходит для разнообразных типов запросов, в то время как хэш-индексы более узкоспециализированы.

 В зависимости от конкретных задач и типов запросов, СУБД может использовать один или оба типа индексов.


 -- Кластерные индексы vs Обычные индексы --

 - Кластерные индексы лучше подходят для операций с диапазонами
 - НЕкластерные индексы — для выборок по конкретным значениям.


| Аспект                 |Кластерные индексы                            | Некластерные индексы
|------------------------|----------------------------------------------|------------------------------------------------
| **Структура**          | Определяет физический порядок хранения строк.| Создает отдельную структуру с указателями на строки.
| **Количество**         | Только один на таблицу.                      | Много на таблицу.
| **Производительность** | Быстрый для диапазонных запросов.            | Быстрый для точных запросов.
| **Обновления**         | Более затратные при изменениях данных.       | Менее затратные операции при изменениях.

 Кластерные индексы играют важную роль в оптимизации производительности базы данных за счет упорядочивания данных.

 -- END Что такое ИНДЕКСЫ в SQL и как их использовать --



 -- Что такое Транзакции в SQL Атомарное действие --

 Транзакция, если по-простому - это совокупность неких действий, причем такая, что либо все эти действия
 выполняются успешно, либо ни одно не выполняется вообще.

 Соответственно, транзакция в sql - это последовательность операторов, которая либо выполняется целиком, либо целиком
 же откатывается. (почитайте про ключевые слова TRANSACTION, COMMIT, ROLLBACK)

 TCL (Transaction Control Language)
 BEGIN TRANSACTION    — начало транзакции.
 COMMIT TRANSACTION   — изменение команд транзакции.
 ROLLBACK TRANSACTION — отказ в транзакции.
 SAVE TRANSACTION     — формирование промежуточной точки сохранения внутри

 Транзакции в SQL — это набор операций, которые выполняются как единое целое. Они обеспечивают атомарность,
 консистентность, изолированность и долговечность (ACID-принципы). Это значит, что либо все операции в транзакции
 выполняются успешно, либо, в случае ошибки, все изменения отменяются.


 -- ACID-принципы --
 ACID — это набор свойств, которые гарантируют надежность транзакций в системах управления базами данных (СУБД).

 ACID расшифровывается как:

 1. **Atomicity (Атомарность)**: Транзакция выполняется полностью или не выполняется вообще. Частичные изменения не допустимы.

 2. **Consistency (Согласованность)**: Транзакция переводит базу данных из одного согласованного состояния в другое.
    Это означает, что все бизнес-правила и ограничения базы данных должны соблюдаться.

 3. **Isolation (Изолированность)**: Одновременно выполняющиеся транзакции не влияют друг на друга.
    Результаты транзакции не доступны другим до её завершения.

 4. **Durability (Надежность)**: После завершения транзакции её изменения сохраняются в базе данных, даже в случае сбоя системы.

 Эти свойства помогают обеспечить надежность и целостность данных в реляционных базах данных.


 -- CAP теорема --
 CAP теорема (или теорема CAP) — это концепция, которая описывает баланс между тремя важными свойствами распределённых
  вычислений в системах хранения данных:

 1. **Consistency (Согласованность)**: Все узлы системы видят одни и те же данные в одно и то же время. Это значит,
  что после успешного завершения операции все пользователи будут видеть обновленные данные.

 2. **Availability (Доступность)**: Каждый запрос к системе получает ответ, даже если некоторые узлы недоступны.
  Система всегда отвечает, но не обязательно корректно.

 3. **Partition Tolerance (Устойчивость к разделению)**: Система продолжает функционировать даже в случае сетевых
  разделений, когда узлы не могут обмениваться данными.

 Согласно теореме CAP, в любой распределенной системе можно одновременно гарантировать только два из трёх свойств:

 - Если вы стремитесь к **согласованности** и **доступности**, у вас может возникнуть проблема при сетевом разделении (Partition).
 - Если вы стремитесь к **доступности** и **устойчивости к разделению**, вы можете потерять **согласованность**.
 - Если вы пытаетесь обеспечить **согласованность** и **устойчивость к разделению**, вы можете столкнуться с проблемами
   в **доступности**.

 Таким образом, теорема CAP помогает разработчикам и архитекторам систем понимать компромиссы, которые необходимо
 учитывать при проектировании распределенных систем.


 ### Пример транзакции:

 Предположим, у нас есть две таблицы: `Accounts` (для учета балансов пользователей) и `Transactions` (для учета сделанных транзакций).

 #### 1. Создание таблиц

 sql
 CREATE TABLE Accounts (
     AccountID INT PRIMARY KEY,
     Balance DECIMAL(10, 2)
 );

 CREATE TABLE Transactions (
     TransactionID INT PRIMARY KEY AUTO_INCREMENT,
     FromAccount INT,
     ToAccount INT,
     Amount DECIMAL(10, 2),
     TransactionDate DATETIME DEFAULT CURRENT_TIMESTAMP
 );


 #### 2. Пример транзакции

 Предположим, мы хотим перевести 100 единиц денег от одного аккаунта к другому:

 sql
 BEGIN;

 UPDATE Accounts
 SET Balance = Balance - 100
 WHERE AccountID = 1;

 UPDATE Accounts
 SET Balance = Balance + 100
 WHERE AccountID = 2;

 INSERT INTO Transactions (FromAccount, ToAccount, Amount)
 VALUES (1, 2, 100);

 COMMIT;


 ### Объяснение:

 1. **BEGIN**  - Начинает транзакцию.
 2. **UPDATE** - Первое обновление уменьшает баланс первого аккаунта.
 3. **UPDATE** - Второе обновление увеличивает баланс второго аккаунта.
 4. **INSERT** - Записывает информацию о транзакции в таблицу `Transactions`.
 5. **COMMIT** - Подтверждает все изменения, сделанные в рамках транзакции.

 Если на каком-то этапе возникает ошибка (например, недостаточно средств на первом аккаунте), можно выполнить `ROLLBACK`,
 чтобы отменить все изменения, сделанные в транзакции.

 -- END Что такое Транзакции в SQL Атомарное действие --



 Оконные функции SQL:
 Оконная функция вычисляет значение по набору данных, связанных с текущей строкой, то есть данные из одной группы,
 если используется Partition by.


 `PARTITION BY` в SQL — это предложение, используемое в оконных функциях для разделения набора строк на более мелкие
 подгруппы (партиции) перед применением функции.

 Оконные функции в SQL — это функции, которые позволяют выполнять вычисления по наборам строк, связанным с текущей
 строкой, не сводя их в одну строку, как это делает, например, агрегатная функция. Они "окружают" строки и позволяют
 выполнять аналитику, сохраняя при этом все строки в результирующем наборе. Оконные функции особенно полезны для
 вычисления скользящих средних, рангов, сумм и т.д.


 Пример:
 SELECT
     id,
     amount,
     sale_date,
     SUM(amount) OVER (ORDER BY sale_date) AS cumulative_sum
 FROM
     sales;

 ### Объяснение:

 - `SUM(amount)`: Это оконная функция, которая вычисляет сумму по столбцу `amount`.
 - `OVER (ORDER BY sale_date)`: Это определяет "окно" для функции, указывая, что суммы должны быть рассчитаны в порядке дат.


 DDL (Data Definition Language)
 CREATE  — создание нового объекта в существующей базе.
 ALTER   — изменение существующего объекта.
 DROP    — удаление объекта из базы.

 DML (Data Manipulation Language)
 SELECT — позволяет выбрать данные в соответствии с необходимым условием.
 INSERT — осуществляют добавление новых данных.
 UPDATE — производит замену существующих данных.
 DELETE — удаление информации.

 DCL (Data Control Language)
 GRANT  — предоставляет доступ к объекту.
 REVOKE — аннулирует выданное ранее разрешение на доступ.
 DENY   — запрет, который прекращает действие разрешения.

 TCL (Transaction Control Language)
 BEGIN TRANSACTION    — начало транзакции.
 COMMIT TRANSACTION   — изменение команд транзакции.
 ROLLBACK TRANSACTION — отказ в транзакции.
 SAVE TRANSACTION     — формирование промежуточной точки сохранения внутри


 # Как начать работу с SQL
 SELECT    — выбор данных.
 FROM      — источник информации, откуда брать данные.
 JOIN      — добавление таблиц.
 WHERE     — при каком условии.
 GROUP BY  — сформируй группу данных по заданному признаку.
 ORDER BY  — сортировка данных по нужному признаку.
 LIMIT     — количество результатов.
 ;         — конец предложения


 WITH позволяет дать блоку подзапроса имя/псевдоним, на которое можно ссылаться в нескольких местах основного SQL-запроса.
 Имя, присвоенное подзапросу, обрабатывается так, как если бы оно было встроенным представлением или таблицей.
 SQL оператор WITH по сути является заменой обычному подзапросу.

 Cинтаксис Oracle PL/SQL WITH с одним подзапросом:
 WITH query_name AS (SELECT expressions FROM table_A)

 SELECT column_list

   FROM query_name [,table_name]

 [WHERE conditions]

 # Выбор уникальных элементов столбца  DISTINCT                                                                 <-----
 На самом деле это тяжёлая операция - группировка и сортировку в себя включает, на маленьких объёмах это незаметно,
 а вот на больших его лучше не использовать


 # *Агрегирующие функции (иногда их ещё называют агрегатными) обрабатывают набор строк для подсчета и возвращают одно
 обобщенное значение: SUM, MAX, MIN, COUNT, AVG

 COUNT(*) —  подсчитывает  все записи, относящиеся к группе, в том числе и со значением NULL;
 COUNT(имя_столбца) — возвращает количество записей конкретного столбца (только NOT NULL), относящихся к группе.


 --- JOINS ---
 В pandas это - pandas.DataFrame.join   pandas.DataFrame.merge

 <join_type> ::=
    [ { INNER | { { LEFT | RIGHT | FULL } [ OUTER ] } } [ <join_hint> ] ]       # [] - НЕобязательное
    JOIN

 Ключевое слово OUTER отмечено как необязательное (заключено в квадратные скобки). В этом конкретном случае OUTER
 не имеет значения, указываете вы его или нет. Обратите внимание, что хотя другие элементы предложения join также
 отмечены как необязательные, их исключение будет иметь значение.

 Например, вся часть type в JOIN предложении является необязательной, в этом случае по умолчанию будет, INNER
 если вы просто укажете JOIN. Другими словами, это допустимо:

 SELECT *
 FROM A JOIN B ON A.X = B.Y

 Вот список эквивалентных синтаксисов:
 A LEFT JOIN B            A LEFT OUTER JOIN B
 A RIGHT JOIN B           A RIGHT OUTER JOIN B
 A FULL JOIN B            A FULL OUTER JOIN B
 A INNER JOIN B           A JOIN B


 На верхнем уровне в основном существуют 3 типа соединений:

 1.INNER JOIN извлекает данные, если они присутствуют в обеих таблицах.

 2.OUTER JOINs бывают 3 типов:
    1.LEFT OUTER JOIN  - извлекает данные, если они присутствуют в левой таблице.
    2.RIGHT OUTER JOIN - извлекает данные, если они присутствуют в нужной таблице.
    3.FULL OUTER JOIN  - извлекает данные, если они присутствуют в любой из двух таблиц.
 3. CROSS JOIN, как следует из названия, делает n раз m пар, которые соединяют все со всем. Это похоже на то, как мы
 просто перечисляем таблицы для соединения (в предложении FROM оператора SELECT), используя запятые для их разделения.

 Следует отметить следующее:
 - Если вы просто упомянули JOIN, то по умолчанию это INNER JOIN.
 - Соединение OUTER должно быть LEFT| RIGHT| FULL; вы не можете просто сказать OUTER JOIN.
 - Вы можете опустить OUTER ключевое слово и просто сказать LEFT JOIN или RIGHT JOIN или FULL JOIN.  <-----



  --- Pandas vs SQL ---

 # ! Абстрактный код
 # новая переменная
 sorted_df = df.sort_values("col1")
 # перезапись исходного `DataFrame`
 df = df.sort_values("col1")

 -- Операция SQL SELECT --

 tips = pd.read_csv(url)

 SELECT total_bill, tip, smoker, time FROM tips;       tips[["total_bill", "tip", "smoker", "time"]]

 # Вызов DataFrame без списка имен столбцов отобразит все столбцы (аналогично * в SQL).
 SELECT * FROM tips;                                   df

 # В SQL можно добавить вычисляемый столбец:
 SELECT *, tip/total_bill as tip_rate FROM tips;       tips.assign(tip_rate=tips["tip"] / tips["total_bill"])


 -- Операция SQL WHERE --

 SELECT * FROM tips WHERE time = 'Dinner';                  tips['time' == 'Dinner']
 SELECT * FROM tips WHERE time = 'Dinner' AND tip > 5.00;   tips[(tips['time'] == 'Dinner') & (tips['tip'] > 5.00)]
 SELECT * FROM tips WHERE size >= 5 OR total_bill > 45;     tips[(tips['size'] >= 5) | (tips['total_bill'] > 45)]

 SELECT * FROM tips WHERE 'day'  IN ('Sun', 'Sat') and sex='Female'
 # Pandas
 tips[(tips['day'].isin(['Sun', 'Sat'])) & (tips['sex'] == 'Female')]


 frame = pd.DataFrame(
    {"col1": ["A", "B", np.nan, "C", "D"], "col2": ["F", np.nan, "G", "H", "I"]}
 )

 # Проверка NULL выполняется с помощью методов .notna() и .isna().
 SELECT * FROM frame WHERE col2 IS NULL;                    frame[frame['col2'].isna()]
 SELECT * FROM frame WHERE col1 IS NOT NULL;                frame[frame['col2'].notna()]


 -- Операция SQL GROUP BY --

 SELECT sex, count(*) FROM tips GROUP BY sex;               tips.groupby("sex").size()
                                                            tips.groupby("sex")["total_bill"].count() # Альтернатива

 Обратите внимание, что в коде с pandas используется .size(), а не .count(). Это связано с тем, что метод .count()
 применяет функцию к КАЖДОМУ столбцу, возвращая количество записей NOT NULL в КАЖДОМ столбце.

 tips.groupby("sex").count()

 SELECT day, AVG(tip), COUNT(*) FROM tips GROUP BY day;     tips.groupby('day').agg({'tip': 'mean', 'day': 'size'})


 SELECT smoker, day, COUNT(*), AVG(tip) FROM tips GROUP BY smoker, day;

 # Pandas
 tips.groupby(["smoker", "day"]).agg({"tip": ["size", "mean"]})


 -- INNER JOIN --

 SELECT *                     # Pandas                      Метод DataFrame.merge() по умолчанию выполняет INNER JOIN.
 FROM df1                     pd.merge(df1, df2, on="key")
 INNER JOIN df2
   ON df1.key = df2.key;


 merge() также предлагает параметры для случаев, когда вы хотите объединить столбец одного DataFrame с индексом другого DataFrame.
 indexed_df2 = df2.set_index("key")
 pd.merge(df1, indexed_df2, left_on="key", right_index=True)


 -- LEFT OUTER JOIN --

 SELECT *                      # Pandas
 FROM df1                      pd.merge(df1, df2, on="key", how="left")
 LEFT OUTER JOIN df2
   ON df1.key = df2.key;


 -- RIGHT JOIN --

 SELECT *                      # Pandas
 FROM df1                      pd.merge(df1, df2, on="key", how="right")
 RIGHT OUTER JOIN df2
   ON df1.key = df2.key;


 -- FULL JOIN --

 SELECT *                       # Pandas
 FROM df1                       pd.merge(df1, df2, on="key", how="outer")
 FULL OUTER JOIN df2
   ON df1.key = df2.key;


 --- Модуль pandas, анализ данных в Python ---


 Kaggle - это международная платформа, на которой проводятся соревнования по анализу данных.

 Медиана — это число, которое является серединой множества чисел
 Например, медианой для чисел 2, 3, 3, 5, 7 и 10 будет 4.

 from numpy import median

 res = numpy.array([1, 2, 3, 5])
 print(median(res))                  # -> 2.5
 print(median([1, 2, 3, 5, 10]))     # -> 3.0  # Если чётное количество чисел берет число по середине
 print(median([1, 2, 3, 5, 9, 10]))  # -> 4.0  # Если НЕ чётное количество чисел берет 2 числа из середины (a+b)/2

 Медиана В статистике: Нужно чтобы данные были Отсортированные по возрастанию
 Медиана(median) — значение признака, которое делит УПОРЯДОЧЕННОЕ множество данных ПОПОЛАМ.

 Мода — это число, наиболее часто встречающееся в данном наборе чисел.


 ЭКСТРАПОЛЯЦИЯ и ИНТЕРПОЛЯЦИЯ — это методы прогнозирования и оценки значений функций или данных.

 1. **Интерполяция** - это процесс оценки значений функции на основе известных данных в пределах заданного диапазона.
 Например, если у вас есть набор точек, интерполяция позволяет определить значения между этими точками.

 2. **Экстраполяция** - это метод предсказания значений функции за пределами известных данных. Это более рискованная
 операция, так как предполагает, что тенденции, наблюдаемые в имеющихся данных, будут продолжаться и за их пределами.

 В итоге, ИНТЕРПОЛЯЦИЯ работает внутри диапазона данных, а ЭКСТРАПОЛЯЦИЯ — за его пределами.


 -- Разработка модели машинного обучения строится из стандартных этапов, я их кратко опишу ниже. --

 0. Сбор данных и оценка их качества в свете требований к модели.

 1. Предобработка данных: удаление дубликатов и пропусков; преобразование категориальных переменных в числовые (One-Hot
 Encoding, Label Encoding); нормализация числовых переменных; в Вашем случае – добавление/интерполяция данных для дат,
 когда данные отсутствуют.

 2. Разделение данных на обучающую, валидационную и тестовую выборки (например, 70%/15%/15%).

 3. Исследование тенденций в данных (в Вашем случае, например, есть ли сезонность продаж и др.). Это влияет на то,
  что в разработку модели добавляются дополнительные признаки (features): дни недели, месяцы, годы,
  лаги — это некоторые предыдущие значения временного ряда.

 4. Выбор модели, в Вашем случае для задачи регрессии (например, линейная регрессия, случайный лес, градиентный бустинг).

 5. Обучение модели на обучающей выборке, используя выбранные алгоритмы и параметры.

 6. Валидация модели на валидационной выборке, используя метрики, такие как MSE (среднеквадратичная ошибка) или MAE (средняя
 абсолютная ошибка). RMSE (корень среднеквадратичной ошибки) , MAPE (средняя абсолютная процентная ошибка)
 SMAPE (Symmetric mean absolute percentage error) - Чувствительность к нулевым значениям и очень маленьким значениям

 7. Тюнинг модели – настройка гипер-параметров модели для улучшения качества предсказания.

 8. Тестирование модели на тестовой выборке для финальной оценки качества модели.

 9. Деплой модели в продакшн-среде для использования в реальных условиях


 MSE, RMSE, MAE, MAPE, SMAPE - ИЗМЕРЯЮТ ОШИБКИ МЕЖДУ ПРЕДСКАЗАННЫМИ ЗНАЧЕНИЯМИ МОДЕЛИ И ФАКТИЧЕСКИМИ ЗНАЧЕНИЯМИ.


 -- Метрики для регрессии --

 **Mean Squared Error (MSE)**: Среднеквадратичная ошибка; измеряет среднее значение квадратов ошибок между
 предсказанными и фактическими значениями.

 **Root Mean Squared Error (RMSE)**: Квадратный корень из MSE; простое интерпретируемое значение, которое показывает,
 в среднем, насколько предсказания отклоняются от фактических значений.

 **Mean Absolute Error (MAE)**: Средняя абсолютная ошибка; измеряет среднее значение абсолютных ошибок между
 предсказанными и фактическими значениями.

 **R-squared (коэффициент детерминации)**: Показывает, какую долю дисперсии зависимой переменной объясняет модель;
 значение от 0 до 1, где 1 указывает на идеальное соответствие.


 -- Метрики для классификации --

 **Accuracy (точность)**: Доля правильно предсказанных объектов к общему числу объектов.

 **Precision (точность)**: Доля истинных положительных результатов к общему числу предсказанных положительных результатов.

 **Recall (полнота)**: Доля истинных положительных результатов к общему числу фактических положительных результатов.

 **F1-score**: Гармоническое среднее между Precision и Recall, полезно при наличии дисбаланса классов.

 **ROC-AUC (площадь под кривой ROC)**: Измеряет качество бинарного классификатора; выше 0.5 считается хорошим.



 DataSet обычно представляет собой файл с таблицей в формате JSON или CSV.
 Dataset – это обработанная и структурированная информация в табличном виде.

 SciPy — это библиотека для языка Python, построена поверх NumPy, но для более глубоких и сложных научных вычислений
 Matplotlib – это библиотека для визуализации данных на Python.  - для создания графиков и диаграмм
 Seaborn - это библиотека для визуализации данных на Python. Построена поверх Matplotlib - для создания статистических графиков

 Библиотека Pandas построена на базе NumPy.
 Numeric Python (NumPy) - Она ускоряет работу с многомерными массивами и матрицами, а также позволяет вычислять
 много высокоуровневых математических функций при работе с массивами данных.

 matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]            # -> Матрица через списки(list) в Python

 import numpy as np
 matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  # -> Матрица через numpy

  Две основные структуры данных, pandas.Series (1-мерная) и pandas.DataFrame (2-мерная)

 pandas.DataFrame - двумерный массив.  представляет собой двумерную помеченную структуру данных со столбцами
 потенциально разных типов.
 pandas.Series - одномерный массив.    это одномерный помеченный массив, способный хранить данные любого типа
 Каждый pandas.DataFrame и pandas.Series имеют индекс pandas.Index, который представляет собой метки строк данных.

 import pandas as pd

 df_filtered = df[df['column'] == 'condition']

 # только на больших ДФ .query() работает медленнее.
 df_filtered = df.query('column == "condition"')

# Предположим, что game_events — это ваш DataFrame
 game_events = pd.read_csv('путь_к_вашему_файлу.csv')  # Загрузите данные, если это необходимо

 Разные функции pandas              # Так тоже можно
 game_events['revenue'].count()     game_events.revenue.count()
 game_events['revenue'].sum()       game_events.revenue.sum()
 game_events['revenue'].mean()      game_events.revenue.mean()
 game_events['revenue'].max()       game_events.revenue.max()
 game_events['revenue'].min()       game_events.revenue.min()
 game_events['revenue'].median()    game_events.revenue.median()
 game_events['revenue'].fillna(1)   game_events.revenue..fillna(0)    - Замена ПРОПУЩЕННЫХ или НЕОПРЕДЕЛЕННЫХ значений


 # Получить столбец
 df['type']
 game_events['type']

 # Только уникальные
 game_events['user_id'].unique()   #  ['7f0344f8' '00aa49ac' 'f5ef9841' '13d17d67']  сами уникальные элементы
 game_events['user_id'].nunique()  #  4                                              количество уникальных элементов


 # Как посмотреть количество значений `Null` (или `NaN`)
 Пример!!!
 data = {'A': [1, 2, None], 'B': [None, 5, 6], 'C': [7, 8, 9]}
 df = pd.DataFrame(data)

 # Смотрим на количество Null значений в каждом столбце
 null_counts = df.isnull().sum()
 print(null_counts)


 # Фильтрация
 count_events = game_events[game_events['user_id'] == 'f5ef9841']['event_name'].count()

 # Тоже самое но SQL
 SELECT COUNT(event_name) AS count_events
 FROM game_events
 WHERE user_id = 'f5ef9841'


 # Фильтрация с датой
 count_events = game_events[game_events['event_date'] == '2021-01-15']['event_name'].count()

 # Выберите только те строки, где внутриигровые покупки пользователей больше или равны числу 7.49.
 game_events[game_events['revenue'] >= 7.49]


 # Получение/извлечение значений Series/DataFrame

 DataFrame.loc[]    - доступ к срезу данных DataFrame по индексным меткам;
 Series.loc[]       - доступ к срезу данных Series по индексным меткам;
 DataFrame.iloc[]   - доступ к срезу данных DataFrame по позиции;          # целочисленная индексация
 Series.iloc[]      - доступ к срезу данных Series по позиции;             # целочисленная индексация

 df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],
 index=['cobra', 'viper', 'sidewinder'],
 columns=['max_speed', 'shield'])

 print(df) # Вывод ниже

 #             max_speed  shield
 # cobra               1       2
 # viper               4       5
 # sidewinder          7       8

 # Одно значение loc[]                      # Несколько значений loc[[]]
 print(df.loc['cobra']) # Вывод ниже        print(df.loc[['cobra', 'viper']])

 # max_speed    1                           #        max_speed  shield
 # shield       2                           # cobra          1       2
 # Name: cobra, dtype: int64                # viper          4       5

 # Интересные примеры loc
 df.loc[df['shield'] > 6]
 df.loc[df['shield'] > 6, ['max_speed']]
 df.loc[lambda df: df['shield'] == 8]
 df.loc['cobra':'viper', 'max_speed']
 df.loc[['viper', 'sidewinder'], ['shield']] = 50
 df.loc[(df['max_speed'] > 1) & (df['shield'] < 8)]
 df.loc[(df['max_speed'] > 4) | (df['shield'] < 5)]
 df.loc['cobra'] = 10

 # iloc
 df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],
 columns=['max_speed', 'shield'])


 # Одно значение iloc[]                     # Несколько значений iloc[[]]
 print(df.iloc[1])                          print(df.iloc[[1, 2]])

 # max_speed    4                           #    max_speed  shield
 # shield       5                           # 1          4       5
 # Name: 1, dtype: int64                    # 2          7       8

 # Интересные примеры iloc
 df.iloc[[True, False, True]]
 df.iloc[lambda x: x.index % 2 == 0]
 df.iloc[:3]                            # что в отличие от обычных срезов python, включены как начало, так и конец.


 # Применить функцию с столбцу                                                              <-----
 df = pd.DataFrame({'hehe': ['Y', 'N']})

 print(df)  # Вывод ниже

 #   hehe
 # 0    Y
 # 1    N


 def split_it(year):
     return re.findall('Y', year)
                                                  # Тоже самое с lambda
 df['hehe'] = df['hehe'].apply(split_it)          df['hehe'] = df['hehe'].apply(lambda x: x.re.findall('Y', year))

 print(df) # Вывод ниже

 #   hehe
 # 0  [Y]
 # 1   []


 # Чтобы применить функцию только к одному столбцу:                                    <-----  Важно

 df = pd.DataFrame({'hehe': ['Y Yes', 'N No']})

 def get_first_word(s):
     return s.split(maxsplit=1)[0]

 df['first'] = df['hehe'].apply(get_first_word)

 print(df) # Вывод ниже

 #     hehe first
 # 0  Y Yes     Y
 # 1   N No     N


 # Еще вариант   Также можно воспользоваться готовыми векторизированными Pandas методами:
 df['hehe'].str.split(n=1).str[0]               # Тоже самое
 df['hehe'].str.extract(r'(Y)', expand=False)   # Тоже самое


 # Иногда при работе со строковыми данными list comprehension оказывается быстрее встроенных векторизированных функций.

 df['first'] = [n.split(maxsplit=1)[0] for n in df['hehe']]


 # Интересный пример

 def find_products(products: pd.DataFrame) -> pd.DataFrame:
     df = products
     pattern = r'Y'
     filtered_df = df[(df['low_fats'].str.contains(pattern)) & \
     (df['recyclable'].str.contains(pattern))
     ]
     return filtered_df[['product_id']]


 -- Регулярки в Pandas     Использование регулярных выражений в Pandas --

 Метод Series.str.count()     - подсчитывает вхождения шаблона в строке;
 Метод Series.str.replace()   - заменит каждое вхождение шаблона регулярного выражения;
 Метод Series.str.contains()  - проверяет, содержится ли регулярное выражение в каждой строке;
 Метод Series.str.extract()   - извлекает группы захвата из шаблона регулярного выражения;
 Метод Series.str.findall()   - найдет все вхождения регулярного выражения;
 Метод Series.str.match()     - определяет, начинается ли каждая строка с совпадения с регулярным выражением;
 Метод Series.str.split()     - разбивает строки по заданному разделителю;
 Метод Series.str.rsplit()    - разбивает строки по заданному разделителю начиная справа.





 # Пример JOIN в pandas   pandas.DataFrame.join
 DataFrame.join(other, on=None, how='left', lsuffix='', rsuffix='', sort=False, validate=None)

 df.join(other, lsuffix='_caller', rsuffix='_other')
 df.set_index('key').join(other.set_index('key'))
 df.join(other.set_index('key'), on='key')
 df.join(other.set_index('key'), on='key', validate='m:1')


 # Пример JOINs в pandas   pandas.DataFrame.merge

 DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False,
                 sort=False, suffixes=('_x', '_y'), copy=None, indicator=False, validate=None)

 # Примеры!!!
 df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],
                    'value': [1, 2, 3, 5]})
 df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],
                    'value': [5, 6, 7, 8]})

 # Объединить df1 и df2 в столбцах lkey и rkey. Столбцы значений имеют суффиксы по умолчанию, _x и _y, добавленные.

 df1.merge(df2, left_on='lkey', right_on='rkey')

 # Объединить фреймы данных df1 и df2 с указанными левыми и правыми суффиксами, добавленными ко всем перекрывающимся столбцам.

 df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=('_left', '_right'))

 # Объединить DataFrames df1 и df2, но вызвать исключение, если DataFrames имеют перекрывающиеся столбцы.

 df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))



 df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})
 df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})

 df1.merge(df2, how='inner', on='a')
 df1.merge(df2, how='left', on='a')
 df1.merge(df2, how='cross')



   --- Pandas vs SQL ---

 # Таблицы
 df1 = pd.DataFrame({"key": ["A", "B", "C", "D"], "value": np.random.randn(4)})
 df2 = pd.DataFrame({"key": ["B", "D", "D", "E"], "value": np.random.randn(4)})

 -- INNER JOIN --

 SELECT *                     # Pandas
 FROM df1                     pd.merge(df1, df2, on="key")
 INNER JOIN df2
   ON df1.key = df2.key;


 merge() также предлагает параметры для случаев, когда вы хотите объединить столбец одного DataFrame с индексом другого DataFrame.
 indexed_df2 = df2.set_index("key")
 pd.merge(df1, indexed_df2, left_on="key", right_index=True)


 -- LEFT OUTER JOIN --

 SELECT *                      # Pandas
 FROM df1                      pd.merge(df1, df2, on="key", how="left")
 LEFT OUTER JOIN df2
   ON df1.key = df2.key;


 -- RIGHT JOIN --

 SELECT *                      # Pandas
 FROM df1                      pd.merge(df1, df2, on="key", how="right")
 RIGHT OUTER JOIN df2
   ON df1.key = df2.key;


 -- FULL JOIN --

 SELECT *                       # Pandas
 FROM df1                       pd.merge(df1, df2, on="key", how="outer")
 FULL OUTER JOIN df2
   ON df1.key = df2.key;


 -- UNION --

 df1 = pd.DataFrame({"city": ["Chicago", "San Francisco", "New York City"], "rank": range(1, 4)})
 df2 = pd.DataFrame({"city": ["Chicago", "Boston", "Los Angeles"], "rank": [1, 4, 5]})

 SELECT city, rank              # Pandas
 FROM df1                       pd.concat([df1, df2])
 UNION ALL
 SELECT city, rank
 FROM df2;


 -- LIMIT --

 SELECT * FROM tips             # Pandas
 LIMIT 10;                      tips.head(10)


 Jupyter-ноутбук — это среда разработки, где сразу можно видеть результат выполнения кода и его отдельных фрагментов.
 расширение файлов  .ipynb
 IPython – это интерактивная оболочка с широким набором возможностей и ядро для Jupyter
 Jupyter notebook является графической веб-оболочкой для IPython
 jupyter magics — метаязык, команды которого обычно начинаются с % или %%


 -- Чем отличается Machine learning от Deep Learning? --
 Основное различие между глубоким обучением и машинным обучением обусловлено тем, как данные представляются в систему.
  Алгоритмы машинного обучения почти всегда требуют структурированных данных, в то время как сети глубокого обучения
 полагаются на слои ANN (искусственные нейронные сети).


--- ML Data Science ---

 Граф состоит из отдельных точек (вершин) и соединяющих их линий (ребер).
 Граф – это просто набор взаимосвязанных элементов.

 5 типов графов, которые должен знать каждый Data Scientist
 - Разбивка вершин на группы
 - Поиск кратчайшего пути
 - Поиск оптимального пути
 - Рейтинг вершин графа
 - Центральность вершин


 Дерево
 Как и связный список, дерево (tree) использует элементы, которым для хранения объектов НЕ нужно располагаться в
 физической памяти непрерывно. Ячейки здесь тоже имеют указатели на другие ячейки, однако, в отличие от связных списков,
 они располагаются не линейно, а в виде ветвящейся структуры. Деревья особенно удобны для иерархических данных,
 таких как каталоги с файлами или система субординации.


 Двоичное дерево поиска   - Сложность алгоритма - O(log n)
 Двоичное дерево поиска (binary search tree) — это особый тип дерева, поиск в котором выполняется особенно эффективно.
 Узлы в двоичном дереве поиска могут иметь не более двух дочерних узлов. Кроме того, узлы располагаются согласно их
 значению/ключу. Дочерние узлы слева от родителя должны быть меньше него, а справа — больше.

 Сложность операций с бинарным деревом зависит от его структуры:                                        <-----  <-----

 1. **Вставка, удаление и поиск**:
 - В худшем случае (разреженное дерево или не сбалансированное): O(n)
 - В среднем и лучшем случае (сбалансированное дерево): O(log n)

 2. **Обход дерева (инордер, префиксный, постфиксный)**: O(n)

 Таким образом, сложность зависит от того, насколько сбалансировано дерево.

 Балансировка дерева в Python — это процесс, который помогает поддерживать структуру дерева в равновесии, чтобы операции,
 такие как добавление, удаление и поиск элементов, выполнялись быстро.
 Балансировка дерева в Python (и в других языках программирования) помогает поддерживать структуру дерева в равновесии,
 что позволяет выполнять операции добавления, удаления и поиска элементов эффективно. Это достигается за счет
 поддержания небольшой высоты дерева, что обеспечивает логарифмическое время выполнения этих операций.
 Примеры сбалансированных деревьев включают AVL-деревья и красно-черные деревья. Они автоматически поддерживают балансировку,
 чтобы высота дерева оставалась логарифмической относительно количества элементов, что обеспечивает быструю работу с данными.


 Балансировка дерева. Если вставить в двоичное дерево поиска слишком много узлов, в итоге получится очень высокое
 дерево, где большинство узлов имеют всего один дочерний узел. Например, если последовательно вставлять узлы с
 ключами/значениями, которые всегда больше предыдущих, в итоге получится нечто, похожее на связный список. Однако мы
 можем перестроить узлы в дереве так, что его высота уменьшится. Эта процедура вызывается балансировкой дерева.

 Большинство операций с деревом требует обхода узлов по ссылкам, пока не будет найден конкретный узел. Чем больше высота
 дерева, тем длиннее средний путь между узлами и тем чаще приходится обращаться к памяти. Поэтому важно уменьшать высоту
 деревьев.

 Однако балансировка дерева — дорогостоящая операция, поскольку требует сортировки всех узлов. Если делать балансировку
 после каждой вставки или удаления, операции станут значительно медленнее. Обычно деревья подвергаются этой процедуре
 после нескольких вставок и удалений. Но балансировка от случая к случаю является разумной стратегией только в отношении
 редко изменяемых деревьев.     Идеально сбалансированное дерево имеет минимальную высоту   <-----

 Для эффективной обработки двоичных деревьев, которые изменяются часто, были придуманы сбалансированные двоичные
 деревья(self-balancing binary tree) . Их процедуры вставки или удаления элементов гарантируют,
 что дерево остается сбалансированным.

 Красно-черное дерево (red-black tree) — это хорошо известный пример сбалансированного дерева, которое окрашивает узлы
 красным либо черным цветом в зависимости от стратегии балансировки . Красно-черные деревья часто используются для
 реализации словарей: словарь может подвергаться интенсивной правке, но конкретные ключи в нем по-прежнему будут
 находиться быстро вследствие балансировки.


 AVL-дерево (AVL tree) — это еще один подвид сбалансированных деревьев. Оно требует немного большего времени для вставки
 и удаления элементов, чем красно-черное дерево, но, как правило, обладает лучшим балансом. Это означает, что оно
 позволяет получать элементы быстрее, чем красно-черное дерево. AVL-деревья часто используются для оптимизации
 производительности в сценариях, для которых характерна высокая интенсивность чтения.

 Данные часто хранятся на магнитных дисках, которые считывают их большими блоками. В этих случаях используется
 обобщенное двоичное B-дерево (B-tree). В таких деревьях узлы могут хранить более одного элемента и иметь более двух
 дочерних узлов, что позволяет им эффективно оперировать данными в больших блоках.
 B-деревья обычно используются в системах управления


 Двоичная куча
 Двоичная куча (binary heap) — особый тип двоичного дерева поиска, в котором можно мгновенно найти самый маленький
 (или самый большой) элемент.

 Кучи подчиняются тем же правилам размещения узлов, что и двоичные деревья поиска, но есть одно ограничение:
 родительский узел должен быть больше (либо меньше) обоих своих дочерних узлов


 Граф (graph) аналогичен дереву. Разница состоит в том, что у него нет ни дочерних, ни родительских узлов (вершин) и,
 следовательно, нет корневого узла. Данные свободно организованы в виде узлов (вершин) и дуг (ребер) так, что любой
 узел может иметь произвольное число входящих и исходящих ребер. Это самая гибкая из всех структур, она используется
 для представления почти всех типов данных.

 Например, графы идеальны для социальной сети, где узлы — это люди, а ребра — дружеские связи.


 Смежность — непосредственная близость, примыкание, тесное соприкосновение.
 Матрица смежности — один из способов представления графа в виде матрицы.
 Список смежности — один из способов представления графа в виде коллекции списков вершин. Каждой вершине графа
 соответствует список, состоящий из «соседей» этой вершины.


 Обход Графа - процесс систематического просмотра всех вершин или рёбер графа, чтобы найти некоторые вершины,
 удовлетворяющие определённым условиям.

 Простыми словами, обход графа — это переход от одной его вершины к другой в поисках свойств связей этих вершин. Связи
 (линии, соединяющие вершины) называются направлениями, путями, гранями или ребрами графа. Вершины графа также именуются узлами.

 Какие бывают алгоритмы обхода графа?
 - Поиск в глубину (Depth-First Search, DFS) - находит такой путь от данной вершины, до нужной, что этот путь содержит
   МИНИМАЛЬНУЮ СУММУ ребер графа.
 - Поиск в ширину (Breadth-First Search, BFS) - В результате поиска в ширину находится путь кратчайшей длины в
   невзвешенном графе, т.е. путь, содержащий НАИМЕНЬШЕЕ ЧИСЛО рёбер. Это один из основных алгоритмов на графах.

 Алгоритм Дейкстры — это метод нахождения кратчайших путей от одной вершины графа ко всем остальным.
 Алгоритм работает только для графов без рёбер отрицательного веса.
 В отличие от BFS, для запоминания просматриваемых вершин алгоритм Дейкстры использует очередь с приоритетом.  <----


  --- PyTorch ---

 Что такое файлы .pth?
 Файлы с расширением .pth - это файлы, которые показывают Python в каких директориях искать МОДУЛИ Python
 Если нужно импортировать из не стандартных директорий.
 Довольно часто используются с PyTorch.

 PyTorch - это библиотека машинного обучения. 1) Динамические вычисления. 2) Автоматическое дифференцирование.
 Дифференциация (от лат. differentia — «различие») - разделение, разведение процессов или явлений на составляющие части.

 Динамические вычисления. PyTorch динамически пересчитывает граф вычислений «на лету», и это удобно.

  Автоматическое дифференцирование. Дифференцированием называют нахождение производной функции. Это математическая операция,
 важная для машинного обучения — с помощью дифференцирования как раз пересчитывают веса. Так работает алгоритм обратного
 распространения ошибки, ключевой в обучении нейронных сетей.

 Нейронная сеть или модель для машинного обучения представлена как ГРАФ — структура, состоящая из вершин и связей между ними.
 Это «нейроны» и «синапсы», которые хранят и передают информацию. В ВЕРШИНАХ ХРАНЯТСЯ ФОРМУЛЫ, А НА РЕБРАХ — ВЕСА,
 числовые значения, которые пересчитываются на каждом шаге.


 PyTorch поддерживает автоматическое дифференцирование. Совместно с динамическими вычислениями выходит так, что фреймворк
 строит граф и вычисляет веса «на лету», а человеку достаточно написать пару строк кода. Это удобно и позволяет реализовать
 более сложные проекты.

 Некоторые говорят, что PyTorch — один из самых «питоновских» фреймворков для ML.


  -- Hadoop   Spark --
 Выполняют операции MapReduce
 MapReduce Техника
 В модели программирования MapReduce большой набор данных сначала разбивается на меньшие части.
 После того как все задачи для всех поднаборов решены, мы может объединить результаты.

 MapReduce — это модель обработки больших данных, состоящая из двух этапов:

 1. **Map**: Разделение данных на пары ключ-значение и их предварительная обработка.
 2. **Reduce**: Агрегация промежуточных результатов по ключам для получения финальных результатов.

 Применяется для параллельной обработки и анализа данных


 -- Модели ML --

 Дисперсия в статистике — это мера, которая показывает разброс между результатами. Если все они близки к среднему,
 дисперсия низкая. А если результаты сильно различаются — высокая.

 СИГМА - Среднеквадратическое отклонение.

 СИГМА В отличии от Дисперсии - показывает РЕАЛЬНОЕ СРЕДНЕЕ ЗНАЧЕНИЕ НАШИХ ОТКЛОНЕНИЙ ОТ СРЕДНЕГО ЗНАЧЕНИЯ В ВЫБОРКЕ

 Может ли показатель стандартного отклонения принимать отрицательные значения?
 Не может, стандартное отклонение всегда НЕотрицательное.

 Квартили — делят УПОРЯДОЧЕННЫЕ ДАННЫХ на 4 равные части
 Квартили — это значения, которые делят распределение на 4 равные части одинакового размера.
 Квартили — это специальные случаи квантилей, которые делят набор данных на четыре равные части

 Квантиль — это общее понятие, которое обозначает значение, делящее набор данных на равные части.

 Перцентили — это статистические меры, которые делят набор данных на 100 равных частей.
 Каждый перцентиль показывает, какой процент данных находится ниже или выше определенного значения.

 Перцентили используются для анализа распределения данных, выявления выбросов и сравнения различных наборов данных.


 Случайная величина — это величина, значение которой не фиксировано и может меняться в зависимости от случайных факторов.
 Проще говоря, это число, которое вы получаете в результате случайного эксперимента.

 -- Основные понятия теории вероятностей включают: --

 1. **Случайная величина**:
 - Это функция, которая сопоставляет каждому элементу исходного пространства (набору возможных исходов эксперимента)
 числовое значение. Случайная величина может быть:
 ДИСКРЕТНОЙ (принимает конечное или счётное множество значений)
 НЕПРЕРЫВНОЙ (принимает значения из непрерывного диапазона).

 2. **Распределение случайной величины**:
 - Это описание вероятностей, с которыми случайная величина принимает различные значения. Для дискретных случайных
 величин используется функция вероятностей, а для непрерывных - функция плотности вероятности.

 3. **Вероятность**:
 Это числовая оценка шанса наступления определённого события, которая выражается в диапазоне
 от 0 (невозможное событие) до 1 (достоверное событие).

 4. **Случайные события**:
 - Это подмножества, состоящие из исходов эксперимента. События могут быть независимыми
 (влияние одного на другое отсутствует) или зависимыми (влияние одного события на другое имеет место).

 5. **Ожидание (математическое ожидание)**:
 - Это среднее значение случайной величины, которое рассчитывается как сумма произведений возможных значений на
 соответствующие вероятности (для дискретных случайных величин) или интеграл (для непрерывных).

 6. **Дисперсия**:
- Это мера разброса значений случайной величины относительно её математического ожидания. Она показывает,
 насколько значения случайной величины варьируются.

 7. **Ковариация**:
 - Это мера, отражающая, как две случайные величины изменяются совместно. Она используется для определения направления
 и силы связи между величинами.

 8. **Стандартное отклонение**:
 - Это квадратный корень из дисперсии, который также измеряет разброс значений случайной величины.


 Ковариация и Корреляция: Методы, позволяющие оценить зависимость между двумя случайными величинами.
 Корреляция — это статистическая мера, которая показывает, насколько сильно и в каком направлении связаны между собой две переменные.

 1. **Положительная корреляция** — при увеличении одной переменной другая также увеличивается.
 2. **Отрицательная корреляция** — при увеличении одной переменной другая уменьшается.
 3. **Отсутствие корреляции**    — изменения одной переменной не влияют на другую.

 Корреляция измеряется с помощью коэффициента корреляции, который варьируется от -1 до +1.
 Значение +1 указывает на идеальную положительную корреляцию, -1 — на идеальную отрицательную, а 0 — на отсутствие корреляции.



 -- МАТЕМАТИЧЕСКИЕ КРИТЕРИИ ПРОВЕРКИ СТАТИСТИЧЕСКИХ ГИПОТЕЗ --

 Проверка статистических гипотез — это важный процесс в статистике, который помогает определить, насколько данные соответствуют
 предположенным утверждениям. Ниже приведены основные математические критерии и методы проверки статистических гипотез:

 1. **Критерий Стьюдента (t-критерий)**:
 - Используется для проверки гипотезы о равенстве средних значений в случае небольших выборок (обычно до 30 наблюдений)
  с неизвестной дисперсией.
 - Различают односторонний и двухсторонний t-критерий в зависимости от формулировки гипотез.

 2. **Критерий Z**:
 - Применяется для проверки гипотез о средних значениях, когда выборка большая (больше 30 наблюдений) или известна
   дисперсия генеральной совокупности.
 - Используется также в случаях проверки пропорций.

 3. **Критерий х² (хи-квадрат)**:
 - Используется для проверки гипотез о распределении категориальных данных и для анализа связи между двумя категориальными переменными.
 - Например, используется в контексте таблиц сопряженности.

 4. **ANOVA (дисперсионный анализ)**:
 - Метод, который позволяет проверить равенство средних значений более чем двух групп.
 - Распределение данных проверяется на нормальность и однородность дисперсий.

 5. **Критерий Манна-Уитни (U-критерий)**:
 - Непараметрический тест, который используется для проверки равенства медиан двух независимых выборок.

 6. **Критерий Уилкоксона**:
 - Непараметрический тест для парных выборок, который сравнивает две связанные выборки, ранжируя данные.

 7. **Критерий Фридмана**:
 - Непараметрический метод для анализа различий между несколькими связанными группами, аналог ANOVA для зависимых выборок.

 8. **Корреляционный анализ**:
 - Проверка гипотезы о наличии зависимости между двумя количественными переменными, обычно с использованием
  коэффициента корреляции (например, Пирсона или Спирмена).

 9. **Бутстрэп-методы**:
 - Непараметрические методы, которые включают повторное выборочное измерение для оценки свойств распределений и проверки гипотез.

 Все эти критерии основаны на статистических распределениях и требуют определения уровня значимости (α), который обычно
 устанавливается на уровне 0.05 или 0.01. Результаты теста используются для принятия решения о начале или отклонении нулевой гипотезы.


 Нормальное распределение, также известное как гауссово распределение - Оно описывает, как значения случайной переменной
 распределены вокруг её среднего.

 Генеральная совокупность — это полное множество всех единиц, объектов или наблюдений, которые соответствуют
 определённым критериям или характеристикам и являются предметом исследования.

 Доверительный интервал — это статистический инструмент, используемый для оценки неопределенности параметра генеральной
 совокупности на основе данных выборки. Он указывает диапазон значений, в котором, с заданной вероятностью
 (например, 95% или 99%), может находиться истинное значение параметра.

 Коэффициент детерминации (R^2) - это статистическая мера, которая используется для оценки качества модели регрессии.
 принимает значения от 0 до 1
 R^2 = 0    - Модель не объясняет вариацию зависимой переменной, т.е. предсказания равны среднему значению.
 R^2 = 1    - Модель идеально объясняет вариацию зависимой переменной, т.е. предсказания совпадают с фактическими значениями.
 0 < R^2 < 1 - Указывает процент общей вариации, который объясняется моделью. Например, \( R^2 = 0.75 \) означает,
               что 75% вариации зависимой переменной объясняется независимыми переменными.
 Высокий  R^2 не всегда означает хорошую модель. - Это может быть связано с ПЕРЕОБУЧЕНИЕМ или некорректным выбором модели.

 Нормализация данных - это процесс приведения различных признаков (например, высота, вес, температура) к общему масштабу,
 чтобы они лучше работали в моделях.
 Нормализация помогает справедливо учитывать все признаки, чтобы они не «перекрывали» друг друга из-за разницы в диапазонах значений.
 В общем, нормализация делает данные более «равноправными», что помогает моделям машинного обучения работать лучше и точнее.

 В библиотеке **scikit-learn** (sklearn) представлены несколько методов нормализации данных. Вот основные из них:

 Min-Max Scaling:
 класс MinMaxScaler   - Приводит данные к заданному диапазону, обычно [0, 1].

 Standardization (Z-score Normalization):
 класс StandardScaler - Приводит данные к нулевому среднему значению и единичному стандартному отклонению.

 Robust Scaling:
 класс RobustScaler   - Использует медиану и интерквартильный размах, что делает его устойчивым к выбросам.

 Normalizer:
 класс Normalizer     - Нормализует каждую строку (образец), приводя к единичной  длине. Полезно для текстовых данных
 и задач, где важно направление вектора.

 Power Transformer:
 классы PowerTransformer или QuantileTransformer - Применяет различные степени и логарифмические трансформации для
 приведения данных к более нормальному распределению.

 Квантиль — это значение, которое делит набор данных на заданное количество равных частей.

 Выбросы в машинном обучении (ML) — это аномальные или экстремальные значения, Очень Высокие или Очень Низкие значения

 Временные ряды(динамический ряд) — это наборы данных, где каждая точка данных связана с определенным моментом времени.
 Временной ряд – упорядоченный набор данных.
 TSFresh - это библиотека Python с открытым исходным кодом и мощными функциями извлечения данных временных рядов. <-----

 Regressors:

 -- LightGBM основан на деревьях решений. --
 LightGBM создает деревья решений, которые растут по листам, что означает, что при заданном условии разделяется только
 один лист, в зависимости от усиления. Деревья с большим числом листьев иногда могут переобучаться, особенно с
 небольшими наборами данных. Ограничение глубины дерева(max_depth)  может помочь избежать переобучения.

 LightGBM - Быстрая, высокая производительность для больших наборов данных, использующая метод ГРАДИЕНТНОГО БУСТИНГА на основе гистограммы.
 Catboost                           - ГРАДИЕНТНЫЙ БУСТИНГ на деревьях решений. Много чего из коробки Акцент на КАТЕГОРИАЛЬНЫЕ переменные
 XGBoost: Scikit-learn              - Деревья с ГРАДИЕНТНЫМ УСИЛЕНИЕМ
 LinearRegression:  SKlearn         - подбирает линейную модель с коэффициентами
 RandomForestRegressor:  SKlearn    - использует усреднение для повышения точности прогнозирования и контроля переобучения
 KNN,  KNeighborsRegressor:sklearn  - Метод K-ближайших соседей, Цель прогнозируется путем локальной интерполяции целей
 AdaBoostRegressor: sklearn         - Она принимает обучающие данные и метки в качестве параметров.
 Sarima                             - определяет, как сезонный, так и несезонный компонент
 ARIMA                              - модель авторегрессии скользящего среднего
 Prophet                            - прогнозирования временных рядов, с Учётом (тренд, сезонность, праздники и т.д.)


 Деревья решений - Это структура данных, которая представляет собой последовательность испытаний (разделений) для
 принятия решений на основе признаков (фич) входных данных.
 Деревья решений могут сильно переобучаться, особенно если глубина дерева велика.

 Узлы НЕ Имеющие дочерних узлов - ЛИСТЬЯ ( по аналогии с листьями настоящего дерева)

 Преимущества дерева решений:
 - Интуитивно понятны
 - Обработка как числовых, так и категориальных данных: Деревья решений могут работать с различными типами данных.
 - Не требуют нормализации данных: Деревья не чувствительны к масштабам признаков.

 Недостатки дерева решений:
 - Переобучение: Глубокие деревья могут хорошо подстраиваться под обучающие данные, приводя к плохой обобщающей
  способности на новых данных.
 - Сложность: При сильной разбросанности данных могут возникать большие и сложные деревья, которые трудно интерпретировать.
 - Неустойчивость: Небольшие изменения в данных могут привести к значительным изменениям в структуре дерева.


 -- Случайный лес (Random Forest) --

 - Принцип работы: Случайный лес создаёт множество деревьев решений (обычно с использованием метода бутстрэпа), каждый
   из которых обучается на случайно выбранном подмножестве данных. Затем предсказания всех деревьев объединяются
   (например, с помощью голосования для классификации или усреднения для регрессии) для получения окончательного результата.

 - Обучение: Деревья в случайном лесу обучаются независимо друг от друга, что делает модель параллельной. Это позволяет
   значительно ускорить процесс обучения, особенно на больших наборах данных.

 - Устойчивость к переобучению: Случайный лес обычно устойчив к переобучению благодаря использованию подвыборок данных
   и случайному выбору признаков при каждом разбиении. Это делает его менее чувствительным к шуму в данных.

 - Применение: хорошо работает на различных типах задач и часто используется как "базовая" линия для сравнения с другими моделями.


 -- Градиентный бустинг (Gradient Boosting) GB --

 - Принцип работы: Градиентный бустинг строит деревья последовательно. Каждое новое дерево обучается на ошибках (остатках)
   предыдущих деревьев с целью их коррекции. Это означает, что каждое следующее дерево пытается улучшить предсказания модели.

 - Обучение: Деревья обучаются последовательно, где каждое следующее дерево зависит от предсказаний предыдущих. Этот подход
   делает градиентный бустинг последовательным, что может увеличить время обучения, но позволяет достигать высокой точности.

 - НЕТ Устойчивости к переобучению: Градиентный бустинг может быть более подвержён переобучению, особенно если не
   контролировать гиперпараметры (например, количество деревьев, глубину деревьев и скорость обучения). Тем не менее,
   существуют методы регуляризации, такие как уменьшение скорости обучения, обрезка деревьев и ранняя остановка.

 - Применение: Градиентный бустинг часто используется в соревнованиях по машинному обучению, таких как Kaggle, поскольку
   при правильной настройке он может обеспечивать выдающиеся результаты.


 Градиентный бустинг - в основном являются деревья решений.
 В отличие от случайного леса, градиентный бустинг строит последовательность деревьев, в которой каждое дерево
 пытается исправить ошибки предыдущего.
 AdaBoost выявляет их на основании высоких значений весов, а GB – на основании градиентов функции потерь.

 Градиентный бустинг — это мощный метод ансамблевого обучения, который используется для решения задач регрессии и
 классификации. Он строит модель заранее определённого типа (обычно деревья решений) и последовательно улучшает её,
 добавляя новые модели, которые корректируют ошибки предыдущих.


 Недостатки градиентного бустинга:
 - Время обучения: Обычно обучение моделей градиентного бустинга занимает больше времени по сравнению с другими методами,
 такими как случайный лес.
 - Переобучение
 - Сложность настройки: Необходимость в умении правильно подбирать гиперпараметры для достижения наилучших результатов.


 -- Градиентный бустинг (Gradient Boosting) GB  vs Случайный лес (Random Forest) --

 ### Случайный лес:
 1. **Принцип работы**:               Создаёт множество независимых деревьев решений и объединяет их предсказания.
 2. **Обучение**:                     Параллельное.
 3. **Устойчивость к переобучению**:  Высокая.
 4. **Параметры**:                    Меньше настроек.
 5. **Производительность**:           Быстрое обучение на больших данных.
 6. **Чувствительность к шуму**:      Менее чувствителен к шуму.

 ### Градиентный бустинг:
 1. **Принцип работы**:               Строит деревья последовательно, каждое следующее исправляет ошибки предыдущих.
 2. **Обучение**:                     Последовательное.
 3. **Устойчивость к переобучению**:  Может быть подвержён переобучению (требует контроля гиперпараметров).
 4. **Параметры**:                    Больше настроек (глубина, скорость обучения и др.).
 5. **Производительность**:           Может медленно обучаться.
 6. **Чувствительность к шуму**:      Более чувствителен к шуму при настройке.

 В контексте машинного обучения **ШУМ** относится к случайным или нерелевантным данным, которые могут искажать или
 усложнять процесс обучения модели. Шум может привести к снижению точности и ухудшению способности модели к обобщению.
 ШУМ - ПЛОХИЕ ДАННЫЕ (например, опечатки в текстовых данных, ошибки измерений и т. д.).

 Главное чтобы не было ПЕРЕОБУЧЕНИЯ!
 Это когда модель настолько хорошо обучена на обучающей выборке данных, что она становится слишком специфичной и в
 результате плохо применимой для НОВЫХ данных.

 «Scikit» и «SkLearn» — это одно и то же!

 Настройка Гиперпараметров:
   - sklearn: GridSearch, RandomizedSearch, BayesSearch.
   - Optuna

   Важные параметры:
   num_leaves - основной параметр, управляющий сложностью модели дерева
   num_iteration - число деревьев (псевдоним n_estimators)
   learning_rate - скорость обучения.
   early_stopping_round - число, определяющее условие останова - если алгоритм не улучшается на протяжении данного количества итераций.
   num_threads/n_jobs - число потоков для обучения.
   max_depth - максимальная разрешенная для деревьев глубина.

 АНСАМБЛЬ МОДЕЛЕЙ/Stacking
 Метод машинного обучения, где несколько моделей обучаются для решения одной и той же проблемы и объединяются
 для получения ЛУЧШИХ результатов

 A/B-тестирование — это метод сравнения двух версий продукта (например, веб-страницы, приложения или рекламного объявления)
 для определения, какая из них лучше работает по заданным метрикам. В процессе тестирования пользователи случайным
 образом делятся на две группы: одной группе показывается версия A, а другой — версия B. Сравниваются результаты
 (например, конверсия, клики, время на странице) для того, чтобы выявить наиболее эффективный вариант. Этот метод
 позволяет принимать обоснованные решения на основе данных и улучшать пользовательский опыт.


 -- NLP --
 Обработка естественного языка (NLP, Natural Language Processing) – это технология машинного обучения, которая дает
 компьютерам возможность интерпретировать, манипулировать и понимать человеческий язык.

 import nltk
 NLTK – это библиотека Python, которую можно использовать в любом приложении для обработки естественного языка - NLP.

 NLTK предлагает удобные инструменты для множества задач NLP: токенизация, стемминг, лемматизация, морфологический
 и синтаксический анализ, а также анализ настроений.

 Перед созданием любого приложения, основанного на обработке естественного языка, нам необходимо обработать данные,
 которые мы используем. Ниже приведены некоторые шаги, которые всегда необходимы при создании приложения NLP:
 Анализ текстовых данных с помощью NLTK и Python:

 - Токенизация: разделение фрагмента текста на токены или слова называется токенизацией.
 Токенизация — это процесс разбиения текста на более мелкие части

 - Удаление стоп-слов: стоп-слова – это самые распространенные слова в любом языке. Нет правильного определения
 игнорируемых слов, вы можете думать об этих словах как о словах, которые используются для создания значимого предложения.
 Например, такие слова, как «the», «is», «a», «as», представляют собой определенный тип стоп-слов, которые необходимо
 удалить из текстовых данных, которые вы используете, иначе это может повлиять на производительность вашей модели.

 - Стемминг — это процесс сведения слов к их основной (корневой) форме, удаляя окончания и суффиксы.
 Это помогает уменьшить сложность текста и улучшить производительность алгоритмов анализа.

 - Лемматизация - В отличие от стемминга, лемматизация сводит слова к их лемме — это более сложный процесс,
 который учитывает морфологический анализ слов. Лемматизация более точно обрабатывает слова, приводя их к словарной форме.

 - Анализ настроений (или сентимент-анализ) в NLTK часто сводится к классификации текста на ПОЗИТИВНЫЙ или НЕГАТИВНЫЙ.



 -- Рекомендательные системы --
 Система рекомендаций — это тип системы машинного обучения, которая предоставляет пользователям персонализированные
 рекомендации на основе их прошлого поведения, предпочтений и шаблонов .


 Для разработки рекомендательных систем используют различные библиотеки и инструменты,
 в зависимости от подхода и архитектуры системы. Вот некоторые из наиболее популярных библиотек и инструментов:

 1. **Scikit-learn** — библиотека для машинного обучения на Python, которая предоставляет множество алгоритмов для
 построения моделей, включая методы, используемые для рекомендаций.

 2. **Surprise** — специализированная библиотека для построения рекомендательных систем. Она поддерживает различные
 алгоритмы, включая матричную факторизацию и коллаборативную фильтрацию.

 3. **LightFM** — библиотека для построения гибридных рекомендательных систем, которая объединяет коллаборативную
 фильтрацию и контентную фильтрацию.

 4. **TensorFlow и Keras** — библиотеки для глубокого обучения, которые можно использовать для создания сложных моделей
 рекомендательных систем (например, нейронные сети).

 5. **PyTorch** — еще одна популярная библиотека для глубокого обучения, используемая для построения рекомендательных
 систем и реализации различных архитектур.

 LightFM — это библиотека для Python, которая поддерживает гибридные рекомендательные системы

 Ранжирование – это сортировка сайтов поисковыми системами в результатах выдачи по соответствующему поисковому запросу.


 -- LLM (Large Language Model) --
 LLM (Large Language Model) — это большие языковые модели, разработанные для обработки и генерации текста на основе
 обширного обучения на текстовых данных. Они могут выполнять задачи, такие как ответ на вопросы, написание текстов,
 перевод, способны понимать и генерировать человеческий язык и многие другие. Они используют архитектуру трансформеров
 для обработки контекста и связи между словами.

 Чтобы использовать LLM:

 1. **Выбор модели**: Определите, какая модель подходит для вашей задачи (например, GPT, BERT и т.д.).
 2. **Доступ к API**: Многие модели доступны через API (например, OpenAI API для GPT).
 3. **Интеграция**: Встраивайте модель в свои приложения для генерации текста или обработки данных.
 4. **Настройка**: При необходимости настраивайте модель под специфические задачи или данные.
 5. **Чат-боты**: Ответы на вопросы и поддержка диaлoгов в реальном времени.
 6. **Перевод**: Автоматический перевод текстов с одного языка на другой.
 7. **Обработка естественного языка (NLP)**: Текстовая классификация, анализ настроений и извлечение информации.

 Использование LLM может значительно улучшить качество обработки языка в ваших проектах.



 CI/CD — это важный элемент современных подходов к разработке программного обеспечения,
 обеспечивающий эффективное и качественное сопровождение проектов.

 CI — это практика, связанная с регулярной интеграцией изменений кода в общий репозиторий.
 CD — это практика, которая подразумевает, что код, который прошел этапы тестирования в CI, готов к развертыванию на любой среде

 CI/CD объединяет разработку, тестирование и развёртывание приложений.

 Инструменты для CI/CD:
 Существует множество инструментов, которые поддерживают CI/CD процессы, включая:
 - Jenkins
 - GitLab CI/CD
 - Travis CI
 - CircleCI
 - GitHub Actions
 - Azure DevOps и другие.


 Legacy проект в контексте разработки программного обеспечения — это проект, который был создан с использованием
 УСТАРЕВШИХ ТЕХНОЛОГИЙ, БИБЛИОТЕК или ПОДХОДОВ.

 Пайплайн (или PIPELINE) — это концепция, используемая в разных областях, таких как разработка программного обеспечения,
 машинное обучение и обработка данных. Основная идея пайплайна заключается в том, что он представляет собой
 последовательность шагов, через которые проходят данные или процесс для достижения конечного результата.

 UI (User Interface) — это пользовательский интерфейс.

 Файлы cookie — это небольшие текстовые файлы, которые веб-сайты сохраняют на вашем устройстве при посещении. Они нужны для:

 1. **Хранения пользовательских настроек** — запоминают предпочтения пользователя, такие как язык или настройки оформления.
 2. **Аутентификации** — помогают идентифицировать пользователей и сохранять сессии входа.
 3. **Аналитики** — собирают информацию о том, как пользователи взаимодействуют с сайтом, что помогает улучшить его.
 4. **Персонализации контента** — позволяют предлагать пользователям релевантные рекомендации и рекламу.

 В целом, файлы cookie делают веб-серфинг более удобным и персонализированным.


 1. **Разработка программного обеспечения**: В CI/CD (непрерывная интеграция и непрерывное развертывание) пайплайн
 представляет собой автоматизированный процесс, который проходит через несколько этапов — от написания кода и
 тестирования до развертывания на сервере.

 2. **Машинное обучение**: В этой области пайплайн состоит из последовательности этапов, таких как подготовка данных,
 выбор признаков, обучение модели и оценка ее производительности. Каждая из этих стадий последовательно передает
 свои результаты следующей.

 3. **Обработка данных**: В системах обработки данных, таких как ETL (извлечение, преобразование, загрузка), пайплайн
 описывает процесс перемещения и преобразования данных из одной системы в другую.

 Основная цель пайплайнов — автоматизировать и стандартизировать процессы, чтобы сделать их более надежными и эффективными.


 PuTTY — это терминальный эмулятор, позволяющий устанавливать SSH-соединение с удаленными серверами.
 SSH (Secure Shell) С С АШ — это протокол для безопасного удаленного доступа к другим компьютерам, позволяющий шифровать
 данные и обеспечивать защищенное подключение.

 -- Через терминал подключение по SSH к СЕРВЕРУ --
 SSH tech_deleloper14@tvlds-0126      # По ХОСТУ
 SSH tech_deleloper14@10.10.207.111   # По ПРОКСИ

 Активировать venv  (Виртуальное окружение)        .\venv\Scripts\activate
 ДЕ-Активировать venv  (Виртуальное окружение)     .\venv\Scripts\deactivate
 Чтобы деактивировать активное виртуальное окружение, просто введите команду deactivate в командной строке.   <-----


 Notepad++
 Plugins -> JsonFormat                  # Чтобы преобразовать в удобный формат
 Plugins -> JsonFormat -> JsonViewer    # Чтобы добавить слева табличку

 SyntaxWarning: invalid escape sequence - ошибка связанная с многострочной строкой используем r перед строкой r''' '''


 Авторы книг: Марк Лутц, Дасти Филлипс, Дэвид Бизли, Кент Бек, Лучано Рамальо, Мартин Фаулер, Ден Бейдер, Фаулер Мэттью,
 Фило Владстон Феррейра

 Можно почитать:  Эндрю Таненбаум



































































"""