
### Нас невозможно сбить с пути, нам все равно, куда идти :)

### TODO Фишка: если сразу подсматривать — мозг “не тренируется”. Поэтому сначала ВСЛУХ из памяти, а потом проверка.  <-----

### ЧИТАЕШЬ вопрос ВСЛУХ

# Пауза 2–5 сек
# Отвечаешь вслух своими словами (по памяти)
# Сверяешься с ответом и если что — повторяешь ответ ещё раз вслух уже правильно
### Важно: НЕ просто прочитать готовый ответ, а попробовать сначала сказать самому — так запоминается быстрее.
### Чтение вслух = хорошо. Ответ по памяти вслух = ещё лучше.


### TODO ЗАМЕТКИ ALL_EVERY_DAY





# НОВЫЕ!!! Замеры размеров структур Python  ПРОСТО ПОСМОТРЕТЬ!!!   РАЗМЕР СТАНОВИТСЯ МЕНЬШЕ С КАЖДОЙ НОВОЙ ВЕРСИЕЙ!
"""
import sys
from pympler import asizeof

                            # -- Сравнение slots vs no_slots --                                  <-----
                            # -- @dataclass(slots=True)  vs  @dataclass() --
                            
from dataclasses import dataclass

@dataclass(slots=True)
class WithSlots:
    value: int

with_slots = WithSlots(10)
print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  40 байт
print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  72 байт


@dataclass
class NoSlots:
    value: int

no_slots = NoSlots(10)
print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    48 байт
print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    424 байт


                            -- Сравнение slots vs no_slots --                                  <-----
                            -- @dataclass(slots=True)  vs  @dataclass() --
                            
from dataclasses import dataclass

@dataclass(slots=True)
class WithSlots:pass

with_slots = WithSlots()
print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  32 байт
print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  32 байт


@dataclass
class NoSlots:pass

no_slots = NoSlots()
print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    48 байт
print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    344 байт


                            -- Обычные классы По размеру тоже самое что @dataclass(slots=True)  vs  @dataclass() --
                            -- Сравнение slots vs no_slots --
                            
class WithSlots:__slots__ = ()

with_slots = WithSlots()
print(f'getsizeof WithSlots:  {sys.getsizeof(with_slots)} байт')    # -> getsizeof WithSlots:  32 байт
print(f'asizeof   WithSlots:  {asizeof.asizeof(with_slots)} байт')  # -> asizeof   WithSlots:  32 байт


class NoSlots:pass

no_slots = NoSlots()
print(f'getsizeof NoSlots:    {sys.getsizeof(no_slots)} байт')      # -> getsizeof NoSlots:    48 байт
print(f'asizeof   NoSlots:    {asizeof.asizeof(no_slots)} байт')    # -> asizeof   NoSlots:    344 байт



 --- Замеры ПУСТЫХ обьектов встроенных Python ---
 
                            -- Примеры list vs [] vs deque() vs heapq --                       <-----
                            
my_list = list()
print(f'getsizeof list():      {sys.getsizeof(my_list)} байт')    # -> getsizeof list():       56 байт
print(f'asizeof   list():      {asizeof.asizeof(my_list)} байт')  # -> asizeof   list():       56 байт
 
my_list = []
print(f'getsizeof []:          {sys.getsizeof(my_list)} байт')    # -> getsizeof []:           56 байт
print(f'asizeof   []:          {asizeof.asizeof(my_list)} байт')  # -> asizeof   []:           56 байт


from collections import deque

my_deque = deque()
print(f'getsizeof deque():     {sys.getsizeof(my_deque)} байт')    # -> getsizeof deque():     760 байт
print(f'asizeof   deque():     {asizeof.asizeof(my_deque)} байт')  # -> asizeof   deque():     760 байт


import heapq

my_heapq = []
heapq.heapify(my_heapq)
print(f'getsizeof heapq:       {sys.getsizeof(my_heapq)} байт')    # -> getsizeof heapq:       56 байт
print(f'asizeof   heapq:       {asizeof.asizeof(my_heapq)} байт')  # -> asizeof   heapq:       56 байт



                            -- Примеры set() vs frozenset() --                                 <-----


my_set = set()
print(f'getsizeof set():       {sys.getsizeof(my_set)} байт')    # -> getsizeof set():         216 байт
print(f'asizeof   set():       {asizeof.asizeof(my_set)} байт')  # -> asizeof   set():         216 байт
  
my_set = frozenset()
print(f'getsizeof frozenset(): {sys.getsizeof(my_set)} байт')    # -> getsizeof frozenset():   216 байт
print(f'asizeof   frozenset(): {asizeof.asizeof(my_set)} байт')  # -> asizeof   frozenset():   216 байт



                            -- Примеры tuple() vs namedtuple() vs () --                        <-----


my_tuple = tuple()
print(f'getsizeof tuple():    {sys.getsizeof(my_tuple)} байт')    # -> getsizeof tuple():      40 байт
print(f'asizeof   tuple():    {asizeof.asizeof(my_tuple)} байт')  # -> asizeof   tuple():      40 байт
print(f'asizeof   ():         {asizeof.asizeof(())} байт')        # -> asizeof   ():           40 байт


from collections import namedtuple

my_tuple = namedtuple('C', '')
nt_tuple = my_tuple()
print(f'getsizeof namedtuple: {sys.getsizeof(nt_tuple)} байт')    # -> getsizeof namedtuple:   40 байт
print(f'asizeof   namedtuple: {asizeof.asizeof(nt_tuple)} байт')  # -> asizeof   namedtuple:   40 байт



                            -- Примеры dict() vs {} vs OrderedDict() vs defaultdict() vs ChainMap() --        <-----
my_dict = dict()
print(f'getsizeof dict():       {sys.getsizeof(my_dict)} байт')      # -> getsizeof dict():       64 байт
print(f'asizeof   dict():       {asizeof.asizeof(my_dict)} байт')    # -> asizeof   dict():       64 байт


my_dict = {}
print(f'getsizeof {{}}:         {sys.getsizeof(my_dict)} байт')      # -> getsizeof {}:           64 байт
print(f'asizeof   {{}}:         {asizeof.asizeof(my_dict)} байт')    # -> asizeof   {}:           64 байт


from collections import OrderedDict
 
my_OrDt = OrderedDict()
print(f'getsizeof OrderedDict:  {sys.getsizeof(my_OrDt)} байт')      # -> getsizeof OrderedDict:  128 байт
print(f'asizeof   OrderedDict:  {asizeof.asizeof(my_OrDt)} байт')    # -> asizeof   OrderedDict:  128 байт

                            -- Примеры int() float() complex() True False str() range(0) bytes() bytearray() vs None -- 
                            -- И пустые объекты Тоже самое 0  ''  0.0  0j  b""  bytearray(b"")  object()  --     <-----
                            
my_int = int()
print(f'getsizeof int():  {sys.getsizeof(my_int)} байт')            # -> getsizeof int():      28 байт
print(f'asizeof   int():  {asizeof.asizeof(my_int)} байт')          # -> asizeof   int():      32 байт
print(f'asizeof   0:      {asizeof.asizeof(0)} байт')               # -> asizeof   0:          32 байт


my_float = float()
print(f'getsizeof float():  {sys.getsizeof(my_float)} байт')        # -> getsizeof float():    24 байт
print(f'asizeof   float():  {asizeof.asizeof(my_float)} байт')      # -> asizeof   float():    24 байт
print(f'asizeof   0.0:      {asizeof.asizeof(my_float)} байт')      # -> asizeof   0.0:        24 байт


my_comp = complex()
print(f'getsizeof complex():  {sys.getsizeof(my_comp)} байт')       # -> getsizeof complex():  32 байт
print(f'asizeof   complex():  {asizeof.asizeof(my_comp)} байт')     # -> asizeof   complex():  32 байт
print(f'asizeof   0j:         {asizeof.asizeof(my_comp)} байт')     # -> asizeof   0j:         32 байт


# True
print(f'getsizeof True:  {sys.getsizeof(True)} байт')               # -> getsizeof True:       28 байт
print(f'asizeof   True:  {asizeof.asizeof(True)} байт')             # -> asizeof   True:       32 байт


# False
print(f'getsizeof False:  {sys.getsizeof(False)} байт')             # -> getsizeof False:      28 байт
print(f'asizeof   False:  {asizeof.asizeof(False)} байт')           # -> asizeof   False:      32 байт


# None  занимает фиксированное количество памяти!!!        Один из самых маленьких объектов по памяти!!!      <-----
print(f'getsizeof None:  {sys.getsizeof(None)} байт')               # -> getsizeof None:       16 байт
print(f'asizeof   None:  {asizeof.asizeof(None)} байт')             # -> asizeof   None:       16 байт


my_str = str()                                                                                           ### БЫЛО!!! <-----
print(f'getsizeof str():  {sys.getsizeof(my_str)} байт')            # -> getsizeof str():      41 байт   # 49 байт
print(f'asizeof   str():  {asizeof.asizeof(my_str)} байт')          # -> asizeof   str():      48 байт   # 56 байт
print(f'asizeof   "":     {asizeof.asizeof("")} байт')              # -> asizeof   "":         48 байт   # 56 байт


my_range = range(0)
print(f'getsizeof range(0):  {sys.getsizeof(my_range)} байт')       # -> getsizeof range(0):   48 байт
print(f'asizeof   range(0):  {asizeof.asizeof(my_range)} байт')     # -> asizeof   range(0):   48 байт


my_bytes = bytes()
print(f'getsizeof bytes():  {sys.getsizeof(my_bytes)} байт')        # -> getsizeof bytes():    33 байт
print(f'asizeof   bytes():  {asizeof.asizeof(my_bytes)} байт')      # -> asizeof   bytes():    40 байт
print(f'asizeof   b"":      {asizeof.asizeof(b"")} байт')           # -> asizeof   b"":        40 байт


my_b_arr = bytearray()
print(f'getsizeof bytearray():  {sys.getsizeof(my_b_arr)} байт')    # -> getsizeof bytearray():              56 байт
print(f'asizeof   bytearray():  {asizeof.asizeof(my_b_arr)} байт')  # -> asizeof   bytearray():              56 байт
print(f'asizeof   bytearray(b""):  {asizeof.asizeof(bytearray(b""))} байт')  # -> asizeof   bytearray(b""):  56 байт


my_object = object()
print(f'getsizeof object():  {sys.getsizeof(my_object)} байт')      # -> getsizeof object():   16 байт
print(f'asizeof   object():  {asizeof.asizeof(my_object)} байт')    # -> asizeof   object():   0 байт

object() возвращает 0 байт, потому что функция `asizeof` не находит вложенных объектов или атрибутов для учета,
так как `object` не содержит информации.  Сам по себе object() - весит 16 байт
object() - является базовым пустым объектом без дополнительных атрибутов или содержимого.

"""



# # type - это тип всех типов, для которых не указан явно иной метакласс
"""
# type - это тип всех типов, для которых не указан явно иной метакласс
print(type(type))   # -> <class 'type'>
print(type(object)) # -> <class 'type'>
print(type(list))   # -> <class 'type'>
print(type(set))    # -> <class 'type'>
print(type(dict))   # -> <class 'type'>
print(type(bool))   # -> <class 'type'>
print(type(int))    # -> <class 'type'>
print(type(str))    # -> <class 'type'>
print(type(collections.deque))  # -> <class 'type'>
"""


# -- КОНКУРЕНТНОСТЬ vs ПАРАЛЛЕЛИЗМ --
"""
В случае КОНКУРЕНТНОСТИ несколько задач работают в течении одного промежутка времени, но только одна активна в каждый
момент.
В случае ПАРАЛЛЕЛИЗМА несколько задач активно одновременно.

В случае КОНКУРЕНТНОСТИ мы переключаемся между двумя приложениями.
В случае ПАРАЛЛЕЛИЗМА мы активно выполняем два приложения одновременно.


Примеры I/O-bound задач в Python:
**Загрузка данных из веб-API**:            - ожидание ответа от сервера.
**Чтение и запись файлов**:                - работа с файлами на диске
**Взаимодействие с базами данных**:        - выполнение запросов и получение данных.
**Отправка и получение данных по сети**:   - работа с сокетами и сетевыми протоколами.

 Для I/O-bound задач особенно эффективны многопоточность и асинхронное программирование, так как они позволяют
выполнять другие задачи, пока происходит ожидание.

"""


# -- Встроенные библиотеки Python для конкурентности --
"""
- threading          – потоки (I/O-bound, GIL).
- multiprocessing    – процессы (CPU-bound).
- asyncio            – асинхронность (I/O-bound, корутины).
- concurrent.futures – пулы потоков/процессов (ThreadPoolExecutor, ProcessPoolExecutor).
- queue              – очереди для обмена данными.

GIL не делает Python потокобезопасным на 100% (только для байт-кода).
GIL не гарантирует 100% потокобезопасности в Python, но он защищает операции с байт-кодом и объектами Python от
самых опасных race conditions.
"""


# --- Многопоточность (multithreading) ---
"""
multithreading - многопоточность, подходит для IO-bound задач, использует ОС, страдает от GIL(важно помнить)
Полезно для ускорения выполнения задач для того, чтобы текущий поток занялся другой задачей
Любая программа это минимум один процесс и один поток
Полезно использовать daemon=True (чтобы остальные потоки не висели), Queue(очереди), pool executor,
НО в любом случае все зависит от программиста!

Плюсы:
+ просто(сравнительно)
+ быстро
+ не умирает из-за одного(!)

Минусы:
- потребление ресурсов(ОС)
- неуправляемость(старт, приостановка, переключение)
- проблема потоков(гонка, блокировки)


Race condition (состояние гонки) - это ситуация, при которой несколько ПОТОКОВ (ИЛИ ПРОЦЕССОВ) одновременно пытаются
выполнить операции чтения или записи к общим ресурсам без должной синхронизации.

Состояние гонки могут возникать, когда два потока одновременно обращаются к одному обьекту Python

Если два потока одновременно увеличивают счетчик ссылок, то может случиться, что счетчик обнулится, хотя обьект еще
используется.

Состояние гонки, возникающее, когда два потока одновременно пытаются увеличить счетчик ссылок.


В чем отличие тредов от мультипроцессинга?
Главное отличие в разделении памяти. Процессы независимы друг от друга, имеют раздельные адресные пространства,
идентификаторы, ресурсы. Треды исполняются в совместном адресном порстранстве,
имеют общий доступ к памяти, переменным, загруженным модулям.
"""



# -- Потоки (multithreading) vs Процессы (multiprocessing) --
"""
Потоки   - лучше для задач, где много ожидания (сеть, файлы, базы данных).
Процессы - лучше для тяжелых вычислений (математика, обработка данных).

Потоки (threads) в операционных системах — это механизм, который позволяет выполнять несколько задач в рамках
ОДНОГО процесса, разделяя его ресурсы (память, файлы и т. д.).

Ключевые особенности:

- Потоки легче процессов, создаются быстрее.
- Все потоки процесса используют общее адресное пространство.
- Переключение между потоками требует меньше затрат, чем между процессами.
- Подходят для I/O-задач, но могут быть неэффективны для CPU-задач из-за GIL (в Python).

В отличие от процессов (multiprocessing):

- Процессы изолированы, имеют отдельную память.
- Подходят для CPU-интенсивных задач (особенно в Python).
- Создание и управление процессами требует больше ресурсов.

Потоки - для параллельных I/O-операций в одном процессе, процессы — для истинного параллелизма на многоядерных CPU.


Критерий	        | Потоки (Threads)	                     | Процессы (Processes)            |
-------------------|----------------------------------------|---------------------------------|
Общая память	    | Да (разделяют)	                     | Нет (изолированы)               |
Скорость создания	| Быстро	                             | Медленнее                       |
Переключение	    | Дешевле	                             | Дороже                          |
Параллелизм	    | Псевдопараллелизм (из-за GIL в Python) | Истинный параллелизм            |
Использование	    | I/O-bound задачи (сеть, диски)	     | CPU-bound задачи (вычисления)   |
-------------------|----------------------------------------|---------------------------------|

Дополнение про Python:
- GIL (Global Interpreter Lock) в CPython мешает потокам выполняться по-настоящему параллельно на многоядерных CPU.
- Multiprocessing обходит GIL, создавая отдельные интерпретаторы Python для каждого процесса.
"""


# -- Lock vs Semaphore --
"""
Lock (блокировка):
- Позволяет одному потоку выполнять критическую секцию.
- Как mutex — владелец один, другие ждут.

Semaphore (семафор):
- Ограничивает количество потоков в критической секции.
- Например, 3 потока могут работать одновременно.

Коротко:
Lock — 1 поток, Semaphore — N потоков.

Когда что использовать?
- Lock - когда нужно гарантировать эксклюзивный доступ к ресурсу (только один поток)
- Semaphore - когда нужно разрешить доступ ограниченному числу потоков одновременно
"""


# КРИТИЧЕСКИЕ СЕКЦИИ
"""
КРИТИЧЕСКИЕ СЕКЦИИ – участки кода, где доступ к общим ресурсам защищается блокировками (например, threading.Lock),
чтобы избежать состояний гонки.

Байткод и GIL
Каждый поток поочередно получает GIL для выполнения байткода.

GIL не защищает от race conditions – нужны критические секции.
КРИТИЧЕСКИЕ СЕКЦИИ – ручное управление доступом к данным.       <-----
"""


#  --- multiprocessing (многопроцессорность) ---
"""
multiprocessing - использование нескольких ядер
Породить процессы с помощью multiprocessing.Pool. Число, которое будет передано в Pool(),
будет равно числу порожденных процессов
import multiprocessing, from multiprocessing import Process, Pool

--- Multiprocessing выдуман чтобы побороть GIL! Как его побороть? - Сделать много GIL каждый из которых независимый ---
--- Multiprocessing обходит GIL, создавая отдельные процессы, у каждого СВОЙ ИНТЕРПРЕТАТОР И СВОЙ GIL. <---- ВАЖНО
--- Multiprocessing работает только с обьектами которые поддерживают сериализацию через pickle
Но не все объекты можно сериализовать (например, файловые дескрипторы, соединения с БД и т. д.).
В других ЯП: параллельность вычислений достигается с помощью 2-х средств - ПОТОКИ или asyncio. Потому что у них нет GIL
Multiprocessing - позволяет решать любые задачи (IO-bound или CPU-bound)
Ускорение любые задачи, распараллеливая их на ядра процессора (лишь до определенного предела, закон Амдала)
закон Амдала - с какого-то момента бессмысленно добавлять дополнительных рабочих(ЦПУ, доп.ядра) быстрее не станет
Ускорение не идеально и возможно только до определенного предела, смотрим закон Амдала.
Создает несколько процессов, у каждого из которых своя память и свой GIL, каждый выполняет свою задачу, взаимодействие
между ними требует pickle(сериализация, превращение обьектов python в байты и наоборот байтов в обьекты python)
API принципиально похоже на многопоточность, выгодно использовать Pool, а для взаимодействия между процессами Queue и Pipe
Pipe - когда нужно 2 процесса между собой подружить чтобы они передавали друг другу данные.

закон Амдала: ускорение упирается в ту часть работы, которую нельзя распараллелить.
Закон Амдала: ускорение ограничено частью кода, которую нельзя распараллелить.
Пример: если 20% работы всегда последовательно, то максимум ускорения ≈ 1 / 0.2 = 5×, хоть 100 ядер.


Плюсы:
+ реальная параллельность любых задач
+ не умирает из-за одного(!)
+ процессы не зависят друг от друга(у каждого процесса своя память и GIL)
Минусы:
- потребление ресурсов (памяти, процессора, времени)
- необходимость сериализации в pickle
- проблемы синхронизации (взаимодействие между процессами)


# конструкция if __name__ == "__main__": критически важна не только для multiprocessing, но и для
# concurrent.futures.ProcessPoolExecutor, поскольку он тоже использует механизм порождения процессов (аналогично multiprocessing).

# if __name__ == "__main__": для защиты от рекурсии.


-- ЦЕЛЬ СЕРИАЛИЗАЦИИ с помощью pickle в multiprocessing --

ЦЕЛЬ СЕРИАЛИЗАЦИИ с помощью pickle в multiprocessing заключается в том, чтобы преобразовать объекты Python в формат,
который можно сохранить в файл или передать между процессами. Это необходимо, поскольку каждый процесс в
multiprocessing имеет своё собственное пространство памяти. Сериализация позволяет копировать данные и передавать их,
обеспечивая тем самым возможность взаимодействия между процессами.
"""


# --- Основы асинхронного программирования в Python ---
"""
Асинхронное программирование позволяет запускать операции параллельно, НЕ дожидаясь выполнения последовательности.

Кроме того, в асинхронном коде используются конструкции await и async with,
которые позволяют приостановить выполнение текущей задачи и переключиться на другую,
пока выполняется асинхронная операция. Это позволяет эффективно использовать ресурсы процессора и снизить время ожидания.

--- asyncio ---
asyncio - Асинхронное выполнение, подходит для IO-bound задач, работает ровно 1 поток - использует Конкурентность по полной
Плюсы:
+ скорость и экономия времени, вместо x + y + z -> max(x, y, z)
+ управляемость
+ меньше потребление ресурсов (в сравнении с потоками)
Минусы:
- "умирает" из-за одного блокирующего вызова (!)
- event loop НЕ безразмерный, нужно понимать, что корутины не бесплатные

1) корутина работает как генератор
2) async - явный флаг, что данная функция является асинхронной (корутиной)
3) await - явный флаг, что в этом месте функция встает на паузу и дает работать другим, пока ждёт свои данные
4) event loop - цикл событий, механизм, который отвечает за планирование и запуск корутин. Можно представить как
список/очередь, из которого в вечном цикле достаются и запускаются корутины
Частые ошибки:
- не использование await внутри корутины
- создание корутины, но использование ее, как функции
- использование внутри корутин синхронного(блокирующего) кода, в том числе IO


asyncio.sleep(0) приостанавливает выполнение текущей корутины, позволяя циклу событий выполнить другие задачи.
Это полезно для перераспределения управления между корутинами без фактической задержки.
"""


#  -- Как работает Event Loop в asyncio --
"""
Event Loop — Как лифт без маршрута:
Не едет по порядку (1 → 2 → 3...), а реагирует на события (как кнопки вызова в лифте)

- Едет по этажам (задачам) циклически, проверяет:
- Готово? (например, данные пришли или таймер сработал) → выполняет кусок кода.
- Не готово? (ждёт I/O, sleep и т.д.)                   → пропускает и едет дальше.
- РАБОТАЕТ пока есть активные задачи

Где "живут" задачи?
- В очереди событий (Event Queue), которую Event Loop постоянно проверяет.

Ключевая фишка:
- Не блокирует поток на ожидании I/O. Пока одна задача "спит" (await), Event Loop выполняет другие.
"""


#  -- Синхронные задачи в EVENT LOOP --
"""
Если в asyncio попадает синхронная (блокирующая) задача (например, CPU-задача или расчеты), то она блокирует Event Loop,
пока не завершится. Это значит, что другие асинхронные задачи не смогут выполняться в это время, и весь цикл событий "зависнет".

Почему так???
Event Loop блокируется, потому что синхронный код не отдаёт управление.

Event Loop в asyncio работает в одном потоке и выполняет задачи по очереди. Если попадает синхронная (блокирующая) задача,
она не отдаёт управление обратно в цикл событий, пока не завершится. В результате другие асинхронные задачи просто НЕ
получают шанса запуститься, пока она не закончит работу
"""


#  -- Когда Потоки (threads) могут быть лучше asyncio --
"""
Потоки лучше asyncio, когда:

- Много блокирующего I/O (например, requests, файлы, БД без async).
- CPU-bound задачи (тяжёлые вычисления → ThreadPoolExecutor).
- Очень много задач (десятки тысяч → event loop тормозит).
- Код синхронный и переписывать сложно.
- Библиотеки без async (например, pandas, numpy).

asyncio лучше, когда:

- Чистый I/O (веб-запросы, асинхронные БД).
- Тысячи соединений (потоки жрут память).
- Код уже async.
- Комбинируй: asyncio + run_in_executor() для CPU-bound.
"""


#  -- asyncio.gather vs asyncio.TaskGroup --
"""
gather – просто запускает корутины параллельно. Если одна упадет, остальные работают дальше.

TaskGroup (Python 3.11+) – отменяет все задачи, если хоть одна сломалась. Удобнее для сложных сценариев.

Пример:
# gather (просто)
await asyncio.gather(task1(), task2())

# TaskGroup (надёжнее)
async with asyncio.TaskGroup() as tg:
    tg.create_task(task1())
    tg.create_task(task2())

Выбор:

gather – если не важно падение отдельных задач.
TaskGroup – если нужна автоматическая отмена при ошибках.
"""



# Идемпотентные/НЕ Идемпотентные  HTTP-МЕТОДЫ
"""

ИДЕМПОТЕНТНЫЕ запросы приводят к одному и тому же состоянию системы НЕЗАВИСИМО от количества вызовов,
а НЕ ИДЕМПОТЕНТНЫЕ могут изменять состояние при каждом вызове.

ИДЕМПОТЕНТНЫЕ:

GET, HEAD, PUT, DELETE, OPTIONS, TRACE — повторный запрос не меняет итоговое состояние ресурса.

НЕИДЕМПОТЕНТНЫЕ (обычно):

POST — повтор часто создаёт новые ресурсы/действия.

ЗАВИСИТ ОТ РЕАЛИЗАЦИИ:

PATCH — может быть идемпотентным или нет (например: “set status=done” — да, “+1 к счётчику” — нет).


GET     - получение ресурса.
POST    - создание ресурса или выполнение операции.                      НЕ ИДЕМПОТЕНТЕН
PUT     - полное обновление ресурса (или создание, если не существует).
PATCH   - частичное обновление ресурса.                                  НЕ ИДЕМПОТЕНТЕН
DELETE  - удаление ресурса.
HEAD    - аналогичен GET, но возвращает только заголовки (без тела).
OPTIONS - получение информации о доступных методах для ресурса.

PATCH НЕ всегда НЕИДЕМПОТЕНТНЫЙ (зависит от логики), а DELETE считается идемпотентным, даже если ресурс уже удалён.
PATCH Обычно неидемпотентный, но может быть идемпотентным, если логика обновления детерминирована
(например, замена поля { "status": "completed" }).
Если PATCH применяет операции типа { "increment": 1 }, то он неидемпотентный.
"""


# REST VS RESTful
"""
REST — стиль проектирования HTTP-API: работаем с ресурсами (URL) и используем HTTP-методы (GET/POST/PUT/DELETE) и статусы.
RESTful — API, которое реально следует принципам REST (ресурсы, корректные методы/статусы, без “глаголов” в URL типа /createUser).

REST    - это концепция (набор принципов)
RESTful - её практическая реализация (например, API, которое следует этим принципам).

REST    = теория (что такое "хорошее API"?).
RESTful = практика (API, которое соответствует этой теории).
"""


# Монолит vs Микросервисы
"""
Преимущества МОНОЛИТА:
- Простота разработки и деплоя   - один код, одна база, меньше зависимостей.
- Высокая производительность     - нет накладных расходов на межсервисное взаимодействие (RPC, REST, JSON-сериализация).
- Легче тестировать и отлаживать - всё в одном месте.

Минусы МОНОЛИТА:
- Масштабируемость    - тяжело расти, приходится масштабировать весь сервис, даже если нагрузка только на одну часть.
- Гибкость технологий - сложно внедрять новые технологии или языки.
- Надёжность          - падение одной функции может уронить всю систему.


Преимущества МИКРОСЕРВИСОВ:
- Гибкость и независимость - каждый сервис на своём стеке, можно обновлять отдельно.
- Масштабируемость         - можно масштабировать только нужные сервисы.
- Отказоустойчивость       - падение одного сервиса не убивает всю систему.

Минусы МИКРОСЕРВИСОВ:
- Сложность              - оркестрация, мониторинг, логирование, дебаг (Kubernetes, Istio, Grafana и т. д.).
- Накладные расходы      - да, JSON-сериализация, сетевые задержки, RPC-вызовы съедают время
  (особенно если сервисов много и они часто общаются).
- Согласованность данных - сложно поддерживать транзакции (нужен Saga, Outbox и т. д.).

RPC (Remote Procedure Call) – это вызов функции или метода на удалённом сервисе, как если бы он был локальным.


Если микросервисов много и они активно обмениваются данными, сериализация/десериализация JSON
(или даже Protobuf/GRPC) добавляет задержки.
gRPC – это современный RPC-фреймворк от Google для быстрого обмена данными между сервисами.

Решение:
- Оптимизировать API (меньше данных, batch-запросы).
- Использовать binary-форматы (Protobuf, Avro).
- Кэшировать данные (Redis).
- Объединять сервисы, если они слишком часто общаются.


МОНОЛИТ      - быстрее и проще, но плохо масштабируется.
МИКРОСЕРВИСЫ - гибкие и масштабируемые, но сложные и медленнее из-за сетевых накладных расходов.
"""



# ВЕРТИКАЛЬНОЕ МАСШТАБИРОВАНИЕ (МОНОЛИТ)  ГОРИЗОНТАЛЬНОЕ МАСШТАБИРОВАНИЕ (МИКРОСЕРВИСЫ)
"""
-- Вертикальное масштабирование (Монолит)  Горизонтальное масштабирование (Микросервисы) --

Монолит чаще масштабируется ВЕРТИКАЛЬНО (мощнее сервер).
Суть: Увеличение ресурсов одной машины (CPU, RAM, GPU, диск).

Микросервисы масштабируются ГОРИЗОНТАЛЬНО (больше серверов).
Суть: Добавление новых машин/контейнеров в кластер.

Вертикально   = больше ресурсов на 1 машину.
Горизонтально = больше машин.


Вертикальное масштабирование (Монолит)

Что значит:

- Увеличение мощности одного сервиса (CPU, RAM, диск).
- Пример: У вас есть сервер с 4 ядрами → апгрейдим до 16 ядер.

Как связано с монолитом:

- Монолит обычно масштабируется вертикально, потому что:
  - Легче запустить один мощный сервер, чем разбивать логику на части.
  - Нет сетевых накладных расходов между компонентами.

Минусы:
- Есть физический предел (максимальная мощность сервера).
- Если упадёт сервер – упадёт весь сервис.



Горизонтальное масштабирование (Микросервисы)

Что значит:

- Добавление новых экземпляров сервиса (а не увеличение мощности одного).
- Пример: Было 2 сервера → стало 10 серверов.

Как связано с микросервисами:

- Каждый микросервис можно масштабировать отдельно.
- Можно добавлять инстансы только для нагруженных частей системы.

Плюсы:
- Нет предела масштабирования (можно добавлять тысячи серверов).
- Отказоустойчивость (если один сервер упадёт, другие продолжат работать).

Минусы:
- Нужна балансировка нагрузки (Nginx, Kubernetes).
- Сложнее управлять (сеть, задержки, согласованность данных).

"""


# Модель OSI (Open Systems Interconnection) — 7 уровней
"""
- L1 Физический (Physical) — передача битов по среде (сигналы, кабель/радио).
- L2 Канальный (Data Link) — кадры, MAC-адреса, доступ к среде, контроль ошибок на линии.
- L3 Сетевой (Network) — пакеты, IP-адреса, маршрутизация между сетями.
- L4 Транспортный (Transport) — доставка “конец-конец”: порты, надёжность/порядок (TCP) или без гарантий (UDP).
- L5 Сеансовый (Session) — управление сеансом (установить/поддерживать/завершить); часто функции на практике в приложениях/протоколах.
- L6 Представления (Presentation) — формат данных: кодировки, сериализация, сжатие, шифрование/дешифрование (часто через TLS).
- L7 Прикладной (Application) — протоколы приложений (HTTP, DNS, SMTP и т.д.).

# КОРОТКО
L1 биты → L2 кадры/MAC → L3 пакеты/IP → L4 сегменты/порты → L7 протоколы приложений (HTTP/DNS/SMTP).
"""


# TCP-сокеты  UDP-сокеты        L4 Транспортный (Transport)
"""
 Существует ДВА основных типа сокетов:
 
-- TCP-сокеты (Transmission Control Protocol) — обеспечивают надежное соединение между двумя устройствами. Они обычно
используются в веб-серверах и клиентах, почтовых серверах и других приложениях, где требуется надежность и порядок доставки данных.

TCP — соединение-ориентированный: держит состояние (установление, номера последовательности, ACK, окна, ретрансляции),
что требует больше ресурсов.

-- UDP-сокеты (User Datagram Protocol) — обеспечивают передачу данных без установления соединения. Они обычно
используются в видео и аудио потоковых приложениях, играх и других приложениях, где скорость важнее, чем надежность.

UDP — протокол без установления соединения (connectionless): нет состояния соединения
(handshake, подтверждений, контроля порядка и т.п.), поэтому меньше накладных расходов и проще обработка.

Нюанс: UDP всё равно может иметь “состояние” на уровне приложения/ОС (например, таблицы NAT/файрвола),
но в самом протоколе состояния соединения нет.


Сокет - низкоуровневая обстракция отправки и получения данных по сети. Именно с помощью ее производится обмен данными
между клиектами и серверами. Сокеты поддерживают две основные операции: отправку и получение байтов.


TCP   — соединение, надёжно и по порядку (ACK/ретраи/окна) → больше накладных.
UDP   — без соединения, датаграммы без гарантий (потери/порядок не обещан) → меньше накладных.
Сокет — интерфейс ОС для сетевого обмена: send/recv (байты/датаграммы) между клиентом и сервером.
"""


# БРОКЕР СООБЩЕНИЙ
"""
Брокер сообщений — сервер-посредник для обмена сообщениями между сервисами: продюсеры публикуют, консюмеры получают.
Оптимизирован под доставку/буферизацию/маршрутизацию (ACK, ретраи, pub/sub), а НЕ под сложные запросы и долгосрочное хранение как СУБД.

Есть два основных алгоритма передачи сообщений в очередь:
1) Есть Producer, который направляет сообщения в брокер, и есть Consumer, который их получает.
2) Есть Publisher, который закидывает сообщения в очередь, и есть n-ое количество Subscriber'ов, которые их обрабатывают в дальнейшем.

Паттерны обмена информации:
--- 1) Request-Response (Запрос-Ответ);
--- 2) One-Way (Односторонний) или Fire and Forget (Отправил и забыл);
--- 3) Publish-Subscribe (Публикация-Подписка) или сокращённо Pub-Sub;
--- 4) Point-to-Point (Точка-Точка).

1) В интернете Request-Response — это работа HTTP: клиент выполняет запрос на сервер, ждёт и получает ответ.
2) One-Way, когда приложение по сети отправляет UDP (User Datagram Protocol) пакет на сервер без ожидания ответа
3), 4) Publish-Subscribe, Point-to-Point - связаны с брокерами сообщений,
В паттернах Pub-Sub и Point-to-Point происходит асинхронное общение через посредника.
"""


# Различие между Rabbit vs Kafka
"""
Разница между RabbitMQ и Kafka значительная, хотя оба являются системами обмена сообщениями, но с разными архитектурами и подходами.
RabbitMQ использует push-модель: брокер сам отправляет сообщения подписчикам.

- Плюсы: низкая задержка, простота для потребителей.
- Минусы: риск перегрузить медленных потребителей.

Kafka использует pull-модель: потребители сами запрашивают данные.

- Плюсы: контроль скорости, балансировка нагрузки.
- Минусы: небольшая задержка из-за опросов.


RabbitMQ лучше для задач с очередями и гарантированной доставкой.
Kafka — для потоковой обработки, больших данных и долгосрочного хранения событий.

Интегрировать их можно через промежуточные сервисы или коннекторы.


Kafka    — PULL: потребители сами запрашивают сообщения, когда готовы их обработать.
RabbitMQ — PUSH: брокер сам отправляет сообщения потребителям, как только они поступают.
"""


# ГАРАНТИИ ДОСТАВКИ СООБЩЕНИЙ (At most once / At least once / Exactly once)
"""
At most once (не более одного раза)
- Сообщение может быть обработано 0 или 1 раз.
- Ретраев нет → возможны потери, зато дублей почти не бывает.
- Пример: fire-and-forget отправка без ACK/ретраев (часто поверх UDP/логирования/метрик).


At least once (хотя бы один раз)
- Сообщение будет доставляться/переотправляться до ACK.
- Потери минимизируются, но возможны дубли (например, ACK потерялся или потребитель упал после обработки).
- Пример: большинство очередей по умолчанию (RabbitMQ/SQS) при использовании ACK + retry; Kafka consumer при коммите
  offset’ов “после” обработки.


Exactly once (ровно один раз — в пределах системы)
- Эффект обработки достигается ровно один раз (нет потерь и дублей “в результате”).
- Обычно требует: идемпотентного producer’а + транзакций/атомарности “прочитал → обработал → записал результат/offset”.
- Пример: Kafka Exactly-Once Semantics (идемпотентный producer + транзакции) для связки “прочитал из Kafka → записал 
  в Kafka/состояние” в рамках поддерживаемых сценариев.
  
  
ACK (acknowledgement) — это подтверждение получения/обработки: получатель сообщает отправителю/брокеру 
“сообщение принято (и часто уже обработано)”, чтобы его не переотправляли.
  
ACK — сообщение от consumer/worker брокеру: “обработал успешно” → брокер не переотправляет и снимает сообщение.
"""


# ЧТО ПРОИСХОДИТ В МОМЕНТ ВЫЗОВА ФУНКЦИИ? (PYTHON)
"""
1) Вычисление аргументов
- Сначала вычисляются выражения-аргументы в месте вызова (слева направо).

2) Фаза связывания (binding) аргументов с параметрами
Python связывает аргументы с параметрами в строгом порядке:

- Позиционные аргументы -> по порядку параметров.
- Затем ключевые аргументы -> по имени параметра (они НЕ становятся позиционными).
- Лишние позиционные могут попасть в *args (если есть).
- Лишние ключевые могут попасть в **kwargs (если есть).

Запрещены конфликты:
- Один параметр не может получить значение дважды (и позиционно, и по ключу).

    def func(a, b): pass
    func(1, a=2)  # TypeError: параметр 'a' задан дважды

Если после связывания остались незаполненные параметры:
- Берутся значения по умолчанию (если есть),
- иначе -> TypeError (не хватает обязательного аргумента).

3) Значения по умолчанию — важный нюанс
- Default-значения вычисляются ОДИН раз при выполнении 'def', а не при каждом вызове.
- Изменяемые default-объекты (list/dict/set) разделяются между вызовами -> опасно.

    i = 5
    def f(arg=i):   # значение по умолчанию "запоминается" здесь (в момент def)
        print(arg)
    i = 6
    f()  # -> 5

4) Выполнение тела функции
- Создаётся новый frame (контекст вызова) с локальными переменными.
- Выполняются инструкции тела функции.

5) Возврат результата
- return X -> возвращается X
- нет return -> возвращается None
- frame завершается, управление возвращается вызывающему коду.
"""



# ЧТО ПРОИСХОДИТ ПРИ IMPORT ПОД КАПОТОМ?
"""
При выполнении команды import в Python происходит несколько шагов:

1) Поиск модуля: Python ищет указанный модуль в системных путях, определённых в sys.path, включая
   стандартные библиотеки и пользовательские директории.

2) Компиляция: Если модуль не был ранее загружен, Python компилирует его в байт-код (файлы с расширением .pyc).

3) Инициализация: Выполняется код модуля, и создаются его объекты (функции, классы и переменные).

Кэширование: Загруженный модуль сохраняется в кэше sys.modules, чтобы при следующем импорте избежать повторной загрузки и компиляции.
"""


# Sum type vs Product type
"""
Sum type (тип-сумма):
- “одно из” нескольких вариантов.
- Python: A | B  (Union[A, B]), Optional[T] = T | None.

Product type (тип-произведение):
- “вместе” несколько полей/частей.
- Python: tuple[T1, T2], dataclass/NamedTuple (несколько полей разных типов).
"""


# Alembic
"""
Alembic — тулза миграций для SQLAlchemy: версионирует схему БД и применяет изменения шагами (upgrade/downgrade).
Хранит “ревизии” (migration scripts) и текущую версию в таблице alembic_version.

Что умеет:
- autogenerate миграций по моделям SQLAlchemy (нужно проверять руками!)
- apply/rollback миграций
- история/текущая версия

Команды:
- alembic revision --autogenerate -m "msg"  # создать миграцию
- alembic upgrade head                      # применить все до последней
- alembic downgrade -1                      # откатить на 1 миграцию
- alembic current                           # текущая ревизия в БД
- alembic history                           # цепочка ревизий

Про “2 heads”:
- это две ветки миграций (разошлась история). Правильно: сделать merge-ревизию:
  alembic merge <rev1> <rev2> -m "merge heads"
  (а не “подменять номера” руками).

Django:
- Alembic обычно не используют: там встроенные миграции через manage.py.
"""


# Alembic: как добавить/поменять колонки без потери данных
"""
ADD COLUMN (без потери данных):
- делаем новую миграцию
- добавляем колонки через op.add_column
- откат: op.drop_column

from alembic import op
import sqlalchemy as sa

def upgrade():
    op.add_column("users", sa.Column("phone", sa.String(20), nullable=True))

def downgrade():
    op.drop_column("users", "phone")


Если нужен NOT NULL на существующей таблице (без боли):
1) add_column nullable=True (+ server_default при необходимости)
2) backfill существующих строк (UPDATE)
3) alter_column nullable=False
4) при желании убрать server_default

Изменение существующей колонки:
- op.alter_column(...)

def upgrade():
    op.alter_column("users", "email", nullable=False)

def downgrade():
    op.alter_column("users", "email", nullable=True)


Памятка:
- server_default = дефолт на стороне БД (важно)
- индексы добавляем отдельно: op.create_index(...)
"""


# 3) Django Migrations vs Alembic (интервью-сравнение)
"""
→ Django migrations:
- встроено в Django ORM
- команды: makemigrations / migrate
- хранит историю в таблице django_migrations
- автогенерация из моделей Django (очень удобно “из коробки”)

→ Alembic:
- миграции для SQLAlchemy (де-факто стандарт)
- команды: revision --autogenerate / upgrade head / downgrade -1
- хранит текущую ревизию в alembic_version
- гибкая модель ревизий (граф), можно словить “несколько heads” и делать merge revision

Ключевые отличия:
- Экосистема: Django ↔ Alembic+SQLAlchemy.
- Autogenerate: у обоих есть, но Alembic обычно требует аккуратнее ревью (особенно constraints/индексы).
- Прод-практики: в Postgres “без блокировок” часто делают отдельными шагами (например, CREATE INDEX CONCURRENTLY) —
  и в Django, и в Alembic это возможно, но в Alembic обычно больше “ручного контроля”.

Собеседовательная фраза:
“Django migrations — удобный встроенный путь для Django ORM; Alembic — аналогичный инструмент в мире SQLAlchemy,
более гибкий по ревизиям и часто требует чуть больше дисциплины при автогенерации.”
"""




# КАК УСТРОЕНА ПАМЯТЬ В CPYTHON
"""
Стек (call stack)
- Хранит фреймы вызовов (кадры функций).
- Локальные переменные/аргументы внутри фрейма — это ССЫЛКИ (PyObject*), а не сами объекты.
- Объекты живут НЕ “в стеке”, а в куче.

Куча (heap)
- Здесь находятся Python-объекты (int/str/list/dict/и т.д.).
- Управление памятью:
  - основной механизм “освобождения” — reference counting (счётчик ссылок)
  - доп. GC (generational) нужен в основном для циклов ссылок
- Аллокация:
  - pymalloc оптимизирует выделение мелких объектов (обычно до ~512 байт) поверх malloc/allocator ОС.

Переменная = ссылка
- a = [1, 2]:
  - a в фрейме → ссылка на list-объект в куче
  - list хранит массив ССЫЛОК на элементы (1 и 2 — отдельные объекты в куче)

Важные уточнения
- “GC следит за объектами” — точнее: RC освобождает сразу при 0 ссылок,
  а GC периодически ищет циклы среди контейнеров (list/dict/set/классы).
- Кэширование:
  - small integers обычно -5..256 кэшируются (деталь реализации CPython)
  - строки могут быть interned (не “все строки кэшируются”, а некоторые)
  
RC (reference counting) — это подсчёт ссылок на объект: у каждого объекта есть счётчик, и когда он становится 0, объект сразу освобождается
  
В общих чертах — да, если ты про классическую картинку в учебниках:

stack (стек вызовов)           — обычно рисуют “СВЕРХУ”
heap (куча, где живут объекты) — обычно “СНИЗУ”

Но важно понимать нюанс: это НЕ “физически сверху/снизу”, а просто удобная схема. В реальности в адресном пространстве
процесса есть много областей (код, данные, mmap-регионы, heap, stack и т.д.), и точное расположение зависит от ОС/архитектуры/настроек.

Главная мысль, которую тебе нужно держать:
- локальные переменные/аргументы лежат во фрейме (stack) как ссылки
- сами Python-объекты лежат в куче (heap)
"""


# MALLOC VS PYMALLOC
"""
malloc — аллокатор из libc (внутри может использовать brk и/или mmap в зависимости от размера/реализации).

RSS (Resident Set Size) — это сколько физической RAM прямо сейчас занимает процесс (его “резидентная” память).

brk/sbrk — “двигает” границу heap (кучи) процесса → память берётся из растущей кучи.
mmap — выделяет отдельный регион памяти (mapping) в адресном пространстве процесса → удобно для больших блоков и проще вернуть ОС.

libc — это стандартная библиотека языка C: набор базовых функций, которыми пользуются программы
(например malloc, printf, memcpy, работа с файлами/строками). На Linux обычно это glibc или musl.


CPython — это основная (самая распространённая) реализация интерпретатора Python, 
написанная на C (именно она обычно устанавливается как “Python”).


pymalloc (obmalloc) — объектный аллокатор CPython поверх malloc:
- Оптимизирован для мелких объектов (≤512 байт) через size classes.
- Архитектура: arenas → pools → blocks (быстрое выделение/освобождение).
- Для больших объектов (>512 байт) переключается на системный malloc.
- Память часто не возвращается ОС сразу: держит арены/пулы для повторного использования (RSS может не падать).
- Дополнительно CPython использует freelists для некоторых типов (ещё меньше аллокаций).

Разница malloc vs pymalloc:
pymalloc уменьшает накладные расходы на частые аллокации мелких объектов в Python.

malloc vs pymalloc:

Особенность         malloc                 pymalloc
Размер объектов     Любые                  Только ≤ 512 байт (дальше → malloc)
Скорость            Медленнее              Быстрее (пулы/size classes)
Использование       libc / ОС              Внутренний CPython (obmalloc)


pymalloc часто переиспользует память внутри процесса и не обязан возвращать её ОС сразу; возврат зависит от того,
удалось ли освободить целые арены/куски и от поведения системного аллокатора    <------------

Важно про возврат памяти ОС:
- pymalloc (obmalloc) часто НЕ возвращает память ОС сразу: он переиспользует её внутри процесса,
  удерживая pools/arenas для будущих мелких аллокаций, поэтому RSS может не падать после освобождения объектов.
- Системный malloc тоже НЕ обязан сразу возвращать память ОС: он может держать освобождённые блоки
  в своих структурах (free-lists/heap) и переиспользовать их. Чаще реально возвращаются ОС большие
  выделения через mmap (их можно munmap), а память из brk-heap обычно остаётся у процесса.



obmalloc (pymalloc) — встроенный аллокатор CPython для мелких объектов; держит пулы и переиспользует память,
поэтому RSS может не падать после освобождения. То есть в большинстве контекстов можно считать  pymalloc == obmalloc


obmalloc — это реализация/подсистема в CPython (файл/модуль allocator’а).
pymalloc — это название алгоритма/режима/поведения этого аллокатора (как его обычно называют в документации/обсуждениях).
В большинстве разговорных контекстов реально говорят как синонимы, но точнее: pymalloc реализован в obmalloc.
"""


# Как хранятся элементы массива и списка в памяти
"""
Массив (C-array / numpy array):

- элементы одного типа и фиксированного размера
- лежат плотно, подряд (contiguous)
- быстрый доступ по индексу, хорошо для CPU cache

Python list:

- это динамический массив указателей
- внутри хранит ссылки (pointers) на PyObject, а сами объекты лежат “где угодно” в heap
- list держит capacity с запасом → append часто O(1) амортизированно
"""



### Big O различных операций в CPython
# n — количество элементов в контейнере
# k — размер входного параметра (например, длина среза/итерируемого)

# LIST (list)
"""
Внутри: динамический массив ссылок (pointers).
Дорого: вставки/удаления НЕ с конца (нужен сдвиг), resize (редко) при росте capacity.

Operation            Average        Worst
Copy                 O(n)           O(n)
Append               O(1) аморт.    O(n)          # при resize
Extend (+= iter)     O(k)           O(k)
Pop last             O(1)           O(1)
Pop(i)               O(n)           O(n)          # сдвиг
Insert(i, x)         O(n)           O(n)          # сдвиг
Get item (a[i])      O(1)           O(1)
Set item (a[i]=x)    O(1)           O(1)
Del item (del a[i])  O(n)           O(n)          # сдвиг
Iteration            O(n)           O(n)
Get slice (a[l:r])   O(k)           O(k)          # копирование k элементов
Del slice            O(n)           O(n)
Set slice            O(n+k)         O(n+k)
Sort (Timsort)       O(n log n)     O(n log n)    # best ~ O(n) на почти отсортированном
x in a               O(n)           O(n)
min(a), max(a)       O(n)           O(n)
len(a)               O(1)           O(1)

Подсказка:
- append/pop() с конца — быстрые
- частые операции с обоих концов → лучше collections.deque
"""

# DEQUE (collections.deque)
"""
Внутри: блоки (чанки) + ссылки между блоками (эффективно для концов).
Сильное место: операции слева/справа.
Слабое место: доступ/вставка в середине.

Operation                 Average        Worst
Copy                      O(n)           O(n)
append                    O(1)           O(1)
appendleft                O(1)           O(1)
pop                       O(1)           O(1)
popleft                   O(1)           O(1)
extend                    O(k)           O(k)
extendleft                O(k)           O(k)
rotate(r)                 O(|r|)         O(|r|)
remove(x)                 O(n)           O(n)
len(d)                    O(1)           O(1)
"""

# SET (set)
"""
Внутри: хеш-таблица.
Обычно: O(1) на in/add/remove.
Worst: O(n) при плохих/злонамеренных коллизиях (редко), либо при rehash.

Operation                                   Average                    Worst
x in s                                      O(1)                       O(n)
add / remove / discard                       O(1) аморт.               O(n)
Union (s | t)                                O(len(s)+len(t))          O(len(s)+len(t))
Intersection (s & t)                         O(min(len(s),len(t)))     O(len(s)*len(t))   # если второй НЕ set/не hashable контейнер
Difference (s - t)                           O(len(s))                 O(len(s)*len(t))   # аналогично, если t "плохой" контейнер
Symmetric diff (s ^ t)                       O(len(s)+len(t))          O(len(s)*len(t))

Примечание:
- Если t тоже set → intersection обычно O(min)
"""

# DICT (dict)
"""
Внутри: хеш-таблица.
Обычно: O(1) на get/set/del/in.
Worst: O(n) при коллизиях/rehash (редко).
Важно: операции зависят от качества hash и нагрузки таблицы.

Operation                 Average        Worst
k in d                    O(1)           O(n)
Copy (d.copy)             O(n)           O(n)
Get item (d[k])           O(1)           O(n)
Set item (d[k]=v)         O(1) аморт.    O(n)      # при resize/rehash
Del item (del d[k])       O(1)           O(n)
Iteration (keys/values)   O(n)           O(n)
len(d)                    O(1)           O(1)
"""


# global и nonlocal
"""
global и nonlocal нужны только для изменения значений
global может создать переменную, nonlocal не может!
nonlocal ищет только во внешних скоупах, но не в глобальном и не builtins
НЕ используйте global, nonlocal


Использовать редко, но они нормальны в очень конкретных случаях.
nonlocal — ок для замыканий (счётчик/кэш в closure).
global — почти всегда запах, но в маленьких скриптах/конфигах встречается
"""


# PATCH vs MOCK vs STUB в pytest
"""
Stub – простая заглушка, возвращает фиктивные данные без логики. Возвращает заранее заданные данные.

# Пример Stub
def stub(): return 42

Mock – объект, имитирующий поведение и проверяющий вызовы (из unittest.mock или pytest-mock).
это заглушка с возможностью проверки (был ли вызван, сколько раз и т. д.)

# Пример Mock
from unittest.mock import Mock
mock = Mock(return_value=100)
assert mock() == 100
mock.assert_called_once()

Mock vs MagicMock

Mock      – базовая заглушка с проверкой вызовов.
MagicMock – автоматически создаёт методы-заглушки (полезно для магических методов, например __len__).

Patch – временная подмена объекта (через pytest-mock или monkeypatch).

# Пример Patch:
def test_func(mocker):  # pytest-mock
   mocker.patch("module.func", return_value=123)
   assert module.func() == 123

Разница:

Stub  – только возвращает данные.
Mock  – подменяет + проверяет, как его использовали.
Patch – способ временно заменить объект (на stub/mock).

	                Stub	                Mock	                    Patch
Возвращает данные	Да (фиксированные)	    Да (можно настраивать)	    Сам по себе нет, но заменяет объект на stub/mock
Проверяет вызовы	Нет	                    Да (assert_called и др.)	Нет (но можно патчить mock)
Где используется	Простая подмена	        Проверка взаимодействия	    Временная замена объекта
"""


# PATCH vs MOCK vs STUB (pytest)
"""
Stub  — подставной ответ (test double): возвращает заранее заданные данные, без проверок вызовов.
Mock  — test double с проверками: позволяет assert'ить, как/сколько раз/с какими аргументами вызывали.
Patch — способ временно подменить зависимость (функцию/класс/атрибут) на stub/mock.

Важно:
- Patch сам по себе не "тип двойника", он просто ставит двойник на место реального объекта.
- mocker.patch(...) обычно создаёт и возвращает Mock/MagicMock.


# Stub (простой)
def get_rate_stub():
    return 42

# Mock (проверяем взаимодействие)
from unittest.mock import Mock
rate = Mock(return_value=100)
assert rate() == 100
rate.assert_called_once()

# Patch (подмена зависимости на время теста)
def test_service_uses_rate(mocker):
    mocked = mocker.patch("module.get_rate", return_value=123)
    assert module.get_rate() == 123
    mocked.assert_called_once()


Mock vs MagicMock:
- Mock      — обычные атрибуты/вызовы.
- MagicMock — умеет магические методы (__len__, __iter__ и т.п.) "из коробки".
"""




# ЧТО БЫСТРЕЕ: СИНХРОННАЯ ИЛИ АСИНХРОННАЯ ЗАПИСЬ В ФАЙЛ?
"""
--- (ОС vs Python) --- 

ОС = операционная система (Windows / Linux / macOS)

1) ОС (синхронная/асинхронная запись на диск)   АСИНХРОННАЯ/БУФЕРИЗОВАННАЯ (через кэш ОС) - БЫСТРЕЕ!!!

- write() без fsync/O_SYNC → асинхронная/буферизованная (через кэш ОС) (без гарантии на диск): ОС кладёт в кэш и сразу возвращает.
- write() + fsync() или O_SYNC → синхронная (с гарантией на диск): ждёшь, пока запишется на SSD/HDD.

2) Python (синхронный код vs asyncio):          СИНХРОННЫЙ - БЫСТРЕЕ!!!

- Синхронный Python write() для ОДНОГО файла часто быстрее (МЕНЬШЕ накладных).
- Асинхронный Python (asyncio/aiofiles) диск не ускоряет — просто не блокирует при многих операциях.
"""


# Как Устроены СЛОВАРИ В ПИТОНЕ
"""
dict в CPython устроен так: есть таблица слотов (indices) и плотный список записей (entries) в порядке вставки.
реализация использует разреженную хеш-таблицу + плотный массив записей (split table / compact dict).

 1. `indices` - разреженная таблица индексов (размер минус степень 2)
 2. `entries` - плотный массив записей, сохраняющий порядок вставки


# Пример (УПРОЩЁННО, как можно представить dict внутри)
d = {"a": 1, "b": 2}

# разреженная таблица слотов: slot -> индекс в entries (-1 = пусто)
indices = [-1, 0, -1, -1, -1, 1, -1, -1]

# плотный массив записей (в порядке вставки)
entries = [
    (h_a, "a", 1),   # entries[0]
    (h_b, "b", 2),   # entries[1]
]

# смысл:
# indices[1] = 0  -> в entries[0] лежит ("a":1)
# indices[5] = 1  -> в entries[1] лежит ("b":2)



# УТОЧНЕНИЯ (чтобы было "идеально правильно")

1) В реальном CPython entries не хранит (hash, key, value) буквально как tuple.
   Там массив структур (PyDictKeyEntry / PyDictUnicodeEntry) и отдельное хранение value
   в зависимости от режима (combined/split).

2) Коллизии: это НЕ просто линейный пробинг "+1".
   В CPython используется схема пробинга с "perturb" (псевдослучайный шаг).

3) split table — это отдельный режим (часто для __dict__ объектов):
   keys общие, values отдельно. "compact dict" — общее свойство современного dict,
   но это не синоним split table.
"""


# DI (Dependency Injection, Внедрение зависимостей)
"""
DI (Dependency Injection, Внедрение зависимостей) - это внедрение зависимостей, техника, при которой объект получает
свои зависимости извне (а не создаёт их сам).

Dependency Injection (DI) — это способ реализации Inversion of Control: объект не создаёт свои зависимости,
а получает уже готовые (через конструктор, параметры метода/функции, свойства или контейнер).


### ПРИМЕР
# DI (Dependency Injection) через конструктор: зависимость передаётся в __init__

from typing import Protocol


class Notifier(Protocol):
    def send(self, to: str, message: str) -> None: ...


class EmailNotifier:
    def send(self, to: str, message: str) -> None:
        print(f"[EMAIL] to={to} | {message}")


class SmsNotifier:
    def send(self, to: str, message: str) -> None:
        print(f"[SMS] to={to} | {message}")


class UserService:
    # <-- Внедрение зависимости через конструктор (constructor injection)
    def __init__(self, notifier: Notifier) -> None:
        self.notifier = notifier

    def register(self, phone_or_email: str) -> None:
        # сервис использует зависимость, но не создаёт её
        self.notifier.send(phone_or_email, "Добро пожаловать!")


if __name__ == "__main__":
    service_email = UserService(EmailNotifier())
    service_email.register("user@example.com")

    service_sms = UserService(SmsNotifier())
    service_sms.register("+380991112233")

"""


# SOLID (коротко) — плохой / хороший пример + 2–3 строки объяснения


# 1) SRP — Single Responsibility
"""
Идея: у класса 1 ответственность → 1 причина для изменения.


# ПЛОХО: и данные, и БД, и email в одном месте
class User:
    def __init__(self, name: str):
        self.name = name

    def save(self):
        print("save to DB")

    def send_email(self, msg: str):
        print("send email")

# ХОРОШО: разделяем ответственности
class User:
    def __init__(self, name: str):
        self.name = name

class UserRepo:
    def save(self, user: User):
        print("save to DB")

class EmailService:
    def send(self, user: User, msg: str):
        print("send email")


Почему: меняется БД/почта → не трогаем модель User, проще тестировать и поддерживать.
"""


# 2) OCP — Open/Closed
"""
Идея: добавляем новое поведение расширением, а не правкой if/else.


# ПЛОХО: новый тип скидки = правка метода
def discount(price: float, kind: str) -> float:
    if kind == "VIP":
        return price * 0.8
    if kind == "REG":
        return price * 0.9
    return price

# ХОРОШО: полиморфизм
from abc import ABC, abstractmethod

class Discount(ABC):
    @abstractmethod
    def apply(self, price: float) -> float: ...

class VipDiscount(Discount):
    def apply(self, price: float) -> float:
        return price * 0.8

class RegDiscount(Discount):
    def apply(self, price: float) -> float:
        return price * 0.9


Почему: добавили PremiumDiscount новым классом — старый код не меняется.
"""


# 3) LSP — Liskov Substitution  Полиморфизм также иногда называют принципом подстановки Барбары Лисков <----
# LSP напрямую про ПОЛИМОРФИЗМ
"""
Идея: подкласс не должен ломать контракт базового класса.


# ПЛОХО: "птица" обещает fly, но пингвин падает с исключением
class Bird:
    def fly(self) -> str:
        return "fly"

class Penguin(Bird):
    def fly(self) -> str:
        raise RuntimeError("can't fly")

# ХОРОШО: разделяем возможности
class Bird: ...
class FlyingBird(Bird):
    def fly(self) -> str:
        return "fly"

class Penguin(Bird):
    def swim(self) -> str:
        return "swim"


Почему: код, который работает с FlyingBird, никогда не получит объект без fly().
"""


# 4) ISP — Interface Segregation
"""
Идея: лучше несколько маленьких интерфейсов, чем один огромный.


# ПЛОХО: заставляем всех уметь всё
class Device(ABC):
    @abstractmethod
    def print_(self): ...
    @abstractmethod
    def scan(self): ...

class OldPrinter(Device):
    def print_(self): ...
    def scan(self): raise NotImplementedError

# ХОРОШО: разделили интерфейсы
class Printer(ABC):
    @abstractmethod
    def print_(self): ...

class Scanner(ABC):
    @abstractmethod
    def scan(self): ...

class SimplePrinter(Printer):
    def print_(self): ...


Почему: классы не обязаны реализовывать ненужные методы.
"""


# 5) DIP — Dependency Inversion
"""
Идея: зависим от абстракций, внедряем реализацию снаружи.


# ПЛОХО: жёстко привязаны к LightBulb
class LightBulb:
    def on(self): print("bulb on")

class Switch:
    def __init__(self):
        self.bulb = LightBulb()
    def press(self):
        self.bulb.on()

# ХОРОШО: зависим от интерфейса
class Switchable(ABC):
    @abstractmethod
    def on(self): ...

class LightBulb(Switchable):
    def on(self): print("bulb on")

class Fan(Switchable):
    def on(self): print("fan on")

class Switch:
    def __init__(self, device: Switchable):
        self.device = device
    def press(self):
        self.device.on()


Почему: Switch работает с любым устройством (Fan/LightBulb) без правок.
"""



# ИНТЕРФЕЙСЫ В PYTHON (ABC vs Protocol)
"""
Интерфейсы в Python = “контракт” (какие методы/атрибуты ожидаются).
Отдельного ключевого слова interface нет, но есть два стандартных подхода:

1) ABC (abc.ABC)
- ДАЁТ runtime-гарантию: если не реализовал @abstractmethod → экземпляр не создать.
- Использую, когда нужно “жёстко обязать” реализацию.

2) Protocol (typing.Protocol)
- ДАЁТ гарантию на уровне типизации (mypy/IDE): важно “похоже по форме”, наследование не нужно.
- Использую для утиной типизации и зависимостей от абстракций (DIP).

Итог:
- ABC = runtime enforcement
- Protocol = static type checking (structural typing)

Вывод: Интерфейсы в Python — это договорённость о методах.

from abc import ABC, abstractmethod
from typing import Protocol

class IWriter(ABC):
    @abstractmethod
    def write(self, data: str) -> None: ...

class FileWriter(IWriter):
    def write(self, data: str) -> None:
        print(data)

class IReader(Protocol):
    def read(self) -> str: ...

class DBReader:
    def read(self) -> str:
        return "data"
"""



# DESCRIPTORS (дескрипторы)
"""
Дескриптор = объект, который управляет доступом к атрибуту.

Если у атрибута класса есть:
- __get__()           → non-data descriptor (пример: функции/методы)
- __get__() + __set__/__delete__ → data descriptor (пример: property)


property     → data descriptor (__get__ + обычно __set__/__delete__ если есть setter/deleter)
classmethod  → non-data descriptor (__get__)
staticmethod → non-data descriptor (__get__)

Приоритет поиска атрибута:
1) data-descriptor (в классе)
2) obj.__dict__ (атрибут экземпляра)
3) non-data descriptor / атрибут класса
4) __getattr__ (если задан)

Где встречается:
- property, classmethod, staticmethod, методы (функции в классе) — всё через дескрипторы.


 -- ТОЛЬКО Data-descriptor  ПЕРЕБИВАЕТ obj.__dict__ --
 Data-descriptor (например property) → имеет приоритет над obj.__dict__.                            <-------
 Non-data descriptor (только __get__, например обычная функция-метод) → НЕ перебивает:
 если в obj.__dict__ есть атрибут с тем же именем, он его перекроет.

 data-descriptor (__get__ + __set__/__delete__) ПЕРЕБИВАЕТ obj.__dict__
 non-data descriptor (только __get__) НЕ перебивает → obj.__dict__ может его перекрыть

 ### “Перебивает” obj.__dict__ только data-descriptor (у которого есть __set__ и/или __delete__).   <-------
 ### DATA-DESCRIPTOR - ПЕРЕБИВАЕТ!
 class A:
     @property
     def x(self): return 1

 a = A()
 a.__dict__['x'] = 999
 print(a.x)          # 1  (property перебил)

 ### NON-DATA-DESCRIPTOR - НЕ ПЕРЕБИВАЕТ!
 class A:
     def f(self): return "method"

 a = A()
 a.__dict__['f'] = 999
 print(a.f)       # 999 (obj.__dict__ перебил non-data descriptor)
"""


# COMPOSITION vs INHERITANCE (композиция vs наследование)
"""
Наследование (is-a):
- "X является Y" (Dog is an Animal)
- Плюсы: переиспользование кода, полиморфизм через общий базовый тип
- Минусы: жёсткая связность, сложнее менять иерархию, риск нарушить LSP

Композиция (has-a):
- "X имеет Y" (Car has an Engine)
- Плюсы: гибче, проще подменять части (стратегии/зависимости), меньше связность
- Минусы: иногда больше кода (прокидывание методов)

Правило для собеса:
- Композиция — по умолчанию.
- Наследование — когда реально “один вид другого” и подстановка корректна (LSP).

Агрегация vs композиция (про жизненный цикл):
- Агрегация: часть может жить отдельно от контейнера.
- Композиция: часть принадлежит контейнеру и обычно создаётся/умирает вместе с ним.


# --- Пример: наследование (is-a) ---
class Animal:
    def speak(self) -> str:
        return "..."

class Dog(Animal):
    def speak(self) -> str:
        return "Bark"


# --- Пример: композиция (has-a) ---
class Engine:
    def start(self) -> str:
        return "Engine starts"

class Car:
    def __init__(self, engine: Engine):
        self.engine = engine  # has-a

    def start(self) -> str:
        return self.engine.start() + " and car moves"


# --- Пример: агрегация (слабое владение) ---
class Product:
    def __init__(self, name: str):
        self.name = name

class Store:
    def __init__(self):
        self.products: list[Product] = []  # продукты могут существовать без магазина

    def add_product(self, p: Product) -> None:
        self.products.append(p)


# --- Пример: композиция (сильное владение) ---
class Room:
    def __init__(self, name: str):
        self.name = name

class House:
    def __init__(self):
        self.rooms = [Room("Bedroom"), Room("Kitchen")]  # комнаты создаются внутри дома


if __name__ == "__main__":
    # Наследование
    a: Animal = Dog()
    print(a.speak())  # Bark

    # Композиция
    car = Car(Engine())
    print(car.start())  # Engine starts and car moves

    # Агрегация
    apple = Product("Apple")
    store = Store()
    store.add_product(apple)
    del store
    print(apple.name)  # Apple (продукт жив)

    # Композиция
    house = House()
    print([r.name for r in house.rooms])  # ['Bedroom', 'Kitchen']
"""




### TODO SQL (CORE + MID/SENIOR) — PostgreSQL FOCUS


# Графы / деревья / BST / DAG (очень кратко для собеса)
"""
Граф:
- Вершины (nodes) + рёбра (edges). Может быть ориентированным/неориентированным, взвешенным/невзвешенным.

Ацикличность:
- В неориентированном графе “без циклов” = лес, а связный лес = дерево.
- В ориентированном графе “без циклов” = DAG (Directed Acyclic Graph).
  Примеры DAG: зависимости задач/пакетов, граф коммитов Git, топологическая сортировка.

Дерево:
- Связный ацикличный неориентированный граф.
- Есть корень (в корневом дереве), у каждого узла (кроме корня) ровно один родитель, листья — без детей.
- Чем меньше высота, тем быстрее операции.

Бинарное дерево:
- У каждого узла ≤ 2 детей (левый/правый).

BST (Binary Search Tree):
- Левое поддерево < ключ < правое поддерево (политика для “равно” оговаривается).
- Поиск/вставка/удаление: среднее O(log n), худшее O(n) (если выродилось в “список”).
- Самобалансирующиеся варианты: AVL, Red-Black → гарантируют O(log n).

B-Tree:
- “Много ключей в узле”, оптимально для диска/страниц → поэтому часто используется в индексах БД.

Heap (двоичная куча):
- Это НЕ BST.
- Свойство кучи: родитель ≤ (min-heap) или ≥ (max-heap) детей → быстрый доступ к min/max.
"""


# КАК ВСТАВИТЬ ЭЛЕМЕНТ В БИНАРНОЕ ДЕРЕВО
"""
- Начинаем с корня.
- Сравниваем новый элемент с текущим узлом:
   - Если меньше → идём в левое поддерево.
   - Если больше → идём в правое поддерево.
- Дошли до пустого места? → Вставляем новый узел.

- Где применяется: сортировка, поиск, базы данных.


1) Это именно BST, а не “просто бинарное дерево”.
   В “бинарном дереве” нет правила “меньше влево, больше вправо”.

2) Что делать с равными ключами?
   Нужно явно определить политику:
   - хранить счётчик (freq),
   - или класть равные всегда влево/вправо,
   - или запрещать дубликаты (ничего не вставлять)
"""


# UNION vs UNION ALL
"""
Разница между UNION и UNION ALL заключается в том, что UNION удаляет дубликаты из результата,
а UNION ALL сохраняет все строки, включая дубликаты.

UNION
- Объединяет результаты и УДАЛЯЕТ дубликаты (как DISTINCT по всем колонкам результата).
- Обычно дороже: нужно убрать дубли (сортировка/хеширование).

UNION ALL
- Просто “склеивает” результаты и ОСТАВЛЯЕТ дубликаты.
- Обычно быстрее.

Практика:
- Если дубликаты не страшны/невозможны по логике — почти всегда UNION ALL.
- UNION — только когда реально нужна дедупликация.
"""



# “DELETE сделал, а строк/размер не уменьшились” (коротко для собеса, Postgres)
"""
→ В PostgreSQL из-за MVCC DELETE не вырезает строку сразу: она становится “dead tuple”.
Селекты её не видят, но физически место освобождается позже.

Почему “не стало меньше”:
- COUNT(*) уже меньше, но размер таблицы/файла может не уменьшиться сразу (место переиспользуется).
- Долгие транзакции держат старый snapshot → vacuum не может зачистить мусор.

Что делать:
- Норма: autovacuum со временем очистит dead tuples (освободит место для повторного использования).
- Если нужно реально уменьшить физический размер: VACUUM FULL (или CLUSTER) — но это долго и берёт сильные блокировки.
- Проверить long-running transactions (pg_stat_activity) и настройки autovacuum, если bloat растёт.

TTL:
- В Postgres “TTL из коробки” нет; делают через job (cron/pg_cron) по expires_at + периодический DELETE.

MVCC — это механизм конкурентного доступа, где СУБД хранит несколько версий строк, 
поэтому чтения обычно НЕ блокируют записи и наоборот (читатель видит “снимок” данных).


MVCC (Multi-Version Concurrency Control) - это механизм в СУБД для работы с данными без блокировок.
MVCC именно в том, чтобы избежать read/write блокировок.
"""



# TRUNCATE vs DELETE (коротко для собеса)
"""
→ Оба удаляют данные, но разная семантика и “цена”.

TRUNCATE
- Быстро очищает ВСЮ таблицу (без WHERE).
- Обычно минимально логирует (не по строкам) → быстрее на больших объёмах.
- Сбрасывает identity/sequence (в Postgres: RESTART IDENTITY опционально).
- Триггеры ON DELETE не вызываются (в Postgres TRUNCATE-триггеры есть отдельные).
- Берёт более сильные блокировки (Postgres: ACCESS EXCLUSIVE) → может стопорить конкурентные операции.
- В Postgres можно откатить внутри транзакции (BEGIN ... TRUNCATE ... ROLLBACK).

DELETE
- Удаляет строки выборочно (WHERE) или все (без WHERE).
- Логирует удаление по строкам → медленнее на больших таблицах.
- Не сбрасывает sequence/identity.
- Вызывает ON DELETE триггеры, соблюдает FK-логику построчно.
- В Postgres умеет RETURNING, и всегда откатывается в транзакции.

Когда что:
- TRUNCATE: “быстро обнулить таблицу целиком”, обычно для staging/тестов/ETL, когда ок сильный lock.
- DELETE: “нужно условие / нужно вернуть удалённые / важны триггеры / мягче по конкурентности”.


TRUNCATE
- Удаляет все строки таблицы быстро (НЕ записывает в журнал удаление каждой строки).
- Нельзя использовать с WHERE (удаляет всё).
- Сбрасывает счетчик автоинкремента (если есть).
- Нельзя откатить (ROLLBACK в некоторых СУБД).

DELETE
- Удаляет строки медленнее (записывает каждое удаление в журнал).
- Можно использовать с WHERE (удаление выборочно).
- Не сбрасывает автоинкремент.
- Можно откатить (ROLLBACK).

DELETE   — удаляет строки (можно WHERE), медленнее, обычно логирует построчно.
TRUNCATE — быстро очищает всю таблицу (без WHERE), часто сбрасывает автоинкремент и работает как DDL.
"""



# 1) Как “выполняется” SELECT: логический порядок. WHERE vs HAVING.
"""
→ Важно: логический порядок выполнения ≠ порядок написания запроса.

Логический порядок:
1) FROM (+ JOIN/ON)      — формируем набор строк (джойны)
2) WHERE                — фильтруем строки ДО агрегации
3) GROUP BY             — группируем
4) HAVING               — фильтруем группы ПОСЛЕ агрегации
5) SELECT               — вычисляем выражения/алиасы
6) DISTINCT             — удаляем дубликаты
7) ORDER BY             — сортируем
8) LIMIT/OFFSET         — ограничиваем результат

WHERE vs HAVING:
- WHERE  — фильтрует строки (быстрее и правильнее, если можно).
- HAVING — фильтрует группы (когда условие зависит от агрегатов: SUM/COUNT/AVG).

Пример:
SELECT product_id, SUM(amount) AS total
FROM sales
WHERE created_at >= '2026-01-01'
GROUP BY product_id
HAVING SUM(amount) > 100;
"""

# 1.1) Follow-up: Почему агрегаты нельзя писать в WHERE?
"""
→ WHERE выполняется до GROUP BY, агрегатов ещё “нет”.
HAVING выполняется после группировки, агрегаты уже посчитаны.
"""


# 2) JOIN’ы: какие бывают и где типичные баги (row explosion).
"""
→ Типы JOIN:
- INNER JOIN: только совпавшие строки
- LEFT JOIN: все слева + совпадения справа, иначе справа NULL
- RIGHT JOIN: как LEFT, но наоборот (часто избегают ради читаемости)
- FULL OUTER JOIN: всё слева и справа, где нет пары — NULL
- CROSS JOIN: декартово произведение (опасно: взрыв строк)

Типичный прод-баг: JOIN “размножил” строки → агрегаты/суммы стали больше.
Причина: one-to-many / many-to-many и неправильная гранулярность.

Правило:
- агрегаты считаем на правильной гранулярности,
- либо сначала агрегируем “many” подтаблицу, потом JOIN.
"""

# 2.1) Follow-up: ON vs WHERE при LEFT JOIN — почему важно?
"""
→ Условие в WHERE может превратить LEFT JOIN в INNER JOIN.

Плохо (потеряем строки без матчей):
SELECT *
FROM users u
LEFT JOIN orders o ON o.user_id = u.id
WHERE o.status = 'paid';

Хорошо (оставим всех users, а paid заказы подтянем где есть):
SELECT *
FROM users u
LEFT JOIN orders o
  ON o.user_id = u.id AND o.status = 'paid';
"""

# 2.2) Follow-up: Как сделать “нет записей справа” правильно? (anti-join)
"""
→ Предпочтительно NOT EXISTS (надёжно с NULL).

SELECT u.*
FROM users u
WHERE NOT EXISTS (
  SELECT 1
  FROM orders o
  WHERE o.user_id = u.id
);

NOT IN опасен, если подзапрос содержит NULL:
WHERE u.id NOT IN (SELECT user_id FROM orders)  -- может дать пустой результат / неожиданный результат
"""


# 3) NULL в SQL: трёхзначная логика и ловушки.
"""
→ В SQL есть TRUE / FALSE / UNKNOWN (из-за NULL).
Сравнение с NULL через '=' не работает:
- x = NULL -> UNKNOWN
Правильно:
- x IS NULL / x IS NOT NULL

Полезные функции:
- COALESCE(x, default)  — подставить значение по умолчанию
- NULLIF(a, b)          — вернуть NULL если a == b (удобно против деления на 0)
"""

# 3.1) Follow-up: Почему “NOT IN” ломается на NULL?
"""
→ Потому что сравнение с NULL даёт UNKNOWN, а в WHERE проходят только TRUE.
Если в подзапросе есть NULL, выражение может стать UNKNOWN для всех строк.
Решение: NOT EXISTS или фильтровать NULL в подзапросе.
"""


# 4) DISTINCT vs GROUP BY — что выбрать?
"""
→ DISTINCT убирает дубликаты строк результата.
GROUP BY группирует и обычно используется вместе с агрегатами.

Если нужно “уникальные строки” → DISTINCT.
Если нужно “агрегация по ключу” → GROUP BY.

Нюанс: иногда DISTINCT применяют как костыль против row explosion после JOIN — это симптом.
Лучше чинить гранулярность/агрегацию, а не “замазывать” DISTINCT’ом.
"""


# 5) Подзапросы vs CTE (WITH). Когда CTE может быть “медленнее”? (materialization нюансы).
"""
→ CTE (WITH) улучшает читаемость и позволяет строить этапы запроса.
Но иногда CTE может материализоваться (выполниться как временный результат),
и это может ухудшить план (особенно если CTE большой, а снаружи фильтры могли бы сузить выборку).

Практика:
- используем CTE для читабельности/переиспользования,
- но для performance — смотрим EXPLAIN (ANALYZE, BUFFERS).

Пример:
WITH recent AS (
  SELECT *
  FROM events
  WHERE created_at >= now() - interval '7 days'
)
SELECT user_id, count(*)
FROM recent
GROUP BY user_id;
"""

# 5.1) Follow-up: Когда CTE особенно полезен?
"""
→ Когда:
- сложный запрос удобно разложить на шаги,
- нужно переиспользовать подрезультат,
- нужно “зафиксировать” набор (иногда сознательно материализовать),
- рекурсивные запросы (WITH RECURSIVE).
"""

# 5.2) Follow-up: Коррелированные подзапросы — когда норм, когда боль.
"""
→ Коррелированный подзапрос выполняется “зависимо” от внешней строки.
Иногда оптимизатор превращает это в semi-join (норм), но иногда получаем N раз работу.

Частые паттерны:
- EXISTS обычно читаемее и чаще оптимизируется как semi-join.
- scalar subquery в SELECT может быть дорогим при больших N.

Правило: если выглядит как “для каждой строки делаем отдельный поиск” — проверить план.
"""


# 6) Оконные функции (window functions): что это и зачем?
"""
→ Оконные функции считают “агрегат по окну”, но НЕ схлопывают строки как GROUP BY.
Идеальны для ранжирования, скользящих сумм, “топ-N в группе”, сравнения с предыдущей строкой.

Ключевые слова:
- OVER (PARTITION BY ... ORDER BY ...)

Пример: топ-1 заказ по сумме на пользователя:
SELECT *
FROM (
  SELECT
    o.*,
    row_number() OVER (PARTITION BY user_id ORDER BY total DESC) AS rn
  FROM orders o
) t
WHERE rn = 1;

Пример: доля от суммы по пользователю:
SELECT
  user_id,
  order_id,
  total,
  total / SUM(total) OVER (PARTITION BY user_id) AS share
FROM orders;
"""

# 6.1) Follow-up: Чем window отличается от GROUP BY?
"""
→ GROUP BY уменьшает количество строк (агрегирует).
Window сохраняет строки, добавляя вычисленные колонки.


GROUP BY      — сжимает: из группы строк делает одну строку с агрегатом.
WINDOW (OVER) — НЕ сжимает: строки остаются, агрегат/ранг добавляется к каждой строке.
"""


# 7) Индексы (CORE + глубже): B-Tree, composite leading column, covering INCLUDE, partial, expression, GIN/GiST/BRIN.
"""
→ Индекс ускоряет чтение, но удорожает запись и занимает место.

B-Tree — стандарт для:
- равенства (=)
- диапазонов (>, <, BETWEEN)
- ORDER BY (если совпадает порядок)

Composite index (a, b) и правило “leading column”:
- отлично для WHERE a = ...
- отлично для WHERE a = ... AND b = ...
- но WHERE b = ... часто плохо (нет “левой” части)

Covering index (Postgres: INCLUDE):
- позволяет Index Only Scan, если все нужные колонки есть в индексе
Пример:
CREATE INDEX CONCURRENTLY idx_orders_user_created
ON orders(user_id, created_at) INCLUDE (total, status);

Partial index:
- индексируем только часть строк (уменьшаем размер и ускоряем типовой запрос)
Пример:
CREATE INDEX CONCURRENTLY idx_orders_paid
ON orders(created_at)
WHERE status = 'paid';

Expression index:
- если в WHERE используем выражение/функцию, индекс на “сырой” колонке может не помочь
Пример:
CREATE INDEX CONCURRENTLY idx_users_email_lower
ON users (lower(email));
Запрос:
SELECT * FROM users WHERE lower(email) = lower(:email);

Доп. Postgres индексы:
- GIN: jsonb, массивы, full-text (поиск по вхождениям)
- GiST: гео/range/некоторые “похожести”
- BRIN: огромные таблицы с коррелированным по времени столбцом (очень дешёвый индекс)
"""

# 7.1) Follow-up: Почему “индекс не используется”, хотя он есть?
"""
→ Причины:
- низкая селективность (почти все строки подходят)
- маленькая таблица (seq scan дешевле)
- условие “оборачивает” колонку (нужен expression index)
- несоответствие leading columns у composite index
- статистика устарела (ANALYZE / autovacuum)
- параметризация/план (generic plan) иногда даёт сюрпризы
"""

# 7.2) Follow-up: Почему “индекс может сделать хуже”?
"""
→ Каждый INSERT/UPDATE/DELETE обновляет индексы:
- падает write throughput
- растёт размер, maintenance, vacuum/реиндекс
Поэтому индексы ставим “под запрос”, проверяем планом и метриками.
"""


# 8) Query patterns: EXISTS vs IN, UNION vs OR, COUNT(*) нюансы, DISTINCT ON.
"""
→ EXISTS vs IN:
- EXISTS обычно безопаснее (особенно с NULL) и часто хорошо оптимизируется (semi-join).
- IN ок для маленьких списков/результатов, но следим за NULL и планом.

UNION vs OR:
- OR по разным колонкам иногда мешает использовать индекс (зависит от плана).
- Иногда быстрее переписать как UNION ALL двух индексных запросов (и потом DISTINCT при необходимости).

COUNT(*) vs COUNT(col):
- COUNT(*) считает строки (включая NULL в колонках).
- COUNT(col) считает только НЕ-NULL значения col.
- COUNT(DISTINCT col) дорогой (sort/hash), иногда нужен индекс/перепись/агрегация.

Postgres-фишка: DISTINCT ON
- “взять по одной строке на ключ, по правилу сортировки”
Пример:
SELECT DISTINCT ON (user_id) *
FROM orders
ORDER BY user_id, created_at DESC;
"""


# 9) EXPLAIN / EXPLAIN ANALYZE / BUFFERS: что смотреть и как “читать” план.
"""
→ EXPLAIN показывает план оптимизатора (как он собирается выполнять).
EXPLAIN ANALYZE показывает фактическое выполнение (time/rows/loops).
BUFFERS показывает профиль I/O: shared hit vs read и т.п.

Что обычно смотрим:
- Seq Scan vs Index Scan / Index Only Scan
- Nested Loop / Hash Join / Merge Join (тип JOIN)
- estimated rows vs actual rows (ошибки статистики)
- loops (сколько раз выполнялся узел) — часто вскрывает “скрытый N раз”
- sort/hash memory, spill to disk (если видно)
- total time и “где узкое место”

Практика:
- сначала EXPLAIN (ANALYZE, BUFFERS) на проблемном запросе,
- затем индекс/перепись запроса,
- снова ANALYZE, чтобы подтвердить улучшение.
"""

# 9.1) Follow-up: “Index Only Scan” — когда он возможен и почему не всегда “only”?
"""
→ Возможен, когда все нужные колонки есть в индексе (covering index).
Но могут быть heap fetches, если страница не помечена как “вся видима” (visibility map).
Чем лучше VACUUM/VM — тем больше реальных index-only.
"""


# 10) Транзакции и ACID “в терминах практики”. MVCC. Держим транзакции короткими.
"""
→ Транзакция = атомарный блок изменений.

ACID:
- Atomicity: либо всё, либо ничего (rollback)
- Consistency: инварианты/constraints не нарушаются
- Isolation: конкурентные транзакции не ломают друг друга (уровни изоляции)
- Durability: после commit данные переживут падение

Postgres/MVCC:
- чтения видят snapshot и обычно не блокируют записи
- UPDATE/DELETE создают новые версии строк → появляются dead tuples → нужен VACUUM/autovacuum

Практические правила:
- транзакции держим короткими
- внешние HTTP вызовы не делаем внутри транзакции (иначе удерживаем локи/соединение)
- deadlock/serialization failure — нормальны под конкуренцией → нужен ограниченный retry транзакционного блока

# КОРОТКО:
ACID — свойства надежных транзакций в БД:

Атомарность — либо всё, либо ничего.
Пример: Перевод денег: если списание прошло, а зачисление нет — операция отменится.

Согласованность — данные всегда корректны.
Пример: Невозможно записать отрицательный баланс, если есть ограничение balance >= 0.

Изолированность — транзакции не мешают друг другу.
Пример: Пока один пользователь не завершит редактирование записи, другой не увидит её изменения.

Долговечность — изменения сохраняются даже при сбое.
Пример: После подтверждения платежа он останется в БД, даже если сервер упадёт.

Короче: «Либо всё, либо ничего; только по правилам; без помех; навсегда».    <-----    <-----
"""

# 10.1) Follow-up: Read-modify-write и lost update — как защищаться?
"""
→ Lost update: две транзакции читают одно значение, обе пишут “поверх”, одно обновление теряется.

Защита:
- SELECT ... FOR UPDATE (пессимистическая блокировка)
- optimistic locking (version column)
- атомарные UPDATE с условием (update ... where version = ...)
"""


# 11) Уровни изоляции (Postgres-ориентировано): что обычно спрашивают.
"""
→ Коротко:
- Read Committed: каждый запрос видит snapshot на момент начала запроса
- Repeatable Read (в Postgres близко к snapshot isolation): стабильный snapshot на всю транзакцию, возможен write skew
- Serializable: эквивалентно последовательному выполнению, но возможны откаты (serialization failures) → retry

Собеседование-посыл:
- “Serializable даёт максимум, но требует ретраев и может быть дороже”.
"""


# 12) Locks и конкурентность: FOR UPDATE, уровни блокировок, SKIP LOCKED, deadlocks.
"""
→ Row-level locks:
- SELECT ... FOR UPDATE: блокируем выбранные строки (другие не смогут обновить/взять FOR UPDATE)
- FOR NO KEY UPDATE / FOR SHARE / FOR KEY SHARE — более тонкие режимы

Классика “очереди задач” (несколько воркеров):
SELECT id
FROM jobs
WHERE status = 'new'
ORDER BY created_at
FOR UPDATE SKIP LOCKED
LIMIT 10;

NOWAIT:
- FOR UPDATE NOWAIT — сразу ошибка, если строка заблокирована (для low-latency)

Deadlock:
- транзакции взяли блокировки в разном порядке и ждут друг друга
Профилактика:
- одинаковый порядок захвата ресурсов
- короткие транзакции
- правильные индексы/условия (меньше лишних блокировок)

Диагностика (Postgres):
- логи “deadlock detected”
- pg_stat_activity (кто что делает)
- pg_locks (кто кого держит/ждёт)
"""

# Блокировки (Locks) vs Deadlock в SQL
"""
Блокировки (Locks) - Механизм контроля доступа к данным при параллельных транзакциях.

Типы:

- Shared (S) – чтение (много транзакций).
- Exclusive (X) – запись (только одна).
- Update (U) – подготовка к изменению.
- Intent – намерение заблокировать данные.

Deadlock - Взаимная блокировка: две транзакции ждут ресурсы друг друга, создавая тупик.

Пример:

Транзакция A блокирует строку 1, B – строку 2.
A хочет строку 2, B – строку 1.
Обе зависают.
"""



# Как “решают” блокировки
"""
- Избегают долгих транзакций: меньше держишь locks → меньше конфликтов.
- Индексы + точные WHERE: чтобы блокировать меньше строк (не сканировать таблицу).
- Правильный уровень изоляции: где можно — ниже (READ COMMITTED вместо SERIALIZABLE).
- MVCC/снимки: чтения часто идут без блокировок (зависит от СУБД).
- Пессимистические/оптимистические подходы: SELECT ... FOR UPDATE (пессимистично) или проверка версии/таймстампа (оптимистично).
"""


# Как решают deadlock    ---->  Deadlock обычно решает сама СУБД
"""
- Детект: СУБД строит граф ожиданий, находит цикл → убивает одну транзакцию (victim) и откатывает.
- Таймаут: если слишком долго ждёт → ошибка/откат.
- Профилактика (в коде)
  - Одинаковый порядок захвата ресурсов (всегда сначала строка 1, потом 2).
  - Короткие транзакции и меньше “ручных” блокировок.
  - Retry: при deadlock-ошибке обычно делают повтор транзакции (с backoff).
  
Твоя задача в приложении: поймать ошибку deadlock и сделать retry (и по возможности уменьшать шанс дедлока:
один порядок блокировок, короткие транзакции, индексы, меньше “долгих” lock’ов).
"""


# 12.1) Follow-up: Table-level locks и DDL — почему миграции опасны?
"""
→ Некоторые DDL берут сильные table locks.
Например, CREATE INDEX без CONCURRENTLY может блокировать записи/иногда чтения.
Вывод: миграции делаем “безопасно” (CONCURRENTLY, NOT VALID, батчи).
"""


# 13) UPSERT и уникальные ограничения: как делать идемпотентную запись.
"""
→ Идемпотентность на уровне БД часто делают через unique constraint + upsert.

PostgreSQL:
INSERT INTO payments(operation_id, user_id, amount)
VALUES (:op_id, :user_id, :amount)
ON CONFLICT (operation_id) DO NOTHING;

Или обновить при конфликте:
... ON CONFLICT (operation_id) DO UPDATE
SET amount = EXCLUDED.amount;
"""

# 13.1) Follow-up: Почему unique constraint — защита от гонок?
"""
→ Это гарантия на уровне БД, а не “проверил — потом вставил” в приложении,
которое ломается при параллельных запросах.
"""


# 14) Pagination: OFFSET/LIMIT vs keyset pagination (seek).
"""
→ OFFSET/LIMIT прост, но OFFSET становится дорогим на больших значениях:
СУБД всё равно “пролистывает” OFFSET строк.

Keyset pagination (seek) — быстрее и стабильнее:
- используем “последний seen key” (created_at/id)
- WHERE (created_at, id) > (:last_created_at, :last_id)
- ORDER BY created_at, id
- LIMIT N

Пример:
SELECT *
FROM events
WHERE (created_at, id) > (:ts, :id)
ORDER BY created_at, id
LIMIT 50;

OFFSET — это “сколько строк пропустить” перед тем, как вернуть LIMIT строк.
Например: LIMIT 50 OFFSET 100 = пропусти первые 100, верни следующие 50.
"""

# 14.1) Follow-up: Почему keyset лучше для “бесконечной ленты”?
"""
→ Потому что:
- нет деградации с ростом страницы
- устойчивее к вставкам/удалениям (меньше “скачков”)
- лучше p99 на больших объёмах
"""


# 15) N+1 в SQL/ORM: как распознать и чинить.
"""
→ N+1:
- 1 запрос на список сущностей
- и затем N запросов на связи (ленивая догрузка)

Как распознать:
- рост “queries per request”
- много одинаковых запросов в логах/трейсах

Как чинить:
- eager loading (join/selectin/prefetch)
- батчинг по id (IN (...) чанками)
- пересмотр модели выдачи (проекции, отдельные запросы агрегатами)
"""

# 15.1) Follow-up: Почему JOIN eager loading иногда “хуже”?
"""
→ Из-за row explosion в one-to-many:
- больше данных по сети
- больше памяти
- сложнее DISTINCT/агрегации
Иногда selectin/prefetch (2–3 запроса) быстрее и предсказуемее.
"""


# 16) Constraints и “границы данных”: PK/UK/FK/CHECK/NOT NULL — что уметь проговорить.
"""
 Constraint (ограничение) в SQL — это правило, которое ограничивает данные в таблице для сохранения их целостности.

 Constraints  Основные типы:
 - PRIMARY KEY — уникальный идентификатор (не NULL).
 - FOREIGN KEY — связь с другой таблицей.
 - UNIQUE      — все значения в столбце уникальны.
 - NOT NULL    — запрет на пустые значения.
 - CHECK       — проверка условия (например, age > 0).

Зачем:
- держат инварианты ближе к данным
- уменьшают класс багов “приложение забыло проверить”
- помогают корректности при параллельных операциях
"""

# 16.1) Follow-up: Почему “валидация только в приложении” хуже?
"""
→ Потому что:
- БД — единая защита для всех клиентов/воркеров/скриптов
- меньше гонок
- проще гарантировать инварианты
"""


# 17) Нормальные формы и денормализация: когда “осознанно нарушаем ради чтений”.
"""
→ Нормализация (идея):
- уменьшаем дублирование данных
- избегаем аномалий вставки/обновления/удаления

Интервью-рамка:
- 1NF: атомарные значения, нет “списков в ячейке”
- 2NF: нет частичных зависимостей от составного ключа
- 3NF: нет транзитивных зависимостей (неключевые зависят только от ключа)

Денормализация (осознанно):
- когда чтений много и они дорогие (join-heavy), а обновления редкие/контролируемые
- когда нужны быстрые агрегаты (счётчики/итоги) и допустим eventual consistency

Техники:
- summary tables
- materialized views
- derived fields (например, total_amount в заказе)
- JSONB для “редких/гибких” атрибутов (осторожно, индексирование/валидация)

Цена:
- риск рассинхронизации (нужны инварианты и стратегия обновления)
- сложнее запись (триггеры/фоновые джобы/outbox)
- больше тестов/наблюдаемости

Senior-посыл:
- “нормализую по умолчанию, денормализую только под измеренный bottleneck и с чёткой стратегией консистентности”.

Нормализация — для целостности и удобства обновлений:
 - уменьшает дублирование
 - уменьшает риск аномалий (update/insert/delete)
 - обычно увеличивает число таблиц и JOIN’ов → чтение иногда медленнее, запись/обновления проще и безопаснее

Денормализация — для скорости чтения и простых запросов:
 - уменьшает JOIN’ы → чтение часто быстрее
 - увеличивает дублирование и размер данных
 - усложняет записи/обновления (надо обновлять несколько мест) и повышает риск рассинхронизации данных
"""


# 18) Cardinality estimation и статистика (Postgres): почему план “плохой”.
"""
→ Оптимизатор выбирает план на основе оценок:
- сколько строк пройдёт фильтр (селективность)
- распределения значений
- корреляция данных с физическим порядком
- статистика по колонкам/таблицам

Если оценки сильно мимо:
- выбираются плохие join стратегии
- не используются индексы
- план “взрывается”

Инструменты:
- ANALYZE (обновить статистику)
- statistics_target на проблемных колонках
- extended statistics (dependencies, ndistinct)
- следить за autovacuum (он же обычно делает analyze)

Ключ:
- “плохой план часто начинается с плохих оценок”.
"""


# 19) Partitions (Postgres): зачем, когда, и какие риски.
"""
→ Partitioning полезен, когда:
- таблица очень большая
- типовые запросы отрезают небольшой диапазон (например, по времени)
- нужен быстрый drop старых данных (drop partition)

Типы:
- RANGE (часто time-series)
- LIST (по категории/тенанту)
- HASH (равномерное распределение)

Плюсы:
- partition pruning (сканируем только нужные партиции)
- управление жизненным циклом данных проще (drop/attach/detach)

Риски:
- слишком много партиций → overhead планирования/maintenance
- индексы/constraints сложнее
- не заменяет индексы “магически”: запросы должны быть “prunable”
"""


# 20) VACUUM/autovacuum, bloat, long-running transactions, visibility map (Index Only Scan).
"""
→ Почему Postgres “раздувается”:
- UPDATE/DELETE создают новые версии (MVCC)
- старые версии становятся dead tuples и ждут VACUUM

Bloat:
- таблица/индекс растут из-за накопления мёртвых версий и фрагментации
- падает cache locality, растёт I/O

Autovacuum:
- должен убирать мусор и обновлять статистику
- если не успевает: bloat, деградация запросов, риск проблем с wraparound

Long-running transactions:
- удерживают старые snapshot’ы → vacuum не может зачистить старые версии
- классическая причина “почему vacuum не помогает”

Visibility map:
- помогает Index Only Scan (страницы “полностью видимы” всем транзакциям)
- чем лучше vacuum/VM, тем больше настоящих index-only
"""


# 21) Безопасные миграции в Postgres: CONCURRENTLY, NOT VALID, expand/contract.
"""
→ CREATE INDEX CONCURRENTLY:
- уменьшает блокировки (но дольше, и нельзя внутри транзакции)
- must-have для больших таблиц в проде

DROP INDEX CONCURRENTLY:
- безопасное удаление без долгих блокировок

Constraints “постепенно”:
- добавить FOREIGN KEY / CHECK как NOT VALID (не проверять исторические данные сразу)
- потом VALIDATE CONSTRAINT (проверка без “жёсткого” lock, как у некоторых вариантов “сразу всё”)

Expand/Contract:
- Expand: добавить новое (колонку/таблицу/индекс) с обратной совместимостью
- затем выкатить код, который использует новое
- Contract: убрать старое после миграционного окна

Ловушка:
- некоторые ALTER TABLE могут переписывать таблицу/брать сильные lock’и (зависит от операции/версии).
Практика:
- add nullable → backfill батчами → default → not null (аккуратно и измеримо)
"""


# 22) Типичные perf-ловушки в запросах (интервью-топ).
"""
→ Частые источники боли:
- SELECT * (лишние колонки, сеть/IO)
- функции/касты на колонках в WHERE (ломают индекс → нужен expression index)
- OR по разным колонкам (иногда лучше UNION ALL двух индексных запросов)
- JOIN без нужных условий (случайный CROSS JOIN)
- отсутствие LIMIT на админских выборках
- сортировка больших наборов без индекса (дорогой sort)
- long transactions (локи, пул, MVCC bloat)
- “замазывание” логической ошибки DISTINCT’ом

Подход “по-взрослому”:
- сначала EXPLAIN (ANALYZE, BUFFERS)
- затем точечная правка (индекс/перепись/ограничение результата)
- снова измерение и подтверждение эффекта
"""



### TODO PYTHON DATA TYPES (CORE + MID)


# 1) Какие базовые типы данных есть в Python? (high-level карта)
"""
→ Условно делим на категории:

Scalar / примитивы:
- int, float, bool, complex
- NoneType (None)

Текст/байты:
- str (Unicode текст)
- bytes (неизменяемые байты)
- bytearray (изменяемые байты)

Коллекции:
- list, tuple, range (последовательности)
- dict (map: key -> value)
- set, frozenset (множества)

Прочее часто в интервью:
- function / callable
- type (классы), object (база всех объектов)
- iterator / generator
"""


# 1.1) Follow-up: Что важно сказать на собесе “сеньорским тоном” про типы?
"""
→ Python — язык объектов:
- “всё — объект” (и типы тоже объекты)
- у объекта есть identity (id), type, value/состояние
- семантика сильно завязана на мутабельность/хешируемость/протоколы (__iter__, __len__, __hash__, ...)
"""


# 2) Mutable vs Immutable: какие типы изменяемые и почему это важно?
"""
→ Mutable (изменяемые):
- list, dict, set, bytearray
- большинство пользовательских классов (у них обычно есть __dict__)

Immutable (неизменяемые):
- int, float, bool, str, bytes, tuple, frozenset, range, NoneType

Почему важно:
- immutable обычно хешируемые → могут быть ключами dict / элементами set
- mutable нельзя безопасно хешировать (изменился объект → “потерялся” в hash table)
- мутабельность = риск сайд-эффектов (особенно в функциях/кэше/конкурентности)
"""

# Hashable
"""
Hashable - обьект является Hashable если у него есть Hash и за время жизни обьекта этот Hash никогда не меняется

Hashable Строки, Числа, Кортежи(если они состоят из неизменяемых обьектов, строк, чисел)
Hashable != Immutable
Для встроенных типов Hashable == Immutable
Hashable - (int, float, bool, str, frozenset, tuple(если он состоит из НЕизменяемых обьектов), bytes, complex, range, ЭК)
unhashable type - (list, set, dict, tuple(если есть хоть 1 изменяемый обьект), bytearray)
"""

# 2.1) Follow-up: Почему tuple “immutable”, но может быть “mutable по смыслу”?
"""
→ tuple нельзя изменить как контейнер (нельзя присвоить t[0] = ...),
но его элементы могут быть изменяемыми:

t = ({"x": 1}, 2)
t[0]["x"] = 99   # валидно → изменился вложенный dict
"""


# 3) Hashable: что значит “хешируемый тип” и кто хешируемый?
"""
→ Hashable = у объекта есть стабильный __hash__ и согласованный __eq__.
Hashable можно:
- использовать как ключ dict
- класть в set

Обычно hashable:
- int/float/bool/str/bytes/None
- tuple/frozenset — только если все элементы hashable

Не hashable:
- list/dict/set/bytearray (и любые mutable по умолчанию)
"""


# 3.1) Follow-up: Почему float “опасный ключ”?
"""
→ Из-за особенностей сравнения и округления (0.1 + 0.2 != 0.3).
Как ключ можно, но обычно лучше избегать float-ключей для бизнес-идентификаторов.


-- float небольшие погрешности, которые накапливаются при вычислениях --

from math import isclose, fsum

# Результат 0.1 + 0.2 на самом деле равен 0.30000000000000004, что не равно 0.3.
print(0.1 + 0.2 == 0.3)         # -> False
# isclose из модуля math сравнивает числа с учетом небольшой погрешности
print(isclose(0.1 + 0.2, 0.3))  # -> True

print((0.1 + 0.1 + 0.1) == 0.3)                                                 # -> False
# sum складывает числа последовательно, что может приводить к накоплению ошибки
print(sum([0.1 + 0.1 + 0.1]) == 0.3)                                            # -> False
print(sum([0.1] * 2)== 0.3)                                                     # -> False
# предназначена для более точного суммирования чисел с плавающей точкой, но даже она не всегда может устранить все ошибки
print(fsum([0.1] * 2)== 0.3)                                                    # -> False
# Странно как-то
print((0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1) == 1.0)       # -> False
# sum в данном случае работает достаточно точно из-за особенностей реализации.
print(sum([0.1] * 10) == 1.0)                                                   # -> True
#fsum специально оптимизирована для минимизации ошибок при суммировании.
print(fsum([0.1] * 10) == 1.0)                                                  # -> True


# Ответ почему так происходит !!!                              <-----      <-----
from decimal import Decimal

# Накопление мусора
print(Decimal(0.1))        # -> 0.1000000000000000055511151231257827021181583404541015625
print(Decimal(0.2))        # -> 0.200000000000000011102230246251565404236316680908203125

# Разница возникает из-за погрешностей двоичного представления float:
# 1. 0.1 + 0.2 в float даёт 0.30000000000000004, что передаётся в Decimal.
# 2. 0.3 в float уже имеет свою погрешность, которая отличается от результата сложения.
print(Decimal(0.1 + 0.2))  # -> 0.3000000000000000444089209850062616169452667236328125
print(Decimal(0.3))        # -> 0.299999999999999988897769753748434595763683319091796875


from decimal import Decimal

# Используйте строки для точного представления чисел
print(Decimal("0.1"))                   # -> 0.1
print(Decimal("0.2"))                   # -> 0.2
print(Decimal("0.1") + Decimal("0.2"))  # -> 0.3
print(Decimal("0.3"))                   # -> 0.3
"""


# 4) str vs bytes: в чём разница “по делу”?
"""
→ str = текст (Unicode). bytes = сырые байты (часто данные сети/файла/протокола).

Правило:
- “на границе” (I/O) мы работаем с bytes
- внутри приложения с текстом — str

Переход:
- encode: str -> bytes  (например, 'utf-8')
- decode: bytes -> str

Пример:
b = "привет".encode("utf-8")
s = b.decode("utf-8")
"""


# 4.1) Follow-up: Почему нельзя “смешивать” str и bytes?
"""
→ Это разные домены: текст vs бинарные данные.
Python специально требует явного encode/decode, чтобы не было тихих багов с кодировками.
"""


# 5) list vs tuple: чем отличаются и когда что выбирать?
"""
→ list — изменяемая последовательность: удобно накапливать/менять.
→ tuple — неизменяемая “запись” значений: безопаснее шарить между слоями, может быть ключом dict (если hashable элементы).

Практика:
- list: строим/мутируем
- tuple: фиксируем “результат” / return values / ключи


# АННОТАЦИИ
# Кортежи (tuple) могут иметь фиксированную длину (shape) или переменную длину в аннотациях типов.
# Списки (list) НЕ имеют фиксированной длины (shape) и всегда описываются как список элементов одного типа.

# tuple фиксированной длины (shape)
b: tuple[int, str, float] = (1, "x", 2.5)

# tuple переменной длины (один тип, любая длина)
c: tuple[int, ...] = (1, 2, 3, 4)


Нюанс: технически и list, и tuple могут хранить разные типы — просто смысловое использование отличается.

Этот пункт больше касается передовой практики.
- Кортежи следует использовать как гетерогенные структуры данных (предназначенные для хранения разнородных, разнотипных данных),
- Списки — как однородные последовательности.
Гетерогенный = неоднородный, “из разных типов”.
"""


# 6) dict / set: как работают “на пальцах”?
"""
→ dict и set — hash table:
- быстрое среднее O(1) на lookup/insert
- требуют hashable ключей/элементов
- коллизии решаются внутри реализацией (нам важно: __hash__ + __eq__)

Важный собеседовательный поинт:
- порядок в dict сохраняется (с Python 3.7+ как контракт языка)
- но это не значит, что можно строить логику на “порядке set”
"""


# 6.1) Follow-up: Почему нельзя полагаться на порядок set?
"""
→ set — неупорядоченная структура, и порядок итерации зависит от хешей (и может меняться между запусками).
Если нужен порядок — сортируем или используем list/tuple.
"""


# 7) Truthiness: какие значения считаются False?
"""
→ False в условии:
- False
- None
- 0, 0.0, 0j
- пустые контейнеры: "", b"", [], (), {}, set()
- объекты, у которых __bool__ возвращает False или __len__ == 0

Правильный стиль:
- if items: ...
- if x is None: ...  (для None отдельно)
"""


# 8) Numeric tower: int / float / bool и нюансы
"""
→ bool — это подкласс int:
isinstance(True, int) == True
True + True == 2

int — произвольной точности.
float — IEEE 754 double (ограниченная точность).
complex — комплексные числа (a + bj).
"""


# 9) range: что это за тип и почему он “лёгкий”?
"""
→ range — ленивое представление диапазона:
- не хранит все числа в памяти
- умеет быстро проверять принадлежность (in) и получать длину

range(10**12) не создаёт список из триллиона элементов.
"""


# 10) Итераторы и генераторы: какой “тип” и что это даёт?
"""
→ Iterator — объект с __next__ (и обычно __iter__ возвращает self).
Generator — частный случай iterator (создаётся через yield / generator expression).

Практика:
- генераторы экономят память
- одноразовые (исчерпываются)
"""


# 11) Пользовательские типы: классы, dataclass, Enum — что сказать кратко?
"""
→ class — свой тип, по умолчанию экземпляры mutable (есть __dict__).
dataclass — удобный “record/DTO” (авто __init__/__repr__/__eq__).
Enum — фиксированный набор значений (часто удобно для статусов/режимов).

На интервью:
- dataclass(frozen=True) = “почти immutable контракт”
- Enum-члены — синглтоны (часто сравнивают через is)
"""


# 12) Мини-шпаргалка: “тип → куда обычно применяют”
"""
- str: тексты, имена, json-поля после парсинга
- bytes: сеть/файлы/протоколы/крипто
- list: рабочий буфер, накопление элементов
- tuple: фиксированные результаты, ключи (если hashable)
- dict: маппинги, конфиги, JSON-объекты
- set: уникальность, membership check
- frozenset: “set как ключ”
- None: “нет значения” (sentinel)
"""



### TODO PYTHON CORE

# 1) Объясни, что такое GIL в CPython. В каких сценариях потоки дадут выигрыш, а в каких — нет?
"""
→ GIL (Global Interpreter Lock) — глобальная блокировка в интерпретаторе CPython, из-за которой в один момент времени
только один поток может исполнять Python-байткод. Это упрощает реализацию (защищает внутренние структуры CPython:
refcount, менеджер памяти, многие инварианты объектов) и делает ряд операций над объектами CPython потокобезопасными
на уровне интерпретатора, но ограничивает параллелизм для CPU-bound кода на чистом Python.

Где потоки помогают:
- I/O-bound: сеть, файловые операции, ожидание БД/внешних сервисов. Поток часто простаивает в ожидании,
  а I/O-реализации в C обычно освобождают GIL на время блокирующего системного вызова → другие потоки могут работать.
- “Обвязка” вокруг блокирующих библиотек: параллельное ожидание множества запросов, когда переписывать всё на async дорого.

Где потоки не помогают (или вредят):
- CPU-bound чистый Python: потоки конкурируют за GIL → прироста почти нет, часто деградация из-за overhead планирования
  и контеншена на GIL/локах.

Важные детали, которые любят на интервью:
- GIL ограничивает исполнение Python-байткода, но не запрещает параллельность в нативном коде (если он release’ит GIL).
- Переключение между потоками — НЕ “магия”: есть time-slicing (sys.setswitchinterval) + точки release GIL в C.  <-----

 GIL может быть удержан потоком не дольше 5 мс  - 0.005
 time.sleep(t) останавливает текущий поток минимум на t секунд (поток “спит”, CPU не жрёт).
 В CPython на время sleep поток отпускает GIL, поэтому другие потоки могут выполнять Python-код — из-за этого 
 и видно переключение/перемешивание.
 
 - Переключение между потоками — НЕ “магия”: есть time-slicing (sys.setswitchinterval) + точки release GIL в C.  <-----

 import sys
 print(sys.getswitchinterval())  # -> 0.005
 
 Потоки ОС переключает планировщик ОС (например, в Linux - CFS).                        <-----
  
 ОС переключает потоки своим планировщиком (в Linux обычно CFS для обычных задач).
 CPython/GIL лишь решает, какой поток может выполнять Python-байткод в данный момент;
 а когда именно поток получит CPU — решает планировщик ОС.

Что делают вместо:
- multiprocessing/ProcessPoolExecutor (реальный параллелизм, но дороже IPC/память),
- перенос hot-path в C/Numba/Cython/NumPy (можно отпускать GIL),
- оптимизация алгоритма/структур данных (часто самый большой эффект),
- для большого числа I/O-коннектов — asyncio (меньше overhead на потоки).
"""



# 1.1) Follow-up: Когда GIL реально “отпускается” и почему это важно для I/O?
"""
→ В CPython GIL освобождают там, где поток надолго уходит в C и в этот момент не трогает Python-объекты:
- на блокирующих системных вызовах (socket read/write, file I/O) — при корректной реализации,
- в C-расширениях на тяжёлых вычислениях (NumPy/BLAS), если они явно release’ят GIL.

Техническая деталь: в расширениях это обычно делается через макросы
Py_BEGIN_ALLOW_THREADS / Py_END_ALLOW_THREADS (или эквиваленты).
Если библиотека не освобождает GIL (или CPU-bound в Python) — выигрыш от потоков будет минимальным.

Почему важно для I/O:
- пока один поток ждёт I/O (GIL отпущен), другие могут исполнять Python-код,
  и мы получаем реальную конкурентность на ожидании.
"""

# 1.2) Follow-up: Почему multiprocessing ускоряет CPU-bound и какие у него издержки?
"""
→ Multiprocessing даёт реальный параллелизм, потому что каждый процесс имеет свой интерпретатор и свой GIL.

Издержки:
- передача данных между процессами (pickle/IPC) → latency/CPU overhead,
- больше памяти (отдельные процессы, копии данных; особенно ощутимо на больших структурах),
- сложнее shared state (очереди/пайпы/shared_memory/manager),
- вопросы старта процессов (spawn vs fork): на Windows/macOS чаще spawn (дороже старт), fork может наследовать состояние,
- управление пулом: graceful shutdown, обработка ошибок воркеров, timeouts.

На собеседовании обычно ждут: “даёт параллелизм, но дороже по IPC и памяти”.
"""


#  -- fork spawn --
"""
- fork (Linux/macOS): новый процесс создаётся как “копия” текущего — быстрее старт, наследует память/состояние
(но из-за наследования возможны тонкие баги с потоками/ресурсами).

- spawn (Windows): новый процесс стартует “с нуля” — медленнее, ничего лишнего не наследует; поэтому нужно,
 чтобы код был импортируемым и был if __name__ == "__main__":.
"""


# 1.3) Follow-up: Как C-расширения/NumPy обходят ограничения GIL?
"""
→ Тяжёлая часть вычислений выполняется в нативном коде. На время вычисления расширение может отпустить GIL,
если ему не нужен доступ к Python-объектам. Поэтому NumPy/BLAS могут эффективно распараллеливаться на уровне
нативных потоков.

Ключевая мысль: “GIL ограничивает байткод Python, но не нативный код при release GIL”.

Практический нюанс из проды:
- возможна oversubscription: процессы Python × потоки BLAS → “стало хуже”.
  Решение: лимитировать потоки BLAS (OMP_NUM_THREADS / MKL_NUM_THREADS) и/или подбирать стратегию параллелизма.

BLAS — набор быстрых низкоуровневых функций линейной алгебры, используемых NumPy для dot и A @ B (часто многопоточно).
"""

# 1.4) Follow-up: Почему “добавили потоки — стало хуже”? Приведи причины.
"""
→ Частые причины деградации:
- overhead на переключения контекста и конкуренцию за GIL,
- contention на локах/общих структурах (очереди, кэш, глобальные объекты),
- исчерпание внешних лимитов: пул соединений, лимиты БД/API → 429/timeout, рост очередей,
- увеличение tail latency (планирование, очереди задач, lock contention),
- oversubscription ресурсов (слишком много потоков/воркеров на одно CPU).

Правильный подход: измерить (профиль/метрики), ограничить параллелизм, добавить backpressure,
таймауты и ретраи осознанно.

Backpressure — это механизм, который не даёт системе принимать/производить задачи быстрее,
чем потребитель успевает обрабатывать, заставляя источник замедляться (или копить ограниченно/отказывать).
"""

# 1.5) Follow-up: Чем thread pool отличается от “просто создать много Thread”, и почему это важно?
"""
→ Пул ограничивает параллелизм и переиспользует потоки:
- меньше overhead на создание/уничтожение потоков,
- проще контролировать нагрузку на внешние зависимости,
- предсказуемее tail latency.
В проде чаще используют ThreadPoolExecutor и подбирают max_workers под I/O и лимиты пулов (БД/HTTP).
"""

# 1.6) Follow-up: Почему “CPU-bound в Python” не ускоряется потоками, но иногда ускоряется “CPU-bound в NumPy”?
"""
→ Потому что чистый Python исполняет байткод под GIL → реального параллелизма нет.
NumPy/BLAS считают в нативном коде и могут отпускать GIL + сами распараллеливаться нативными потоками.
Поэтому “CPU-bound” может ускоряться, если hot-path не в Python-байткоде, а в C/Fortran/BLAS.
"""


# 2) Чем отличаются list и tuple, и когда что выбирать?
"""
→ list — изменяемая последовательность: можно добавлять/удалять/менять элементы.
Хорошо подходит для коллекций, которые строятся постепенно или будут мутировать.
Важно: list реализован как динамический массив и растёт с over-allocation, поэтому append обычно amortized O(1).

tuple — неизменяемая последовательность: фиксированный набор значений.
Практические плюсы:
- семантика “значения/записи” (record-like), удобно передавать через слои без риска случайной мутации,
- может быть ключом dict/элементом set (если все элементы хешируемы),
- обычно чуть компактнее по памяти и иногда немного быстрее на доступ/итерацию (но это вторично).

Критерий выбора на интервью: “мутируется ли структура и нужна ли неизменяемость/хешируемость”.
"""

# 2.1) Follow-up: Что значит “хешируемость” и почему tuple не всегда хешируемый?
"""
→ Хешируемый объект должен иметь стабильный __hash__ и корректное равенство (__eq__ согласовано с hash).
tuple хешируемый только если хешируемы все элементы.
tuple с list внутри не хешируемый, потому что list изменяемый → hash был бы нестабилен.
"""

# 2.2) Follow-up: Shallow copy vs deep copy — что будет с вложенными объектами?
"""
→ Shallow copy копирует только контейнер, элементы остаются теми же объектами (общие ссылки).
Deep copy рекурсивно копирует всё дерево объектов (дорого и может иметь проблемы с циклами и внешними ресурсами).

Инструменты:
- copy.copy(x) → shallow
- copy.deepcopy(x) → deep (иногда требует __deepcopy__, может быть “опасен” для ресурсов)

Важно: tuple неизменяемый как контейнер, но его элементы могут быть изменяемыми (tuple с dict внутри).
"""

# 2.3) Follow-up: Почему list[T] инвариантен в типизации, а Sequence[T] ковариантна?
"""
→ list — изменяемый: если бы он был ковариантным, можно было бы подставить list[Cat] туда, где ждут list[Animal],
и затем добавить Dog в list[Cat] → нарушение типов.
Sequence — read-only интерфейс, поэтому ковариантность безопаснее.

Инвариантность — тип-параметр нельзя “поднимать/опускать” по иерархии: даже если Cat <: Animal, 
всё равно list[Cat] не является ни подтипом list[Animal], ни надтипом.
Пример: list[T] обычно инвариантен.

Ковариантность — если Cat <: Animal, то и Sequence[Cat] <: Sequence[Animal]. То есть “контейнер” можно использовать там,
где ожидают более общий тип, если он только для чтения.
Пример: Sequence[T] часто ковариантен.


Ковариант — когда можно подставить более “узкий” тип вместо более “широкого” и всё ок.
Пример: если Кот = Животное, то List[Кот] можно использовать там, где ждут List[Животное] (это ковариантность).

Инвариант — когда так подставлять нельзя: List[Кот] не считается List[Животное], даже если Кот — это Животное.


Инвариантность: “НЕЛЬЗЯ подменять”      — G[Cat] нельзя использовать вместо G[Animal], даже если Cat — это Animal.
Ковариантность: “можно подменять вверх” — G[Cat] можно использовать там, где ждут G[Animal].


В МАТЕМАТИКЕ:
Ковариант — величина меняется вместе с другой (в ту же сторону).
Инвариант — величина НЕ меняется, хотя ты что-то преобразуешь/меняешь условия.
Пример-интуиция: повернули/сдвинули фигуру → площадь обычно инвариант, а координаты точек ковариант (меняются).
"""

# 2.4) Follow-up: Какие типичные ошибки с “изменяемыми значениями по умолчанию”?
"""
→ Частая ловушка:
def f(x=[]): ...
Список создаётся один раз при определении функции и переиспользуется между вызовами.

Правильно:
def f(x=None):
    if x is None:
        x = []

Ещё одна частая форма (dataclasses):
- field(default_factory=list) вместо default=[]
"""

# 2.5) Follow-up: Чем tuple отличается от “замороженного” объекта вроде dataclass(frozen=True)?
"""
→ tuple — просто последовательность без имён полей, удобно для коротких “пакетов” значений.
dataclass(frozen=True) даёт:
- именованные поля (лучше читаемость),
- более явную модель данных,
- (при корректных типах) может быть хешируемым.
Выбор: tuple для простых return values, dataclass/NamedTuple для сущностей/DTO.
"""


# 2.6) Follow-up: Почему tuple “immutable”, но всё равно может быть “изменяемым по смыслу”?
"""
→ Нельзя заменить элементы в tuple, но сами элементы могут быть изменяемыми (например, dict/list внутри).
То есть неизменяемость относится к контейнеру, а не к графу объектов.
"""


# 3) Что такое генераторы и чем они лучше списков/итераторов в реальных задачах?
"""
→ Генератор — это итератор, который:
- вычисляет элементы лениво (по мере запроса),
- сохраняет состояние между yield,
- позволяет строить “потоковые” пайплайны обработки данных.

Плюсы в проде:
- экономия памяти (не материализуем большие коллекции),
- можно начинать обработку раньше (лучше latency), не дожидаясь чтения всего набора,
- удобно строить пайплайны (filter/map/chunks) и работать с “бесконечными” потоками,
- естественный backpressure: потребитель определяет скорость, производитель не “убегает”.

Минусы/ограничения:
- одноразовый проход: генератор “исчерпывается”,
- сложнее дебажить и повторно использовать (иногда надо materialize),
- на элемент может быть небольшой overhead, особенно в tight loop (тогда измеряем).


Tight loop — цикл с миллионами итераций и минимумом работы внутри, где важен overhead каждой итерации.

s = 0
for i in range(10_000_000):
    s += 1
    
Materialize (материализовать) — это превратить ленивый поток/итератор/генератор в реальную коллекцию в памяти
(обычно list, иногда tuple, set).

gen = (x*x for x in range(5))  # генератор (лениво)
lst = list(gen)               # materialize -> [0, 1, 4, 9, 16]
"""

# 3.1) Follow-up: Чем генераторное выражение отличается от list comprehension?
"""
→ list comprehension сразу строит list (eager) и занимает память под все элементы.
Генераторное выражение возвращает генератор (lazy) и отдаёт элементы по мере итерации.

Практический нюанс: list comprehension часто быстрее (внутренние оптимизации),
но выбираем по требованиям к памяти/стримингу и по профилю.
"""

# 3.2) Follow-up: Iterable vs Iterator — в чём разница?
"""
→ Iterable — объект, по которому можно получить iterator (есть __iter__).
Iterator — объект, который возвращает элементы через __next__ и обычно __iter__ возвращает self.
Генератор — это iterator.

Пример:
- list — iterable, iter(list) — iterator,
- генератор — одноразовый iterator: повторно “пройти” нельзя без создания заново.
"""

# 3.3) Follow-up: Как закрывать генераторы и зачем нужны close()/throw()?
"""
→ close() завершает генератор, внутри возникает GeneratorExit (можно освободить ресурсы в finally).
throw(exc) “вбрасывает” исключение внутрь генератора (для управления потоком).

Это важно, если генератор управляет ресурсами:
- внутри генератора делают try/finally для гарантированного cleanup.
"""

# 3.4) Follow-up: Когда генераторы плохой выбор?
"""
→ Когда нужно:
- многократное повторное прохождение (лучше список/кэширование),
- случайный доступ по индексу / len(),
- параллельная обработка с повторным чтением источника,
- сложная отладка важнее экономии памяти,
- нужно сортировать/переиспользовать данные несколько раз.
Тогда материализация оправдана.
"""

# 3.5) Follow-up: Как правильно делать “генератор, который держит ресурс” (файл/соединение)?
"""
→ Либо используем контекстный менеджер (with) и возвращаем итератор внутри,
либо внутри генератора обязательно try/finally, чтобы cleanup сработал при close()/исключениях.
Иначе ресурс может утечь при преждевременном прекращении итерации.
"""


# 4) Как работает `yield from` и что он даёт на практике?
"""
→ `yield from iterable` делегирует выдачу элементов вложенному итератору/генератору.
Для генераторов это особенно полезно, потому что `yield from`:
- прокидывает send/throw/close во вложенный генератор,
- корректно обрабатывает StopIteration и позволяет “получить return value” подгенератора.

 `return value` в ГЕНЕРАТОРЕ приводит к StopIteration(value)   <-----

Практический эффект:
- проще композиция генераторов (меньше boilerplate),
- меньше ошибок с закрытием/исключениями,
- удобнее строить читаемые пайплайны/парсеры/стриминговые обработчики.
"""

# 4.1) Follow-up: Что возвращает генератор через `return` и как это связано со StopIteration?
"""
→ `return value` в генераторе приводит к StopIteration(value).
`yield from` умеет извлечь это значение:
result = yield from subgen

Важно помнить про PEP 479:
- StopIteration, “вылетевший” изнутри генератора не как return, преобразуется в RuntimeError,
  чтобы избежать тихых багов.

### ПРИМЕРЫ!

# --- 1) return в генераторе -> StopIteration(value) ---

def subgen():
    yield 1
    return 42          # когда subgen исчерпается (следующий next), Python "завершит" его через StopIteration(42)

def gen():
    # yield from:
    # 1) "прокидывает" наружу все yield из subgen (тут: 1)
    # 2) когда subgen завершится через return 42, yield from вернёт это значение в res
    res = yield from subgen()   # res == 42
    yield res                   # 42 отдаём как ОБЫЧНЫЙ yield (не StopIteration наружу)

print(list(gen()))  # [1, 42]


# Хотим ЯВНО увидеть StopIteration(value) от subgen:
s = subgen()
print(next(s))  # 1
try:
    next(s)     # здесь subgen исчерпался -> выбросится StopIteration(42)
except StopIteration as e:
    print("subgen StopIteration value:", e.value)  # 42


# --- 2) PEP 479: StopIteration внутри генератора (НЕ через return) -> RuntimeError ---

def bad():
    yield 1
    raise StopIteration("oops")  # НЕ return => Python превращает в RuntimeError (PEP 479)

g = bad()
print(next(g))  # 1
try:
    print(next(g))
except RuntimeError as e:
    print("RuntimeError:", e)
"""


# 4.2) Follow-up: Чем `yield from` лучше, чем `for x in it: yield x`?
"""
→ Цикл делегирует только значения.
`yield from` делегирует ещё send/throw/close и корректно прокидывает завершение генератора,
поэтому это “правильная” прокси-делегация.
Плюс часто немного быстрее.
"""

# 4.3) Follow-up: Где это реально встречается в проде?
"""
→ В генераторных пайплайнах, парсерах (особенно рекурсивных: дерево → токены),
стриминговых обработчиках, библиотечном коде (итерация как интерфейс).
"""

# 4.4) Follow-up: Как `yield from` связан с async/await исторически?
"""
→ Исторически до полноценного async/await были generator-based coroutines (PEP 342/380),
и `yield from` был механизмом делегирования выполнения/ожидания.
Современный async/await концептуально похож: “корутина ждёт другую корутину”.
"""


# 5) Как устроены контекстные менеджеры и зачем нужен протокол `__enter__/__exit__`?
"""
→ Контекстный менеджер задаёт гарантированный lifecycle ресурса:
- __enter__ подготавливает ресурс и возвращает объект, который используется в with,
- __exit__ выполняет cleanup и получает информацию об исключении (тип/объект/traceback).

Ключевой смысл: корректное освобождение ресурсов при любом исходе:
- файл/сокет/соединение,
- транзакция (commit/rollback),
- lock (acquire/release),
- временные изменения состояния (переключить настройки и вернуть обратно).

Полезные инструменты:
- contextlib.contextmanager / asynccontextmanager — лаконичная реализация,
- contextlib.ExitStack — управлять набором контекстов (в т.ч. условно добавляемых).
"""

# 5.1) Follow-up: Когда __exit__ должен подавлять исключение?
"""
→ Почти никогда “по умолчанию”.
Подавлять (return True) стоит только в строго контролируемых сценариях,
когда ошибка ожидаема и безопасна (например, contextlib.suppress(FileNotFoundError) для best-effort).

Важно: не скрывать баги и не ломать observability — если подавляем, делаем это осознанно и обычно логируем/метрим.
"""

# 5.2) Follow-up: Чем отличается `with` от `try/finally`?
"""
→ По сути `with` — стандартизированный wrapper над try/finally + возможность подавления исключений через __exit__.
Плюс `with` делает интерфейс ресурса явным и переиспользуемым (меньше copy-paste).
"""

# 5.3) Follow-up: Как работают async контекстные менеджеры?
"""
→ `async with` использует `__aenter__`/`__aexit__`, которые могут быть awaitable.
Нужно для async ресурсов: async HTTP-сессии, async соединения, транзакции в async драйверах.
"""

# 5.4) Follow-up: Пример контекстного менеджера для транзакции: что должно быть внутри?
"""
→ На входе — begin, на выходе:
- если исключений нет → commit,
- если есть исключение → rollback,
и обязательно закрыть соединение/курсор (или вернуть в пул) в finally.

Ключевые слова: “границы транзакции + rollback на исключении + ресурс всегда освобождаем”.
"""

# 5.5) Follow-up: Зачем нужен ExitStack и где он спасает?
"""
→ Когда количество/набор ресурсов динамический:
- открыть N файлов,
- выбрать один из нескольких контекстов по условию,
- аккуратно сделать cleanup всего, что успели открыть.
ExitStack гарантирует корректный порядок закрытия и снижает количество вложенных with.

ExitStack — “один with для многих ресурсов”, которые добавляются по ходу, и он закроет их все при выходе.

from contextlib import ExitStack

with ExitStack() as s:
    f1 = s.enter_context(open("a.txt"))
    f2 = s.enter_context(open("b.txt"))
# f2 закрыт, потом f1
"""


# 6) В чём отличие `@staticmethod` от `@classmethod`? Когда какой использовать?
"""
→ `@staticmethod` — метод без self/cls: логически рядом с классом, но не использует состояние класса/экземпляра.
Часто это “утилита”, хотя нередко лучше вынести в модуль.

`@classmethod` получает cls и полезен когда:
- нужен альтернативный конструктор/фабрика (from_json/from_config),
- важно корректное наследование (возвращать экземпляр подкласса через cls(...)),
- нужно работать с состоянием класса (registries, конфигурация на уровне класса).
"""

# 6.1) Follow-up: Пример, где staticmethod сломает наследование, а classmethod — нет.
"""
→ Если фабрика жёстко создаёт Base(...), то Subclass вызовет метод и получит Base, а не Subclass.
С classmethod: return cls(...)
Это классический пример “ожидаем услышать на интервью”.
"""

# 6.2) Follow-up: Что такое дескрипторы и почему методы вообще “привязываются” к объекту?
"""
→ Функции в классе — дескрипторы: при доступе через экземпляр они превращаются в bound method,
потому что вызывается function.__get__(obj, type) и self подставляется автоматически.
staticmethod отключает привязку, classmethod привязывает cls.
"""

# 6.3) Follow-up: Когда лучше вообще не использовать ни staticmethod, ни classmethod?
"""
→ Если функция не относится к ответственности класса — вынести в модуль/сервис.
На интервью любят услышать: “не тащу всё в классы ради стиля”.
"""


# 7) Что такое MRO и как `super()` работает при множественном наследовании?
"""
→ MRO (Method Resolution Order) — линейный порядок обхода классов в иерархии (C3-линеаризация),
по которому Python ищет атрибуты/методы. Python также проверяет, что MRO “согласован” (иначе будет TypeError при создании класса).
`super()` — это вызов “следующего класса по MRO”, а не “прямого родителя”.

Это критично при diamond inheritance и миксинах:
при кооперативном наследовании каждый класс вызывает super(),
и цепочка выполняется ровно один раз в правильном порядке.
"""



# Diamond problem
"""
В Python, наследование ромбовидной формы (или diamond inheritance) возникает, когда класс наследует от двух классов,
которые, в свою очередь, наследуют от одного общего базового класса. Это может привести к неоднозначности,
когда необходимо определить, какой метод или атрибут должен быть использован.

# Пример ромбовидного наследования:
class A:
    def say_hello(self):
        print("Hello from A")

class B(A):
    def say_hello(self):
        print("Hello from B")

class C(A):
    def say_hello(self):
        print("Hello from C")

class D(B, C):
    pass

d = D()
d.say_hello()  # Вывод: Hello from B
"""


# 7.1) Follow-up: Почему прямой вызов Parent.method(self) опасен?
"""
→ Он обходит MRO: можно пропустить часть цепочки или вызвать кого-то дважды.
При множественном наследовании это ломает кооперативность и приводит к трудноуловимым багам.
"""

# 7.2) Follow-up: Какие требования к “кооперативным” классам, чтобы super() работал правильно?
"""
→ Все классы в цепочке должны:
- вызывать super() и не “обрывать” цепочку,
- иметь совместимые сигнатуры (часто используют *args/**kwargs, особенно в миксинах),
- не делать жёстких предположений о конкретном родителе и порядке наследования.
"""

# 7.3) Follow-up: Покажи, как понять MRO на практике.
"""
→ Использовать Class.__mro__, Class.mro() или inspect.getmro(Class).
На интервью: “смотрю MRO, чтобы понять порядок вызовов и отладить”.
"""

# 7.4) Follow-up: Где множественное наследование оправдано?
"""
→ Обычно для mixins с маленькой ответственностью (логирование, сериализация, трейсинг),
или когда фреймворк так устроен. В бизнес-модели чаще предпочитают композицию.
"""


# 8) В чём разница между `is` и `==`? Приведи практические примеры ошибок.
"""
→ `is` проверяет идентичность (тот же объект). `==` проверяет равенство значений (через __eq__).
Корректный кейс для `is`: сравнение с None (`x is None`) и синглтоны.

Частая ошибка: сравнивать через `is` числа/строки/кортежи:
- `x is 0` или `s is "abc"` может “случайно” работать из-за интернирования/кэшей CPython,
  но это не контракт языка и может отличаться между версиями/реализациями.

Важно на интервью:
- `is` — O(1) (сравнение указателей),
- `==` может быть O(n) и зависит от реализации __eq__.
"""

# 8.1) Follow-up: Почему `==` может быть дорогим или опасным?
"""
→ `==` вызывает __eq__, который может:
- сравнивать большие структуры (O(n)),
- быть переопределённым неочевидно,
- в плохих реализациях иметь побочные эффекты (антипаттерн, но встречается).
Поэтому в hot-path важны правильные сравнения и понимание стоимости.
"""

# 8.2) Follow-up: Что такое интернирование строк и кэширование small ints, и почему нельзя на это полагаться?
"""
→ CPython может интернировать строки и кэшировать небольшие целые числа,
поэтому `a is b` иногда “случайно True”. Но это детали реализации, не спецификация.
Правило: `is` только для None/синглтонов, `==` для значений.
"""

# 8.3) Follow-up: Как правильно сравнивать Enum/Singleton-объекты?
"""
→ Для Enum часто используют `is` (члены Enum — синглтоны), `==` тоже обычно работает.
Для None — `is`.
Главное: понимать семантику и не путать “значение” с “идентичностью”.
"""

# 8.4) Follow-up: Можно ли использовать `is` для сравнения с True/False?
"""
→ Обычно нет: это не добавляет смысла и может быть ошибкой стиля.
Правильно:
- if flag: ... (для truthiness)
- if flag is True: ... — только если принципиально нужно отличить True от truthy-объектов (редко).
"""


# 9) LEGB, closures, late binding
"""
→ LEGB: Local → Enclosing → Global → Builtins.
Python ищет имя в этом порядке.

Closure (замыкание) — функция "помнит" переменные из enclosing scope.
Late binding — значения из closure берутся при вызове, поэтому в циклах часто ловят баг.

# Пример LEGB (поиск имени)
x = "global"

def outer():
    x = "enclosing"
    def inner():
        x = "local"
        return x
    return inner()

print(outer())  # local

# Баг: lambda в цикле (late binding)
funcs = [lambda: i for i in range(3)]
print([f() for f in funcs])  # [2, 2, 2]

# Fix: “захватить” текущее значение через default arg
funcs = [lambda i=i: i for i in range(3)]
print([f() for f in funcs])  # [0, 1, 2]
"""


# 9.1) Follow-up: Какие частые докапы?
"""
- Почему [2,2,2]? -> closure хранит ссылку на переменную i, а не значение.
- Как ещё фиксить? -> factory-функция или functools.partial.
- nonlocal/global? -> nonlocal меняет enclosing переменную, global — модульную.
"""

# 10) Декораторы и functools.wraps
"""
→ Декоратор: принимает функцию и возвращает обёртку.
wraps сохраняет метаданные (name/doc/signature), чтобы работали inspect/FastAPI/дебаг.


from functools import wraps
import inspect

def timing(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        return fn(*args, **kwargs)
    return wrapper

@timing
def add(a: int, b: int) -> int:
    '''sum two ints'''
    return a + b

print(add.__name__)             # add (без wraps было бы wrapper)
print(add.__doc__)              # sum two ints
print(inspect.signature(add))   # (a: int, b: int) -> int
"""




# 10.1) Follow-up: Докапы
"""
- Декоратор с аргументами? -> def deco(x): def inner(fn): ...
- Порядок декораторов? -> применяется снизу вверх.
- Как типизировать декоратор? -> ParamSpec + TypeVar (частый senior-докап).
"""


# ParamSpec + TypeVar
"""
TypeVar   — “переменная типа” для значения (например, что функция возвращает).
ParamSpec — “переменная типа” для параметров функции (чтобы декоратор сохранил сигнатуру).


# Это просто имена для типовых переменных (условные буквы), можно назвать как угодно.
# P обычно читают как Parameters → “набор параметров функции” (позиционные + именованные).
# R обычно читают как Return     → “тип возвращаемого значения”.

from typing import Callable, TypeVar, ParamSpec
from functools import wraps

P = ParamSpec("P")
R = TypeVar("R")

def log(fn: Callable[P, R]) -> Callable[P, R]:
    @wraps(fn)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        print(fn.__name__)
        return fn(*args, **kwargs)
    return wrapper

"""


# 11) Итерация / протоколы: __iter__, __next__, __len__, __contains__
"""
→ Iterable: есть __iter__ (возвращает iterator).
→ Iterator: есть __next__ и обычно __iter__ возвращает self.
for-loop ловит StopIteration и воспринимает его как "конец итерации".

Оператор `in`:
- сначала пробует __contains__
- иначе пробует итерацию (через __iter__/__next__)


class Counter:
    def __init__(self, n: int):
        self.n = n
        self.i = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.i >= self.n:
            raise StopIteration
        self.i += 1
        return self.i

print(list(Counter(3)))  # [1, 2, 3]


class Box:
    def __init__(self, items):
        self.items = set(items)

    def __contains__(self, x):
        return x in self.items

print(2 in Box([1, 2, 3]))  # True
"""




# 11.1) Follow-up: Докапы
"""
- Почему for не падает на StopIteration? -> он перехватывает его внутри.
- Что даёт __len__? -> len(obj), иногда влияет на truthiness (если нет __bool__).
- Можно ли сделать объект iterable, но не iterator? -> да, __iter__ возвращает отдельный iterator.
"""


# 12) Контекст исключений: try/except/else/finally, raise from
"""
→ try/except/else/finally:
- except: обработка ошибки
- else: выполняется только если ошибки не было
- finally: выполняется всегда (cleanup)

raise vs raise e:
- raise сохраняет исходный traceback
- raise e может “перепривязать” traceback (хуже для диагностики)

chaining:
- raise X from e -> сохраняет причину (cause)
- raise X from None -> скрывает причину наружу (но внутри обычно логируем e)


def parse_int(s: str) -> int:
    try:
        x = int(s)
    except ValueError as e:
        raise ValueError("bad input") from e
    else:
        return x
    finally:
        pass

try:
    parse_int("x")
except Exception as e:
    print(type(e).__name__, e)
"""


#  -- finally когда работает а когда НЕТ --
"""
# В Python блок finally выполняется ВСЕГДА, даже если был вызван sys.exit() в блоке except
import sys

try:
    print(2 / 0)
except ZeroDivisionError:
    print("Деление на ноль!")
    sys.exit(0)  # Программа завершается, но finally всё равно выполнится
finally:
    print("Этот код выполнится в любом случае!")

Почему так происходит?
sys.exit() вызывает исключение SystemExit, но перед завершением интерпретатор всегда выполняет блок finally.


Блок finally НЕ выполняется только в исключительных случаях:

- Принудительное завершение через os._exit().
- Аварийный краш Python (segfault, stack overflow и т. д.).
- Бесконечный цикл или зависание.
- Убийство процесса извне (kill -9, диспетчер задач).


os._exit() завершает процесс немедленно, без вызова обработчиков finally, финализаторов (__del__) и других завершающих действий.

# Пример 1: Принудительное завершение через os._exit()
import os

try:
    print(2 / 0)
except ZeroDivisionError:
    print("Деление на ноль!")
    os._exit(1)  # Программа завершится сразу, finally не выполнится
finally:
    print("Этот код не выполнится!")


# Пример 2: Принудительный краш через os.abort()
try:
    print("Вызов os.abort()...")
    os.abort()  # Немедленное аварийное завершение (как SIGABRT)
except:                             # except без указания исключения ловит все ошибки (но так делать не рекомендуется).
    print("Except не сработает!")
finally:
    print("Этот finally не выполнится!")

Во всех остальных случаях (включая sys.exit(), исключения и обычное завершение) finally гарантированно выполнится.


# finally выполняется ДАЖЕ при raise
try:
    print(2 / 0)  # → ZeroDivisionError
except ZeroDivisionError:
    print("Деление на ноль!")  # Выполнится
    raise KeyError  # Выбрасывается новое исключение, но сначала выполнится finally!
finally:
    print("Этот код выполнится в любом случае!")  # Выполнится до завершения программы
    print("РАБОТАЕТ FINALLY ВМЕСТЕ С RAISE")

# Почему так?
# - Блок finally гарантированно выполняется перед выходом из try-except, даже если внутри есть raise.
# - После выполнения finally программа завершится с исключением KeyError.
# - Это полезно, например, для освобождения ресурсов (закрытия файлов, соединений с БД и т. д.), чтобы избежать утечек.


# finally всегда выполняется, даже если:

# - есть raise,
# - есть return,
# - произошло другое исключение.
"""


# 12.1) Follow-up: Докапы
"""
- Когда использовать from None? -> чтобы не светить внутренности клиенту (безопасность/UX).
- Почему нельзя ловить BaseException? -> туда входят KeyboardInterrupt/SystemExit/CancelledError.
- Как добавить контекст и не потерять причину? -> raise NewError(...) from e.
"""


# 13) Контейнеры/сложности (big-O “на пальцах”)
"""
→ list.append: амортизированно O(1)
→ list.pop(): O(1) с конца, но pop(0) = O(n) (сдвиг)
→ dict/set lookup: обычно O(1) в среднем (hash table)
→ deque: O(1) для очереди (popleft/append)


from collections import deque

lst = [1, 2, 3]
lst.append(4)     # ~O(1)
lst.pop()         # O(1)
# lst.pop(0)      # O(n)

q = deque([1, 2, 3])
q.append(4)       # O(1)
print(q.popleft())# O(1)
"""



# 13.1) Follow-up: Докапы
"""
- Почему append “амортизированно”? -> динамический массив растёт рывками (over-allocation).
- Почему dict “в среднем”? -> зависит от распределения hash и коллизий.
- Почему deque лучше для очереди? -> не сдвигает элементы как list.pop(0).
"""


# 14) dataclass / __slots__
"""
→ dataclass: удобные "records/DTO": автогенерация __init__/__repr__/__eq__.
→ __slots__: экономит память (нет __dict__), быстрее доступ к атрибутам,
  но меньше гибкости (нельзя динамически добавлять поля).
  

from dataclasses import dataclass

@dataclass(frozen=True)
class UserDTO:
    id: int
    email: str

class Point:
    __slots__ = ("x", "y")
    def __init__(self, x: int, y: int):
        self.x = x
        self.y = y

"""

# 14.1) Follow-up: Докапы
"""
- frozen=True зачем? -> неизменяемый контракт, можно безопасно шарить между слоями.
- weakref со slots? -> нужен "__weakref__" в __slots__, если требуется.
- trade-off slots? -> сложнее расширять, меньше “динамики”.
"""

# 15) import system: sys.path, пакет/модуль, относительные импорты
"""
→ Python ищет модули по sys.path.
Пакет = директория (обычно с __init__.py).
Прод-баг “локально ок, в контейнере падает” часто из-за:
- запуска не из корня проекта (sys.path другой)
- неправильного PYTHONPATH
- shadowing: файл с именем как у библиотеки (например, json.py)


import sys
print(sys.path[:3])
"""



# Пример shadowing (антипример):
# если у вас есть файл "jwt.py" в проекте, import jwt может импортировать ваш файл, а не библиотеку.


# 15.1) Follow-up: Докапы
"""
- Почему `python script.py` vs `python -m package.module` различаются? -> разный sys.path и __package__.
- Относительные импорты зачем? -> внутри пакета, но важно запускать модуль как часть пакета.
- Как чинить в проде? -> правильная структура пакета + запуск через -m / корректный entrypoint.

Быстрый чек: модуль vs пакет

python -m some_module — запускает модуль some_module.py
python -m some_package — запускает some_package/__main__.py (если есть)
"""




#  -- ЦИКЛИЧЕСКИЙ ИМПОРТ --
"""
КАК ПОЧИНИТЬ?
Вынеси общее в третий модуль (common.py), чтобы модули не импортировали друг друга.
Сделай “ленивый” импорт: from b import X внутри функции/метода.
"""



###  -- Вытесняющая и Кооперативная многозадачность --
"""
Вытесняющая и кооперативная многозадачность
(По книге Мэтью Фаулера "Конкурентность в Python" и в контексте asyncio)

- Вытесняющая многозадачность   – ОС принудительно переключает потоки (как в threading), даже если задача не завершена.
- Кооперативная многозадачность – задачи добровольно уступают управление (как в asyncio через await).
"""


### ПОТОКИ/ASYNCIO
"""
 # Потоки ИСПОЛНЯЮТСЯ на уровне ОС а asyncio на уровне Интерпретатора                                 <-----
 ### Потоки — на уровне ОС, asyncio — на уровне приложения (event loop) внутри одного потока.
 
 В целом — да, идея правильная: threads = OS-level, asyncio = user-level (event loop) внутри процесса.

 Но на собесе я бы сказал так (чуть точнее, чтобы не придрались):
 Потоки: реальные потоки ОС, планируются ядром, но В CPython GIL НЕ даёт потокам параллелить байткод.
 asyncio: обычно один поток ОС, конкурентность делает event loop (кооперативно, в точках await),
 это НЕ “интерпретатор”, а рантайм/библиотека в user-space.

 Если сказать “на уровне интерпретатора” — тебя, скорее всего, поправят, но смысл ты передаёшь верно.
"""


# 16) asyncio “must know”: Task vs coroutine, gather vs TaskGroup, cancellation
"""
→ coroutine — объект, который НЕ выполняется, пока его не await/не запланировать.
→ Task — корутина, запланированная в event loop (конкурентно).

gather:
- ждёт несколько awaitable
- по умолчанию при ошибке “валит” await (остальные могут быть отменены)

TaskGroup (py3.11+):
- structured concurrency: если одна задача упала, остальные отменяются автоматически
- проще reasoning и cleanup

Cancellation:
- отмена поднимает CancelledError
- важно НЕ “глотать” CancelledError, иначе shutdown может зависнуть


import asyncio

async def work(name: str, t: float):
    await asyncio.sleep(t)
    return name

async def main_gather():
    res = await asyncio.gather(work("A", 0.1), work("B", 0.2))
    print(res)  # ['A', 'B']

async def main_taskgroup():
    async with asyncio.TaskGroup() as tg:
        t1 = tg.create_task(work("A", 0.1))
        t2 = tg.create_task(work("B", 0.2))
    print(t1.result(), t2.result())

asyncio.run(main_gather())
asyncio.run(main_taskgroup())


# Cancellation: правильный шаблон
async def loop_forever():
    try:
        while True:
            await asyncio.sleep(1)
    except asyncio.CancelledError:
        print("cleanup")
        raise  # важно!

async def main_cancel():
    t = asyncio.create_task(loop_forever())
    await asyncio.sleep(0.1)
    t.cancel()
    try:
        await t
    except asyncio.CancelledError:
        print("cancelled")

asyncio.run(main_cancel())

"""



# 16.1) Follow-up: Докапы (что часто спрашивают)
"""
- Чем Task отличается от coroutine? -> Task = запланированная корутина, сразу выполняется конкурентно.
- Что будет если забыть await? -> корутина не выполнится + RuntimeWarning.
- gather vs TaskGroup? -> TaskGroup структурирует жизненный цикл, безопаснее по ошибкам/отмене.
- Почему нельзя глотать CancelledError? -> ломает cancellation, зависает shutdown/drain.
- Как ограничить конкуренцию? -> Semaphore / bounded Queue / лимиты пулов соединений.
- Как вынести блокирующее из loop? -> asyncio.to_thread/run_in_executor + лимиты.
"""


# 17) Модель данных Python: __new__ vs __init__, атрибуты, __repr__/__str__
"""
→ __new__(cls, ...) создаёт объект (выделение/конструирование), __init__(self, ...) инициализирует уже созданный.
Докап: __new__ нужен для immutable (str/int/tuple), singleton, кеширования, контроля создания.

__repr__ — "для разработчика" (желательно однозначный/полезный), __str__ — "для пользователя".
"""

# 17.1) Follow-up: Докапы
"""
- Почему __init__ не вызывается, если __new__ вернул НЕ экземпляр cls? -> потому что объект "другой".
- Где реально нужен __new__? -> immutable типы, singleton, flyweight/caching.
- Почему важен __repr__? -> логи/дебаг/диагностика (особенно в проде).
"""


# 18) Исключения: finally + return, suppression, Exception vs BaseException
"""
→ finally выполняется всегда, даже если был return/raise.
Опасная тонкость: return в finally "перетирает" исключение/return из try.

BaseException ловить нельзя почти никогда (там KeyboardInterrupt/SystemExit/CancelledError).
"""

# Пример: return в finally "убивает" исключение
"""
def bad():
    try:
        1 / 0
    finally:
        return "oops"

print(bad())  # "oops" (ZeroDivisionError потерян!)  -> анти-паттерн
"""

# 18.1) Follow-up: Докапы
"""
- Когда допустимо подавлять исключение? -> contextlib.suppress для ожидаемых "best-effort" кейсов.
- Почему нельзя except Exception: в asyncio иногда? -> CancelledError может быть не Exception (зависит от версии/контекста), лучше явно.
"""


# 19) mutable/immutable + identity: интернирование, кэш small ints, copy pitfalls
"""
→ immutable: int/str/tuple/frozenset (контейнер immutable, элементы могут быть mutable).
mutable: list/dict/set.

'is' только для None/синглтонов, не для значений.
Интернирование/кэши CPython — деталь реализации, не контракт.
"""

# 19.1) Follow-up: Докапы
"""
- Почему tuple с list внутри “логически mutable”? -> потому что меняется вложенный объект.
- Что сломает shallow copy? -> вложенные структуры остаются общими ссылками.
- Почему иногда a is b для строк True? -> интернирование, но нельзя полагаться.
"""


# 20) Hash / dict / set: __hash__, __eq__, коллизии, почему mutable нельзя ключом
"""
→ dict/set используют hash table: сначала hash(obj), потом __eq__ для разрешения коллизий.
Ключ обязан быть хешируемым: hash стабилен во времени, __eq__/__hash__ согласованы.

Mutable нельзя в ключи: меняешь объект -> hash меняется -> объект "теряется" в таблице.
"""

# 20.1) Follow-up: Докапы
"""
- Что будет если переопределить __eq__ и не __hash__? -> объект станет unhashable (по умолчанию).
- Почему dict lookup O(1) "в среднем"? -> зависит от распределения хэшей и resize.
- Как Python борется с hash-flooding? -> рандомизация hash для str/bytes (в общих чертах).
"""


# 21) Функции/вызовы: *args/**kwargs, позиционные-only и keyword-only, unpacking
"""
→ *args собирает позиционные (КОРТЕЖ), **kwargs — именованные (СЛОВАРЬ).
Важно: уметь читать сигнатуры и делать keyword-only параметры, чтобы API было безопаснее.

- def f(a, /, b, *, c): 
  a — positional-only
  c — keyword-only
  
# Где “вылезает ошибка” — в вызовах, если нарушить правила:
f(a=1, b=2, c=3)   # TypeError: a positional-only
f(1, 2, 3)         # TypeError: c keyword-only
f(1, c=3, b=2)     # OK
f(1, 2, c=3)       # OK
"""

# 21.1) Follow-up: Докапы
"""
- Зачем keyword-only? -> предотвращает ошибки при добавлении новых параметров, улучшает читаемость.
- Чем опасно принимать **kwargs без валидации? -> silent bugs/опечатки параметров.
- Что такое unpacking? -> f(*lst), f(**dict), {**a, **b}, [*a, *b]
"""


# 22) async “докапы уровня прод”: timeouts, shielding, backpressure, bounded queues
"""
→ В async почти всегда спрашивают:
- как ставить таймаут (asyncio.wait_for)
- как ограничить конкуренцию (Semaphore/Queue)
- как не дать отмене "убить" критический блок (asyncio.shield) — осторожно
- как делать backpressure: bounded Queue + producer ждёт

Главное: таймауты/лимиты/корректная отмена.
"""

"""
import asyncio

async def call_api():
    await asyncio.sleep(10)

async def main():
    try:
        await asyncio.wait_for(call_api(), timeout=0.5)
    except asyncio.TimeoutError:
        print("timeout")

asyncio.run(main())
"""

# 22.1) Follow-up: Докапы
"""
- asyncio.shield зачем? -> защитить awaitable от отмены, но не злоупотреблять (иначе утечки/фоновые задачи).
- Как сделать backpressure? -> Queue(maxsize=N), producer await put(), consumer await get().
- Чем опасны "fire-and-forget" tasks? -> потеря ошибок, утечки, неуправляемый lifecycle.
"""


# 23) async + threads/process: когда to_thread, когда ProcessPool, и что с GIL
"""
→ asyncio не ускоряет CPU-bound: event loop один.
Если есть блокирующий sync-код:
- asyncio.to_thread(fn) / run_in_executor -> вынести блокирующее I/O или легкий CPU.
Для тяжёлого CPU:
- ProcessPoolExecutor (обход GIL), но дороже IPC/serialization.

Важно: лимитировать параллелизм и помнить про ресурсы (DB pools, HTTP).
"""

# 23.1) Follow-up: Докапы
"""
- Почему нельзя просто вызвать блокирующую функцию в async? -> блокирует event loop (всё встанет).
- Когда to_thread плохо? -> если CPU-bound чистый Python (GIL), и будет contention.
- Почему process pool бывает "медленно"? -> pickle/IPC, старт процессов, большие данные.
"""



### TODO ОШИБКИ, ИСКЛЮЧЕНИЯ, НАДЁЖНОСТЬ

# 9) Когда ловить конкретные исключения, а когда допустимо `except Exception:`?
"""
→ Внутри бизнес-логики (домен/юзкейсы) обычно ловят конкретные исключения, потому что:
- это фиксирует ожидаемые ветки (валидация, “не найдено”, конфликт состояния),
- не маскирует баги (TypeError, AttributeError и т.п. должны всплывать),
- упрощает тестирование и контроль поведения.

`except Exception:` уместен на границах системы (HTTP handler, consumer loop, CLI entrypoint), где задача другая:
- нормализовать ошибку (перевести в 4xx/5xx или статус задачи),
- залогировать с контекстом (trace_id/request_id, user_id, параметры, версия релиза),
- выставить метрики (тип ошибки, код ответа, endpoint),
- гарантировать cleanup (закрыть сессию, вернуть соединение в пул, откатить транзакцию).

Важно на интервью: даже на границе `except Exception` не должен “глотать” ошибку.
Либо:
- возвращаем стандартизированный ответ/статус и сохраняем информацию (лог/трейс),
- либо re-raise после логирования, если это crash-only компонент.

Полезная эвристика:
- ловлю исключение там, где могу сделать осмысленную реакцию (recover/translate/retry),
  а не “просто чтобы не падало”.
"""

# 9.1) Follow-up: Почему не стоит ловить `BaseException` и что туда входит?
"""
→ `BaseException` включает `KeyboardInterrupt`, `SystemExit`, `GeneratorExit`
(а в async — ещё важно `asyncio.CancelledError`).
Если ловить всё подряд, можно:
- сломать корректную остановку сервиса (SIGINT/SIGTERM обработка),
- нарушить cancellation в asyncio (задачи не отменяются, shutdown зависает),
- скрыть критические причины завершения процесса.

Практика: ловим `Exception` или конкретные типы; `BaseException` — только в “последнем рубеже”
с обязательным re-raise/exit (и часто отдельно пропускаем CancelledError).

asyncio.CancelledError — это исключение, которое выбрасывается внутри coroutine/Task, когда её отменяют (task.cancel() или при shutdown).
Обычно его не ловят, либо ловят и сразу пробрасывают дальше (raise), чтобы отмена сработала корректно.

"""

# 9.2) Follow-up: Как “правильно” классифицировать ошибки в сервисе?
"""
→ Обычно делят на:
- expected/business errors (валидация, not found, conflict) → 4xx, без stacktrace (или с урезанным),
- transient/infrastructure errors (timeouts, network, deadlocks) → ретраи/backoff, 5xx/503,
- bugs (programmer errors) → 5xx, алерты, нужно чинить код, ретраи редко помогают.

Практическая ось (для ретраев): transient vs permanent.
Transient — “повтор может помочь”, permanent — “повтор не поможет без изменения входных данных/кода”.

На интервью ожидают: “я различаю категории, по-разному логирую и по-разному ретраю”.
"""

# 9.3) Follow-up: Что именно логировать при исключении (и что нельзя)?
"""
→ Логировать:
- stacktrace (logger.exception),
- корреляцию (trace_id/request_id), endpoint/метод, latency, статус,
- ключевые параметры, но без секретов и PII (пароли, токены, номера карт).
→ Дополнительно: версия релиза, имя сервиса, host/pod, user_id (если безопасно).

Нельзя:
- секреты (tokens/keys/passwords),
- чувствительные персональные данные (PII) без маскирования,
- “сырые” payload’ы целиком, если они большие/чувствительные (лучше whitelisting полей).

Идея: по одному событию можно восстановить контекст и быстро найти первопричину.
"""

# 9.4) Follow-up: Ретраи: что ретраить, что не ретраить, и как не “DDOS-ить” зависимость?
"""
→ Ретраим только transient: timeouts, connection reset, 502/503/504, deadlock/serialization failure.
Не ретраим:
- валидацию/4xx (кроме 429),
- операции без идемпотентности (если повтор создаст дубль/двойной сайд-эффект).

Правильный retry:
- exponential backoff + jitter,
- max attempts,
- общий deadline/timeout (retry budget: не ретраим бесконечно внутри уже истекшего SLA),
- circuit breaker / bulkhead,
- rate limiting (чтобы не устроить retry storm).

На интервью ожидают услышать: “ретраи ограничены, с backoff/jitter, и только при идемпотентности”.
"""

# 9.5) Follow-up: Почему иногда “retry ухудшает ситуацию” и как это диагностировать?
"""
→ Потому что ретраи увеличивают нагрузку на уже деградирующую зависимость (amplification),
растёт очередь и p99, начинается каскадный отказ.
Диагностика: рост retries, рост latency, рост saturation (пулы/очереди), рост 5xx/timeout.
Лечение: лимиты, backoff+jitter, breaker, снижение concurrency, деградация (stale/cache/fallback).
"""


# 10) Почему `except:` считается плохой практикой?
"""
→ `except:` ловит вообще всё (включая наследников `BaseException`), и это опасно:
- перехватывает `KeyboardInterrupt`/`SystemExit` → сервис может не завершиться корректно,
- перехватывает CancelledError → ломает cancellation/graceful shutdown,
- перехватывает ошибки, которые должны “вскрыть” баг (и тогда баг превращается в молчаливую деградацию),
- затрудняет отладку: нет явного списка, что именно ожидается и почему.

Практика:
- в обычном коде ловим конкретные исключения,
- на границах ловим `Exception`, логируем и конвертируем в контролируемый ответ,
- `except:` — почти никогда; если используется как last resort, то с обязательным re-raise/корректным exit.
"""

# 10.1) Follow-up: Чем отличается `raise` от `raise e` внутри except?
"""
→ `raise` пробрасывает текущее исключение с оригинальным traceback (это обычно то, что нужно).
`raise e` может перезаписать/потерять часть traceback и ухудшить диагностику.

Правило для интервью: “использую `raise` для сохранения стека”.
"""

# 10.2) Follow-up: Как добавить контекст к ошибке и не потерять первопричину?
"""
→ Использовать chaining: `raise NewError("...") from e`.
Так сохраняется причинная цепочка (cause), и в логах видно и исходную, и обёртку.
Это особенно важно на границах слоёв (infra → domain → API).

Редкий, но полезный приём: `raise NewError(...) from None` — скрыть исходную причину,
когда это осознанно нужно для UX/безопасности (но внутри всё равно логируем реальную причину).
"""

# 10.3) Follow-up: Как проектировать “ошибки домена”, чтобы они не протекали наружу?
"""
→ Делают собственные доменные исключения/типы ошибок (ValidationError, NotFoundError, ConflictError),
а на границе маппят их в HTTP-коды/ответы (или в статусы задач/событий).
Инфраструктурные детали (psycopg errors, boto errors) не должны уходить напрямую наружу.

На интервью ожидают: “ошибки нормализованы и маппятся на границе, наружу — стабильный error model”.
"""


# 11) Что делать с транзакциями и побочными эффектами при исключениях?
"""
→ Цель: консистентность и отсутствие “полувыполненных” операций.

Для БД:
- держать транзакции короткими (минимальный scope),
- rollback на исключении (обычно через контекстный менеджер/Unit of Work),
- понимать изоляцию и конфликты (deadlocks/serialization failures → возможно ретраи транзакций).

Для внешних побочных эффектов (HTTP вызовы, публикация в брокер, отправка email):
- проектировать операции идемпотентными,
- разделять “изменение состояния” и “публикацию события” (outbox pattern),
- для распределённых сценариев использовать саги/компенсации,
- ретраить только transient и только при безопасном повторе (идемпотентность / idempotency key).

Ключевой посыл: “транзакция не должна охватывать внешний вызов,
а сайд-эффекты защищены идемпотентностью и/или outbox/inbox”.
"""

# 11.1) Follow-up: Почему опасно делать внешний HTTP-вызов внутри транзакции БД?
"""
→ Потому что транзакция удерживает локи и соединение, пока ждём сеть:
- растут блокировки и вероятность дедлоков,
- исчерпывается пул соединений,
- увеличивается bloat (MVCC) из-за долгих транзакций,
- хвостовая latency растёт.

Обычно делают: сначала фиксируем состояние в БД (и при необходимости пишем outbox),
затем выполняем внешнее действие отдельно (или через воркер/сагу).

MVCC — это механизм конкурентного доступа, где СУБД хранит несколько версий строк, 
поэтому чтения обычно НЕ блокируют записи и наоборот (читатель видит “снимок” данных).
"""

# 11.2) Follow-up: Что такое outbox pattern и какую проблему он решает?
"""
→ Решает dual-write problem: “БД обновили, а событие в брокер не отправили” или наоборот.

Подход:
- в той же транзакции, что и бизнес-изменения, пишем запись в outbox таблицу,
- отдельный publisher читает outbox и публикует в брокер (с ретраями/дедупом).

Так достигается атомарность: изменение состояния + фиксация намерения отправить событие.
"""

# 11.3) Follow-up: Как обрабатывать дубли при at-least-once доставке?
"""
→ Делать обработчик идемпотентным:
- хранить processed_message_ids / idempotency key (обычно с TTL, чтобы таблица не росла бесконечно),
- использовать уникальные ограничения + upsert,
- хранить статус операции (state machine) и проверять, выполнено ли уже.

На интервью ожидают: “дубли неизбежны, поэтому дедуп/идемпотентность — обязательны”.
"""

# 11.4) Follow-up: Что такое “poison message” и как не сломать очередь?
"""
→ Poison message — сообщение, которое стабильно падает (например, из-за данных или бага).
Стратегия:
- ограничение количества ретраев,
- DLQ (dead-letter queue) для ручного/отложенного разбора,
- алерты и наблюдаемость,
- возможность переобработки после фикса (replay).

Важно: не крутить бесконечные ретраи, иначе очередь “забьётся”.
"""

# 11.5) Follow-up: Как безопасно ретраить транзакцию при deadlock/serialization failure?
"""
→ Делать retry на уровне “целого юзкейса” или транзакционного блока:
- ограниченное число попыток,
- backoff (+ jitter при высокой конкуренции),
- строго идемпотентные операции внутри (или защита уникальными ключами),
- логирование факта ретрая/конфликта как метрика.

Это типичный Staff-уровень “докап” про консистентность.
"""

# 11.6) Follow-up: В чём разница между outbox и saga, и когда что выбирать?
"""
→ Outbox решает надёжную публикацию события из локальной транзакции (атомарность “БД + событие”).
Saga решает координацию нескольких шагов/сервисов (распределённая бизнес-операция) через:
- оркестрацию (центральный orchestrator) или
- хореографию (события между сервисами),
и использует компенсации при частичных отказах.
Выбор: outbox — почти базовая техника для событий; saga — когда есть многошаговый процесс и нужен откат/компенсации.

Outbox — паттерн “БД + событие атомарно”: в той же транзакции, где меняешь данные, пишешь событие в таблицу outbox,
 а отдельный воркер потом надежно публикует его в брокер.

Saga — паттерн для многошаговой распределённой операции между сервисами: шаги выполняются по цепочке,
 а при сбое делаются компенсирующие действия (откат бизнес-эффекта), через оркестрацию или хореографию.
 
 
Saga   — распределённая операция + компенсации.
Outbox — “БД + событие” атомарно (публикация через воркер).
"""




### TODO ТИПИЗАЦИЯ И КАЧЕСТВО КОДА

# 12) Зачем типизация (type hints) в реальном продакшене и что она даёт на senior-уровне?
"""
→ Type hints — это “контракт” между частями кода и командами. В проде они дают не абстрактную красоту, а практические выгоды:
- раньше ловим ошибки (mypy/pyright в CI) до рантайма: перепутали типы, не тот return, забыли обработать Optional,
  сломали интерфейс при рефакторинге;
- ускоряем рефакторинг: можно безопасно менять сигнатуры/модели и быстро находить места, где всё сломалось;
- повышаем читаемость API: в больших кодовых базах по типам сразу видно вход/выход и ожидания;
- улучшаем IDE/автодополнение и уменьшаем регрессии при доработках.

Что важно на senior-уровне:
- типизирую в первую очередь границы и публичные интерфейсы (API слоёв, сервисы, DTO, адаптеры),
- избегаю Any, но не делаю типизацию самоцелью (прагматично, итеративно),
- подключаю типчекер в CI и договариваюсь о правилах (strictness повышаю постепенно),
- проектирую “типовые” контракты так, чтобы они отражали реальные инварианты (Optional, Literal, Protocol).


DTO (Data Transfer Object) — простой объект/структура для передачи данных между слоями/сервисами, обычно без бизнес-логики.
Пример: UserDTO(id, name, email) для ответа API или передачи в сервис.
"""

# 12.1) Follow-up: Где типизация реально приносит максимальный эффект?
"""
→ На границах:
- между модулями/слоями (transport ↔ use-cases ↔ domain ↔ infra),
- в публичных API функций/классов,
- в моделях данных (DTO, события, ответы),
- в местах активного рефакторинга.

Там типы предотвращают “тихие” изменения контрактов и снижают стоимость коммуникации.
"""

# 12.2) Follow-up: Как бороться с “типовым хаосом” в проектах с ORM/динамикой?
"""
→ Типизировать не всё подряд, а ключевые контуры:
- вход/выход: dataclass/pydantic/TypedDict для DTO (контракт на границе),
- зависимости: Protocol для репозиториев/клиентов,
- stubs/py.typed/локальные typing-обёртки для библиотек без типов,
- Any держать “на входе” (шлюз), но не протаскивать внутрь домена.

Практическая техника: нормализовать данные “один раз” на границе
(парсинг/валидация → дальше код работает с типизированными структурами).
"""

# 12.3) Follow-up: mypy vs pyright — что выберешь и почему?
"""
→ pyright обычно быстрее и строже “из коробки”, хорошая интеграция с VS Code/Pylance.
mypy гибче по настройкам, очень распространён и имеет богатую экосистему плагинов.

Ответ уровня senior: “выбираю по стеку команды/репо; важно, чтобы:
- типчекер был в CI,
- правила были согласованы (strictness, Any, Optional),
- были понятные исключения (per-module/per-package), а не хаос”.
"""

# 12.4) Follow-up: Какие практики помогают внедрять типизацию без боли?
"""
→ Постепенно:
- начать с публичных интерфейсов и новых модулей,
- включить типчекер в non-blocking режиме (report-only), затем ужесточать,
- договориться о минимальных правилах (Optional handling, возвращаемые типы, запрет “широкого Any”),
- ограничивать зоны без типизации (typing “границами”, не “везде сразу”),
- добавлять pre-commit (format + lint + typecheck для изменённых файлов).
"""

# 12.5) Follow-up: Примеры типовых ошибок, которые типизация ловит чаще всего?
"""
→ Частые кейсы:
- забыли обработать Optional (None),
- функция возвращает не тот тип (например, str vs int),
- перепутали порядок/названия параметров,
- несовместимые структуры dict (TypedDict помогает),
- неверно используем API библиотеки (если есть stubs),
- неверные generic-типы коллекций (list[str] vs list[int]).
"""

# 12.6) Follow-up: Как “правильно” относиться к `Any`?
"""
→ `Any` допустим как “шлюз” на границе (входной JSON, не типизированная библиотека),
но его нужно локализовать:
- как можно раньше преобразовать/провалидировать в строгий тип,
- не тащить Any в доменные модели и бизнес-логику,
- для библиотек без типов — лучше stub/Protocol, чем разносить Any повсюду.
"""


# 13) Что такое `Protocol` и когда он предпочтительнее ABC?
"""
→ `Protocol` — структурная типизация: объект подходит, если у него есть нужные методы/атрибуты (duck typing),
без обязательного наследования. Это удобно для слабой связанности и “контрактов по поведению”.

Когда Protocol особенно уместен:
- зависимости/адаптеры (репозитории, клиенты внешних сервисов) — домен зависит от интерфейса, а не от реализации,
- тесты: легко подменить fake/stub объектом без наследования и тяжёлых моков,
- небольшие интерфейсы, где важнее гибкость.

ABC уместнее, когда:
- нужна общая реализация (template method / shared behavior),
- важны инварианты/состояние базового класса,
- хочется явно зафиксировать иерархию/семантику наследования.
"""

# 13.1) Follow-up: Как Protocol помогает тестируемости по сравнению с ABC?
"""
→ Не нужно наследоваться: любой объект с нужным методом подходит.
Можно написать простой fake/stub в тесте без мок-фреймворков и без привязки к иерархии.
Это снижает coupling и ускоряет тесты.
"""

# 13.2) Follow-up: Когда Protocol может быть опасен/неочевиден?
"""
→ Когда интерфейс разрастается:
- “скрытый контракт” сложнее поддерживать,
- больше мест “случайно” удовлетворяют протоколу, но семантически не подходят.

Решение:
- дробить на маленькие протоколы,
- давать имена и документацию,
- иногда перейти на ABC, если нужна более жёсткая дисциплина.
"""

# 13.3) Follow-up: Что такое `@runtime_checkable` Protocol и почему с ним осторожно?
"""
→ Позволяет делать `isinstance(obj, Proto)` на рантайме,
но проверка поверхностная: наличие атрибутов/методов, не семантика.
Использую редко: чаще для “best-effort” адаптеров и диагностики.
"""

# 13.4) Follow-up: Пример, где ABC лучше Protocol.
"""
→ Когда базовый класс задаёт общий алгоритм (template method) и требует реализации частей (hook methods),
или когда нужно общее состояние/инварианты, которые должны обеспечиваться базовым классом.
"""

# 13.5) Follow-up: Как бы ты типизировал репозиторий/клиент внешнего сервиса?
"""
→ Через Protocol + generics:
- методы get/save/list,
- типы доменных сущностей и идентификаторов,
- ошибки/результаты (Optional/Result-подобные структуры) фиксируются в контракте.

Реализации (PostgresRepo, MemoryRepo, HttpClientRepo) подменяются без изменения домена.
Ключ: “домен зависит от абстракции, а не от инфраструктуры”.
"""

# 13.6) Follow-up: Protocol vs ABC: влияет ли это на рантайм?
"""
→ Почти нет:
- Protocol — в основном для статической проверки (type checker),
- ABC — влияет на рантайм (наследование, isinstance/issubclass, абстрактные методы).
Поэтому Protocol часто легче и “дешевле” для архитектуры, если не нужен общий код.
"""


# 14) Объясни covariance/contravariance на примере коллекций и Callable.
"""
→ Variance — это про то, можно ли подставлять “производный” тип вместо “базового” в обобщённых типах.

Коллекции:
- `Sequence[T]` ковариантна, потому что это read-only интерфейс:
  `Sequence[Cat]` можно использовать как `Sequence[Animal]`, ведь мы только читаем Animal.
- `list[T]` инвариантен, потому что list изменяемый:
  если бы list был ковариантным, можно было бы передать list[Cat] туда, где ждут list[Animal],
  и затем добавить Dog → сломаем list[Cat].

Callable:
- аргументы контрвариантны, возвращаемый тип ковариантен.
Интуиция: функция, которая умеет принимать более общий тип (Animal),
может заменить функцию, ожидающую более конкретный (Cat),
потому что ей “не страшно” получить Cat. А вот наоборот — опасно.
"""

# 14.1) Follow-up: Почему на практике рекомендуют принимать `Iterable/Sequence`, а не `list`?
"""
→ Потому что это расширяет совместимость API: можно передать list/tuple/generator.
И это честнее по контракту: если функция не мутирует коллекцию, ей не нужен именно list.
Плюс это улучшает тестируемость и позволяет стриминг (generator).
"""

# 14.2) Follow-up: Приведи пример, где неправильная variance приводит к багу.
"""
→ Если предположить ковариантность list:
func(animals: list[Animal]) может добавить Dog.
Если туда передали list[Cat], мы получили Dog в списке котов → нарушение инварианта и потенциальные рантайм-ошибки.
"""

# 14.3) Follow-up: Что такое `Final`, `Literal`, `TypeGuard` и где это реально полезно?
"""
→ `Final`:
- фиксирует, что имя/атрибут не должен переопределяться (константы, синглтоны, публичные “точки расширения”).

`Literal`:
- уточняет конкретные значения (режимы "read"/"write", коды/состояния),
- полезно для “конфигов” и API с ограниченным набором значений.

`TypeGuard`:
- помогает тайпчекеру сузить тип после проверки (особенно для Union),
- полезно в валидации/парсинге, где после проверки хочется строгий тип.

Использую точечно, когда повышает безопасность и читаемость.
"""

# 14.4) Follow-up: Как типизировать функцию, которая принимает либо str, либо Path?
"""
→ Использовать Union (`str | Path`) и внутри нормализовать в Path.
Если важно, что возвращаем тот же “вид” (редко), можно добавить overload’ы.

Пример:
def f(p: str | Path) -> Path:
    return Path(p)
"""

# 14.5) Follow-up: Зачем нужны overload’ы и где они реально помогают?
"""
→ overload помогает описать разные сигнатуры и связать входы с выходами:
- разные типы аргументов → разные возвращаемые типы,
- улучшает автодополнение и уменьшает касты.

Полезно в обёртках над API, фабриках и функциях “принимает много форматов, но возвращает предсказуемо”.
"""



### TODO ПАМЯТЬ И ПРОИЗВОДИТЕЛЬНОСТЬ

# 15) Если в CPython есть refcount, зачем ещё GC? Какие проблемы он решает?
"""
→ В CPython основной механизм управления жизненным циклом объектов — refcount:
как только счётчик ссылок падает до 0, объект освобождается сразу
(это даёт довольно “детерминированное” освобождение *в CPython*).

Но refcount не умеет разрывать циклы:
- если A ссылается на B, а B на A, их refcount не станет 0, даже если цикл больше никому не нужен.

Для этого нужен cyclic GC (поколенческий сборщик), который периодически:
- находит контейнерные объекты, участвующие в циклах (list/dict/set/instance и т.п.),
- строит/анализирует граф ссылок внутри поколения,
- выделяет unreachable cycles и освобождает их.

Практическая формулировка:
- refcount решает “обычное” освобождение,
- GC нужен для *циклов* и сложных графов объектов.
"""

# 15.1) Follow-up: Чем опасны циклы с `__del__` (финализаторами)?
"""
→ Циклы с финализаторами (`__del__`) исторически проблемны:
порядок финализации объектов в цикле неопределён, поэтому CPython может не собирать такие циклы автоматически
(или откладывать/класть в gc.garbage в некоторых сценариях).

Практика:
- избегать `__del__` для управления ресурсами,
- использовать context managers (with) / явный close(),
- использовать `weakref.finalize` как более безопасную альтернативу финализатору.

НЕ делает __del__ надёжным для критического освобождения ресурсов, потому что:

момент вызова __del__ не гарантирован по времени (когда именно соберётся GC);
__del__ может не вызваться при аварийном завершении процесса;
сложные случаи (например, “воскрешение” объекта в __del__, исключения, взаимодействие с другими финализаторами) остаются источником проблем.

Поэтому правило: ресурсы закрываем через with / try/finally, а __del__ — не основной механизм cleanup.   <-----
"""

# 15.2) Follow-up: Как искать утечки памяти в Python-сервисе?
"""
→ Подход “по-взрослому”:
1) Подтвердить рост метриками:
   - RSS vs heap (если есть), графики по времени, связь с трафиком/релизами.
   - понять “утечка” vs “кэш/буферы/фрагментация/аллокатор держит арену”.

2) Найти источник аллокаций:
   - tracemalloc snapshots + diff (топ файлов/строк),
   - sampling профайлеры памяти (например, scalene) — “кто аллоцирует”.

3) Понять, кто удерживает ссылки:
   - gc.get_referrers / objgraph (осторожно: тяжёлые инструменты),
   - смотреть на глобальные коллекции, кеши, очереди, “накопители” логов/трасс.

4) Типовые причины:
   - lru_cache без maxsize/TTL,
   - кеши/мапы без eviction,
   - очереди/буферы без backpressure,
   - накопление “контекста” (например, логгеры/handlers/трейсы),
   - сторонние библиотеки (особенно C-расширения, mmap, изображения).

Важно на интервью: “сначала измеряю и локализую, потом чиню”.
"""

# 15.3) Follow-up: Что такое weakref и где он полезен?
"""
→ weakref — слабая ссылка: не увеличивает refcount,
объект может быть собран, даже если на него есть weakref.

Полезно для:
- кешей (WeakValueDictionary/WeakKeyDictionary), чтобы кеш не превращался в “утечку”,
- observer pattern и графов объектов, где иначе легко создать циклы.
"""

# 15.4) Follow-up: Почему рост RSS не всегда означает утечку в Python?
"""
→ RSS может расти без “логической утечки”:
- pymalloc/арены: память может не возвращаться ОС сразу, остаётся в процессе для повторного использования,
- фрагментация (внутренняя/в ОС),
- кеши/пулы/буферы,
- mmap, большие страницы, сторонние библиотеки.

Поэтому важны:
- heap-профиль/allocations (tracemalloc),
- поведение после снижения нагрузки,
- сравнение “живых объектов” и удерживающих ссылок.

RSS (Resident Set Size) — это сколько физической RAM сейчас занимает процесс (его “живущие” в памяти страницы),
т.е. видимый объём памяти процесса в ОС.
"""

# 15.5) Follow-up: Как устроен “поколенческий GC” в CPython и зачем поколения?
"""
→ GC в CPython поколенческий: объекты, пережившие сборку, перемещаются в “старшие” поколения,
которые проверяются реже.
Идея: большинство объектов живёт недолго → чаще проверяем молодое поколение и экономим CPU.
Это помогает держать overhead GC контролируемым на реальных нагрузках.
"""


# 16) Как бы ты профилировал Python-сервис, если он стал медленным/память растёт?
"""
→ Начинаю с данных и гипотез, а не с “магии”:

1) Метрики:
- latency p95/p99, RPS, error rate,
- saturation (CPU/mem),
- saturation пулов (DB/HTTP), queue depth/lag,
- GC/alloc rate, pauses (если есть наблюдаемость).

2) Локализация:
- какой endpoint/тип запросов ухудшился,
- что поменялось (релиз, конфиг, трафик, зависимость).

3) CPU:
- локально: cProfile/pyinstrument,
- в проде: py-spy sampling (без рестарта), flamegraph: “куда уходит CPU”.

4) Память:
- tracemalloc snapshots/diff по аллокациям,
- при необходимости: scalene/objgraph/gc.get_referrers для “кто удерживает”.

5) Трейсинг:
- OpenTelemetry: time in DB / downstream / app, чтобы не гадать.

Ожидаемая мысль: “я разделяю CPU/IO/DB, считаю хвосты, использую метрики+трейсы+профиль”.
"""

# 16.1) Follow-up: Почему важны p95/p99, а не только среднее?
"""
→ Среднее прячет хвосты.
p95/p99 отражают реальные проблемы пользователя и перегрузки (очереди, блокировки, GC паузы, pool waits).
Часто именно хвостовая latency “ломает” SLO.
"""

# 16.2) Follow-up: Как понять, что тормозит БД, а не Python?
"""
→ По трейсам/метрикам:
- рост времени в DB span,
- saturation connection pool (wait time на пул),
- slow queries / EXPLAIN/ANALYZE (seq scan, плохой план),
- блокировки/locks, deadlocks.

Если time-in-app растёт, а time-in-db нет — ищем CPU/аллоцирование/локи/контеншен в коде.
"""

# 16.3) Follow-up: Как профилировать в проде “безопасно”?
"""
→ Использовать low-overhead sampling:
- py-spy (sampling), ограниченное время профиля,
- canary/один инстанс,
- собирать только нужные стеки/период,
- параллельно полагаться на трейсинг/метрики (это почти всегда дешевле, чем “всё профилировать”).
"""

# 16.4) Follow-up: Что такое “alloc rate” и почему он влияет на latency?
"""
→ alloc rate — темп аллокаций объектов.
Высокий alloc rate:
- давит на CPU cache и память,
- увеличивает расходы на управление объектами,
- повышает churn в GC (для циклов) и общий overhead рантайма,
что особенно бьёт по p99 (через очереди и косвенные задержки).
"""

# 16.5) Follow-up: Как понять “CPU-bound” или “I/O-bound” по симптомам?
"""
→ CPU-bound:
- CPU near 100%, py-spy показывает горячие функции Python,
- latency растёт вместе с CPU, очередь задач растёт при росте RPS.

I/O-bound:
- CPU умеренный, но растёт время ожидания (DB/HTTP spans),
- saturation пулов/очередей, много времени в ожидании (pool wait, network),
- часто помогает лимитировать concurrency и чинить внешние зависимости.
"""


# 17) Как оптимизировать горячий участок кода “по-взрослому”?
"""
→ Последовательность, которую ждут на интервью:

1) Измерить:
- профилировщик/трейсы, найти bottleneck (не оптимизировать “на глаз”).

2) Алгоритм/асимптотика:
- часто даёт x10+, в отличие от микрооптимизаций.

3) Структуры данных:
- set/dict вместо линейного поиска, heap/deque, правильные индексы/кеши.

4) Уменьшить аллокации и копирования:
- меньше промежуточных списков,
- стриминг/генераторы там, где это помогает,
- батчи и vectorized операции (NumPy), переиспользование буферов.

5) Вынос из Python, если надо:
- C/Numba/Cython/векторизация,
- процессы для CPU-bound.

6) Контроль регрессий:
- бенчмарки/нагрузочные, сравнение метрик до/после,
- держать читаемость и поддерживаемость.

Ключевая мысль: “оптимизация — управляемый процесс с измерениями и контролем регрессий”.
"""

# 17.1) Follow-up: Какие “микрооптимизации” обычно бессмысленны без профиля?
"""
→ Без профиля почти всегда пустая трата времени:
- спор comprehension vs for-loop,
- “вынести локальную переменную ради скорости”,
- преждевременный кеш “на всякий случай”.

Сначала профиль → потом точечные улучшения.
"""

# 17.2) Follow-up: Когда кэш в приложении оправдан?
"""
→ Когда:
- есть повторяемые запросы/данные,
- есть понятный TTL/инвалидация,
- есть метрики hit/miss и контроль размера (eviction),
- кэш не ломает критичную консистентность.

На интервью важно: “инвалидация — главная сложность” и “кэш должен быть управляемым”.
"""

# 17.3) Follow-up: Что делать, если узкое место — сериализация JSON/парсинг?
"""
→ Варианты (после измерения):
- быстрее библиотека (orjson/ujson, если допустимо по требованиям),
- уменьшить payload (меньше полей/размер),
- убрать лишние преобразования (dict ↔ model ↔ dict),
- ускорить валидацию (pydantic v2, точечная настройка моделей),
- кешировать сериализованный вид для hot-path (осторожно с памятью).

Но решение выбираю после профиля: часто bottleneck не там, где кажется.
"""

# 17.4) Follow-up: Как не “сломать” систему оптимизацией?
"""
→ Техники снижения риска:
- тесты + бенчмарки, сравнение метрик до/после,
- маленькие PR, canary rollout,
- наблюдаемость на выкате (p95/p99, error rate, saturation),
- помнить про trade-offs: скорость ↔ память ↔ сложность ↔ консистентность.
"""

# 17.5) Follow-up: Что делать, если “стало быстрее”, но выросла память/GC pressure?
"""
→ Считать полный баланс:
- возможно, улучшили CPU ценой аллокаций → вырос alloc rate и p99,
- смотреть heap/alloc diff, искать рост промежуточных объектов/кешей,
- добавлять лимиты (cache size), батчи, переиспользование буферов,
- выбирать вариант, который улучшает SLO, а не только “среднюю скорость”.
"""



### TODO CONCURRENCY / ASYNC

# 18) Когда выбирать asyncio, threads или processes? Примеры.
"""
→ Выбор делаю от профиля нагрузки и ограничений стека (драйверов/библиотек).

asyncio выбираю, когда нужно много конкурентного I/O и есть async-драйверы:
- веб-серверы/гейтвеи с большим числом соединений,
- много параллельных запросов к HTTP/DB (async client + async DB driver),
- websockets/streaming, long-polling.

threads выбираю, когда:
- стек синхронный, но нужно перекрывать I/O (requests/boto/старые SDK),
- нельзя/дорого переписывать архитектуру на async,
- нужно “обернуть” блокирующую библиотеку (через ThreadPoolExecutor) и контролировать параллелизм.

processes выбираю для CPU-bound:
- тяжёлые вычисления/ETL, обработка изображений/видео, криптография, парсинг больших объёмов,
- нужен настоящий параллелизм (обойти GIL) и можно терпеть IPC/память.

Важные нюансы для интервью:
- “asyncio ≠ быстрее всегда”: это про эффективность конкурентного I/O и меньше overhead на потоки.
- threads в CPython не ускоряют чистый CPU Python из-за GIL, но отлично перекрывают ожидание (I/O).
- processes дают параллелизм, но цена — сериализация/память/сложность управления.
"""

# 18.1) Follow-up: Как ограничить параллелизм в asyncio, чтобы не убить БД/внешний сервис?
"""
→ Инструменты:
- asyncio.Semaphore/BoundedSemaphore вокруг “дорогого” участка,
- bounded asyncio.Queue (backpressure), worker pool,
- лимиты на количество задач (не создавать миллионы create_task),
- ограничения на пул соединений клиента (HTTP/DB) + таймаут ожидания соединения,
- rate limiting (token bucket) на границе.

Обязательные практики:
- timeouts на каждый downstream вызов и на ожидание пула,
- общий deadline запроса (deadline propagation),
- метрики: pool wait time, in-flight, errors, p95/p99.
"""

# 18.2) Follow-up: Какие типичные ошибки при смешивании sync и async?
"""
→ Частые ошибки:
- блокирующие вызовы внутри event loop (requests, boto3, time.sleep, файловые операции без offload),
- CPU-heavy цикл внутри async → стопорит loop,
- запуск слишком многих задач без лимита → взрыв памяти/дескрипторов,
- “глотание” CancelledError → зависания на shutdown,
- смешивание разных event loop/loop-per-thread без понимания.

Решения:
- async аналоги (aiohttp, async DB drivers),
- run_in_executor/to_thread для блокирующего + лимиты пула,
- вынос CPU-bound в процессы,
- структурированная конкурентность (TaskGroup), явный cancel/cleanup.
"""

# 18.3) Follow-up: Что выбрать для CPU-bound внутри async-сервиса?
"""
→ Обычно:
- ProcessPoolExecutor / отдельные воркеры / отдельный сервис,
потому что это реальный параллелизм.

ThreadPool редко помогает из-за GIL, если вычисление в чистом Python.
Исключение: CPU в нативном коде, который release’ит GIL (NumPy/BLAS), но там надо следить за oversubscription.
"""

# 18.4) Follow-up: Что такое structured concurrency и зачем TaskGroup (py3.11+)?
"""
→ Structured concurrency: задачи живут внутри явных “областей” (scope),
и их жизненный цикл управляется предсказуемо:
- если одна задача упала — группа корректно отменяет остальные,
- гарантируется cleanup при выходе из блока,
- проще reasoning про ошибки и отмену, меньше “висячих” задач.

TaskGroup упрощает корректную обработку ошибок по сравнению с россыпью create_task/gather.
"""

# 18.5) Follow-up: Чем отличается concurrency от parallelism (и почему это важно)?
"""
→ Concurrency: много задач “в процессе” (перекрываем ожидание, переключаемся).
Parallelism: реально выполняем одновременно на разных ядрах.

asyncio даёт concurrency (супер для I/O), processes дают parallelism (нужно для CPU-bound).
"""


# 19) Что такое event loop и почему блокирующий код внутри async — проблема?
"""
→ Event loop — планировщик корутин: они выполняются кооперативно и переключаются в точках await.
Если внутри async-функции выполнить блокирующий код:
- CPU-heavy (долгий цикл),
- I/O без await (requests, time.sleep),
то loop не может переключаться → “зависают” все остальные задачи:
растут latency, timeouts, tail p99.

Решения:
- использовать async-библиотеки (aiohttp, async DB драйверы),
- выносить блокирующее в executor (asyncio.to_thread / run_in_executor) + лимиты,
- выносить CPU-bound в процессы,
- timeouts + лимиты конкурентности + backpressure.
"""

# 19.1) Follow-up: Что такое cancellation в asyncio и почему её нельзя “глотать”?
"""
→ Отмена задачи поднимает CancelledError (BaseException-подобная логика).
Если перехватить и не пробросить, задача может “не отмениться”:
- ресурсы не освободятся,
- shutdown зависнет.

Правильно:
- ловим cancellation только ради cleanup (finally),
- затем re-raise CancelledError.
"""

# 19.2) Follow-up: Что будет, если забыть await?
"""
→ Корутин-объект не выполнится:
- логика “молча не сработает”,
- возможны RuntimeWarning (“coroutine was never awaited”),
- ресурсы/сессии могут не закрыться (если ожидалось, что код выполнит cleanup).

Ключ: корутина — это объект; пока её не await/не запланировали, она не исполняется.
"""

# 19.3) Follow-up: Как дебажить “подвисание” event loop?
"""
→ Подход:
- включить debug mode loop (asyncio debug),
- искать долгие callbacks/участки без await,
- добавить таймауты (на операции и на shutdown),
- py-spy/cProfile (если CPU), трейсинг (если I/O),
- проверить saturation пулов: DB/HTTP pool wait, очередь задач, лимиты сокетов/FD.

Типичный реальный culprit: pool exhaustion + нет таймаута ожидания пула → “вечное ожидание”.
"""

# 19.4) Follow-up: Когда run_in_executor/to_thread — плохая идея?
"""
→ Когда блокирующая работа массовая/тяжёлая:
- легко создать contention и исчерпать ресурсы (много потоков/памяти),
- tail latency ухудшается из-за очередей и планирования.

Тогда лучше:
- отдельные воркеры/процессы,
- отдельный сервис,
- ограниченный worker pool + backpressure.
"""

# 19.5) Follow-up: Почему “async быстрее” может оказаться мифом в конкретном проекте?
"""
→ Потому что:
- нет async-драйверов (всё равно придётся в threads),
- bottleneck в БД/наружных лимитах, а не в приложении,
- добавили слишком много конкурентности → перегрузили зависимость,
- CPU-bound куски блокируют loop.
Ответ: измеряем и подбираем модель под реальный bottleneck.
"""


# 20) Что такое backpressure и как его внедрить в пайплайне/очереди?
"""
→ Backpressure — механизм контроля входного потока, чтобы продюсер не перегрузил консьюмера/систему.
Без него очереди растут, память заканчивается, p99 улетает, начинаются каскадные отказы.

Как внедряют:
- bounded queues (ограниченный размер),
- лимиты параллелизма (Semaphore/worker concurrency),
- rate limiting (token bucket/leaky bucket), 429/503 при перегрузе,
- управление prefetch (в брокерах/воркерах),
- деградация: отдавать stale из кэша, отключать необязательные функции,
- “shed load”: быстро отказывать вместо бесконечного ожидания.

Ключ для интервью: backpressure — стратегия защиты системы, а не только “очередь”.
"""

# 20.1) Follow-up: Как backpressure связан с connection pool?
"""
→ Пул — естественный backpressure: если соединения закончились, запросы ждут.
Но без таймаутов ожидание превращается в “залипание”:
- растёт очередь,
- растёт p99,
- начинается каскадное падение.

Правильно:
- таймаут ожидания пула,
- лимит concurrency выше по стеку,
- метрики pool wait + alarms.
"""

# 20.2) Follow-up: Что будет, если backpressure не настроен?
"""
→ Типовые последствия:
- OOM (очереди/буферы растут),
- исчерпание file descriptors/sockets,
- saturation CPU, рост p99/p999,
- timeouts и retry storm,
- вы начинаете DDOS’ить БД/внешний API → каскадные отказы.

Backpressure — это механизм, когда медленный потребитель “тормозит” производителя, чтобы тот не переполнял очереди/память.

Пример: если обработчик не успевает читать из очереди, очередь заполняется → put()/запись начинает блокироваться
или ждать, и продюсер вынужден замедлиться.
"""

# 20.3) Follow-up: Как реализовать rate limiting в API?
"""
→ Механики:
- token bucket / leaky bucket,
- лимиты по user/ip/api-key/endpoint,
- разные лимиты на тяжёлые операции,
- распределённо: Redis/gateway/service mesh,
- ответы 429 + Retry-After.

Важно: лимиты должны быть наблюдаемыми (метрики/логи) и согласованы с SLO.
"""


#  -- Алгоритмы ограничения трафика (rate limiting) или Алгоритмы ограничения ЧАСТОТЫ ЗАПРОСОВ --
"""
Алгоритмы ограничения ЧАСТОТЫ ЗАПРОСОВ (rate limiting) — это способы ограничивать число запросов/скорость от пользователя,
сервиса или IP за единицу времени, чтобы защитить систему от перегрузки, злоупотреблений и обеспечить стабильную работу.

Основные виды:
- Token bucket (маркерная корзина) — ограничивает среднюю скорость, разрешает короткие всплески.
- Leaky bucket (дырявое ведро) — выпускает запросы равномерно, сглаживает трафик (очередь/сброс лишнего).
- Fixed window counter (фиксированное окно) — считает запросы в интервале (например, за минуту); просто, но есть проблема на границе окна.
- Sliding window log (журнал скользящего окна) — хранит времена запросов за последние T секунд; точно, но дорого.
- Sliding window counter (счётчик скользящего окна) — приближённый вариант скользящего окна; дешевле, почти точно.
"""


# 20.4) Follow-up: Что делать при перегрузе: “резать” трафик или “очередить”?
"""
→ Зависит от типа нагрузки:
- интерактивные запросы: лучше резать/деградировать (иначе пользователи ждут бесконечно и всё равно падают),
- фоновые задачи: можно очередить, но с bounded queue, приоритетами и DLQ.

Ключ: очередь не бесконечная — у неё должны быть лимиты и политика.
"""

# 20.5) Follow-up: Чем backpressure отличается от rate limiting?
"""
→ Rate limiting ограничивает скорость входа (сколько запросов/сек).
Backpressure управляет “внутренним давлением” системы:
- ограничивает in-flight,
- ограничивает очереди/буферы,
- заставляет продюсера замедлиться или получить отказ.

Обычно они работают вместе: rate limiting на входе + backpressure внутри.
"""



### TODO АРХИТЕКТУРА И ДИЗАЙН

# 21) Что ты понимаешь под “границами” (boundaries) в сервисе? Зачем они нужны?
"""
→ Границы — это места, где один слой/контекст взаимодействует с другим: transport (HTTP/gRPC/CLI), application/use-cases,
domain, infrastructure (БД/кэш/брокер/внешние API).

Зачем нужны:
- разделение ответственности: домен содержит бизнес-правила и инварианты, инфраструктура — детали хранения/сети,
- тестируемость: домен/юзкейсы тестируются без реальной БД/брокера через интерфейсы/адаптеры,
- управляемость изменений: смена БД/клиента/формата транспорта не ломает домен,
- безопасность и наблюдаемость: на границах удобно делать валидацию, авторизацию, нормализацию ошибок, логирование/трейсинг.

Что обычно делают на границах:
- валидация входа (s syntactic): типы/формат/обязательные поля,
- маппинг DTO ↔ доменные сущности (не тащить ORM-модели в домен),
- нормализация ошибок (domain errors → 4xx, infra errors → 5xx/ретраи),
- постановка deadline/timeouts, correlation_id/trace_id, метрики.
"""

# 21.1) Follow-up: Как отличить “валидацию на входе” от “доменной валидации”?
"""
→ На входе (transport): проверяем формат/типы/обязательные поля, лимиты (max длина, диапазоны) — это syntactic validation.
В домене: проверяем бизнес-инварианты (например, “нельзя списать больше баланса”, “статус нельзя перевести назад”) — semantic validation.
Ключ: доменные правила должны жить в домене, чтобы не зависеть от HTTP/формы запроса.
"""

# 21.2) Follow-up: Почему плохо “протаскивать” ORM-модели через весь сервис?
"""
→ ORM-модель тащит:
- детали хранения (lazy loading, сессии, side effects),
- сложно тестировать (без БД/сессии),
- легко получить N+1 и непредсказуемые запросы,
- домен становится зависим от инфраструктуры.
Лучше: маппинг ORM → доменные структуры/DTO на границе репозитория.
"""

# 21.3) Follow-up: Где проводить границу транзакции и почему это важно?
"""
→ Транзакция должна охватывать минимальный атомарный бизнес-кусок (use-case), но не включать внешние HTTP вызовы.
Долгие транзакции = блокировки, исчерпание пула, MVCC bloat.
Обычно: транзакция вокруг изменения состояния + запись outbox (если нужно событие), а внешние эффекты — отдельно.
"""

# 21.4) Follow-up: Как бы ты организовал зависимости между слоями?
"""
→ Зависимости направлены внутрь: domain не зависит от infrastructure.
Инфраструктура реализует интерфейсы (ports), домен использует абстракции.
Практически: Protocol/ABC для репозиториев/клиентов, DI (ручной/контейнер), адаптеры на краях.
"""

# 21.5) Follow-up: Когда DDD/слои не нужны и достаточно “простого CRUD”?
"""
→ Если домен простой, нет сложных инвариантов, проект маленький — можно сделать проще.
Но всё равно полезно иметь хотя бы минимальные границы: transport ↔ service ↔ storage,
чтобы не смешивать HTTP/SQL в одном месте.
Ожидаемая мысль: “архитектура соразмерна сложности”.

DDD — это подход, где модель и код строятся вокруг предметной области и её правил (инвариантов), а не вокруг БД/CRUD
"""


# 22) Как бы ты организовал обработку ошибок в микросервисе?
"""
→ Я строю систему ошибок как часть контракта сервиса:

1) Классификация:
- ожидаемые бизнес-ошибки (валидация, not found, conflict) → 4xx, без ретраев,
- transient инфраструктурные (timeout, network, deadlock/serialization failure) → 5xx/503, ретраи с backoff,
- баги (programmer errors) → 5xx, алертинг/фикс.

2) Нормализация:
- единый error model: code/type, message (без секретов), details, request_id/trace_id,
- маппинг доменных исключений в конкретные HTTP статусы,
- маппинг infra исключений в 503/500 (и, если нужно, проброс причины во внутренние логи/трейсы).

3) Надёжность:
- таймауты на все внешние вызовы + deadline propagation,
- retries только там, где безопасно (идемпотентно) + exponential backoff + jitter + лимиты,
- circuit breaker/bulkhead, чтобы не утопить сервис при деградации зависимости,
- graceful degradation (stale cache, partial response, отключение необязательных функций).

4) Наблюдаемость:
- structured logs + logger.exception,
- метрики error rate по кодам/категориям,
- трассировка (OpenTelemetry) с разделением “time in DB/downstream/app”,
- алерты по SLO (p95/p99, burn rate/error budget).
"""

# 22.1) Follow-up: Как сделать error model, чтобы он был полезен клиентам и безопасен?
"""
→ Клиенту: стабильный code/type, понятный message, иногда field-level details (validation).
Безопасность: не отдаём stacktrace, SQL, внутренние ids/секреты. Внутрь (логи/трейс) — наоборот, даём максимум контекста.
Ожидаемая мысль: “внешний контракт стабилен, детали только во внутренней наблюдаемости”.
"""

# 22.2) Follow-up: Когда ставить 500, когда 503, когда 429?
"""
→ 500: ошибка сервера/баг, “повтор может не помочь”.
503: сервис временно не готов/зависимость недоступна (retry вероятно поможет).
429: rate limit — клиент превысил лимит, возвращаем retry-after.
На интервью любят: “правильные коды + правильная политика ретрая”.
"""

# 22.3) Follow-up: Что такое circuit breaker и какие состояния он имеет?
"""
→ Обычно: closed (всё ок), open (быстро фейлим, не ходим в зависимость), half-open (пробуем ограниченно).
Нужны пороги по ошибкам/таймаутам и окна времени. Цель: не усугублять деградацию зависимости и защитить свой сервис.

Circuit breaker — паттерн, который отключает обращения к падающей зависимости после ошибок и быстро фейлит, а затем периодически пробует снова.
"""

# 22.4) Follow-up: Что такое bulkhead и как его применить в Python-сервисе?
"""
→ Изоляция ресурсов: отдельные пулы/лимиты для разных направлений (например, отдельный пул запросов к платежам и к профилю),
чтобы “падающая” зависимость не съела все потоки/соединения.
Практика: разные connection pools, разные semaphores/queues, лимиты на executor.

Bulkhead — паттерн изоляции: разделяем ресурсы на отдельные “отсеки” (пулы/лимиты), чтобы сбой одной части не “утопил” весь сервис.
"""

# 22.5) Follow-up: Как избежать “retry storm” при падении зависимости?
"""
→ Ограничить ретраи (max attempts), backoff + jitter, circuit breaker, кеширование ошибок на короткое время,
и, главное, таймауты. Также важно: не ретраить неидемпотентные операции.
Ожидаемый ответ: “таймауты + backoff/jitter + breaker”.
"""


# 23) Как отличить рефакторинг от “переписывания” и как снижать риск изменений?
"""
→ Рефакторинг:
- сохраняет внешнее поведение и контракты,
- делается маленькими шагами (small PR),
- под защитой тестов/метрик,
- можно выкатывать постепенно и откатывать.

Переписывание:
- меняет поведение/контракты и часто “всё сразу”,
- высокий риск регрессий и “big bang” релиза.

Как снижать риск:
- инкрементальные PR + code review,
- feature flags/конфиги, чтобы включать поэтапно,
- canary/blue-green, постепенный rollout,
- миграции данных по паттерну expand/contract,
- наблюдаемость и явный rollback plan (как откатиться быстро и безопасно).
"""

# 23.1) Follow-up: Что такое expand/contract migrations и зачем они нужны?
"""
→ Expand: добавляем новые поля/таблицы так, чтобы старый код продолжал работать (обратная совместимость).
Дальше: dual-write/dual-read при необходимости.
Contract: после выката нового кода удаляем старые поля/код.
Это снижает риск “сломать прод” при изменениях схемы.
"""

# 23.2) Follow-up: Какие метрики ты смотришь во время canary?
"""
→ Golden signals: error rate, p95/p99 latency, saturation (CPU/mem/pool), плюс бизнес-метрики (заказы/конверсия).
И сравниваю canary vs baseline.

Canary — это релиз, когда новую версию выкатывают на небольшой процент трафика/серверов, чтобы проверить, что всё ок,
прежде чем раскатывать всем.
"""

# 23.3) Follow-up: Как понять, что тестов “достаточно” для безопасного рефакторинга?
"""
→ Есть покрытие критичных инвариантов и интеграций:
- unit на домен/юзкейсы,
- интеграционные тесты на БД/брокер/внешние контракты,
- e2e точечно на критические пользовательские сценарии.
Плюс: мониторинг регрессий в проде (алерты, SLO).
"""

# 23.4) Follow-up: Что делать, если рефакторинг требует изменения контракта API?
"""
→ Версионирование (v1/v2), backward compatibility, депрекейшн политика, dual-read/dual-write,
документация и миграционный период для клиентов.
Ожидаемый ответ: “не ломаю клиентов без плана миграции”.
"""


### TODO БАЗЫ ДАННЫХ И КОНСИСТЕНТНОСТЬ


# ACID
"""
# КОРОТКО:
ACID — свойства надежных транзакций в БД:

Атомарность — либо всё, либо ничего.
Пример: Перевод денег: если списание прошло, а зачисление нет — операция отменится.

Согласованность — данные всегда корректны.
Пример: Невозможно записать отрицательный баланс, если есть ограничение balance >= 0.

Изолированность — транзакции не мешают друг другу.
Пример: Пока один пользователь не завершит редактирование записи, другой не увидит её изменения.

Долговечность — изменения сохраняются даже при сбое.
Пример: После подтверждения платежа он останется в БД, даже если сервер упадёт.

Короче: «Либо всё, либо ничего; только по правилам; без помех; навсегда».    <-----    <-----


### УРОВНИ ИЗОЛЯЦИИ ТРАНЗАКЦИЙ
Какие уровни изоляции транзакций вы знаете?

Read Uncommitted - возможны "грязные" чтения.
Read Committed   - только committed данные (стандарт в PostgreSQL).
Repeatable Read  - защита от "неповторяемого чтения".
Serializable     - полная изоляция (как последовательное выполнение).
"""


# 24) Какие уровни изоляции транзакций знаешь и какие аномалии они предотвращают?
"""
→ Уровни изоляции = какие “аномалии” допускаем при конкурентных транзакциях.


- Read Uncommitted – Видны даже незафиксированные изменения других транзакций (грязное чтение).
Классические аномалии:
- dirty read: читаем незакоммиченные изменения другой транзакции,
- non-repeatable read: повторное чтение той же строки даёт другое значение,
- phantom read: повторный запрос по условию возвращает другой набор строк,
- write skew / lost update: “тонкие” аномалии при одновременных проверках/записях.

Read Committed:
- предотвращает dirty reads,
- допускает non-repeatable reads и phantoms.
Практика: это дефолт во многих СУБД, хорошо для большинства CRUD, но требует осторожности при read-modify-write.

Repeatable Read:
- гарантирует “стабильный снимок” для повторных чтений в рамках транзакции,
- убирает non-repeatable reads,
- phantoms/аномалии зависят от реализации (в терминах стандарта — phantoms возможны, но в MVCC-системах нюанснее).
Важно про PostgreSQL:
- в Postgres Repeatable Read = Snapshot Isolation (SI): читаем консистентный snapshot,
  но возможен write skew (классический кейс SI).

Serializable:
- поведение эквивалентно некоторому последовательному выполнению транзакций,
- предотвращает write skew/phantoms и прочие аномалии,
- цена: больше конфликтов и откатов → serialization failures, которые приложение обязано уметь ретраить.

Важно на интервью (практика):
- длинные транзакции опасны (локи/пул/ MVCC bloat),
- deadlock и serialization failure — “нормальные” события под конкуренцией,
- ретраи должны быть ограниченными и безопасными (идемпотентность/уникальные ключи).
"""

# 24.1) Follow-up: Что такое MVCC и какие у него практические последствия?
"""
→ MVCC (Multi-Version Concurrency Control): при изменениях создаются версии строк,
а чтения видят snapshot, поэтому чтения обычно не блокируют записи.

Практические последствия (Postgres-подобные):
- накапливаются “мертвые версии” (dead tuples) → нужен VACUUM/autovacuum,
- долгие транзакции удерживают старые версии → vacuum хуже работает, таблицы/индексы “раздуваются” (bloat),
- важно следить за: long-running transactions, autovacuum, bloat, transaction ID wraparound,
- многие конфликты проявляются не как “грязные чтения”, а как конфликты записи/serialization failure.
"""

# 24.2) Follow-up: Что такое deadlock и как его чинить/предотвращать?
"""
→ Deadlock — цикл ожиданий: транзакции держат ресурсы и ждут друг друга.

Диагностика:
- логи БД (deadlock detected),
- анализ блокировок (pg_locks/lock graphs), поиск паттерна запросов.

Профилактика:
- одинаковый порядок захвата ресурсов (таблиц/строк) во всех путях,
- уменьшение scope/времени транзакции,
- правильные индексы (меньше “лишних” блокировок),
- избегать пользовательских “ручных” lock’ов без нужды,
- ретраи на deadlock detected (ограниченно, с backoff/jitter).
"""

# 24.3) Follow-up: Что такое serialization failure и как правильно ретраить?
"""
→ При Serializable (и в некоторых реализациях под высокой конкуренцией) БД может отклонить транзакцию,
чтобы сохранить сериализуемость.

Правильный retry:
- ретраим весь транзакционный блок целиком (а не куски),
- ограниченное число попыток,
- backoff + jitter при конкуренции,
- операции внутри должны быть идемпотентны или защищены уникальными constraint’ами/upsert,
- логируем/метрим факт ретраев как сигнал конкуренции.
"""

# 24.4) Follow-up: Почему “transaction per request” может быть плохой идеей?
"""
→ Транзакция на весь HTTP запрос часто становится “длинной”:
- удерживает соединение из пула,
- удерживает локи/версии строк (MVCC),
- растёт bloat, ухудшается vacuum,
- растут p95/p99 и вероятность дедлоков.

Обычно: транзакция охватывает минимальный атомарный бизнес-кусок (use-case),
а внешние вызовы — вне транзакции (с outbox/сагами при необходимости).

Bloat — это “раздувание” таблиц/индексов: на диске копится мёртвое/устаревшее место (старые версии строк, удалённые записи),
из-за чего растёт размер и падает производительность.
"""

# 24.5) Follow-up: Потерянное обновление (lost update) — что это и как защищаться?
"""
→ Lost update: две транзакции читают одно значение, обе пишут “поверх” друг друга, и одно обновление теряется.

Защита:
- SELECT ... FOR UPDATE (пессимистическая блокировка) для read-modify-write,
- optimistic locking (version column / compare-and-swap),
- атомарные UPDATE (update balance = balance - x where ...) с проверками,
- Serializable + retry (но не всегда удобно/дёшево).
"""


# 25) Когда индексы вредят и как понять, что индекс нужен?
"""
→ Индексы — компромисс: ускоряют чтение, но удорожают запись и занимают место.

Когда вредят:
- частые INSERT/UPDATE/DELETE: каждый индекс надо обновлять → падает write throughput,
- рост размера индексов → больше IO/кэша, выше стоимость vacuum,
- плохие/лишние индексы могут ухудшать планы (planner выбирает неудачный путь) и увеличивать maintenance.

Как понять, что индекс нужен:
- есть конкретные “дорогие” запросы (slow log / tracing),
- EXPLAIN (ANALYZE, BUFFERS) показывает seq scan / высокие cost / много buffer reads,
- есть предикат/джойн, который реально селективен,
- проверяем кардинальность/селективность, статистику,
- подтверждаем эффект на нагрузке/профиле.

На интервью: “индекс ставлю под запрос, проверяю EXPLAIN/ANALYZE, измеряю эффект”.
"""

# 25.1) Follow-up: Почему индекс может не использоваться, даже если он есть?
"""
→ Причины:
- низкая селективность (seq scan дешевле),
- условие оборачивает колонку (функции/касты) → нужен expression index,
- несовпадение “leading columns” у составного индекса (индекс (a,b), фильтр только по b),
- устаревшая статистика → planner ошибся (нужен ANALYZE),
- маленькая таблица — seq scan дешевле,
- параметризация запроса/plan caching может приводить к generic plan.
"""

# 25.2) Follow-up: B-tree vs GIN/GiST — когда какой?
"""
→ (PostgreSQL-ориентировано)
- B-tree: равенство/диапазоны, сортировки, джойны, большинство стандартных кейсов.
- GIN: массивы/JSONB/полнотекст, “membership/containment” (`@>`, `?`), быстрый поиск по вхождениям.
- GiST: геоданные, range types, некоторые “похожести”/поиск ближайших (в зависимости от расширений).

Интервью-уровень: “B-tree стандарт, GIN для jsonb/fts/arrays, GiST для geo/range”.
"""

# 25.3) Follow-up: Что такое covering index / index-only scan и когда он помогает?
"""
→ Если все нужные поля запроса содержатся в индексе (в Postgres: INCLUDE),
БД может сделать index-only scan и почти не ходить в heap → быстрее и меньше IO.

Нюанс: эффективность зависит от visibility map (актуальности vacuum).
Если VM неактуален, всё равно будут heap fetches.
"""

# 25.4) Follow-up: Как безопасно добавить индекс на большой таблице в проде?
"""
→ PostgreSQL:
- CREATE INDEX CONCURRENTLY (меньше блокировок записи, но дольше и требует ресурсов),
- мониторить IO/нагрузку, делать в “тихие” часы,
- иметь план отката (DROP INDEX CONCURRENTLY),
- учитывать, что CONCURRENTLY нельзя внутри транзакции.

Ключ: “не блокирую прод на создание индекса”.
"""

# 25.5) Follow-up: Что ещё кроме индекса часто лечит “медленный запрос”?
"""
→ Часто проблема не в отсутствии индекса:
- переписать запрос (убрать N+1, лишние JOIN, подзапросы),
- добавить LIMIT/pagination, убрать SELECT *,
- исправить кардинальность/статистику (ANALYZE),
- проверить план на parameter sniffing/generic plan,
- денормализация/материализованный view (редко, но бывает),
- кэширование на уровне приложения (если допустимо по свежести).
"""


# 26) Что такое N+1 и как его находить/фиксить в ORM?
"""
→ N+1:
- 1 запрос на список сущностей,
- затем ORM лениво догружает связь для каждой сущности отдельным запросом (ещё N).
Итог: рост latency и нагрузки, особенно на больших списках.

Как находить:
- логирование SQL (dev/test),
- метрика “queries per request/endpoint” + алерты,
- трассировка: видно множество одинаковых DB spans,
- профилирование: время в DB и количество запросов.

Как фиксить:
- eager loading: selectinload/prefetch (обычно безопаснее) или joined load (JOIN),
- проекции (вытащить только нужные поля),
- батчинг/чанки + pagination,
- пересмотр модели доступа (иногда лучше отдельный запрос агрегатом),
- кеширование/денормализация для hot-path (если оправдано).

На интервью: “вижу по логам/трейсам и выбираю selectin vs join осознанно”.

Шпаргалка (SQLAlchemy ↔ Django ORM):
- joinedload()   ≈ select_related()     # JOIN
- selectinload() ≈ prefetch_related()   # отдельные запросы + merge
"""

# 26.1) Follow-up: Когда JOIN eager loading может сделать хуже?
"""
→ JOIN может “раздуть” результат (row explosion) из-за one-to-many/many-to-many:
- больше трафика и памяти,
- сложнее DISTINCT/агрегации,
- иногда хуже план.

В таких случаях selectin (2–3 запроса) часто быстрее и предсказуемее.
"""

# 26.2) Follow-up: Как не убить БД при догрузке связей для 10k объектов?
"""
→ Практики:
- pagination/лимиты на выдачу,
- чанки по id (batch size),
- вытягивать только нужные поля,
- контролировать размер ответа,
- ограничить concurrency на endpoint,
- метрика query count + защита от “случайно вернули 10k”.
"""

# 26.3) Follow-up: Как тестами/инструментами предотвратить регрессию N+1?
"""
→ Автоматизация:
- assert на число SQL-запросов в интеграционном тесте для критичного endpoint’а,
- включить SQL logging и анализ в CI (точечно),
- метрика “queries per request” + алерт на рост,
- профилирование в staging под нагрузкой (до релиза).

Идея: N+1 ловим не только вручную, но и делаем regression guard.
"""

# 26.4) Follow-up: Почему N+1 иногда “не видно” на маленьких данных, но убивает прод?
"""
→ На dev данных N маленький → кажется “норм”.
В проде N растёт (сотни/тысячи) → запросов становится лавина:
- перегружается пул соединений,
- растёт p99,
- растёт нагрузка на БД.
Поэтому нужны лимиты/pagination и метрики query count.
"""



### TODO КЭШИРОВАНИЕ И РАСПРЕДЕЛЁННЫЕ СИСТЕМЫ

# 27) Что такое cache stampede и как от него защищаться?
"""
→ Cache stampede (dogpile effect) — ситуация, когда популярный ключ истекает (или массово инвалидируется),
и множество запросов одновременно идут в origin (БД/сервис) пересчитывать значение. Итог:
- пик нагрузки на БД/зависимость,
- рост p95/p99,
- возможные timeouts и каскадные отказы.

Базовые техники защиты:
1) TTL jitter (рандомизация TTL)
   - чтобы ключи не истекали одновременно и не создавали “волны” промахов.
2) Soft TTL / stale-while-revalidate
   - некоторое время отдаём “устаревшее” значение, параллельно один воркер обновляет кэш.
3) Lock / singleflight
   - при промахе только один поток/процесс делает пересчёт, остальные ждут или получают stale.
4) Prefill / warming
   - прогрев кэша при деплое/старте или планово по расписанию.
5) Backpressure и лимиты
   - ограничить количество одновременных пересчётов (Semaphore/queue), чтобы не утопить БД.

На интервью важно проговорить trade-off:
- stale данные vs свежесть,
- ожидание vs быстрый ответ,
- сложность реализации vs выигрыш по устойчивости.

TTL (Time To Live) — это время жизни данных (обычно в кэше): сколько ключ хранится, после чего истекает и удаляется/пересчитывается.
"""

# 27.1) Follow-up: Как выбрать TTL и почему “одинаковый TTL всем ключам” — плохая идея?
"""
→ TTL выбирают от требований свежести и нагрузки на origin. “Одинаковый TTL всем” часто приводит к синхронным истечениям.
Решение: jitter (например ±10–30%), разные TTL для разных классов данных.
Ожидаемая мысль: “TTL — это про баланс свежести и стоимости пересчёта”.

Jitter — это случайное небольшое отклонение (разброс) параметра, например TTL, чтобы ключи истекали не одновременно.
"""

# 27.2) Follow-up: Cache-aside vs write-through vs write-behind — в чём разница?
"""
→ Cache-aside: приложение читает из кэша, при промахе читает из БД и кладёт в кэш. Простой, но нужен контроль stampede.
Write-through: запись идёт в кэш и БД синхронно. Чтения быстрые, но запись дороже.
Write-behind (write-back): запись сначала в кэш, потом асинхронно в БД. Быстро, но сложнее (риски потерь, консистентность).
На интервью часто ждут: “в проде чаще cache-aside, остальное — по необходимости”.
"""

# 27.3) Follow-up: Как измерять эффективность кэша и что смотреть в метриках?
"""
→ Основные метрики:
- hit rate / miss rate,
- latency кэша vs latency origin,
- eviction rate / memory usage,
- “stale served” (если есть),
- нагрузка на origin (QPS, CPU, pool saturation) и корреляция с misses.
Важно: высокий hit rate не всегда хорошо, если данные stale или кэш огромный/дорогой.
"""

# 27.4) Follow-up: Какие типичные баги с кэшем встречаются в проде?
"""
→ Частые проблемы:
- “вечные” ключи без TTL → устаревание + memory leak,
- некорректная инвалидация (данные не обновляются),
- кэширование ошибок/пустых результатов без осторожности,
- слишком крупные значения (перегруз сети/Redis),
- stampede на популярные ключи,
- несогласованные ключи (разный формат ключей приводит к miss storm).
Ожидаемая мысль: “инвалидация и размер — главные источники боли”.
"""

# 27.5) Follow-up: Как делать инвалидацию кэша при изменениях в БД?
"""
→ Подходы:
- TTL-based (самый простой, но не мгновенный),
- explicit invalidation по событию (publish event → удалить/обновить ключ),
- versioned keys (ключ включает версию/etag; смена версии = естественная инвалидация),
- write-through для некоторых данных.
Выбор зависит от требований к свежести и сложности системы.
"""


# 28) Что такое идемпотентность и как её обеспечить в API/воркерах?
"""
→ Идемпотентность означает: повтор одного и того же запроса/сообщения не приводит к повторному побочному эффекту.
Это критично, потому что в распределённых системах:
- ретраи неизбежны (таймауты/сбои сети),
- брокеры часто дают at-least-once доставку (дубли).

Как обеспечить в HTTP API:
- Idempotency-Key (клиент присылает ключ, сервер хранит результат/статус по ключу с TTL),
- уникальные ограничения в БД (например, уникальный order_id/operation_id) + upsert,
- хранение “журнала” выполненных операций (operation log/state machine).

Как обеспечить в воркерах/консьюмерах:
- dedup по message_id/correlation_id (таблица processed_messages),
- транзакционный guard: “пометить обработанным” + “записать результат” атомарно,
- идемпотентные операции (upsert, compare-and-set, versioning).

На интервью важно: “ретраи и дубли — норма, поэтому идемпотентность проектируется заранее”.
"""

# 28.1) Follow-up: Чем идемпотентность отличается от “безопасного повторного чтения”?
"""
→ Идемпотентность про побочный эффект: повтор не меняет состояние повторно.
GET обычно “безопасен”, но POST/PUT/DELETE тоже могут быть идемпотентными, если правильно спроектировать ключи/операции.
На интервью любят уточнение: “HTTP семантика ≠ гарантия идемпотентности реализации”.
"""

# 28.2) Follow-up: Как реализовать Idempotency-Key: что хранить и как долго?
"""
→ Обычно храним:
- ключ операции,
- статус (in_progress/succeeded/failed),
- результат (response payload или ссылка на ресурс),
- TTL (чтобы не хранить бесконечно).
Повторный запрос с тем же ключом возвращает тот же результат или “в обработке”.
Важно: защита от гонок (лок/уникальный индекс по ключу).
"""

# 28.3) Follow-up: Как сделать идемпотентным создание ресурса (например, “создать заказ”)?
"""
→ Варианты:
- клиент задаёт внешний request_id/order_id (уникальный) → сервер делает upsert,
- или сервер принимает idempotency key и маппит на созданный order_id.
Главное: уникальное ограничение в БД + транзакция, чтобы два параллельных запроса не создали два заказа.
"""

# 28.4) Follow-up: Где лучше хранить дедуп: Redis или БД?
"""
→ Redis:
- быстро, удобно TTL, но нужно учитывать персистентность и возможность потерь (если критично).
БД:
- надёжнее для строгой уникальности и “истины”, но дороже по latency.
Часто: Redis как быстрый слой + БД как окончательная защита уникальным индексом (или только БД для критичных операций).
"""

# 28.5) Follow-up: Какие операции нельзя безопасно ретраить без дополнительных мер?
"""
→ Неидемпотентные сайд-эффекты: списание денег, отправка письма, создание записи без уникального ключа.
Нужны: idempotency key, уникальные ограничения, outbox/саги, подтверждения провайдера.
Ожидаемая мысль: “ретраи без идемпотентности = дубли”.
"""


# 29) Что значит exactly-once и почему в реальности чаще делают at-least-once?
"""
→ Exactly-once в практическом смысле: “каждое сообщение приводит к эффекту ровно один раз” (без дублей и пропусков),
включая запись в БД и любые внешние сайд-эффекты.

Почему сложно:
- сеть ненадёжна, ack/commit могут потеряться,
- процесс может упасть после выполнения эффекта, но до коммита оффсета/ack,
- чтобы гарантировать exactly-once сквозь брокер + БД, нужен координированный коммит (транзакции, EOS),
  что дорого и усложняет систему.

Поэтому типичный промышленный подход:
- выбираем at-least-once доставку (возможны дубли),
- делаем обработчик идемпотентным (dedup, upsert, уникальные ключи),
- используем outbox/inbox паттерны, чтобы избежать dual-write проблем.

На интервью важно: “exactly-once часто достигается как ‘effectively-once’ за счёт идемпотентности”.
"""

# 29.1) Follow-up: Что важнее: не потерять событие или не допустить дубль? Почему?
"""
→ В большинстве бизнес-сценариев потеря события хуже, чем дубль, потому что потеря = нарушение целостности данных.
Дубли решаем идемпотентностью, а потери обычно сложнее обнаружить и восстановить.
Поэтому и выбирают at-least-once + dedup.
"""

# 29.2) Follow-up: Где появляется дубль в Kafka/RabbitMQ типично?
"""
→ Классический сценарий:
- консьюмер обработал сообщение (сделал запись в БД),
- упал до коммита оффсета/ack,
- брокер считает, что сообщение не обработано → доставляет повторно.
Решение: идемпотентный consumer (dedup/unique constraints).
"""

# 29.3) Follow-up: Как сделать “effectively exactly-once” для записи в БД?
"""
→ Техники:
- уникальный ключ операции (message_id/operation_id) + upsert/insert on conflict do nothing,
- таблица processed_messages с уникальным индексом,
- транзакция: “пометить обработано + записать бизнес-изменение” атомарно.
Это самый частый “реальный” ответ на интервью.
"""

# 29.4) Follow-up: Что такое inbox/outbox и как они связаны с exactly-once?
"""
→ Outbox: гарантирует, что событие будет опубликовано, если бизнес-изменение закоммичено (решает dual-write).
Inbox: хранит входящие message_id и защищает от повторной обработки (dedup).
Вместе дают “эффективно один раз” на уровне эффекта.
"""

# 29.5) Follow-up: Почему “exactly-once” в брокере не гарантирует exactly-once “в бизнесе”?
"""
→ Потому что эффект часто включает внешние системы: платежи, email, сторонние API.
Даже если брокер гарантирует EOS на уровне записи/чтения, сайд-эффекты вне транзакции могут дублироваться.
Поэтому всё равно нужна идемпотентность и дизайн операций с ключами/статусами.
"""



### TODO ТЕСТИРОВАНИЕ

# 30) Объясни “пирамиду тестов” и как ты применяешь её на проекте.
"""
→ Пирамида тестов — это баланс скорости/стоимости/надёжности:

1) Unit-тесты (основание пирамиды)
- много, быстрые, стабильные;
- проверяют бизнес-логику, чистые функции, доменные инварианты;
- почти без внешних зависимостей (БД/сеть), часто с фейками/стабами.

2) Интеграционные тесты (середина)
- меньше, медленнее;
- проверяют интеграции: БД (SQL/транзакции/миграции), брокер, кэш, внешние API через контракт/стабы;
- ловят ошибки схемы, запросов, сериализации, конфигурации.

3) E2E (вершина)
- немного, самые дорогие и хрупкие;
- проверяют критические пользовательские сценарии “как в проде”;
- полезны как smoke/regression на самые важные флоу, но не должны покрывать всё.

Как применяю:
- максимум логики держу в unit, чтобы получать быстрый фидбек,
- интеграционные пишу на самые рискованные места (SQL, транзакции, очереди, авторизация, схемы),
- e2e — только на ключевые цепочки (например, “создать заказ → оплатить → получить статус”).
В CI важно: быстрый этап (unit+часть интеграционных) и более тяжёлый (nightly/по тегу) для e2e/нагрузочных.
"""

# 30.1) Follow-up: Что мокать нельзя (или нежелательно)?
"""
→ Хорошее правило: не мокать то, что вы хотите проверить.
Если цель — проверить SQL/транзакции/поведение ORM, мок БД бессмысленен.
Часто не мокают:
- слой репозитория целиком (лучше реальная БД в контейнере),
- сериализацию/контракты API,
- критические интеграции (очередь/кэш) — хотя бы контрактно.

Мокаются обычно “чужие” внешние системы и нестабильные части:
- сторонние HTTP API (через stub server/recording),
- время/рандом/UUID (для детерминизма),
- “дорогие” операции, если они не предмет теста.
"""

# 30.2) Follow-up: Как сделать интеграционные тесты стабильными, а не flaky?
"""
→ Основные практики:
- изоляция данных: отдельная БД/схема на прогон или транзакции + rollback,
- детерминизм: никаких sleep “на авось”, фиксированные фикстуры,
- тестовые контейнеры (docker) с зафиксированными версиями Postgres/Redis,
- корректное ожидание асинхронных событий (polling с таймаутом, а не sleep),
- чистка ресурсов (соединения, очереди, файлы) после теста.

На интервью любят услышать: “flaky тесты убивают доверие, я лечу источник недетерминизма”.

Flaky (тест) — это тест, который то проходит, то падает без изменения кода, обычно из-за недетерминизма
(гонки, тайминги, зависимость от окружения/сети/порядка выполнения).
"""

# 30.3) Follow-up: Как тестировать миграции БД?
"""
→ Минимум:
- прогон миграций на чистой базе (upgrade),
- проверка, что приложение работает с новой схемой.
Для более строгих требований:
- upgrade со “старой” схемы на “новую” с тестовыми данными,
- backward compatibility (если нужен период dual-read/dual-write),
- проверка индексов/constraints (что реально создалось).

Важно: миграции должны быть безопасны для прод-объёма (CONCURRENTLY, avoid долгих locks).
"""

# 30.4) Follow-up: Как покрывать тестами “distributed” сценарии (ретраи, дубли, идемпотентность)?
"""
→ Явно моделировать:
- повторную доставку сообщения,
- падение между сайд-эффектом и ack/commit,
- повторные HTTP запросы с одним idempotency key,
- частичные отказы внешних зависимостей.
Проверять, что результат:
- не дублируется (dedup/unique constraints),
- корректно ретраится (backoff/лимиты),
- не оставляет “полусостояния”.
"""

# 30.5) Follow-up: Как измерять качество тестов, кроме “coverage %”?
"""
→ Coverage полезен, но не цель.
Смотрю на:
- покрытие критичных инвариантов и сценариев,
- количество регрессий, пойманных до прода,
- скорость CI и стабильность (flaky rate),
- “mutation testing” точечно (если нужно),
- наличие контрактных тестов на API/события.
Ожидаемая мысль: “важно покрытие рисков, а не проценты”.
"""


# 31) Как тестировать async-код и что важно проверить кроме “оно работает”?
"""
→ Async тестирую так же системно, как sync, но добавляю проверки конкурентности и отмены.

Инструменты:
- pytest-asyncio или anyio (в зависимости от стека),
- async фикстуры для loop/клиентов,
- моки async методов (AsyncMock), stub-серверы для HTTP.

Что проверяю сверх “happy path”:
1) Таймауты и дедлайны
- что запрос не висит бесконечно, правильно отрабатывает timeout.

2) Cancellation (отмена задач)
- задача корректно освобождает ресурсы (finally),
- CancelledError не “глотается” случайно.

3) Лимиты параллелизма
- семафоры/пулы работают, нет взрывного количества задач,
- нет превышения лимитов БД/HTTP (pool exhaustion).

4) Отсутствие блокирующих вызовов в loop
- не используем requests/долгий CPU в async-части,
- если нужно — executor/процессы.

5) Закрытие ресурсов
- сессии/коннекты закрываются (async with), нет утечек.
"""

# 31.1) Follow-up: Как детерминированно тестировать конкурентность и избегать sleep()?
"""
→ Избегаю “sleep для синхронизации” — это источник флапов.
Использую:
- asyncio.Event/Condition/Semaphore как точки синхронизации,
- ограничение concurrency, чтобы воспроизвести гонки,
- polling с таймаутом вместо sleep, если ждём внешний эффект,
- фиктивные “барьеры” в тестовых фейках, чтобы управлять порядком шагов.
"""

# 31.2) Follow-up: Как тестировать ретраи/backoff в async без замедления тестов?
"""
→ Подменяю sleep/таймер:
- dependency injection “sleeper” функции,
- фейковые часы (fake clock),
- конфиг: backoff=0 в тестах, но проверяю количество попыток и условия ретрая.
Цель: тест быстрый, но логика ретрая проверена.
"""

# 31.3) Follow-up: Как проверить, что код не блокирует event loop?
"""
→ Практики:
- включить debug режима loop (может логировать долгие callbacks),
- добавить “watchdog” таймауты на тест,
- профилировать/логировать длительные участки,
- в тесте запускать параллельную корутину и проверять, что она получает управление (не залипает).
На интервью ожидают: “я ловлю блокировки loop через таймауты и диагностику”.
"""

# 31.4) Follow-up: Как мокать async зависимости правильно?
"""
→ Использую AsyncMock для awaitable методов, либо реальные stub-сервера (например, aiohttp test server).
Важно: мок должен быть awaitable, иначе тест пройдёт “не так”, как в проде.
Также важно проверять, что await был сделан (assert_awaited).
"""

# 31.5) Follow-up: Что важно в тестах на graceful shutdown async-сервиса/воркера?
"""
→ Проверить:
- прекращение приёма новых задач,
- корректная отмена in-flight задач,
- закрытие пулов/сессий,
- корректные ack/commit (если consumer),
- отсутствие подвисаний на shutdown.
Это частый “senior/staff” докап про эксплуатацию и устойчивость.
"""



### TODO DEVOPS / ЭКСПЛУАТАЦИЯ

# 32) Что обязательно должно быть в продакшен-сервисе (observability/надёжность)?
"""
→ На реальном проде “работает локально” не считается — нужен набор практик, которые дают управляемость и предсказуемость.

Observability:
- Структурные логи (JSON): уровень, message, trace_id/request_id, endpoint, latency, status, user_id (если можно), версия релиза.
- Метрики: RPS/throughput, latency p50/p95/p99, error rate, saturation (CPU/mem), очереди/лаги, pool saturation (DB/HTTP).
- Трассировка (OpenTelemetry): спаны по входящему запросу, БД, внешним API, очередям — чтобы видеть “куда ушло время”.

Надёжность и защита:
- Таймауты на ВСЕ внешние вызовы + таймаут ожидания пула (иначе запросы “залипают”).
- Ретраи только для transient ошибок, с backoff+jitter и лимитами, с учётом идемпотентности.
- Circuit breaker/bulkhead, чтобы деградация зависимости не утопила сервис.
- Backpressure: лимиты concurrency, bounded queues, rate limiting, 429/503 при перегрузе.
- Graceful shutdown: stop accepting, drain in-flight, закрыть пулы/сессии, корректно ack/commit в воркерах.

Операционка:
- Healthchecks: liveness/readiness (readiness учитывает зависимость от БД/очереди/пула).
- Конфигурация через env/секреты, строгая разделённость конфигов по окружениям.
- Миграции БД безопасным способом (expand/contract, CONCURRENTLY), “backward compatible” релизы.
- Безопасные деплои: canary/blue-green + быстрый rollback, наблюдаемость в момент выката.

На интервью обычно ждут: “golden signals, timeouts everywhere, bounded retries, graceful shutdown, safe deploy”.
"""

# 32.1) Follow-up: Какие “golden signals” ты мониторишь и почему?
"""
→ Latency, Traffic, Errors, Saturation — потому что они быстро показывают:
- деградацию UX (latency),
- нагрузку (traffic),
- качество (errors),
- приближение к лимитам ресурсов (saturation: CPU, память, пулы, очередь, FD).
Плюс бизнес-метрики (заказы/конверсия), чтобы видеть реальный impact.
"""

# 32.2) Follow-up: Как строить алерты, чтобы не было шума?
"""
→ Aлертить по SLO и трендам, а не по “каждой ошибке”:
- error budget / burn rate (быстро сгорает бюджет — поднимаем критический алерт),
- разделять warning/critical,
- использовать окна времени и rate-based алерты (например, 5xx rate > X%),
- добавить алерты на saturation (пул БД почти пуст, очередь растёт).
Ожидаемая мысль: “алерты должны быть actionable”.
"""

# 32.3) Follow-up: Чем отличаются liveness и readiness? Что именно проверять?
"""
→ Liveness: “процесс жив” (не завис, не в deadlock). Обычно минимальная проверка.
Readiness: “готов обслуживать трафик” — учитывает критичные зависимости:
- можем получить соединение из пула БД,
- можем писать/читать из брокера/кэша (если критично),
- не в состоянии деградации, когда сервис должен быть выведен из балансера.
На интервью любят: “readiness не должна делать тяжёлые проверки, но должна отражать способность обслуживать запросы”.
"""

# 32.4) Follow-up: Что такое graceful shutdown и что нужно успеть сделать за termination grace period?
"""
→ Правильный shutdown:
1) перестать принимать новые запросы (выключить readiness),
2) дождаться завершения in-flight (с deadline),
3) закрыть соединения/пулы (DB, HTTP clients), корректно остановить background tasks,
4) для воркеров: завершить текущую задачу или корректно requeue, сделать ack/commit там, где нужно.
Иначе: потеря задач, дубли, зависшие соединения.
"""

# 32.5) Follow-up: Как прокидывать таймауты “сквозь” сервисы (deadline propagation)?
"""
→ Входящий запрос получает общий deadline (например, header или внутренний контекст),
дальше каждый downstream вызов использует таймаут меньше общего (оставляем запас).
Это защищает от “висящих хвостов” и уменьшает каскадные отказы.
Ключевые слова: deadline, per-hop timeout, budget.
"""


# 33) Как тюнинговать gunicorn/uvicorn/celery: на что смотришь в первую очередь?
"""
→ Я начинаю не с магических чисел, а с профиля нагрузки и ограничений окружения.

Шаг 1: понять природу нагрузки
- CPU-bound: упираемся в CPU → важны процессы/воркеры, оптимизация кода, возможно вынести тяжёлое.
- I/O-bound: упираемся в ожидание → важны concurrency, пулы соединений, таймауты, лимиты.
Смотрю метрики: CPU%, RSS, p95/p99, error rate, pool saturation, lag/queue depth.

gunicorn/uvicorn:
- Модель воркеров: sync vs async (uvicorn workers) и соответствующая стратегия.
- Кол-во процессов/воркеров: обычно по ядрам и по памяти (и по реальному latency под нагрузкой).
- Таймауты: request timeout, keep-alive, graceful timeout.
- Лимиты: max requests (перезапуск воркера для борьбы с утечками), limit request size.

celery:
- concurrency (prefork/threads/gevent — выбираем осознанно),
- prefetch (fairness vs throughput),
- acks_late (надёжность vs дубли),
- visibility_timeout/late ack связка (чтобы задачи не “пропадали” и не дублировались бесконечно),
- retries/backoff, DLQ (poison messages),
- time limits (soft/hard), rate limits, dead letter/requeue политика.

Шаг 2: подтвердить тюнинг
- нагрузочным тестом + сравнением метрик до/после,
- canary rollout для опасных изменений.
"""

# 33.1) Follow-up: Как выбрать количество воркеров gunicorn? Есть ли “формула”?
"""
→ “Формула 2*cores+1” — грубый ориентир для некоторых sync нагрузок, но в реальности:
- ограничение может быть по памяти (каждый воркер потребляет RSS),
- по пулу БД (слишком много воркеров = слишком много соединений),
- по I/O (async воркер может обслуживать больше соединений).
Правильный ответ на интервью: “подбираю по нагрузочному тесту, смотрю saturation и хвосты latency”.
"""

# 33.2) Follow-up: Что такое pool saturation и как он ломает сервис?
"""
→ Если пул БД/HTTP исчерпан, запросы начинают ждать соединение.
Без таймаута ожидания пула это превращается в “залипание”, рост очереди и p99.
В тюнинге важно согласовать:
- количество воркеров/конкурентность,
- размеры пулов,
- таймауты на ожидание пула.
"""

# 33.3) Follow-up: Celery prefetch — почему большой prefetch может быть проблемой?
"""
→ Большой prefetch = воркер “забирает” много задач заранее:
- ухудшается fairness (другие воркеры простаивают),
- при падении воркера возможны массовые redelivery/дубли,
- увеличивается время ожидания задач в памяти воркера.
Обычно на интервью ждут: “уменьшаю prefetch для долгих/разнородных задач”.
"""

# 33.4) Follow-up: `acks_late`: когда включать и какие риски появляются?
"""
→ acks_late = ack после выполнения:
+ меньше риск потери задач,
- больше дублей при падениях/таймаутах.
Поэтому обязательно: идемпотентность обработчика + корректные time limits + visibility_timeout.
Ожидаемая мысль: “acks_late повышает надёжность, но требует идемпотентности”.
"""

# 33.5) Follow-up: Как бороться с memory leak/фрагментацией в воркерах?
"""
→ Подходы:
- max_requests / max_tasks_per_child (перезапуск воркера после N запросов/задач),
- профилирование памяти и фиксы (кеши без TTL, глобальные коллекции),
- ограничение размера батчей/пейлоадов,
- контроль сторонних библиотек (особенно для обработки файлов/изображений),
- мониторинг RSS + алерты.
На интервью ожидают: “у меня есть стратегия и метрики, а не ‘перезапускать по крону’”.
"""



### TODO SYSTEM DESIGN

# 34) Спроектируй сервис сокращения ссылок: ключевые решения и риски.
"""
→ Я обычно проговариваю дизайн по слоям: API → генерация ключа → хранилище → кэш → редирект → аналитика → защита.

Функциональные требования:
- создать короткую ссылку (опционально кастомный alias),
- редирект по ключу с минимальной latency,
- опционально TTL/удаление,
- аналитика кликов (счётчик, referrer, UA, geo — по требованиям),
- защита от abuse (спам/фишинг, перебор ключей).

API:
- POST /links {url, ttl?, alias?} → {key, short_url}
- GET /{key} → 301/302 + Location
- админ/пользовательские эндпоинты для статистики/удаления (если нужно)
Важно: идемпотентность создания (Idempotency-Key), чтобы ретраи не создавали дубликаты.

Генерация ключа:
- base62 от sequence/snowflake, или крипто-рандом с проверкой уникальности.
Trade-offs:
- sequence/base62: проще уникальность, но нужно центральное выделение id (sequence) и риск “предсказуемости”.
- random: хорошая энтропия (сложнее перебор), но нужен retry на коллизии (обычно редкие при нормальной длине).
Для “защиты от перебора” чаще выбирают random/достаточную длину.

Хранилище:
- Таблица: key (PK), original_url, created_at, expires_at, owner_id, flags/status, metadata.
- Индексы: PK по key, при необходимости (owner_id, expires_at).
- TTL: через expires_at + периодический cleanup job (или partitioning по времени).

Кэш:
- cache-aside: по key → original_url.
- TTL в кэше обычно <= TTL ссылки, плюс jitter.
- защита от stampede: singleflight/lock или stale-while-revalidate.

Редирект:
- 301 (постоянный) vs 302 (временный) — зависит от требований к кешированию браузерами/прокси и возможности менять таргет.
- “горячий путь” должен быть максимально быстрым: кэш hit → редирект без похода в БД.

Аналитика:
- на горячем пути минимизируем работу: счётчики/события лучше отправлять async (в очередь/лог) и агрегировать отдельно.
- можно хранить простые counters в Redis и периодически сливать в БД.

Надёжность и безопасность:
- rate limiting на создание и на редирект (по IP/user),
- защита от фишинга/злоупотреблений: allowlist/denylist доменов, интеграция с safe browsing (если нужно),
- защита от enumeration: ключи с достаточной энтропией, WAF/антибот, 404 без лишней информации.
Наблюдаемость: p95/p99 редиректа, hit rate кэша, ошибки БД/кэша, доля 404, абьюз-метрики.
Горизонтальное масштабирование: stateless API + общий кэш/БД, шардирование при необходимости.
"""

# 34.1) Follow-up: Как выбрать длину ключа и оценить вероятность коллизий/перебора?
"""
→ Длина ключа — баланс между удобством (коротко) и энтропией (защита от перебора и коллизий).
Для random base62 энтропия растёт экспоненциально с длиной: увеличение на 1 символ даёт ×62 вариантов.
Практика:
- для публичного сервиса обычно выбирают длину, чтобы перебор был нерентабелен,
- и чтобы вероятность коллизии при ожидаемом объёме была пренебрежимо мала.
На интервью важно сказать: “random с достаточной энтропией + retry на коллизии”.
"""

# 34.2) Follow-up: 301 или 302? Что выберешь и почему?
"""
→ 301 кэшируется агрессивнее браузерами/прокси, хорошо для “постоянных” ссылок и снижения нагрузки,
но сложнее поменять таргет “на лету”.
302 менее кэшируемый, лучше если ссылка может менять назначение или нужна свежесть.
Часто: по умолчанию 302, а 301 — для “закреплённых” ссылок (по продуктовым требованиям).
"""

# 34.3) Follow-up: Как обеспечить идемпотентность создания короткой ссылки?
"""
→ Варианты:
- Idempotency-Key: сохраняем результат по ключу на время (Redis/БД) и возвращаем тот же key при ретрае.
- Дедуп по исходному URL + owner_id (если “один и тот же URL должен давать одну ссылку”), через уникальный индекс.
Важно: уникальность и защита от гонок обеспечиваются транзакцией и constraint’ами.
"""

# 34.4) Follow-up: Как сделать редирект “очень быстрым” на p99?
"""
→ Оптимизирую горячий путь:
- кэш hit должен быть основным сценарием,
- минимальные аллокации/логика в обработчике,
- keep-alive и пул соединений,
- защита от stampede,
- аналитика — асинхронно (не блокировать редирект),
- CDN/edge caching для популярных ключей (если применимо и безопасно).
Метрики: cache hit rate, p99, ошибки кэша/БД.
"""

# 34.5) Follow-up: Что делать при отказе кэша (Redis down)? Какой fallback?
"""
→ Должен быть деградирующий режим:
- читаем из БД напрямую (с rate limiting/ограничением concurrency),
- включаем более агрессивные локальные кеши/negative caching (осторожно),
- повышаем TTL на edge (если допустимо),
- обязательно: алерты и защита от перегруза БД.
Ключ: не допустить каскадного отказа.
"""

# 34.6) Follow-up: Как организовать TTL/удаление ссылок на масштабе?
"""
→ expires_at + индекс по expires_at, периодический job, который удаляет батчами.
Для очень больших объёмов: partitioning по времени и удаление партиций быстрее.
Важно избегать массовых delete в пик, делать батчи и мониторить vacuum/bloat.
"""


# 35) Как обеспечить надёжность очереди задач/воркеров в продакшене?
"""
→ Надёжность воркеров — это комбинация семантики доставки, идемпотентности, ретраев и наблюдаемости.

1) Семантика доставки
- чаще всего at-least-once (возможны дубли) → проектируем обработчик идемпотентным.
- at-most-once (без дублей) риск потерь — обычно хуже.
- exactly-once в бизнес-смысле достигается “effectively-once” через dedup/unique constraints/outbox/inbox.

2) Идемпотентность и дедуп
- unique operation/message id,
- таблица processed_messages или уникальный ключ в бизнес-таблице + upsert,
- транзакционный guard: записать эффект и отметить обработку атомарно.

3) Ретраи
- только transient ошибки (timeouts, network, временная недоступность),
- exponential backoff + jitter, max attempts, отдельная политика для permanent ошибок.
- важно: не ретраить бесконечно и не создавать retry storm.

4) Poison messages и DLQ
- ограничение ретраев,
- DLQ для проблемных сообщений,
- алерты, ручная/отложенная переобработка после фикса.

5) Таймауты и лимиты
- time limits на задачу (soft/hard),
- лимиты concurrency и prefetch (fairness),
- backpressure на продюсера (если возможно).

6) Эксплуатация
- graceful shutdown: корректно завершать in-flight и делать ack/commit в правильном месте,
- метрики: lag, throughput, retries, DLQ rate, processing time p95/p99, error rate,
- трассировка: корреляция message_id/trace_id.
"""

# 35.1) Follow-up: В каком месте делать ack/commit и какой trade-off?
"""
→ Если ack/commit ДО сайд-эффекта:
- меньше дублей,
- но можно потерять задачу при падении между commit и эффектом (эффект не выполнен, сообщение уже “съедено”).

Если ack/commit ПОСЛЕ эффекта (acks_late):
- меньше потерь,
- но больше дублей при падении до commit.
Поэтому чаще выбирают ack после эффекта + идемпотентность.
На интервью ключевая фраза: “duplicates are cheaper than losses”.
"""

# 35.2) Follow-up: Как обеспечить порядок обработки?
"""
→ Если порядок важен, нужна сериализация по ключу:
- партиционирование по entity_id (Kafka) и один consumer per partition,
- либо локи/очереди по ключу,
- либо state machine, которая не допускает “прыжков” состояния.
Ожидаемая мысль: “порядок = снижает параллелизм, это осознанный trade-off”.
"""

# 35.3) Follow-up: Как выбирать retry policy и отличать transient от permanent?
"""
→ Transient: timeouts, connection reset, 503, deadlock/serialization failure.
Permanent: validation/data errors, “not found”, бизнес-конфликт.
Policy:
- backoff + jitter,
- max retries,
- после исчерпания — DLQ.
Важно: логировать причину, считать метрики по типам ошибок.
"""

# 35.4) Follow-up: Что делать при росте лага очереди?
"""
→ Диагностика:
- вырос вход (producer), упала скорость обработки, или появились ошибки/ретраи.
Действия:
- увеличить количество консьюмеров/воркеров (в пределах партиций/ресурсов),
- оптимизировать обработчик (батчи, меньше I/O, кеш),
- уменьшить ретраи/исправить зависимость,
- включить backpressure/rate limit на продюсере,
- приоритизация задач (critical сначала).
На интервью ожидают: “сначала выясняю причину, потом масштабирую/оптимизирую”.
"""

# 35.5) Follow-up: Как сделать обработчик безопасным к повторной доставке и рестартам?
"""
→ Паттерны:
- идемпотентный state machine (операция имеет уникальный id и статус),
- upsert по уникальному ключу,
- processed_messages (unique index) + транзакция.
Плюс: все внешние вызовы либо идемпотентны (idempotency key), либо вынесены через outbox/компенсации.
"""

# 35.6) Follow-up: Как “приземлить” это на Celery/Kafka/Rabbit: какие настройки важны?
"""
→ Celery/Rabbit:
- acks_late, prefetch, visibility_timeout, time limits, retries/backoff, DLQ.
Kafka:
- commit strategy (manual/auto), max.poll.interval, session.timeout, handling rebalance,
- idempotent consumer через dedup и транзакции на записи в БД.
Главное: настройки должны соответствовать семантике обработки и идемпотентности.
"""




### TODO PYTHON OOP — SENIOR LEVEL (для собеседования) #########################
# Цель: уметь объяснить не только “что это”, но и trade-offs, контракты,
# дизайн-решения, анти-паттерны и продовые кейсы.
###############################################################################


# 0) Как бы ты описал “Senior OOP в Python” одним предложением?
"""
→ Senior-уровень = проектировать границы ответственности и контракты так,
чтобы код был расширяемым, тестируемым, наблюдаемым и безопасным к изменениям
(и понимать, как Python data model/дескрипторы/MRO реально работают).
"""


# 1) Attribute lookup (поиск атрибутов) в Python: порядок + где ломают голову?
"""
→ Для obj.x (упрощённо, но полезно на интервью):
1) type(obj).__dict__ ищем DATA-descriptor с именем x (has __set__ / __delete__)
   - если нашли → вызываем descriptor.__get__(obj, type(obj)) и возвращаем
2) obj.__dict__ (если есть)
3) type(obj).__dict__ ищем non-data descriptor / обычный атрибут
4) дальше по MRO (base classes)
5) если не нашли → __getattr__ (если определён)

Ключевые докапы:
- почему property “перебивает” obj.__dict__? → потому что data-descriptor
- почему методы “привязываются”? → function — дескриптор (__get__)

“Перебивает” obj.__dict__ ТОЛЬКО data-descriptor (у которого есть __set__ и/или __delete__).

Data-descriptor (например property) → ИМЕЕТ приоритет над obj.__dict__.                            <-------
Non-data descriptor (только __get__, например обычная функция-метод) → НЕ перебивает:
если в obj.__dict__ есть атрибут с тем же именем, он его перекроет.

data-descriptor (__get__ + __set__/__delete__) ПЕРЕБИВАЕТ obj.__dict__
non-data descriptor (только __get__) НЕ перебивает → obj.__dict__ может его перекрыть

### “Перебивает” obj.__dict__ только data-descriptor (у которого есть __set__ и/или __delete__).   <-------
### DATA-DESCRIPTOR - ПЕРЕБИВАЕТ!
class A:
    @property
    def x(self): return 1

a = A()
a.__dict__['x'] = 999
print(a.x)          # 1  (property перебил)

### NON-DATA-DESCRIPTOR - НЕ ПЕРЕБИВАЕТ!
class A:
    def f(self): return "method"

a = A()
a.__dict__['f'] = 999
print(a.f)       # 999 (obj.__dict__ перебил non-data descriptor)
"""



# 1.1) Follow-up: __getattr__ vs __getattribute__ — где опасность?
"""
→ __getattribute__ вызывается ВСЕГДА → легко сделать рекурсию/сломать introspection.
Использовать только если реально нужно (прокси/трассировка/виртуальные атрибуты).
Внутри всегда обращаться через object.__getattribute__(...).

__getattribute__ вызывается на любое обращение к атрибуту (всегда первым).
__getattr__ вызывается только если атрибут НЕ найден обычным способом
            (не нашли ни в instance __dict__, ни в классе/родителях, ни через дескрипторы).
            
class A:
    x = 1
    def __getattr__(self, name):
        return f"нет {name}"

a = A()
print(a.x)   # 1      (__getattr__ НЕ вызовется)
print(a.y)   # "нет y" (__getattr__ вызовется)


__getattribute__ вызывается на любое обращение к атрибуту (всегда первым).
__getattr__ вызывается только если атрибут НЕ найден обычным способом
            (не нашли ни в instance __dict__, ни в классе/родителях, ни через дескрипторы).
            
class A:
    x = 1
    def __getattr__(self, name):
        return f"нет {name}"

a = A()
print(a.x)   # 1      (__getattr__ НЕ вызовется)
print(a.y)   # "нет y" (__getattr__ вызовется)


__getattr__ — отдельный “фолбэк” только для чтения, фолбэка для записи нет
              (если запись нельзя/некуда — будет исключение или сработает дескриптор).
              
__setattr__ — “перехватчик” записи (как __getattribute__ для чтения), 
             но “фолбэка” как __getattr__ для записи НЕТ.
"""




# 2) Дескрипторы на уровне “ORM/Framework”: что нужно уметь объяснить?
"""
→ Дескрипторы — база для:
- property
- методов (binding self)
- @classmethod/@staticmethod
- ORM полей (lazy-load, validation, query building)

Senior must know:
- data vs non-data precedence
- __set_name__ (когда дескриптор узнаёт имя поля в owner)
- side effects: доступ к полю может сходить в БД (lazy-loading) → неожиданная стоимость
"""

# 2.1) Follow-up: __set_name__ — зачем?
"""
→ Позволяет дескриптору понять, под каким именем он присвоен в классе.
ORM/validation frameworks используют это, чтобы не писать имя поля руками.
"""


# 3) MRO + super(): кооперативное наследование на практике
"""
→ super() = “следующий по MRO”, не “родитель”.
Правильный стиль для миксинов:
- каждый класс вызывает super()
- сигнатуры совместимы (*args/**kwargs)
- классы не делают жёстких предположений о том, кто “выше/ниже”

Анти-паттерн:
- прямой вызов Parent.method(self) ломает цепочку (пропуски/двойные вызовы).
"""

# 3.1) Мини-пример “кооперативный __init__” (правильно)
"""
class Base:
    def __init__(self, *, config=None, **kwargs):
        super().__init__(**kwargs)
        self.config = config

class LoggingMixin:
    def __init__(self, *, logger=None, **kwargs):
        super().__init__(**kwargs)
        self.logger = logger

class Service(LoggingMixin, Base):
    def __init__(self, *, config=None, logger=None):
        super().__init__(config=config, logger=logger)
"""

# 3.2) Follow-up: “почему super() не работает” — типовые причины
"""
→ Кто-то в цепочке:
- не вызывает super()
- имеет несовместимую сигнатуру (не принимает **kwargs)
- делает ранний return/исключение до super()
"""


# 4) Инварианты/контракты в OOP: “класс хранит инвариант”
"""
→ Senior мыслит так:
- где инварианты? (валидность состояния)
- кто отвечает за поддержание инварианта? (конструктор/методы, а не внешний код)
- как сделать невозможным некорректное состояние? (валидация + сокрытие деталей)

Инструменты:
- методы вместо прямого доступа к полям
- property (валидация)
- фабрики/alternative constructors
- frozen value objects (immutability)
"""

# 4.1) Follow-up: “почему публичные поля иногда ок” (Pythonic ответ)
"""
→ Если это DTO/record без сложных инвариантов — публичные поля норм (dataclass).
Но доменная модель с правилами обычно требует методов/инвариантов.
"""


# 5) SOLID по-взрослому: не определения, а примеры и trade-offs
"""
S — SRP:
- один повод для изменения; если класс отвечает и за бизнес-логику, и за I/O, и за кеш — это запах

O — OCP:
- расширяем через новые реализации интерфейса/протокола, не правим старый код
- в Python часто через композицию + dependency injection

L — LSP:
- подкласс должен сохранять контракт базового
- важно: не “тип”, а поведение/предусловия/постусловия

I — ISP:
- лучше несколько маленьких протоколов, чем один “жирный” интерфейс

D — DIP:
- зависим от абстракций (Protocol/ABC), а не от конкретных реализаций
"""

# 5.1) LSP-ловушка (классика): “квадрат-не-прямоугольник”
"""
→ Любые ограничения в subclass, которые ломают ожидания base-кода:
- base обещает: set_width/set_height независимы
- subclass делает их связанными → ломает контракт
"""

# 5.2) Практический LSP пример (частый в проде)
"""
→ BaseRepo.save(entity) обещает “сохраняет и возвращает entity с id”
Subclass, который иногда не проставляет id/делает lazy, ломает контракт и валит сервисный слой.
Выход: фиксировать контракт (протокол) и тесты на контракт.
"""


# 6) Композиция vs наследование: как объяснить на senior уровне?
"""
→ Наследование “is-a”:
+ удобно для полиморфизма “is-a”
- жёсткая связанность, fragile base class, сложнее менять

Композиция  “has-a”: 
+ лучше для вариативного поведения, тестирования, DI
+ проще заменять зависимости
- надо проектировать интерфейсы/протоколы

Senior-аргумент:
- наследование — про ИЕРАРХИЮ контрактов
- композиция —   про СБОРКУ поведения
"""

# 6.1) Follow-up: Пример замены наследования на Strategy
"""
→ Вместо SubclassPerPaymentProvider:
- PaymentService принимает PaymentGateway (Protocol)
- разные гейтвеи подставляются как зависимости
"""


# 7) Protocol / ABC: как выбирать (и как продавать решение)
"""
→ ABC:
- enforce в runtime (нельзя инстанцировать без реализации)
- полезно, если вам реально нужна runtime-гарантия

Protocol:
- структурная типизация (mypy/pyright)
- “подходит любой, кто реализует методы”
- супер для интеграций/SDK: не заставляете пользователей наследоваться

Senior tip:
- публичные контракты часто Protocol
- внутренние “скелеты/шаблоны” — ABC
"""

# 7.1) Пример Protocol (duck typing + типизация)
"""
from typing import Protocol

class Clock(Protocol):
    def now(self) -> float: ...

class SystemClock:
    def now(self) -> float:
        import time
        return time.time()
"""


# 8) Value Objects vs Entities: как хранить “данные” правильно?
"""
→ Entity:
- есть identity (id), меняется во времени
- equality часто по id

Value Object:
- identity нет, важно значение
- обычно immutable, hashable
- equality по полям

Senior важно:
- не смешивать (иначе странные баги с dict/set и сравнениями)
"""

# 8.1) Follow-up: __eq__/__hash__ — правила
"""
→ Если объект mutable — обычно не делаем hashable.
Если immutable value object — можно __hash__ (или frozen dataclass).
Если определили __eq__ и не определили __hash__ — объект станет unhashable (часто правильно).
"""


# 9) dataclass на senior уровне: “где ножи”
"""
→ dataclass отлично для DTO/value objects, но знать ловушки:
- mutable defaults → только default_factory
- frozen=True: нельзя обычное присваивание (иногда нужен object.__setattr__)
- order=True: сравнение по порядку полей может быть неожиданным для домена
- slots=True: экономия памяти, но ограничения на динамику

Senior подход:
- dataclass для данных, “поведение/инварианты” — отдельные сервисы/методы
"""

# 9.1) Follow-up: Как сделать lazy/cached поле в frozen dataclass?
"""
→ Использовать cached_property (и хранить кэш вне объекта),
или аккуратно object.__setattr__ (но это ломает “чистую” иммутабельность по смыслу).
"""


# 10) __slots__ / память / производительность — как об этом говорить уверенно?
"""
→ __slots__:
+ меньше памяти (нет __dict__)
+ быстрее доступ к атрибутам (часто)
- меньше гибкости (нельзя новые атрибуты)
- careful с наследованием и weakref

Senior должен:
- знать, что выигрыш зависит от количества объектов
- понимать, что premature optimization — зло: измеряем
"""


# 11) Метаклассы и хуки класса: что реально спрашивают у senior?
"""
→ Часто спрашивают не “напиши metaclass”, а:
- зачем metaclass вообще нужен?
- какие есть альтернативы?

Полезные механизмы ДО metaclass:
- __init_subclass__ (хук при создании подкласса)
- decorators for classes
- descriptors + __set_name__
- Pydantic/attrs/dataclasses

Metaclass нужен редко:
- регистрация классов/валидация namespace при создании
- enforcement “в момент определения класса”
"""

# 11.1) Follow-up: __init_subclass__ — чем удобен?
"""
→ Позволяет базовому классу реагировать на создание наследников:
- регистрация плагинов
- проверка наличия атрибутов/конфигурации
- автодобавление поведения
Без metaclass’а.
"""


# 12) Магические методы как протоколы: “Pythonic OOP”
"""
→ Senior умеет проектировать объекты под протоколы:
- контекст: __enter__/__exit__
- итерация: __iter__/__next__
- контейнер: __contains__/__len__
- сериализация: __getstate__/__setstate__ (реже)
- callable: __call__

Плюс: интерфейсы становятся “естественными” для языка.
Минус: важно не превращать это в “магический ад”.
"""


# 13) Паттерны (без фанатизма): что реально уместно в Python
"""
Strategy:
- вариативное поведение через композицию

Adapter:
- оборачиваем чужое API под наш протокол

Decorator:
- добавляем поведение (логирование/метрики/кеш) без изменения объекта

Factory:
- альтернативные конструкторы (classmethod) и сборка графа зависимостей

Observer/Signals:
- событийная модель (осторожно с неявностью)

Repository (в DDD смысле):
- граница домена и хранения (тестируемость, изоляция инфраструктуры)
"""

# 13.1) Follow-up: Когда “паттерны” вредны?
"""
→ Когда:
- усложняют ради абстракции
- появляются раньше требований
- вводят лишние уровни indirection без выигрыша в тестируемости/изменяемости
"""


# 14) Dependency Injection (DI) по-питоновски
"""
→ DI в Python часто простой:
- зависимости передаются в __init__ (constructor injection)
- типизируем через Protocol
- в тестах подменяем фейками/стабами

Senior тезис:
- “dependency direction” важнее конкретного DI-фреймворка
"""

# 14.1) Мини-пример DI + тестируемость
"""
from typing import Protocol

class Notifier(Protocol):
    def send(self, msg: str) -> None: ...

class EmailNotifier:
    def send(self, msg: str) -> None:
        ...

class Service:
    def __init__(self, notifier: Notifier):
        self.notifier = notifier

    def do(self):
        self.notifier.send("done")
"""


# 15) Ошибки и анти-паттерны, которые senior обязан видеть
"""
- mutable class attributes (общие списки/кэши без намерения)
- “god object / manager” (слишком много обязанностей)
- inheritance for reuse (вместо композиции)
- сломанная цепочка super() в миксинах
- сложные side effects в __getattribute__/property (неожиданная стоимость)
- equality/hash несовместимы (ломает dict/set)
- скрытие исключений (особенно в __exit__/finally)
- магия без observability (нет логов/метрик, дебажить невозможно)
"""


# 16) Вопросы “про прод”: как твой OOP дизайн помогает эксплуатации?
"""
Senior ответы обычно про:
- observability (логирование/метрики/трейсы на границах)
- backpressure/лимиты (если есть внешние ресурсы)
- тестируемость (контракты + фейки)
- миграции (как менять реализацию без изменения интерфейса)
- стабильность API (property/протоколы/адаптеры)
"""


# 17) “Если бы переписывал”: как бы ты улучшил дизайн класса?
"""
→ Senior-алгоритм:
1) Найти ответственность (SRP)
2) Определить контракт (Protocol/ABC)
3) Убрать сильные связи (композиция/DI)
4) Сделать инварианты явными
5) Добавить тесты на контракт
6) Проверить стоимость (perf) и наблюдаемость (logs/metrics)
"""


###############################################################################
# Быстрый блок “докапы, которые часто задают” (чеклист)
###############################################################################
"""
- Объясни precedence: property vs instance __dict__
- Почему super() “следующий по MRO”
- Как сделать mixin безопасным (super + совместимые сигнатуры)
- Как проектировать value object (immutability + eq/hash)
- Protocol vs ABC: когда какой
- Где применять dataclass, а где нет
- Почему inheritance for reuse опасно
- Как DI улучшает тестируемость
- Как бы ты сделал этот код расширяемым без правки существующих классов (OCP)
"""





